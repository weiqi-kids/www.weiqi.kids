<!doctype html>
<html lang="ar" dir="rtl" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-alphago/explained/dual-head-resnet" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">الشبكة مزدوجة الرأس وشبكة الباقي | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/ar/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/ar/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/ar/docs/alphago/explained/dual-head-resnet/"><meta data-rh="true" property="og:locale" content="ar"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" name="docusaurus_locale" content="ar"><meta data-rh="true" name="docsearch:language" content="ar"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="الشبكة مزدوجة الرأس وشبكة الباقي | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="تحليل عميق لبنية الشبكة العصبية في AlphaGo Zero - العمود المشترك، Policy Head، Value Head و ResNet بـ 40 طبقة"><meta data-rh="true" property="og:description" content="تحليل عميق لبنية الشبكة العصبية في AlphaGo Zero - العمود المشترك، Policy Head، Value Head و ResNet بـ 40 طبقة"><meta data-rh="true" name="keywords" content="شبكة مزدوجة الرأس,شبكة الباقي,ResNet,Policy Head,Value Head,التعلم العميق,بنية الشبكة العصبية"><link data-rh="true" rel="icon" href="/ar/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/ar/docs/alphago/explained/dual-head-resnet/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/explained/dual-head-resnet/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/alphago/explained/dual-head-resnet/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/alphago/explained/dual-head-resnet/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/alphago/explained/dual-head-resnet/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/alphago/explained/dual-head-resnet/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/alphago/explained/dual-head-resnet/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/alphago/explained/dual-head-resnet/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/alphago/explained/dual-head-resnet/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/alphago/explained/dual-head-resnet/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/alphago/explained/dual-head-resnet/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/alphago/explained/dual-head-resnet/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/explained/dual-head-resnet/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AlphaGo","item":"https://www.weiqi.kids/ar/docs/alphago/"},{"@type":"ListItem","position":2,"name":"完整解析","item":"https://www.weiqi.kids/ar/docs/alphago/explained/"},{"@type":"ListItem","position":3,"name":"الشبكة مزدوجة الرأس وشبكة الباقي","item":"https://www.weiqi.kids/ar/docs/alphago/explained/dual-head-resnet"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/ar/assets/css/styles.fac9ec51.css">
<script src="/ar/assets/js/runtime~main.859acfbd.js" defer="defer"></script>
<script src="/ar/assets/js/main.218ccdbb.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ar/img/logo.svg"><div role="region" aria-label="انتقل إلى المحتوى الرئيسي"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">انتقل إلى المحتوى الرئيسي</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ar/"><div class="navbar__logo"><img src="/ar/img/logo.svg" alt="شعار جمعية ويكي كيدز" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/ar/img/logo.svg" alt="شعار جمعية ويكي كيدز" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">ويكي كيدز</b></a><a class="navbar__item navbar__link" href="/ar/docs/learn/">تعلم غو</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ar/docs/alphago/">AlphaGo</a><a class="navbar__item navbar__link" href="/ar/docs/animations/">استوديو الرسوم المتحركة</a><a class="navbar__item navbar__link" href="/ar/docs/tech/">الوثائق التقنية</a><a class="navbar__item navbar__link" href="/ar/docs/about/">من نحن</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>العربية</a><ul class="dropdown__menu"><li><a href="/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="بحث..." aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="أنتقل للأعلى" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ar/docs/intro/"><span title="دليل الاستخدام" class="linkLabel_REp1">دليل الاستخدام</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/ar/docs/learn/"><span title="學圍棋" class="categoryLinkLabel_ezQx">學圍棋</span></a><button aria-label="Expand sidebar category &#x27;學圍棋&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/ar/docs/alphago/"><span title="AlphaGo" class="categoryLinkLabel_ezQx">AlphaGo</span></a><button aria-label="Collapse sidebar category &#x27;AlphaGo&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ar/docs/alphago/explained/"><span title="完整解析" class="categoryLinkLabel_ezQx">完整解析</span></a><button aria-label="Collapse sidebar category &#x27;完整解析&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/birth-of-alphago/"><span title="ولادة AlphaGo" class="linkLabel_REp1">ولادة AlphaGo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/key-matches/"><span title="استعراض المباريات الرئيسية" class="linkLabel_REp1">استعراض المباريات الرئيسية</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/move-37/"><span title="تحليل معمّق لـ &quot;الحركة الإلهية&quot;" class="linkLabel_REp1">تحليل معمّق لـ &quot;الحركة الإلهية&quot;</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/why-go-is-hard/"><span title="لماذا الغو صعب؟" class="linkLabel_REp1">لماذا الغو صعب؟</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/traditional-limits/"><span title="حدود الأساليب التقليدية" class="linkLabel_REp1">حدود الأساليب التقليدية</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/board-representation/"><span title="تمثيل حالة اللوحة" class="linkLabel_REp1">تمثيل حالة اللوحة</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/policy-network/"><span title="شرح مفصل لشبكة Policy" class="linkLabel_REp1">شرح مفصل لشبكة Policy</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/value-network/"><span title="شرح مفصل لشبكة Value" class="linkLabel_REp1">شرح مفصل لشبكة Value</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/input-features/"><span title="تصميم ميزات المدخلات" class="linkLabel_REp1">تصميم ميزات المدخلات</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/cnn-and-go/"><span title="الشبكات العصبية الالتفافية ولعبة الجو" class="linkLabel_REp1">الشبكات العصبية الالتفافية ولعبة الجو</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/supervised-learning/"><span title="مرحلة التعلم الخاضع للإشراف" class="linkLabel_REp1">مرحلة التعلم الخاضع للإشراف</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/reinforcement-intro/"><span title="مقدمة في التعلم المعزز" class="linkLabel_REp1">مقدمة في التعلم المعزز</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/self-play/"><span title="اللعب الذاتي" class="linkLabel_REp1">اللعب الذاتي</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/mcts-neural-combo/"><span title="دمج MCTS مع الشبكات العصبية" class="linkLabel_REp1">دمج MCTS مع الشبكات العصبية</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/puct-formula/"><span title="شرح تفصيلي لمعادلة PUCT" class="linkLabel_REp1">شرح تفصيلي لمعادلة PUCT</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/alphago-zero/"><span title="نظرة عامة على AlphaGo Zero" class="linkLabel_REp1">نظرة عامة على AlphaGo Zero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ar/docs/alphago/explained/dual-head-resnet/"><span title="الشبكة مزدوجة الرأس وشبكة الباقي" class="linkLabel_REp1">الشبكة مزدوجة الرأس وشبكة الباقي</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/training-from-scratch/"><span title="عملية التدريب من الصفر" class="linkLabel_REp1">عملية التدريب من الصفر</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/distributed-systems/"><span title="الأنظمة الموزعة و TPU" class="linkLabel_REp1">الأنظمة الموزعة و TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ar/docs/alphago/explained/legacy-and-impact/"><span title="إرث AlphaGo" class="linkLabel_REp1">إرث AlphaGo</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ar/docs/animations/"><span title="動畫教室" class="linkLabel_REp1">動畫教室</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/ar/docs/tech/"><span title="技術文件" class="categoryLinkLabel_ezQx">技術文件</span></a><button aria-label="Expand sidebar category &#x27;技術文件&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/ar/docs/about/"><span title="關於我們" class="categoryLinkLabel_ezQx">關於我們</span></a><button aria-label="Expand sidebar category &#x27;關於我們&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="التنقل التفصيلي"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="الرئيسية" class="breadcrumbs__link" href="/ar/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ar/docs/alphago/"><span>AlphaGo</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ar/docs/alphago/explained/"><span>完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">الشبكة مزدوجة الرأس وشبكة الباقي</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">محتويات هذه الصفحة</button></div><div class="theme-doc-markdown markdown"><header><h1>الشبكة مزدوجة الرأس وشبكة الباقي</h1></header>
<p>من أهم ابتكارات AlphaGo Zero البنيوية هو استخدام <strong>الشبكة مزدوجة الرأس (Dual-Head Network)</strong> بدلاً من تصميم الشبكتين المنفصلتين في AlphaGo الأصلي. هذا التغيير الذي يبدو بسيطاً أدى إلى تحسين ملحوظ في الأداء وعملية تعلم أكثر أناقة.</p>
<p>هذه المقالة ستحلل بعمق مبادئ تصميم هذه البنية، أساسها الرياضي، ولماذا هي فعالة جداً.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="تصميم-الشبكة-مزدوجة-الرأس">تصميم الشبكة مزدوجة الرأس<a href="#تصميم-الشبكة-مزدوجة-الرأس" class="hash-link" aria-label="ارتباط مباشر بالعنوان تصميم الشبكة مزدوجة الرأس" title="ارتباط مباشر بالعنوان تصميم الشبكة مزدوجة الرأس" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="البنية-الكلية">البنية الكلية<a href="#البنية-الكلية" class="hash-link" aria-label="ارتباط مباشر بالعنوان البنية الكلية" title="ارتباط مباشر بالعنوان البنية الكلية" translate="no">​</a></h3>
<p>الشبكة العصبية في AlphaGo Zero يمكن تقسيمها إلى ثلاثة أجزاء:</p>
<!-- -->
<p>دعونا نحلل كل جزء بالتفصيل.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="العمود-المشترك-shared-backbone">العمود المشترك (Shared Backbone)<a href="#العمود-المشترك-shared-backbone" class="hash-link" aria-label="ارتباط مباشر بالعنوان العمود المشترك (Shared Backbone)" title="ارتباط مباشر بالعنوان العمود المشترك (Shared Backbone)" translate="no">​</a></h3>
<p>العمود المشترك هو <strong>شبكة باقي عميقة (ResNet)</strong>، مسؤولة عن استخراج الميزات من حالة اللوحة.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="تفاصيل-البنية">تفاصيل البنية<a href="#تفاصيل-البنية" class="hash-link" aria-label="ارتباط مباشر بالعنوان تفاصيل البنية" title="ارتباط مباشر بالعنوان تفاصيل البنية" translate="no">​</a></h4>
<table><thead><tr><th>المكون</th><th>المواصفات</th></tr></thead><tbody><tr><td>طبقة الإدخال</td><td>تلافيف 3×3، 256 قناة</td></tr><tr><td>كتل الباقي</td><td>40 كتلة (أو 20 في النسخة المخففة)</td></tr><tr><td>كل كتلة باقي</td><td>طبقتا تلافيف 3×3، 256 قناة</td></tr><tr><td>دالة التنشيط</td><td>ReLU</td></tr><tr><td>التطبيع</td><td>Batch Normalization</td></tr></tbody></table>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="التمثيل-الرياضي">التمثيل الرياضي<a href="#التمثيل-الرياضي" class="hash-link" aria-label="ارتباط مباشر بالعنوان التمثيل الرياضي" title="ارتباط مباشر بالعنوان التمثيل الرياضي" translate="no">​</a></h4>
<p>لنفرض أن الإدخال هو x (الأبعاد 17 × 19 × 19)، إخراج العمود المشترك هو:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">f(x) = ResNet_40(Conv_3x3(x))</span><br></span></code></pre></div></div>
<p>حيث f(x) (الأبعاد 256 × 19 × 19) هو تمثيل الميزات عالي الأبعاد.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="policy-head-رأس-السياسة">Policy Head (رأس السياسة)<a href="#policy-head-رأس-السياسة" class="hash-link" aria-label="ارتباط مباشر بالعنوان Policy Head (رأس السياسة)" title="ارتباط مباشر بالعنوان Policy Head (رأس السياسة)" translate="no">​</a></h3>
<p>Policy Head مسؤول عن التنبؤ باحتمالية الحركة لكل موقع.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="تفاصيل-البنية-1">تفاصيل البنية<a href="#تفاصيل-البنية-1" class="hash-link" aria-label="ارتباط مباشر بالعنوان تفاصيل البنية" title="ارتباط مباشر بالعنوان تفاصيل البنية" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">إخراج العمود المشترك (256 × 19 × 19)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">تلافيف 1×1 (قناتان)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Batch Normalization</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ReLU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">تسطيح (2 × 19 × 19 = 722)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">طبقة كاملة الاتصال (362)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Softmax</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">الإخراج: 362 احتمالية (361 موقع + Pass)</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="التمثيل-الرياضي-1">التمثيل الرياضي<a href="#التمثيل-الرياضي-1" class="hash-link" aria-label="ارتباط مباشر بالعنوان التمثيل الرياضي" title="ارتباط مباشر بالعنوان التمثيل الرياضي" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">π = Softmax(FC(Flatten(ReLU(BN(Conv_1x1(f(x)))))))</span><br></span></code></pre></div></div>
<p>الإخراج π هو متجه بـ 362 بُعداً، جميع العناصر غير سالبة ومجموعها 1.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="value-head-رأس-القيمة">Value Head (رأس القيمة)<a href="#value-head-رأس-القيمة" class="hash-link" aria-label="ارتباط مباشر بالعنوان Value Head (رأس القيمة)" title="ارتباط مباشر بالعنوان Value Head (رأس القيمة)" translate="no">​</a></h3>
<p>Value Head مسؤول عن التنبؤ بنسبة الفوز للوضع الحالي.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="تفاصيل-البنية-2">تفاصيل البنية<a href="#تفاصيل-البنية-2" class="hash-link" aria-label="ارتباط مباشر بالعنوان تفاصيل البنية" title="ارتباط مباشر بالعنوان تفاصيل البنية" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">إخراج العمود المشترك (256 × 19 × 19)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">تلافيف 1×1 (قناة واحدة)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Batch Normalization</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ReLU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">تسطيح (1 × 19 × 19 = 361)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">طبقة كاملة الاتصال (256)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ReLU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">طبقة كاملة الاتصال (1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Tanh</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">الإخراج: نسبة الفوز [-1, 1]</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="التمثيل-الرياضي-2">التمثيل الرياضي<a href="#التمثيل-الرياضي-2" class="hash-link" aria-label="ارتباط مباشر بالعنوان التمثيل الرياضي" title="ارتباط مباشر بالعنوان التمثيل الرياضي" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">v = Tanh(FC_1(ReLU(FC_2(Flatten(ReLU(BN(Conv_1x1(f(x)))))))))</span><br></span></code></pre></div></div>
<p>الإخراج v في النطاق [-1, 1]:</p>
<ul>
<li class="">v = 1: اللاعب الحالي سيفوز حتماً</li>
<li class="">v = -1: اللاعب الحالي سيخسر حتماً</li>
<li class="">v = 0: القوى متعادلة</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="لماذا-مشاركة-العمود">لماذا مشاركة العمود؟<a href="#لماذا-مشاركة-العمود" class="hash-link" aria-label="ارتباط مباشر بالعنوان لماذا مشاركة العمود؟" title="ارتباط مباشر بالعنوان لماذا مشاركة العمود؟" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="الفهم-الحدسي">الفهم الحدسي<a href="#الفهم-الحدسي" class="hash-link" aria-label="ارتباط مباشر بالعنوان الفهم الحدسي" title="ارتباط مباشر بالعنوان الفهم الحدسي" translate="no">​</a></h3>
<p>&quot;أين يجب أن ألعب الحركة التالية&quot; (Policy) و&quot;من سيفوز&quot; (Value) هما سؤالان يحتاجان فهم نفس أنماط اللوحة:</p>
<ul>
<li class=""><strong>شكل الأحجار</strong>: أي الأشكال جيدة، وأيها سيئة</li>
<li class=""><strong>النفوذ</strong>: أي جانب أكبر، وأين يوجد مساحة</li>
<li class=""><strong>الحياة والموت</strong>: أي الأحجار حية، وأيها في كو</li>
<li class=""><strong>القتال</strong>: أين يوجد هجوم وقتل، وما هي نتيجة القتال المحلي</li>
</ul>
<p>إذا استخدمنا شبكتين مستقلتين، هذه الميزات يجب تعلمها مرتين. العمود المشترك يجعل هذه الميزات الأساسية تُتعلم مرة واحدة فقط، ويمكن للمهمتين استخدامها.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="منظور-التعلم-متعدد-المهام">منظور التعلم متعدد المهام<a href="#منظور-التعلم-متعدد-المهام" class="hash-link" aria-label="ارتباط مباشر بالعنوان منظور التعلم متعدد المهام" title="ارتباط مباشر بالعنوان منظور التعلم متعدد المهام" translate="no">​</a></h3>
<p>من منظور التعلم الآلي، هذا نوع من <strong>التعلم متعدد المهام (Multi-task Learning)</strong>:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L = L_policy + L_value</span><br></span></code></pre></div></div>
<p>المهمتان تتشاركان التمثيل الأساسي، وهذا يجلب عدة فوائد:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-تأثير-التنظيم">1. تأثير التنظيم<a href="#1-تأثير-التنظيم" class="hash-link" aria-label="ارتباط مباشر بالعنوان 1. تأثير التنظيم" title="ارتباط مباشر بالعنوان 1. تأثير التنظيم" translate="no">​</a></h4>
<p>مشاركة المعاملات تعادل تنظيماً ضمنياً. إذا كانت ميزة مفيدة فقط لـ Policy وليس Value (أو العكس)، يصعب تضخيمها بشكل مفرط.</p>
<p>عدد المعاملات الفعلي أقل من عدد معاملات شبكتين مستقلتين.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-كفاءة-البيانات">2. كفاءة البيانات<a href="#2-كفاءة-البيانات" class="hash-link" aria-label="ارتباط مباشر بالعنوان 2. كفاءة البيانات" title="ارتباط مباشر بالعنوان 2. كفاءة البيانات" translate="no">​</a></h4>
<p>كل مباراة تولّد في نفس الوقت تسميات Policy (احتمالية بحث MCTS) وتسميات Value (الفوز/الخسارة النهائي). العمود المشترك يجعل كلا التسميتين تُستخدمان لتدريب الميزات المشتركة، مما يحسن كفاءة استخدام البيانات.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-إشارات-التدرج-الغنية">3. إشارات التدرج الغنية<a href="#3-إشارات-التدرج-الغنية" class="hash-link" aria-label="ارتباط مباشر بالعنوان 3. إشارات التدرج الغنية" title="ارتباط مباشر بالعنوان 3. إشارات التدرج الغنية" translate="no">​</a></h4>
<p>تدرجات المهمتين تتدفق إلى العمود المشترك:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">∂L/∂θ_shared = ∂L_policy/∂θ_shared + ∂L_value/∂θ_shared</span><br></span></code></pre></div></div>
<p>هذا يوفر إشارات إشراف أغنى، ويجعل الميزات المشتركة أكثر متانة.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="الأدلة-التجريبية">الأدلة التجريبية<a href="#الأدلة-التجريبية" class="hash-link" aria-label="ارتباط مباشر بالعنوان الأدلة التجريبية" title="ارتباط مباشر بالعنوان الأدلة التجريبية" translate="no">​</a></h3>
<p>تجارب الاستئصال من DeepMind أظهرت أن الشبكة مزدوجة الرأس تتفوق بشكل ملحوظ على الشبكتين المنفصلتين:</p>
<table><thead><tr><th>التكوين</th><th>تقييم ELO</th><th>الفارق النسبي</th></tr></thead><tbody><tr><td>شبكتا Policy + Value منفصلتان</td><td>الأساس</td><td>-</td></tr><tr><td>شبكة مزدوجة الرأس (عمود مشترك)</td><td>+300 ELO</td><td>~65% فارق في نسبة الفوز</td></tr></tbody></table>
<p>فارق 300 ELO يعني أن الشبكة مزدوجة الرأس لها نسبة فوز حوالي 65% على الشبكتين المنفصلتين. هذا تحسين ملحوظ.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="مبادئ-شبكة-الباقي">مبادئ شبكة الباقي<a href="#مبادئ-شبكة-الباقي" class="hash-link" aria-label="ارتباط مباشر بالعنوان مبادئ شبكة الباقي" title="ارتباط مباشر بالعنوان مبادئ شبكة الباقي" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="معضلة-الشبكات-العميقة">معضلة الشبكات العميقة<a href="#معضلة-الشبكات-العميقة" class="hash-link" aria-label="ارتباط مباشر بالعنوان معضلة الشبكات العميقة" title="ارتباط مباشر بالعنوان معضلة الشبكات العميقة" translate="no">​</a></h3>
<p>قبل اختراع ResNet، واجهت الشبكات العصبية العميقة مفارقة:</p>
<blockquote>
<p>نظرياً، الشبكات الأعمق يجب أن تكون على الأقل بمستوى الشبكات الضحلة (في أسوأ الحالات، الطبقات الإضافية يمكن أن تتعلم تحويل الهوية). لكن عملياً، الشبكات الأعمق غالباً ما يكون أداؤها أسوأ.</p>
</blockquote>
<p>هذه هي <strong>مشكلة التدهور (Degradation Problem)</strong>:</p>
<ul>
<li class="">خطأ التدريب يزداد مع العمق (ليس فرط التكيف، بل صعوبة التحسين)</li>
<li class="">التدرج يتلاشى تدريجياً أثناء الانتشار العكسي (Vanishing Gradient)</li>
<li class="">معاملات الطبقات العميقة يكاد يكون من المستحيل تحديثها بفعالية</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="تصميم-كتلة-الباقي">تصميم كتلة الباقي<a href="#تصميم-كتلة-الباقي" class="hash-link" aria-label="ارتباط مباشر بالعنوان تصميم كتلة الباقي" title="ارتباط مباشر بالعنوان تصميم كتلة الباقي" translate="no">​</a></h3>
<p>في 2015، اقترح هي كايمينغ وآخرون حلاً بسيطاً وأنيقاً: <strong>اتصال الباقي (Skip Connection)</strong>.</p>
<!-- -->
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="التمثيل-الرياضي-3">التمثيل الرياضي<a href="#التمثيل-الرياضي-3" class="hash-link" aria-label="ارتباط مباشر بالعنوان التمثيل الرياضي" title="ارتباط مباشر بالعنوان التمثيل الرياضي" translate="no">​</a></h4>
<p>الشبكة التقليدية: تتعلم التحويل المستهدف H(x)</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">y = H(x)</span><br></span></code></pre></div></div>
<p>شبكة الباقي: تتعلم <strong>تحويل الباقي</strong> F(x) = H(x) - x</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">y = F(x) + x</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="لماذا-اتصال-الباقي-فعال">لماذا اتصال الباقي فعال؟<a href="#لماذا-اتصال-الباقي-فعال" class="hash-link" aria-label="ارتباط مباشر بالعنوان لماذا اتصال الباقي فعال؟" title="ارتباط مباشر بالعنوان لماذا اتصال الباقي فعال؟" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-طريق-التدرج-السريع">1. طريق التدرج السريع<a href="#1-طريق-التدرج-السريع" class="hash-link" aria-label="ارتباط مباشر بالعنوان 1. طريق التدرج السريع" title="ارتباط مباشر بالعنوان 1. طريق التدرج السريع" translate="no">​</a></h4>
<p>اعتبر تدرج الانتشار العكسي:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">∂L/∂x = ∂L/∂y × ∂y/∂x = ∂L/∂y × (1 + ∂F(x)/∂x)</span><br></span></code></pre></div></div>
<p>المفتاح هو ذلك <strong>+1</strong>. حتى لو كان ∂F(x)/∂x صغيراً جداً أو صفراً، التدرج لا يزال يمكنه المرور مباشرة عبر +1.</p>
<p>هذا مثل بناء &quot;طريق تدرج سريع&quot;، يسمح للتدرج بالانتقال دون عوائق من طبقة الإخراج إلى طبقة الإدخال.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-تحويل-الهوية-أسهل-للتعلم">2. تحويل الهوية أسهل للتعلم<a href="#2-تحويل-الهوية-أسهل-للتعلم" class="hash-link" aria-label="ارتباط مباشر بالعنوان 2. تحويل الهوية أسهل للتعلم" title="ارتباط مباشر بالعنوان 2. تحويل الهوية أسهل للتعلم" translate="no">​</a></h4>
<p>إذا كان الحل الأمثل قريباً من تحويل الهوية (H(x) ≈ x)، فإن:</p>
<ul>
<li class="">الشبكة التقليدية: تحتاج تعلم H(x) = x، قد يكون صعباً</li>
<li class="">شبكة الباقي: تحتاج فقط تعلم F(x) ≈ 0، أسهل نسبياً</li>
</ul>
<p>تهيئة الأوزان بصفر أو قريباً من الصفر، كتلة الباقي تميل طبيعياً نحو تحويل الهوية.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-تأثير-التجميع">3. تأثير التجميع<a href="#3-تأثير-التجميع" class="hash-link" aria-label="ارتباط مباشر بالعنوان 3. تأثير التجميع" title="ارتباط مباشر بالعنوان 3. تأثير التجميع" translate="no">​</a></h4>
<p>ResNet العميقة يمكن رؤيتها كـ<strong>تجميع ضمني</strong> لكثير من الشبكات الضحلة. إذا كان هناك n كتلة باقي، المعلومات يمكن أن تتدفق عبر 2^n مسار مختلف.</p>
<p>هذا التأثير التجميعي يزيد متانة النموذج.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="اختراق-resnet-في-imagenet">اختراق ResNet في ImageNet<a href="#اختراق-resnet-في-imagenet" class="hash-link" aria-label="ارتباط مباشر بالعنوان اختراق ResNet في ImageNet" title="ارتباط مباشر بالعنوان اختراق ResNet في ImageNet" translate="no">​</a></h3>
<p>ResNet حققت نتائج مذهلة في مسابقة ImageNet 2015:</p>
<table><thead><tr><th>العمق</th><th>خطأ Top-5</th></tr></thead><tbody><tr><td>VGG-19 (بدون باقي)</td><td>7.3%</td></tr><tr><td>ResNet-34</td><td>5.7%</td></tr><tr><td>ResNet-152</td><td>4.5%</td></tr><tr><td>مستوى البشر</td><td>~5.1%</td></tr></tbody></table>
<p>ResNet بـ<strong>152 طبقة</strong> ليس فقط قابلاً للتدريب، بل أفضل بكثير من VGG بـ 19 طبقة. هذا يثبت أن اتصال الباقي حل فعلاً مشكلة تدريب الشبكات العميقة.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="resnet-بـ-40-طبقة-في-alphago-zero">ResNet بـ 40 طبقة في AlphaGo Zero<a href="#resnet-بـ-40-طبقة-في-alphago-zero" class="hash-link" aria-label="ارتباط مباشر بالعنوان ResNet بـ 40 طبقة في AlphaGo Zero" title="ارتباط مباشر بالعنوان ResNet بـ 40 طبقة في AlphaGo Zero" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="لماذا-اختيار-40-طبقة">لماذا اختيار 40 طبقة؟<a href="#لماذا-اختيار-40-طبقة" class="hash-link" aria-label="ارتباط مباشر بالعنوان لماذا اختيار 40 طبقة؟" title="ارتباط مباشر بالعنوان لماذا اختيار 40 طبقة؟" translate="no">​</a></h3>
<p>DeepMind اختبرت ResNet بأعماق مختلفة:</p>
<table><thead><tr><th>عدد كتل الباقي</th><th>إجمالي الطبقات</th><th>تقييم ELO</th></tr></thead><tbody><tr><td>5</td><td>11</td><td>الأساس</td></tr><tr><td>10</td><td>21</td><td>+200</td></tr><tr><td>20</td><td>41</td><td>+400</td></tr><tr><td>40</td><td>81</td><td>+500</td></tr></tbody></table>
<p>الشبكات الأعمق فعلاً أقوى، لكن العائد الحدي يتناقص. AlphaGo Zero يستخدم 20 أو 40 كتلة باقي:</p>
<ul>
<li class=""><strong>AlphaGo Zero (نسخة الورقة)</strong>: 40 كتلة باقي، 256 قناة</li>
<li class=""><strong>النسخة المخففة</strong>: 20 كتلة باقي، 256 قناة</li>
</ul>
<p>تكوين 40 طبقة يحقق توازناً جيداً بين قوة اللعب وتكلفة التدريب.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="التكوين-المحدد">التكوين المحدد<a href="#التكوين-المحدد" class="hash-link" aria-label="ارتباط مباشر بالعنوان التكوين المحدد" title="ارتباط مباشر بالعنوان التكوين المحدد" translate="no">​</a></h3>
<p>تكوين ResNet في AlphaGo Zero كالتالي:</p>
<!-- -->
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="تقدير-عدد-المعاملات">تقدير عدد المعاملات<a href="#تقدير-عدد-المعاملات" class="hash-link" aria-label="ارتباط مباشر بالعنوان تقدير عدد المعاملات" title="ارتباط مباشر بالعنوان تقدير عدد المعاملات" translate="no">​</a></h4>
<table><thead><tr><th>المكون</th><th>عدد المعاملات (تقريباً)</th></tr></thead><tbody><tr><td>تلافيف الإدخال</td><td>17 × 3 × 3 × 256 ≈ 39K</td></tr><tr><td>كل كتلة باقي</td><td>2 × 256 × 3 × 3 × 256 ≈ 1.2M</td></tr><tr><td>40 كتلة باقي</td><td>40 × 1.2M ≈ 47M</td></tr><tr><td>Policy Head</td><td>~1M</td></tr><tr><td>Value Head</td><td>~0.2M</td></tr><tr><td><strong>الإجمالي</strong></td><td><strong>~48M</strong></td></tr></tbody></table>
<p>حوالي 48 مليون معامل، بالمعايير الحديثة هذه شبكة عصبية متوسطة الحجم.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="دور-batch-normalization">دور Batch Normalization<a href="#دور-batch-normalization" class="hash-link" aria-label="ارتباط مباشر بالعنوان دور Batch Normalization" title="ارتباط مباشر بالعنوان دور Batch Normalization" translate="no">​</a></h3>
<p>بعد كل طبقة تلافيف هناك <strong>Batch Normalization (BN)</strong>، وهذا حاسم لاستقرار التدريب:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-تطبيع-قيم-التنشيط">1. تطبيع قيم التنشيط<a href="#1-تطبيع-قيم-التنشيط" class="hash-link" aria-label="ارتباط مباشر بالعنوان 1. تطبيع قيم التنشيط" title="ارتباط مباشر بالعنوان 1. تطبيع قيم التنشيط" translate="no">​</a></h4>
<p>BN تطبّع قيم التنشيط في كل طبقة إلى متوسط 0 وتباين 1:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">x_hat = (x - μ_B) / sqrt(σ_B² + ε)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">y = γ × x_hat + β</span><br></span></code></pre></div></div>
<p>حيث γ و β معاملات قابلة للتعلم.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-تخفيف-انزياح-المتغيرات-الداخلية">2. تخفيف انزياح المتغيرات الداخلية<a href="#2-تخفيف-انزياح-المتغيرات-الداخلية" class="hash-link" aria-label="ارتباط مباشر بالعنوان 2. تخفيف انزياح المتغيرات الداخلية" title="ارتباط مباشر بالعنوان 2. تخفيف انزياح المتغيرات الداخلية" translate="no">​</a></h4>
<p>في الشبكات العميقة، توزيع الإدخال لكل طبقة يتغير مع تحديث معاملات الطبقات السابقة. BN تجعل توزيع الإدخال لكل طبقة مستقراً، وتسرع تقارب التدريب.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-تأثير-التنظيم">3. تأثير التنظيم<a href="#3-تأثير-التنظيم" class="hash-link" aria-label="ارتباط مباشر بالعنوان 3. تأثير التنظيم" title="ارتباط مباشر بالعنوان 3. تأثير التنظيم" translate="no">​</a></h4>
<p>BN تستخدم إحصائيات mini-batch أثناء التدريب، مما يضيف عشوائية، ولها تأثير تنظيم خفيف.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="المقارنة-مع-بنيات-أخرى">المقارنة مع بنيات أخرى<a href="#المقارنة-مع-بنيات-أخرى" class="hash-link" aria-label="ارتباط مباشر بالعنوان المقارنة مع بنيات أخرى" title="ارتباط مباشر بالعنوان المقارنة مع بنيات أخرى" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="مقابل-cnn-في-alphago-الأصلي">مقابل CNN في AlphaGo الأصلي<a href="#مقابل-cnn-في-alphago-الأصلي" class="hash-link" aria-label="ارتباط مباشر بالعنوان مقابل CNN في AlphaGo الأصلي" title="ارتباط مباشر بالعنوان مقابل CNN في AlphaGo الأصلي" translate="no">​</a></h3>
<table><thead><tr><th>الميزة</th><th>AlphaGo الأصلي</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>نوع البنية</td><td>CNN قياسي</td><td>ResNet</td></tr><tr><td>العمق</td><td>13 طبقة</td><td>41-81 طبقة</td></tr><tr><td>اتصال الباقي</td><td>لا يوجد</td><td>يوجد</td></tr><tr><td>عدد الشبكات</td><td>2 (منفصلة)</td><td>1 (مشتركة)</td></tr><tr><td>BN</td><td>لا يوجد</td><td>يوجد</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="مقابل-شبكات-نمط-vgg">مقابل شبكات نمط VGG<a href="#مقابل-شبكات-نمط-vgg" class="hash-link" aria-label="ارتباط مباشر بالعنوان مقابل شبكات نمط VGG" title="ارتباط مباشر بالعنوان مقابل شبكات نمط VGG" translate="no">​</a></h3>
<p>VGG كانت الوصيفة في ImageNet 2014، تستخدم تكديس تلافيف 3×3:</p>
<table><thead><tr><th>الميزة</th><th>VGG</th><th>ResNet</th></tr></thead><tbody><tr><td>أقصى عمق قابل للتدريب</td><td>~19 طبقة</td><td>152+ طبقة</td></tr><tr><td>تدفق التدرج</td><td>يتناقص طبقة بطبقة</td><td>له طريق سريع</td></tr><tr><td>صعوبة التدريب</td><td>صعب للطبقات العميقة</td><td>قابل للتدريب للطبقات العميقة</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="مقابل-inception--googlenet">مقابل Inception / GoogLeNet<a href="#مقابل-inception--googlenet" class="hash-link" aria-label="ارتباط مباشر بالعنوان مقابل Inception / GoogLeNet" title="ارتباط مباشر بالعنوان مقابل Inception / GoogLeNet" translate="no">​</a></h3>
<p>Inception تستخدم تلافيف متعددة المقاييس بالتوازي:</p>
<table><thead><tr><th>الميزة</th><th>Inception</th><th>ResNet</th></tr></thead><tbody><tr><td>الميزة</td><td>ميزات متعددة المقاييس</td><td>تكديس عميق</td></tr><tr><td>التعقيد</td><td>أعلى</td><td>بسيط</td></tr><tr><td>ملاءمة الغو</td><td>عادية</td><td>ممتازة</td></tr></tbody></table>
<p>تصميم ResNet البسيط أنسب للغو الذي يحتاج استدلالاً عميقاً.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="مقابل-transformer">مقابل Transformer<a href="#مقابل-transformer" class="hash-link" aria-label="ارتباط مباشر بالعنوان مقابل Transformer" title="ارتباط مباشر بالعنوان مقابل Transformer" translate="no">​</a></h3>
<p>بنية Transformer المقترحة في 2017 حققت نجاحاً كبيراً في NLP. البعض حاول تطبيق Transformer على الغو:</p>
<table><thead><tr><th>الميزة</th><th>ResNet</th><th>Transformer</th></tr></thead><tbody><tr><td>الانحياز الاستقرائي</td><td>محلية (تلافيف)</td><td>انتباه شامل</td></tr><tr><td>ترميز الموقع</td><td>ضمني (تلافيف)</td><td>صريح</td></tr><tr><td>أداء الغو</td><td>ممتاز</td><td>ممكن لكن ليس أفضل من ResNet</td></tr><tr><td>كفاءة الحساب</td><td>أعلى</td><td>أقل (O(n²))</td></tr></tbody></table>
<p>لمشاكل مثل الغو ذات البنية المكانية الواضحة، الانحياز الاستقرائي لـ CNN/ResNet أنسب.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="تحليل-عميق-لخيارات-التصميم">تحليل عميق لخيارات التصميم<a href="#تحليل-عميق-لخيارات-التصميم" class="hash-link" aria-label="ارتباط مباشر بالعنوان تحليل عميق لخيارات التصميم" title="ارتباط مباشر بالعنوان تحليل عميق لخيارات التصميم" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="لماذا-استخدام-تلافيف-33">لماذا استخدام تلافيف 3×3؟<a href="#لماذا-استخدام-تلافيف-33" class="hash-link" aria-label="ارتباط مباشر بالعنوان لماذا استخدام تلافيف 3×3؟" title="ارتباط مباشر بالعنوان لماذا استخدام تلافيف 3×3؟" translate="no">​</a></h3>
<p>AlphaGo Zero يستخدم تلافيف 3×3 طوال الوقت، وليس نوى تلافيف أكبر:</p>
<ol>
<li class=""><strong>كفاءة المعاملات</strong>: تلافيفان 3×3 لهما نفس مجال الرؤية كتلافيف 5×5 واحد، لكن معاملات أقل (18 مقابل 25)</li>
<li class=""><strong>شبكة أعمق</strong>: بنفس عدد المعاملات، يمكن تكديس طبقات أكثر</li>
<li class=""><strong>غير خطية أكثر</strong>: بين كل طبقة يوجد ReLU، يزيد القدرة التعبيرية</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="لماذا-استخدام-256-قناة">لماذا استخدام 256 قناة؟<a href="#لماذا-استخدام-256-قناة" class="hash-link" aria-label="ارتباط مباشر بالعنوان لماذا استخدام 256 قناة؟" title="ارتباط مباشر بالعنوان لماذا استخدام 256 قناة؟" translate="no">​</a></h3>
<p>256 قناة خيار تجريبي:</p>
<ul>
<li class=""><strong>قليل جداً</strong> (مثل 64): قدرة تعبيرية غير كافية، لا يستطيع التقاط الأنماط المعقدة</li>
<li class=""><strong>كثير جداً</strong> (مثل 512): عدد المعاملات يتضاعف، تكلفة التدريب تزيد كثيراً، لكن تحسين قوة اللعب محدود</li>
</ul>
<p>تجارب KataGo اللاحقة أظهرت أن عدد القنوات يمكن ضبطه حسب موارد التدريب:</p>
<ul>
<li class="">موارد منخفضة: 128 قناة، 20 كتلة</li>
<li class="">موارد عالية: 256 قناة، 40 كتلة</li>
<li class="">موارد أعلى: 384 قناة، 60 كتلة</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="لماذا-policy-head-تستخدم-softmax-و-value-head-تستخدم-tanh">لماذا Policy Head تستخدم Softmax و Value Head تستخدم Tanh؟<a href="#لماذا-policy-head-تستخدم-softmax-و-value-head-تستخدم-tanh" class="hash-link" aria-label="ارتباط مباشر بالعنوان لماذا Policy Head تستخدم Softmax و Value Head تستخدم Tanh؟" title="ارتباط مباشر بالعنوان لماذا Policy Head تستخدم Softmax و Value Head تستخدم Tanh؟" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="policy-head-softmax">Policy Head: Softmax<a href="#policy-head-softmax" class="hash-link" aria-label="ارتباط مباشر بالعنوان Policy Head: Softmax" title="ارتباط مباشر بالعنوان Policy Head: Softmax" translate="no">​</a></h4>
<p>الحركة هي <strong>مشكلة تصنيف</strong> — اختيار واحد من 361 موقع (زائد Pass). إخراج Softmax يحقق:</p>
<ul>
<li class="">جميع الاحتماليات غير سالبة: π_i &gt;= 0</li>
<li class="">مجموع الاحتماليات 1: Σπ_i = 1</li>
</ul>
<p>هذا يتوافق مع تعريف التوزيع الاحتمالي.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="value-head-tanh">Value Head: Tanh<a href="#value-head-tanh" class="hash-link" aria-label="ارتباط مباشر بالعنوان Value Head: Tanh" title="ارتباط مباشر بالعنوان Value Head: Tanh" translate="no">​</a></h4>
<p>نسبة الفوز هي <strong>مشكلة انحدار</strong> — التنبؤ بقيمة مستمرة. نطاق إخراج Tanh هو [-1, 1]:</p>
<ul>
<li class="">محدود: لا ينتج قيماً متطرفة</li>
<li class="">متماثل: الفوز والخسارة تُعالجان بتماثل</li>
<li class="">قابل للاشتقاق: يسهل حساب التدرج</li>
</ul>
<p>استخدام Tanh بدلاً من إخراج غير محدود (مثل طبقة خطية) يمنع عدم استقرار التدريب.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="تفاصيل-التدريب">تفاصيل التدريب<a href="#تفاصيل-التدريب" class="hash-link" aria-label="ارتباط مباشر بالعنوان تفاصيل التدريب" title="ارتباط مباشر بالعنوان تفاصيل التدريب" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="دالة-الخسارة">دالة الخسارة<a href="#دالة-الخسارة" class="hash-link" aria-label="ارتباط مباشر بالعنوان دالة الخسارة" title="ارتباط مباشر بالعنوان دالة الخسارة" translate="no">​</a></h3>
<p>الخسارة الكلية في AlphaGo Zero هي مجموع ثلاثة عناصر:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L = L_policy + L_value + L_reg</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="policy-loss">Policy Loss<a href="#policy-loss" class="hash-link" aria-label="ارتباط مباشر بالعنوان Policy Loss" title="ارتباط مباشر بالعنوان Policy Loss" translate="no">​</a></h4>
<p>تستخدم <strong>خسارة الإنتروبيا المتقاطعة</strong>، تجعل إخراج الشبكة يقترب من احتمالية بحث MCTS:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L_policy = -Σ π_MCTS(a) × log(π_net(a))</span><br></span></code></pre></div></div>
<p>حيث:</p>
<ul>
<li class="">π_MCTS(a) هي احتمالية بحث MCTS للإجراء a</li>
<li class="">π_net(a) هي الاحتمالية التي تخرجها الشبكة</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="value-loss">Value Loss<a href="#value-loss" class="hash-link" aria-label="ارتباط مباشر بالعنوان Value Loss" title="ارتباط مباشر بالعنوان Value Loss" translate="no">​</a></h4>
<p>تستخدم <strong>الخطأ التربيعي المتوسط (MSE)</strong>، تجعل إخراج الشبكة يقترب من الفوز/الخسارة الفعلي:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L_value = (v_net - z)²</span><br></span></code></pre></div></div>
<p>حيث:</p>
<ul>
<li class="">v_net هي نسبة الفوز المتنبأ بها من الشبكة</li>
<li class="">z هي نتيجة المباراة الفعلية (+1 أو -1)</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="regularization-loss">Regularization Loss<a href="#regularization-loss" class="hash-link" aria-label="ارتباط مباشر بالعنوان Regularization Loss" title="ارتباط مباشر بالعنوان Regularization Loss" translate="no">​</a></h4>
<p>تستخدم <strong>تنظيم L2</strong> لمنع فرط التكيف:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L_reg = c × ||θ||²</span><br></span></code></pre></div></div>
<p>حيث c هو معامل التنظيم، وθ هي معاملات الشبكة.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="تكوين-المحسن">تكوين المحسن<a href="#تكوين-المحسن" class="hash-link" aria-label="ارتباط مباشر بالعنوان تكوين المحسن" title="ارتباط مباشر بالعنوان تكوين المحسن" translate="no">​</a></h3>
<table><thead><tr><th>المعامل</th><th>القيمة</th></tr></thead><tbody><tr><td>المحسن</td><td>SGD + Momentum</td></tr><tr><td>الزخم</td><td>0.9</td></tr><tr><td>معدل التعلم الأولي</td><td>0.01</td></tr><tr><td>تناقص معدل التعلم</td><td>تنصيف كل X خطوة</td></tr><tr><td>حجم الدفعة</td><td>32 × 2048 = 64K (موزع)</td></tr><tr><td>معامل تنظيم L2</td><td>1e-4</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="تعزيز-البيانات">تعزيز البيانات<a href="#تعزيز-البيانات" class="hash-link" aria-label="ارتباط مباشر بالعنوان تعزيز البيانات" title="ارتباط مباشر بالعنوان تعزيز البيانات" translate="no">​</a></h3>
<p>لوحة الغو لها 8 تناظرات (4 دورات × 2 انعكاس). أثناء التدريب، كل وضع يمكن أن ينتج 8 عينات تدريب متكافئة.</p>
<p>هذا يجعل بيانات التدريب الفعالة تزيد 8 مرات، دون الحاجة للعب ذاتي إضافي.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="اعتبارات-التنفيذ">اعتبارات التنفيذ<a href="#اعتبارات-التنفيذ" class="hash-link" aria-label="ارتباط مباشر بالعنوان اعتبارات التنفيذ" title="ارتباط مباشر بالعنوان اعتبارات التنفيذ" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="تحسين-الذاكرة">تحسين الذاكرة<a href="#تحسين-الذاكرة" class="hash-link" aria-label="ارتباط مباشر بالعنوان تحسين الذاكرة" title="ارتباط مباشر بالعنوان تحسين الذاكرة" translate="no">​</a></h3>
<p>تدريب ResNet بـ 40 طبقة يحتاج ذاكرة كبيرة:</p>
<ul>
<li class=""><strong>الانتشار الأمامي</strong>: يحتاج تخزين قيم التنشيط لكل طبقة (للانتشار العكسي)</li>
<li class=""><strong>الانتشار العكسي</strong>: يحتاج تخزين التدرجات</li>
</ul>
<p>استراتيجيات التحسين:</p>
<ol>
<li class=""><strong>نقاط فحص التدرج (Gradient Checkpointing)</strong>: تخزين جزء فقط من قيم التنشيط، إعادة الحساب عند الحاجة</li>
<li class=""><strong>التدريب بدقة مختلطة</strong>: استخدام FP16 لتقليل استهلاك الذاكرة</li>
<li class=""><strong>التدريب الموزع</strong>: توزيع الدفعة على عدة GPU/TPU</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="تحسين-الاستنتاج">تحسين الاستنتاج<a href="#تحسين-الاستنتاج" class="hash-link" aria-label="ارتباط مباشر بالعنوان تحسين الاستنتاج" title="ارتباط مباشر بالعنوان تحسين الاستنتاج" translate="no">​</a></h3>
<p>أثناء الاستنتاج لا نحتاج إحصائيات mini-batch لـ BN، يمكن استخدام المتوسط المتحرك المتراكم أثناء التدريب:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">x_hat = (x - μ_moving) / sqrt(σ_moving² + ε)</span><br></span></code></pre></div></div>
<p>هذا يجعل الاستنتاج أسرع والنتائج حتمية.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="التكميم-والضغط">التكميم والضغط<a href="#التكميم-والضغط" class="hash-link" aria-label="ارتباط مباشر بالعنوان التكميم والضغط" title="ارتباط مباشر بالعنوان التكميم والضغط" translate="no">​</a></h3>
<p>عند النشر يمكن ضغط الشبكة أكثر:</p>
<ul>
<li class=""><strong>تكميم الأوزان</strong>: FP32 → INT8، الذاكرة تقل 4 مرات</li>
<li class=""><strong>التشذيب</strong>: إزالة الاتصالات ذات الأوزان الصغيرة</li>
<li class=""><strong>التقطير المعرفي</strong>: استخدام شبكة كبيرة لتدريب شبكة صغيرة</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="تطابق-الرسوم-المتحركة">تطابق الرسوم المتحركة<a href="#تطابق-الرسوم-المتحركة" class="hash-link" aria-label="ارتباط مباشر بالعنوان تطابق الرسوم المتحركة" title="ارتباط مباشر بالعنوان تطابق الرسوم المتحركة" translate="no">​</a></h2>
<p>المفاهيم الأساسية في هذه المقالة ورقم الرسوم المتحركة المقابل:</p>
<table><thead><tr><th>الرقم</th><th>المفهوم</th><th>التطابق الفيزيائي/الرياضي</th></tr></thead><tbody><tr><td>🎬 E3</td><td>الشبكة مزدوجة الرأس</td><td>التعلم متعدد المهام</td></tr><tr><td>🎬 D12</td><td>اتصال الباقي</td><td>طريق التدرج السريع</td></tr><tr><td>🎬 D8</td><td>الشبكة العصبية التلافيفية</td><td>مجال الرؤية المحلي</td></tr><tr><td>🎬 D10</td><td>Batch Normalization</td><td>تطبيع التوزيع</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="قراءة-موسعة">قراءة موسعة<a href="#قراءة-موسعة" class="hash-link" aria-label="ارتباط مباشر بالعنوان قراءة موسعة" title="ارتباط مباشر بالعنوان قراءة موسعة" translate="no">​</a></h2>
<ul>
<li class=""><strong>المقال السابق</strong>: <a class="" href="/ar/docs/alphago/explained/alphago-zero/">نظرة عامة على AlphaGo Zero</a> — لماذا لا يحتاج سجلات بشرية</li>
<li class=""><strong>المقال التالي</strong>: <a class="" href="/ar/docs/alphago/explained/training-from-scratch/">عملية التدريب من الصفر</a> — التطور التفصيلي من اليوم 0 إلى 3</li>
<li class=""><strong>تعمق تقني</strong>: <a class="" href="/ar/docs/alphago/explained/cnn-and-go/">CNN والغو</a> — لماذا CNN مناسبة للوحة</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="المراجع">المراجع<a href="#المراجع" class="hash-link" aria-label="ارتباط مباشر بالعنوان المراجع" title="ارتباط مباشر بالعنوان المراجع" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">He, K., et al. (2016). &quot;Deep Residual Learning for Image Recognition.&quot; <em>CVPR 2016</em>.</li>
<li class="">Ioffe, S., &amp; Szegedy, C. (2015). &quot;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.&quot; <em>ICML 2015</em>.</li>
<li class="">Caruana, R. (1997). &quot;Multitask Learning.&quot; <em>Machine Learning</em>, 28(1), 41-75.</li>
<li class="">Veit, A., et al. (2016). &quot;Residual Networks Behave Like Ensembles of Relatively Shallow Networks.&quot; <em>NeurIPS 2016</em>.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/explained/17-dual-head-resnet.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>تعديل هذه الصفحة</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="صفحة التوثيق"><a class="pagination-nav__link pagination-nav__link--prev" href="/ar/docs/alphago/explained/alphago-zero/"><div class="pagination-nav__sublabel">السابق</div><div class="pagination-nav__label">نظرة عامة على AlphaGo Zero</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ar/docs/alphago/explained/training-from-scratch/"><div class="pagination-nav__sublabel">التالي</div><div class="pagination-nav__label">عملية التدريب من الصفر</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#تصميم-الشبكة-مزدوجة-الرأس" class="table-of-contents__link toc-highlight">تصميم الشبكة مزدوجة الرأس</a><ul><li><a href="#البنية-الكلية" class="table-of-contents__link toc-highlight">البنية الكلية</a></li><li><a href="#العمود-المشترك-shared-backbone" class="table-of-contents__link toc-highlight">العمود المشترك (Shared Backbone)</a></li><li><a href="#policy-head-رأس-السياسة" class="table-of-contents__link toc-highlight">Policy Head (رأس السياسة)</a></li><li><a href="#value-head-رأس-القيمة" class="table-of-contents__link toc-highlight">Value Head (رأس القيمة)</a></li></ul></li><li><a href="#لماذا-مشاركة-العمود" class="table-of-contents__link toc-highlight">لماذا مشاركة العمود؟</a><ul><li><a href="#الفهم-الحدسي" class="table-of-contents__link toc-highlight">الفهم الحدسي</a></li><li><a href="#منظور-التعلم-متعدد-المهام" class="table-of-contents__link toc-highlight">منظور التعلم متعدد المهام</a></li><li><a href="#الأدلة-التجريبية" class="table-of-contents__link toc-highlight">الأدلة التجريبية</a></li></ul></li><li><a href="#مبادئ-شبكة-الباقي" class="table-of-contents__link toc-highlight">مبادئ شبكة الباقي</a><ul><li><a href="#معضلة-الشبكات-العميقة" class="table-of-contents__link toc-highlight">معضلة الشبكات العميقة</a></li><li><a href="#تصميم-كتلة-الباقي" class="table-of-contents__link toc-highlight">تصميم كتلة الباقي</a></li><li><a href="#لماذا-اتصال-الباقي-فعال" class="table-of-contents__link toc-highlight">لماذا اتصال الباقي فعال؟</a></li><li><a href="#اختراق-resnet-في-imagenet" class="table-of-contents__link toc-highlight">اختراق ResNet في ImageNet</a></li></ul></li><li><a href="#resnet-بـ-40-طبقة-في-alphago-zero" class="table-of-contents__link toc-highlight">ResNet بـ 40 طبقة في AlphaGo Zero</a><ul><li><a href="#لماذا-اختيار-40-طبقة" class="table-of-contents__link toc-highlight">لماذا اختيار 40 طبقة؟</a></li><li><a href="#التكوين-المحدد" class="table-of-contents__link toc-highlight">التكوين المحدد</a></li><li><a href="#دور-batch-normalization" class="table-of-contents__link toc-highlight">دور Batch Normalization</a></li></ul></li><li><a href="#المقارنة-مع-بنيات-أخرى" class="table-of-contents__link toc-highlight">المقارنة مع بنيات أخرى</a><ul><li><a href="#مقابل-cnn-في-alphago-الأصلي" class="table-of-contents__link toc-highlight">مقابل CNN في AlphaGo الأصلي</a></li><li><a href="#مقابل-شبكات-نمط-vgg" class="table-of-contents__link toc-highlight">مقابل شبكات نمط VGG</a></li><li><a href="#مقابل-inception--googlenet" class="table-of-contents__link toc-highlight">مقابل Inception / GoogLeNet</a></li><li><a href="#مقابل-transformer" class="table-of-contents__link toc-highlight">مقابل Transformer</a></li></ul></li><li><a href="#تحليل-عميق-لخيارات-التصميم" class="table-of-contents__link toc-highlight">تحليل عميق لخيارات التصميم</a><ul><li><a href="#لماذا-استخدام-تلافيف-33" class="table-of-contents__link toc-highlight">لماذا استخدام تلافيف 3×3؟</a></li><li><a href="#لماذا-استخدام-256-قناة" class="table-of-contents__link toc-highlight">لماذا استخدام 256 قناة؟</a></li><li><a href="#لماذا-policy-head-تستخدم-softmax-و-value-head-تستخدم-tanh" class="table-of-contents__link toc-highlight">لماذا Policy Head تستخدم Softmax و Value Head تستخدم Tanh؟</a></li></ul></li><li><a href="#تفاصيل-التدريب" class="table-of-contents__link toc-highlight">تفاصيل التدريب</a><ul><li><a href="#دالة-الخسارة" class="table-of-contents__link toc-highlight">دالة الخسارة</a></li><li><a href="#تكوين-المحسن" class="table-of-contents__link toc-highlight">تكوين المحسن</a></li><li><a href="#تعزيز-البيانات" class="table-of-contents__link toc-highlight">تعزيز البيانات</a></li></ul></li><li><a href="#اعتبارات-التنفيذ" class="table-of-contents__link toc-highlight">اعتبارات التنفيذ</a><ul><li><a href="#تحسين-الذاكرة" class="table-of-contents__link toc-highlight">تحسين الذاكرة</a></li><li><a href="#تحسين-الاستنتاج" class="table-of-contents__link toc-highlight">تحسين الاستنتاج</a></li><li><a href="#التكميم-والضغط" class="table-of-contents__link toc-highlight">التكميم والضغط</a></li></ul></li><li><a href="#تطابق-الرسوم-المتحركة" class="table-of-contents__link toc-highlight">تطابق الرسوم المتحركة</a></li><li><a href="#قراءة-موسعة" class="table-of-contents__link toc-highlight">قراءة موسعة</a></li><li><a href="#المراجع" class="table-of-contents__link toc-highlight">المراجع</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>