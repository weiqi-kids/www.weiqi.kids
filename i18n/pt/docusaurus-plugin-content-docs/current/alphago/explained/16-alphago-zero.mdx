---
sidebar_position: 17
title: Vis√£o Geral do AlphaGo Zero
description: Come√ßando do zero, completamente autodidata, como o AlphaGo Zero superou todas as vers√µes anteriores sem usar qualquer registro de partidas humanas
keywords: [AlphaGo Zero, auto-jogo, aprendizado por refor√ßo, aprendizado profundo, IA de Go, aprendizado n√£o supervisionado]
---

# Vis√£o Geral do AlphaGo Zero

Em outubro de 2017, a DeepMind publicou um resultado que chocou o mundo da IA: **AlphaGo Zero**, sem usar qualquer registro de partidas humanas, come√ßando a treinar de um estado completamente aleat√≥rio, superou o AlphaGo original que derrotou Lee Sedol em apenas tr√™s dias, e venceu por **100:0**.

Isso n√£o √© apenas um progresso num√©rico. Representa um novo paradigma: **A IA n√£o precisa de conhecimento humano, pode descobrir tudo do zero**.

---

## Por que N√£o Precisa de Registros de Partidas Humanas?

### Limita√ß√µes dos Registros de Partidas Humanas

O processo de treinamento do AlphaGo original era dividido em duas etapas:

1. **Aprendizado supervisionado**: Treinar a Policy Network com 30 milh√µes de partidas humanas
2. **Aprendizado por refor√ßo**: Melhorar ainda mais atrav√©s de auto-jogo

Este m√©todo tem v√°rios problemas fundamentais:

#### 1. Os registros humanos t√™m um teto

A habilidade dos jogadores humanos tem limites, os registros cont√™m a compreens√£o humana, mas tamb√©m incluem erros e vieses humanos. Quando a IA aprende com registros humanos, ela aprende:

- Jogadas que os humanos consideram boas (mas n√£o necessariamente √≥timas)
- Padr√µes de pensamento humano (que podem limitar a inova√ß√£o)
- Erros humanos (que s√£o aprendidos como exemplos corretos)

#### 2. O gargalo do aprendizado supervisionado

O objetivo do aprendizado supervisionado √© "imitar humanos" ‚Äî prever qual jogada um jogador humano faria. Isso significa que o limite de capacidade da IA √© limitado pela habilidade dos jogadores humanos.

√â como um aprendiz que s√≥ pode imitar o mestre, nunca podendo superar o mestre.

#### 3. Custo de coleta de dados

Registros de partidas humanas de alta qualidade levam muitos anos para acumular, e s√≥ existem para jogos com longa hist√≥ria como o Go. Se quisermos aplicar IA a novos campos (como previs√£o de estrutura de prote√≠nas), simplesmente n√£o existem "registros de especialistas humanos".

### O Avan√ßo do Zero

O AlphaGo Zero pula completamente a etapa de aprendizado supervisionado, come√ßando diretamente o auto-jogo a partir de **inicializa√ß√£o aleat√≥ria**. Isso resolve todos os problemas acima:

| Problema | AlphaGo Original | AlphaGo Zero |
|----------|-----------------|--------------|
| Limite do conhecimento humano | Limitado pela qualidade dos registros | Sem esta limita√ß√£o |
| Objetivo de aprendizado | Imitar humanos | Maximizar taxa de vit√≥ria |
| Requisitos de dados | 30 milh√µes de partidas | 0 |
| Generalizabilidade | Apenas Go | Pode ser generalizado para outros campos |

Esta √© uma mudan√ßa de paradigma fundamental: de "aprender conhecimento humano" para "descobrir conhecimento a partir de primeiros princ√≠pios".

---

## Compara√ß√£o com o AlphaGo Original: 100:0

### Vit√≥ria Esmagadora

A DeepMind fez o AlphaGo Zero treinado jogar contra v√°rias vers√µes do AlphaGo:

| Oponente | Resultado do AlphaGo Zero |
|----------|---------------------------|
| AlphaGo Fan (vers√£o que derrotou Fan Hui) | 100:0 |
| AlphaGo Lee (vers√£o que derrotou Lee Sedol) | 100:0 |
| AlphaGo Master (vers√£o das 60 vit√≥rias consecutivas) | 89:11 |

**100:0** ‚Äî isso significa que em 100 partidas, o AlphaGo original n√£o conseguiu vencer sequer uma.

### Menos Recursos, Mais For√ßa

N√£o apenas venceu, o AlphaGo Zero tamb√©m alcan√ßou maior for√ßa de jogo com menos recursos:

| M√©trica | AlphaGo Lee | AlphaGo Zero |
|---------|-------------|--------------|
| Tempo de treinamento | V√°rios meses | 40 dias (3 dias para superar AlphaGo Lee) |
| Partidas de treinamento | 30 milh√µes de partidas humanas + auto-jogo | 4,9 milh√µes de partidas de auto-jogo |
| TPUs (treinamento) | 50+ | 4 |
| TPUs (infer√™ncia) | 48 | 4 |
| Caracter√≠sticas de entrada | 48 planos | 17 planos |
| Rede neural | Redes duplas SL + RL | √önica rede de cabe√ßa dupla |

Esta √© uma melhoria de efici√™ncia impressionante: **recursos reduzidos em mais de 10 vezes, mas a for√ßa de jogo aumentou significativamente**.

### Por que o Zero √© Mais Forte?

As raz√µes pelas quais o AlphaGo Zero √© mais forte podem ser entendidas de v√°rios √¢ngulos:

#### 1. Aprendizado sem vi√©s

O AlphaGo original aprendeu com registros humanos, herdando vieses humanos. Por exemplo, jogadores humanos podem supervalorizar certos josekis, ou ter avalia√ß√µes incorretas de certas posi√ß√µes.

O AlphaGo Zero n√£o tem essa bagagem. Ele come√ßa de uma tela em branco, aprendendo o que √© uma boa jogada apenas atrav√©s dos resultados de vit√≥ria/derrota. Isso permite descobrir jogadas que os humanos nunca imaginaram.

#### 2. Objetivo de aprendizado consistente

O treinamento do AlphaGo original tinha dois objetivos diferentes:
- Aprendizado supervisionado: Maximizar a precis√£o de previs√£o das jogadas humanas
- Aprendizado por refor√ßo: Maximizar a taxa de vit√≥ria

Estes dois objetivos podem entrar em conflito. O AlphaGo Zero tem apenas um objetivo: **maximiza√ß√£o da taxa de vit√≥ria**. Isso torna o processo de aprendizado mais consistente e eficaz.

#### 3. Arquitetura mais simples

O AlphaGo original usava Policy Network e Value Network separadas. O AlphaGo Zero usa uma √∫nica rede de cabe√ßa dupla (veja o pr√≥ximo artigo), permitindo que a representa√ß√£o de caracter√≠sticas seja compartilhada, aumentando a efici√™ncia de aprendizado.

---

## Caracter√≠sticas de Entrada Simplificadas: De 48 para 17

### 48 Planos de Caracter√≠sticas do AlphaGo Original

A entrada da rede neural do AlphaGo original inclu√≠a 48 planos de caracter√≠sticas 19x19, codificando muitas caracter√≠sticas projetadas por humanos:

| Categoria | N√∫mero de caracter√≠sticas | Conte√∫do |
|-----------|---------------------------|----------|
| Posi√ß√µes das pedras | 3 | Pedras pretas, pedras brancas, pontos vazios |
| Liberdades | 8 | Grupos com 1-8 liberdades |
| Capturas | 8 | Pode capturar 1-8 pedras |
| Ko | 1 | Posi√ß√£o do ko |
| Dist√¢ncia da borda | 4 | Primeira a quarta linha |
| Legalidade de jogada | 1 | Quais posi√ß√µes podem ser jogadas |
| Estado hist√≥rico | 8 | Posi√ß√µes das √∫ltimas 8 jogadas |
| Turno | 1 | Pretas ou brancas |
| Outros | 14 | Escada, olhos, etc. |

Estas 48 caracter√≠sticas foram cuidadosamente projetadas por especialistas de Go, contendo muito conhecimento do dom√≠nio.

### 17 Planos de Caracter√≠sticas do AlphaGo Zero

O AlphaGo Zero simplificou drasticamente a entrada, usando apenas 17 planos de caracter√≠sticas:

| N√∫mero do plano | Conte√∫do | Quantidade |
|-----------------|----------|------------|
| 1-8 | Posi√ß√µes das pedras pretas (√∫ltimas 8 jogadas) | 8 |
| 9-16 | Posi√ß√µes das pedras brancas (√∫ltimas 8 jogadas) | 8 |
| 17 | Turno atual (todo 1 ou todo 0) | 1 |

Estes 17 planos cont√™m apenas:
- **Estado atual do tabuleiro**: Cada posi√ß√£o tem pedra preta, pedra branca ou vazia
- **Informa√ß√£o hist√≥rica**: Estado do tabuleiro das √∫ltimas 8 jogadas
- **Informa√ß√£o de turno**: De quem √© a vez de jogar

Sem liberdades, sem julgamento de escada, sem dist√¢ncia da borda ‚Äî todo esse "conhecimento de Go" √© deixado para a rede neural aprender sozinha.

### Por que a Simplifica√ß√£o √© Boa?

#### 1. Deixar a rede descobrir caracter√≠sticas

Caracter√≠sticas manuais complexas podem perder informa√ß√µes importantes, ou codificar suposi√ß√µes incorretas. Deixar a rede neural aprender a partir de dados brutos pode levar a descobrir melhores representa√ß√µes de caracter√≠sticas.

De fato, o AlphaGo Zero aprendeu todas as caracter√≠sticas que os humanos projetaram (liberdades, escadas, etc.), e tamb√©m aprendeu alguns padr√µes que os humanos n√£o tinham consci√™ncia expl√≠cita.

#### 2. Melhor generalizabilidade

Muitas das 48 caracter√≠sticas s√£o espec√≠ficas do Go (como escadas, dist√¢ncia da borda). Os 17 planos simplificados s√£o gen√©ricos ‚Äî qualquer jogo de tabuleiro pode ser codificado de forma similar.

Isso estabeleceu as bases para o posterior **AlphaZero** (IA de jogos gen√©rica).

#### 3. Redu√ß√£o de erros humanos

Caracter√≠sticas projetadas manualmente podem conter defini√ß√µes incorretas ou incompletas. Simplificar a entrada elimina a possibilidade desses problemas.

---

## Arquitetura de Rede √önica

### Design de Rede Dupla da Vers√£o Original

O AlphaGo original usava duas redes neurais independentes:

```
Policy Network:  Entrada ‚Üí CNN ‚Üí Probabilidades de jogada 19x19
Value Network:   Entrada ‚Üí CNN ‚Üí Estimativa de taxa de vit√≥ria (-1 a 1)
```

Estas duas redes:
- Tinham arquiteturas diferentes (n√∫mero de camadas e canais ligeiramente diferentes)
- Eram treinadas independentemente (primeiro Policy, depois Value)
- N√£o compartilhavam nenhum par√¢metro

### Rede de Cabe√ßa Dupla do Zero

O AlphaGo Zero usa uma √∫nica rede, mas com duas cabe√ßas de sa√≠da (heads):

```
Entrada ‚Üí Backbone ResNet compartilhado ‚Üí Policy Head ‚Üí Probabilidades de jogada 19x19
                                       ‚Üí Value Head  ‚Üí Estimativa de taxa de vit√≥ria
```

As duas Heads compartilham o mesmo backbone ResNet (veja o pr√≥ximo artigo: [Rede de Cabe√ßa Dupla e Rede Residual](../dual-head-resnet)), o que traz v√°rias vantagens:

#### 1. Efici√™ncia de par√¢metros

Backbone compartilhado significa que a maioria dos par√¢metros √© usada por ambas as tarefas. Isso reduz o n√∫mero total de par√¢metros e diminui o risco de overfitting.

#### 2. Compartilhamento de caracter√≠sticas

"Onde devo jogar" (Policy) e "Quem vai ganhar" (Value) precisam entender padr√µes de tabuleiro similares. O backbone compartilhado permite que essas caracter√≠sticas sejam aprendidas e utilizadas por ambas as tarefas simultaneamente.

#### 3. Estabilidade de treinamento

O treinamento conjunto faz com que os sinais de gradiente venham de duas fontes, fornecendo sinais de supervis√£o mais ricos, tornando o treinamento mais est√°vel.

### O Poder da Rede Residual

O backbone do AlphaGo Zero usa uma **rede residual (ResNet) de 40 camadas**, muito mais profunda que a CNN de 13 camadas do AlphaGo original.

As conex√µes residuais (skip connections) permitem que redes profundas sejam treinadas efetivamente, evitando o problema do gradiente desvanecente. Esta foi a tecnologia inovadora da competi√ß√£o ImageNet de 2015, aplicada com sucesso pelo AlphaGo Zero ao campo do Go.

---

## Melhoria na Efici√™ncia de Treinamento

### Crescimento Exponencial do Auto-jogo

O processo de treinamento do AlphaGo Zero demonstra uma efici√™ncia impressionante:

| Tempo de Treinamento | Classifica√ß√£o ELO | Equivalente a |
|---------------------|-------------------|---------------|
| 0 horas | 0 | Jogando aleatoriamente |
| 3 horas | ~1000 | Descobriu regras b√°sicas |
| 12 horas | ~3000 | Descobriu josekis |
| 36 horas | ~4500 | Superou vers√£o Fan Hui |
| 60 horas | ~5200 | Superou vers√£o Lee Sedol |
| 72 horas | ~5400 | Superou AlphaGo original |
| 40 dias | ~5600 | Vers√£o mais forte |

**Tr√™s dias para superar humanos, tr√™s dias para superar IA que levou meses para treinar** ‚Äî isso √© uma melhoria de efici√™ncia exponencial.

### Por que T√£o R√°pido?

#### 1. Guia de busca mais forte

O MCTS do AlphaGo Zero √© completamente guiado pela rede neural, n√£o usa mais a pol√≠tica de jogada r√°pida (rollout). Isso torna a busca mais eficiente e precisa.

#### 2. Auto-jogo mais r√°pido

Como precisa de apenas uma rede (em vez de duas), o custo computacional de cada partida de auto-jogo √© reduzido. Isso significa que mais dados de treinamento podem ser gerados no mesmo tempo.

#### 3. Aprendizado mais eficaz

O treinamento conjunto da rede de cabe√ßa dupla faz com que a informa√ß√£o de cada partida seja utilizada de forma mais eficiente. Os gradientes de Policy e Value se refor√ßam mutuamente, acelerando a converg√™ncia.

### Compara√ß√£o com Aprendizado Humano

Quanto tempo jogadores humanos precisam para alcan√ßar diferentes n√≠veis?

| N√≠vel | Tempo necess√°rio humano | AlphaGo Zero |
|-------|------------------------|--------------|
| Iniciante | V√°rias semanas | Alguns minutos |
| Amateur 1 dan | V√°rios anos | Algumas horas |
| N√≠vel profissional | 10-20 anos | 1-2 dias |
| Campe√£o mundial | 20+ anos de dedica√ß√£o em tempo integral | 3 dias |
| Superar humanos | Imposs√≠vel | 3 dias |

Esta compara√ß√£o n√£o √© para diminuir jogadores humanos ‚Äî eles usam neur√¥nios biol√≥gicos, enquanto o AlphaGo Zero usa TPUs especialmente projetados e v√°rios quilowatts de eletricidade. Mas isso realmente demonstra qu√£o eficiente o m√©todo de aprendizado correto pode ser.

---

## Generalidade: Xadrez, Shogi

### O Nascimento do AlphaZero

Em dezembro de 2017, a DeepMind publicou o **AlphaZero** ‚Äî a vers√£o gen√©rica do AlphaGo Zero. O mesmo algoritmo, apenas modificando as regras do jogo, alcan√ßou n√≠vel mundial em tr√™s jogos de tabuleiro:

| Jogo | Tempo de Treinamento | Oponente | Resultado |
|------|---------------------|----------|-----------|
| Go | 8 horas | AlphaGo Zero | 60:40 |
| Xadrez | 4 horas | Stockfish 8 | 28 vit√≥rias 72 empates 0 derrotas |
| Shogi | 2 horas | Elmo | 90:8:2 |

Note os oponentes:
- **Stockfish** era a engine de xadrez mais forte na √©poca, usando d√©cadas de conhecimento humano e otimiza√ß√£o
- **Elmo** era a IA de shogi mais forte na √©poca

O AlphaZero com algumas horas de treinamento superou esses sistemas especializados que levaram anos para desenvolver.

### O Significado da Generalidade

AlphaGo Zero / AlphaZero provou algo importante:

> **O mesmo algoritmo de aprendizado pode alcan√ßar n√≠vel sobre-humano em diferentes dom√≠nios.**

N√£o s√£o tr√™s IAs diferentes, mas um framework de aprendizado gen√©rico:

1. **Auto-jogo** gera experi√™ncia
2. **Busca em √Årvore de Monte Carlo** explora possibilidades
3. **Rede Neural** aprende fun√ß√£o de pol√≠tica e valor
4. **Aprendizado por refor√ßo** otimiza a fun√ß√£o objetivo

Este framework n√£o depende de conhecimento espec√≠fico do dom√≠nio, isso √© um passo importante para a generaliza√ß√£o da IA.

### Impacto na IA Tradicional

Antes do AlphaZero, as IAs mais fortes de xadrez e shogi eram do estilo "sistema especialista":

- **Muito conhecimento humano**: Livros de abertura, tabelas de finais, fun√ß√µes de avalia√ß√£o
- **D√©cadas de otimiza√ß√£o**: Sangue e suor de incont√°veis jogadores e engenheiros
- **Altamente especializadas**: Stockfish n√£o consegue jogar Go, Elmo n√£o consegue jogar xadrez

O AlphaZero superou tudo isso em horas com um algoritmo gen√©rico. Isso fez muitos pesquisadores de IA reconsiderarem:

> Devemos investir mais esfor√ßos em "algoritmos de aprendizado gen√©ricos" ou "codifica√ß√£o de conhecimento especializado"?

A resposta parece cada vez mais clara: deixar a m√°quina aprender sozinha √© mais eficaz do que ensin√°-la conhecimento.

---

## Estilo de Jogo do AlphaGo Zero

### Est√©tica Al√©m dos Humanos

O mundo do Go tem uma avalia√ß√£o comum das jogadas do AlphaGo Zero: **mais elegantes**.

As jogadas do AlphaGo Lee √†s vezes pareciam "estranhas" ‚Äî como a jogada 37, onde os humanos precisaram de an√°lise posterior para entender sua profundidade. Mas as jogadas do AlphaGo Zero s√£o frequentemente avaliadas posteriormente como "imediatamente reconhec√≠veis como boas jogadas".

Isso pode ser porque:

1. **For√ßa de jogo mais forte**: Zero pode ver mais profundamente, jogar com mais calma
2. **Sem vieses humanos**: N√£o limitado por josekis tradicionais
3. **Objetivo consistente**: Busca apenas taxa de vit√≥ria, n√£o imita humanos

### Redescoberta da Teoria de Go Humana

Curiosamente, o AlphaGo Zero "redescobriu" o conhecimento de Go que os humanos acumularam ao longo de milhares de anos durante o treinamento:

- **Josekis**: Zero descobriu sozinho muitos josekis comuns, porque estes s√£o de fato as solu√ß√µes √≥timas para ambos os lados
- **Princ√≠pios de abertura**: A ordem de import√¢ncia de cantos, bordas e centro
- **Conhecimento de formas**: A diferen√ßa entre formas ruins e formas boas

Isso valida a racionalidade da teoria de Go humana ‚Äî este conhecimento n√£o √© coincid√™ncia, mas reflexo da ess√™ncia do Go.

### Inova√ß√£o Al√©m dos Humanos

Mas o Zero tamb√©m descobriu jogadas que os humanos nunca imaginaram:

- **Aberturas n√£o convencionais**: Varia√ß√µes sobre aberturas tradicionais
- **Sacrif√≠cios agressivos**: Mais disposto que humanos a desistir localmente em troca de vantagem global
- **Formas contra-intuitivas**: "Formas ruins" superficiais que na verdade s√£o a solu√ß√£o √≥tima

Estas inova√ß√µes est√£o mudando a compreens√£o humana do Go. Muitos jogadores profissionais dizem que estudar os registros de partidas do AlphaGo Zero lhes deu uma compreens√£o completamente nova do Go.

---

## Resumo dos Detalhes T√©cnicos

### Compara√ß√£o Completa com o AlphaGo Original

| Aspecto | AlphaGo (Original) | AlphaGo Zero |
|---------|-------------------|--------------|
| **Dados de treinamento** | Registros humanos + auto-jogo | Puro auto-jogo |
| **M√©todo de aprendizado** | Supervisionado + por refor√ßo | Puro por refor√ßo |
| **Caracter√≠sticas de entrada** | 48 planos | 17 planos |
| **Arquitetura de rede** | Policy/Value separadas | ResNet de cabe√ßa dupla |
| **Profundidade da rede** | 13 camadas | 40 camadas (ou mais) |
| **Avalia√ß√£o MCTS** | Rede neural + Rollout | Pura rede neural |
| **Simula√ß√µes** | ~100.000 por jogada | ~1.600 por jogada |
| **TPUs de treinamento** | 50+ | 4 |
| **TPUs de infer√™ncia** | 48 | 4 (escal√°vel) |

### Algoritmo Central

O ciclo de treinamento do AlphaGo Zero √© muito simples:

```
1. Auto-jogo
   - Usar rede atual para MCTS
   - Selecionar jogadas pela probabilidade de busca MCTS
   - Registrar cada passo (posi√ß√£o, probabilidade MCTS, resultado da partida)

2. Treinar rede
   - Amostrar do pool de experi√™ncia
   - Policy Head: minimizar entropia cruzada com probabilidades MCTS
   - Value Head: minimizar erro quadr√°tico m√©dio com resultado real
   - Otimizar ambos os objetivos conjuntamente

3. Atualizar rede
   - Substituir rede antiga pela nova (verificar que nova √© mais forte por auto-jogo)
   - Voltar ao passo 1
```

Este ciclo roda continuamente, e a rede fica cada vez mais forte. Sem dados humanos, sem conhecimento humano, apenas regras do jogo e objetivo de vit√≥ria/derrota.

---

## Li√ß√µes para Pesquisa em IA

### Aprendizado de Primeiros Princ√≠pios

O AlphaGo Zero demonstrou um m√©todo de aprendizado de "primeiros princ√≠pios":

> N√£o diga √† IA como fazer, apenas diga qual √© o objetivo, e deixe-a descobrir o m√©todo por conta pr√≥pria.

Isso forma um contraste marcante com a abordagem tradicional de sistemas especialistas. Sistemas especialistas tentam codificar conhecimento humano na IA, enquanto o AlphaGo Zero deixa a IA descobrir conhecimento por conta pr√≥pria.

O resultado √©: o conhecimento que a IA descobre pode ser mais completo e preciso que o conhecimento humano.

### O Poder do Auto-jogo

O AlphaGo Zero provou que o auto-jogo pode gerar dados de treinamento infinitos, e a qualidade desses dados melhora √† medida que a rede melhora.

Este √© um "ciclo positivo":
- Rede mais forte ‚Üí Dados de auto-jogo melhores
- Dados melhores ‚Üí Rede mais forte

Este ciclo pode continuar rodando at√© atingir o limite te√≥rico do jogo (se existir).

### A Import√¢ncia da Simplifica√ß√£o

O sucesso do AlphaGo Zero prova a import√¢ncia da "simplifica√ß√£o":

- Simplificar entrada (48 ‚Üí 17)
- Simplificar arquitetura (rede dupla ‚Üí rede √∫nica)
- Simplificar treinamento (supervisionado + refor√ßo ‚Üí puro refor√ßo)

Cada simplifica√ß√£o tornou o sistema mais poderoso. Isso nos diz: complexo n√£o significa bom, a solu√ß√£o mais simples frequentemente √© a melhor.

---

## Correspond√™ncia de Anima√ß√µes

Conceitos centrais discutidos neste artigo e n√∫meros de anima√ß√£o:

| N√∫mero | Conceito | Correspond√™ncia F√≠sica/Matem√°tica |
|--------|----------|-----------------------------------|
| üé¨ E7 | Treinamento do zero | Fen√¥meno de auto-organiza√ß√£o |
| üé¨ E5 | Auto-jogo | Converg√™ncia de ponto fixo |
| üé¨ E12 | Curva de crescimento de for√ßa | Crescimento em forma de S |
| üé¨ D12 | Rede residual | Rodovia de gradientes |

---

## Leitura Adicional

- **Pr√≥ximo artigo**: [Rede de Cabe√ßa Dupla e Rede Residual](../dual-head-resnet) ‚Äî An√°lise detalhada da arquitetura de rede neural do AlphaGo Zero
- **Artigo relacionado**: [Auto-jogo](../self-play) ‚Äî Por que o auto-jogo pode produzir n√≠vel sobre-humano
- **Aprofundamento t√©cnico**: [O Processo de Treinamento do Zero](../training-from-scratch) ‚Äî Evolu√ß√£o detalhada dos Dias 0-3

---

## Refer√™ncias

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
3. DeepMind. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
4. Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." *Nature*, 588, 604-609.
