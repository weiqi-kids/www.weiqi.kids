---
sidebar_position: 16
title: F√≥rmula PUCT em Detalhes
description: Compreens√£o aprofundada do mecanismo central de sele√ß√£o do AlphaGo - O princ√≠pio matem√°tico da f√≥rmula PUCT e experi√™ncia de ajuste de par√¢metros
---

import { MCTSTree } from '@site/src/components/D3Charts';

# F√≥rmula PUCT em Detalhes

No artigo anterior, apresentamos a combina√ß√£o de MCTS com redes neurais. Agora, vamos explorar em profundidade o n√∫cleo da fase de sele√ß√£o do MCTS ‚Äî a **f√≥rmula PUCT**.

Esta f√≥rmula parece simples, mas √© uma das chaves do sucesso do AlphaGo. Ela equilibra elegantemente "aproveitar jogadas boas conhecidas" e "explorar jogadas possivelmente melhores" ‚Äî dois objetivos aparentemente contradit√≥rios.

---

## F√≥rmula PUCT

### Defini√ß√£o da F√≥rmula

A f√≥rmula **PUCT (Predictor Upper Confidence Trees)** usada pelo AlphaGo:

$$a^* = \arg\max_a \left[ Q(s,a) + U(s,a) \right]$$

Onde:

$$U(s,a) = c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,a)}$$

Expandindo completamente:

$$a^* = \arg\max_a \left[ Q(s,a) + c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,a)} \right]$$

### Explica√ß√£o dos S√≠mbolos

| S√≠mbolo | Significado | Origem |
|---------|-------------|--------|
| $Q(s,a)$ | Valor m√©dio da a√ß√£o $a$ | Estat√≠sticas MCTS |
| $P(s,a)$ | Probabilidade a priori da a√ß√£o $a$ | Policy Network |
| $N(s)$ | Contagem de visitas do n√≥ pai | Estat√≠sticas MCTS |
| $N(s,a)$ | Contagem de visitas do n√≥ filho | Estat√≠sticas MCTS |
| $c_{\text{puct}}$ | Constante de explora√ß√£o | Hiperpar√¢metro |

### Compreens√£o Intuitiva

A f√≥rmula PUCT pode ser dividida em duas partes:

```
Pontua√ß√£o total = Q(s,a)        + U(s,a)
                    ‚Üì              ‚Üì
               Termo de        Termo de
               aproveitamento  explora√ß√£o
              "Qu√£o boa √©     "Vale a pena
               esta jogada?"   explorar mais?"
```

**Termo de aproveitamento Q(s,a)**:
- O desempenho m√©dio passado desta jogada
- Quanto mais visitas, mais precisa a estimativa
- Encoraja selecionar jogadas "conhecidamente boas"

**Termo de explora√ß√£o U(s,a)**:
- Quanto valor de explora√ß√£o esta jogada ainda tem
- Quanto menos visitas, maior a recompensa de explora√ß√£o
- Encoraja tentar jogadas "possivelmente boas"

---

## Significado de Cada Termo

### Q(s,a): Valor M√©dio

$Q(s,a)$ √© a m√©dia de todas as simula√ß√µes a partir do n√≥ $(s,a)$:

$$Q(s,a) = \frac{W(s,a)}{N(s,a)} = \frac{\sum_i z_i}{N(s,a)}$$

Onde $z_i \in \{-1, +1\}$ √© o resultado da $i$-√©sima simula√ß√£o.

**Caracter√≠sticas**:
- Intervalo: $[-1, +1]$
- Valor inicial: indefinido (precisa de pelo menos uma visita)
- Estabiliza √† medida que a contagem de visitas aumenta

**Interpreta√ß√£o**:
- $Q = 0.6$: Taxa de vit√≥ria desta jogada √© cerca de 80% (pois $Q = 2 \times \text{taxa de vit√≥ria} - 1$)
- $Q = 0$: Metade vit√≥rias, metade derrotas
- $Q = -0.3$: Taxa de vit√≥ria desta jogada √© cerca de 35%

### P(s,a): Probabilidade a Priori

$P(s,a)$ vem da sa√≠da da Policy Network:

$$P(s,a) = \pi_\theta(a|s)$$

**Caracter√≠sticas**:
- Intervalo: $[0, 1]$, e $\sum_a P(s,a) = 1$
- Calculada quando o n√≥ √© expandido pela primeira vez
- Reflete o julgamento da rede neural sobre "qu√£o boa √© esta jogada"

**Fun√ß√£o**:
- A√ß√µes de alta probabilidade s√£o exploradas prioritariamente
- Mesmo com contagem de visitas 0, h√° motiva√ß√£o para explora√ß√£o
- Guia a busca para focar em jogadas que "parecem razo√°veis"

### N(s) e N(s,a): Contagens de Visitas

$N(s)$ √© a contagem total de visitas do n√≥ pai:

$$N(s) = \sum_a N(s,a)$$

**Papel no termo de explora√ß√£o**:

$$\frac{\sqrt{N(s)}}{1 + N(s,a)}$$

O comportamento desta fra√ß√£o:
- Quando $N(s,a) = 0$, fra√ß√£o = $\sqrt{N(s)}$ (m√°xima motiva√ß√£o de explora√ß√£o)
- √Ä medida que $N(s,a)$ aumenta, a fra√ß√£o diminui
- Quando $N(s,a) \gg \sqrt{N(s)}$, a fra√ß√£o se aproxima de 0

Isso garante que:
1. **Cada a√ß√£o seja explorada pelo menos uma vez** (se $P(s,a) > 0$)
2. **Motiva√ß√£o de explora√ß√£o diminui com as visitas**
3. **A sele√ß√£o final √© dominada pelo valor $Q$**

### c_puct: Constante de Explora√ß√£o

$c_{\text{puct}}$ controla o equil√≠brio entre explora√ß√£o e aproveitamento:

| Valor de $c_{\text{puct}}$ | Efeito |
|---------------------------|--------|
| Menor (como 0.5) | Mais inclinado ao aproveitamento, foca rapidamente em boas jogadas |
| Moderado (como 1-2) | Equilibra explora√ß√£o e aproveitamento |
| Maior (como 5) | Mais inclinado √† explora√ß√£o, tenta mais possibilidades |

O valor usado pelo AlphaGo: $c_{\text{puct}} = 1.5$ (segundo o paper).

---

## Rela√ß√£o com UCB1

### Revis√£o da F√≥rmula UCB1

A f√≥rmula **UCB1** usada pelo MCTS tradicional:

$$\text{UCB1}(s,a) = \bar{X}_{s,a} + c \sqrt{\frac{\ln N(s)}{N(s,a)}}$$

Onde $\bar{X}_{s,a}$ √© o retorno m√©dio.

### Compara√ß√£o

| Aspecto | UCB1 | PUCT |
|---------|------|------|
| Termo de aproveitamento | $\bar{X}_{s,a}$ (retorno m√©dio) | $Q(s,a)$ (valor m√©dio) |
| Termo de explora√ß√£o | $\sqrt{\frac{\ln N}{n}}$ (limite de confian√ßa) | $P \cdot \frac{\sqrt{N}}{1+n}$ (guiado por priori) |
| Informa√ß√£o a priori | Nenhuma | Usa Policy Network |
| Decaimento da explora√ß√£o | Logar√≠tmico | Linear |

### Vantagens do PUCT

1. **Utiliza conhecimento a priori**: O $P(s,a)$ fornecido pela Policy Network faz a busca focar em jogadas razo√°veis desde o in√≠cio

2. **Converg√™ncia mais r√°pida**: Decaimento linear ($1/(1+n)$) faz a busca focar mais r√°pido que decaimento logar√≠tmico ($1/\sqrt{\ln N / n}$)

3. **Explora√ß√£o mais control√°vel**: $P(s,a)$ e $c_{\text{puct}}$ fornecem mais meios de controlar a explora√ß√£o

### Contexto Te√≥rico

UCB1 tem garantias te√≥ricas rigorosas (regret bound), mas essas garantias assumem:
- Cada bra√ßo (a√ß√£o) √© independente
- N√£o h√° informa√ß√£o a priori

No Go, temos um forte priori (Policy Network), e o PUCT pode utilizar melhor essa informa√ß√£o.

---

## Deriva√ß√£o Matem√°tica

### A Partir do Problema do Bandido Multi-Bra√ßo

A inspira√ß√£o do PUCT vem do problema do **Bandido Multi-Bra√ßo (Multi-Armed Bandit)**.

Imagine que voc√™ est√° diante de $K$ ca√ßa-n√≠queis, cada um com uma probabilidade de ganho diferente e desconhecida. Seu objetivo √© maximizar o n√∫mero total de vit√≥rias. A estrat√©gia √©:
- **Aproveitamento**: Jogar no que parece melhor
- **Explora√ß√£o**: Tentar outros, pode descobrir um melhor

UCB1 √© a solu√ß√£o cl√°ssica para este problema, PUCT √© uma variante dele.

### Base Te√≥rica do UCB

Para uma vari√°vel aleat√≥ria $X$, pela desigualdade de Hoeffding:

$$P(|\bar{X}_n - \mu| \geq \epsilon) \leq 2 \exp(-2n\epsilon^2)$$

Se queremos errar com probabilidade $1/t^4$, precisamos de:

$$\epsilon = \sqrt{\frac{2 \ln t}{n}}$$

Esta √© a origem do termo de explora√ß√£o do UCB1.

### Modifica√ß√µes do PUCT

O PUCT faz v√°rias modifica√ß√µes ao UCB cl√°ssico:

**1. Adicionar probabilidade a priori**

$$U(s,a) \propto P(s,a) \cdot (\text{termo de explora√ß√£o})$$

Isso concentra a explora√ß√£o em a√ß√µes de alta probabilidade.

**2. Mudar a forma do termo de explora√ß√£o**

De $\sqrt{\frac{\ln N}{n}}$ para $\frac{\sqrt{N}}{1+n}$

Isso acelera a converg√™ncia:

```
Compara√ß√£o (assumindo N = 1000, n = 10):

UCB1:  sqrt(ln(1000) / 10) = sqrt(0.69) ‚âà 0.83
PUCT:  sqrt(1000) / 11 ‚âà 2.87

PUCT d√° mais recompensa de explora√ß√£o, mas decai mais r√°pido
```

**3. Priori aprend√≠vel**

$P(s,a)$ vem da rede neural, que melhora com o treinamento. Isso cria um ciclo positivo entre MCTS e rede neural.

### Por que Esta Forma Funciona?

Explica√ß√£o intuitiva:

$$U(s,a) = c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,a)}$$

1. **$P(s,a)$**: "O especialista diz que esta jogada √© boa"
2. **$\sqrt{N(s)}$**: "Quanto sabemos sobre esta posi√ß√£o"
3. **$1/(1 + N(s,a))$**: "Quanto sabemos sobre esta jogada"

Combinando: **Quando sabemos muito sobre a posi√ß√£o, mas pouco sobre uma jogada, e o especialista acha que a jogada √© razo√°vel, devemos explor√°-la**.

---

## Compreens√£o Visual

### Varia√ß√£o do Termo de Explora√ß√£o

Vamos visualizar como o termo de explora√ß√£o varia com a contagem de visitas:

```
U(s,a) = c_puct √ó P(s,a) √ó ‚àöN(s) / (1 + N(s,a))

Assumindo P(s,a) = 0.1, c_puct = 1.5, N(s) = 1600

N(s,a)  |  U(s,a)
--------|----------
   0    |  6.00      ‚Üê N√£o visitado, m√°xima recompensa de explora√ß√£o
   1    |  3.00
   5    |  1.00
  10    |  0.55
  50    |  0.12
 100    |  0.06
 400    |  0.015     ‚Üê Ap√≥s muitas visitas, recompensa de explora√ß√£o muito pequena
```

### Impacto de Diferentes Probabilidades a Priori

```
Assumindo c_puct = 1.5, N(s) = 1600, N(s,a) = 0

P(s,a)  |  U(s,a)
--------|----------
  0.30  |  18.00     ‚Üê A√ß√µes de alta probabilidade t√™m mais motiva√ß√£o de explora√ß√£o
  0.10  |   6.00
  0.03  |   1.80
  0.01  |   0.60
  0.001 |   0.06     ‚Üê A√ß√µes de baixa probabilidade quase n√£o s√£o exploradas
```

### Explora√ß√£o Interativa

Tente ajustar o par√¢metro $c_{\text{puct}}$ abaixo e observe como ele afeta a sele√ß√£o do MCTS:

<MCTSTree width={700} height={450} showPUCT={true} interactive={true} cPuct={1.5} />

---

## Implementa√ß√£o Espec√≠fica no AlphaGo

### Implementa√ß√£o do AlphaGo Fan/Lee

O AlphaGo original usa uma f√≥rmula ligeiramente diferente:

$$U(s,a) = c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{\sum_b N(s,b)}}{1 + N(s,a)}$$

E o c√°lculo de $Q(s,a)$ considera a perda virtual:

```python
def get_ucb_score(node, action, c_puct=1.5):
    Q = node.W[action] / (node.N[action] + 1)  # Evitar divis√£o por zero
    P = node.prior[action]
    N_parent = sum(node.N.values())
    N_child = node.N[action]

    U = c_puct * P * math.sqrt(N_parent) / (1 + N_child)

    return Q + U
```

### Implementa√ß√£o do AlphaGo Zero

O AlphaGo Zero usa uma implementa√ß√£o mais simples:

```python
def select_action(node, c_puct=1.5):
    """Selecionar a a√ß√£o com maior pontua√ß√£o PUCT"""
    N_parent = sum(node.visit_count.values())

    def puct_score(action):
        Q = node.value_sum[action] / (node.visit_count[action] + 1)
        P = node.prior[action]
        U = c_puct * P * math.sqrt(N_parent) / (1 + node.visit_count[action])
        return Q + U

    return max(node.legal_actions, key=puct_score)
```

### Tratamento de N√≥s N√£o Visitados

Quando $N(s,a) = 0$, $Q(s,a)$ √© indefinido. Formas comuns de tratamento:

**M√©todo 1: Usar valor do n√≥ pai**
```python
Q = parent_value if N[action] == 0 else W[action] / N[action]
```

**M√©todo 2: Usar valor inicial**
```python
Q = 0 if N[action] == 0 else W[action] / N[action]
```

**M√©todo 3: Usar FPU (First Play Urgency)**
```python
# N√≥s n√£o visitados usam valor Q mais baixo
fpu_value = parent_Q - fpu_reduction
Q = fpu_value if N[action] == 0 else W[action] / N[action]
```

O AlphaGo Zero usa FPU, o que faz a busca preferir primeiro n√≥s j√° visitados.

---

## Experi√™ncia Pr√°tica de Ajuste

### Escolha de c_puct

$c_{\text{puct}}$ √© o hiperpar√¢metro mais importante. Diretrizes pr√°ticas:

**1. Relacionado √† qualidade da rede**
- Rede muito forte (alta precis√£o): Pode usar $c_{\text{puct}}$ menor
- Rede mais fraca: Precisa de $c_{\text{puct}}$ maior para corrigir erros

**2. Relacionado ao or√ßamento de busca**
- Muitas simula√ß√µes: Pode usar $c_{\text{puct}}$ maior (tempo suficiente para explorar)
- Poucas simula√ß√µes: Usar $c_{\text{puct}}$ menor (focar rapidamente)

**3. Relacionado √†s caracter√≠sticas do jogo**
- Jogos mais t√°ticos: Pode precisar de mais explora√ß√£o
- Jogos mais estrat√©gicos: Pode confiar mais no priori

### Valores T√≠picos

| Projeto | $c_{\text{puct}}$ |
|---------|-------------------|
| AlphaGo | 1.5 |
| AlphaGo Zero | 1.0 - 2.0 |
| AlphaZero | 1.25 |
| KataGo | 0.5 - 1.0 (ajuste din√¢mico) |
| Leela Zero | 1.5 - 2.0 |

### Ajuste Din√¢mico

Algumas implementa√ß√µes avan√ßadas usam $c_{\text{puct}}$ din√¢mico:

```python
def dynamic_cpuct(visit_count):
    """Ajustar constante de explora√ß√£o com base na contagem de visitas"""
    base = 1.0
    init = 1.5
    log_base = 19652  # Par√¢metro de ajuste

    return math.log((visit_count + log_base + 1) / log_base) + init
```

Isso faz a busca tender mais para explora√ß√£o no in√≠cio e mais para aproveitamento depois.

### An√°lise de Sensibilidade

Impacto de $c_{\text{puct}}$ na for√ßa do jogo:

```
Experimento (fixando outras condi√ß√µes, variando c_puct):

c_puct | Elo Relativo
-------|----------
  0.5  |   -50     ‚Üê Aproveitamento excessivo, perde boas jogadas
  1.0  |   +20
  1.5  |    0      ‚Üê Linha de base
  2.0  |   -10
  3.0  |   -30     ‚Üê Explora√ß√£o excessiva, desperdi√ßa busca
  5.0  |   -80
```

O valor √≥timo geralmente est√° entre 1.0-2.0, mas depende da qualidade da rede e do or√ßamento de busca.

---

## Variantes Avan√ßadas

### Variantes do PUCT

**1. Polynomial PUCT (P-UCT)**

$$U(s,a) = c \cdot P(s,a) \cdot \frac{N(s)^\alpha}{1 + N(s,a)}$$

Onde $\alpha$ √© um par√¢metro ajust√°vel (geralmente $\alpha = 0.5$).

**2. PUCT com Ru√≠do**

Adicionar ru√≠do Dirichlet no n√≥ raiz:

$$P'(s,a) = (1-\varepsilon) P(s,a) + \varepsilon \cdot \eta_a$$

Onde $\eta \sim \text{Dir}(\alpha)$. Isso aumenta a diversidade da explora√ß√£o.

**3. UCT-like PUCT**

$$U(s,a) = c \cdot P(s,a) \cdot \sqrt{\frac{\ln(1 + N(s) + c_{\text{base}})}{1 + N(s,a)}}$$

Isso combina a forma logar√≠tmica do UCT com o guia de priori do PUCT.

### Melhorias do KataGo

O KataGo fez v√°rias melhorias no PUCT:

**1. $c_{\text{puct}}$ din√¢mico**
Ajusta com base na complexidade da posi√ß√£o e progresso da busca.

**2. Incerteza na previs√£o de valor**
Considera a confian√ßa da predi√ß√£o da Value Network.

**3. Aprendizado de objetivo de pol√≠tica**
Aprende diretamente a distribui√ß√£o de visitas do MCTS, n√£o apenas a sa√≠da da cabe√ßa de pol√≠tica.

### Outras F√≥rmulas de Sele√ß√£o

Al√©m do PUCT, existem outras f√≥rmulas de sele√ß√£o:

**RAVE (Rapid Action Value Estimation)**

$$Q_{\text{RAVE}}(s,a) = (1-\beta) Q(s,a) + \beta Q_{\text{AMAF}}(s,a)$$

Usa estat√≠sticas "All Moves As First" para acelerar o aprendizado.

**GRAVE (Generalized RAVE)**

Variante do RAVE que usa estat√≠sticas de n√≥s ancestrais.

---

## An√°lise Te√≥rica

### Converg√™ncia

O PUCT garante converg√™ncia para a solu√ß√£o √≥tima?

**Rigorosamente**: N√£o h√° garantias te√≥ricas como o UCB1.

**Na pr√°tica**: Com simula√ß√µes suficientes, o PUCT converge para uma solu√ß√£o de alta qualidade, porque:
1. O termo de explora√ß√£o eventualmente tende a zero
2. Todas as a√ß√µes s√£o eventualmente visitadas
3. Os valores $Q$ convergem para os valores reais

### An√°lise de Complexidade

**Complexidade de tempo** (por simula√ß√£o):
- Sele√ß√£o: $O(\log N)$ (profundidade da √°rvore)
- Expans√£o: $O(A)$ (n√∫mero de a√ß√µes legais, precisa de infer√™ncia de rede neural)
- Avalia√ß√£o: $O(1)$ (Value Network) ou $O(T)$ (Rollout, $T$ √© o comprimento do jogo)
- Retropropaga√ß√£o: $O(\log N)$

**Complexidade de espa√ßo**:
- Por n√≥: $O(A)$ (armazenar priori e estat√≠sticas de visitas)
- √Årvore inteira: $O(N \cdot A)$ ($N$ √© o n√∫mero de n√≥s)

### Rela√ß√£o com Minimax

Quando $c_{\text{puct}} \to 0$ e n√∫mero de simula√ß√µes $\to \infty$, MCTS + PUCT aproxima a busca Minimax.

Mas com or√ßamento limitado, PUCT √© geralmente mais eficiente que Minimax + Alpha-Beta, porque pode utilizar melhor o conhecimento a priori.

---

## Perguntas Frequentes

### P: Por que n√£o usar diretamente a sa√≠da da Policy Network como sele√ß√£o?

**R: A Policy Network pode cometer erros**. A busca MCTS pode:
1. Verificar o julgamento da rede neural
2. Descobrir boas jogadas que a rede neural ignorou
3. Corrigir vieses sistem√°ticos da rede neural

Experimentos mostram que mesmo com uma rede neural muito forte, adicionar busca ainda melhora significativamente a for√ßa do jogo.

### P: Em quais situa√ß√µes o PUCT n√£o funciona bem?

1. **Probabilidade a priori completamente errada**: Se a Policy Network avalia boas jogadas como baixa probabilidade, o PUCT precisa de muitas simula√ß√µes para corrigir

2. **T√°ticas de longo prazo**: O PUCT pode ter dificuldade em descobrir sequ√™ncias t√°ticas longas que exigem c√°lculo preciso

3. **Falta de modelo do oponente**: O PUCT assume que o oponente tamb√©m usa a estrat√©gia √≥tima, pode n√£o ser √≥timo contra oponentes irracionais

### P: Como lidar com espa√ßos de a√ß√£o muito grandes?

Algumas t√©cnicas:
1. **Filtragem pela Policy Network**: Considerar apenas a√ß√µes com $P(s,a) > \epsilon$
2. **Alargamento progressivo**: Expandir apenas poucas a√ß√µes primeiro, expandir mais quando necess√°rio
3. **Poda din√¢mica**: Remover a√ß√µes claramente ruins

---

## Correspond√™ncia de Anima√ß√µes

Conceitos centrais discutidos neste artigo e n√∫meros de anima√ß√£o:

| N√∫mero | Conceito | Correspond√™ncia F√≠sica/Matem√°tica |
|--------|----------|-----------------------------------|
| üé¨ E4 | Explora√ß√£o e aproveitamento | Bandido multi-bra√ßo |
| üé¨ C3 | Sele√ß√£o PUCT | Limite de confian√ßa |

---

## Resumo

A f√≥rmula PUCT √© o n√∫cleo do MCTS do AlphaGo, aprendemos:

1. **Estrutura da f√≥rmula**: $Q + U$, termo de aproveitamento mais termo de explora√ß√£o
2. **Significado de cada termo**: $Q$ √© valor emp√≠rico, $P$ √© probabilidade a priori, $N$ controla decaimento da explora√ß√£o
3. **Rela√ß√£o com UCB1**: PUCT adiciona priori e usa forma diferente do termo de explora√ß√£o
4. **Deriva√ß√£o matem√°tica**: Do bandido multi-bra√ßo √† sele√ß√£o MCTS
5. **Ajuste pr√°tico**: Escolha e impacto de $c_{\text{puct}}$
6. **Variantes avan√ßadas**: Ajuste din√¢mico, ru√≠do, melhorias do KataGo

A eleg√¢ncia do PUCT est√° em ser simples e eficaz ‚Äî com uma √∫nica f√≥rmula equilibra explora√ß√£o e aproveitamento, e integra elegantemente o conhecimento a priori da rede neural.

---

## Leitura Adicional

- **Pr√≥ximo artigo**: [Vis√£o Geral do AlphaGo Zero](../alphago-zero) ‚Äî O avan√ßo do zero
- **Artigo anterior**: [A Combina√ß√£o de MCTS e Redes Neurais](../mcts-neural-combo) ‚Äî Arquitetura geral
- **Relacionado**: [Limites dos M√©todos Tradicionais](../traditional-limits) ‚Äî Por que precisamos de novos m√©todos

---

## Refer√™ncias

1. Rosin, C. D. (2011). "Multi-armed bandits with episode context." *Annals of Mathematics and Artificial Intelligence*, 61(3), 203-230.
2. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
3. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
4. Kocsis, L., & Szepesv√°ri, C. (2006). "Bandit based Monte-Carlo Planning." *ECML*.
5. Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). "Finite-time analysis of the multiarmed bandit problem." *Machine Learning*, 47(2), 235-256.
6. Wu, D., et al. (2019). "Accelerating Self-Play Learning in Go." *arXiv preprint* (Relat√≥rio t√©cnico do KataGo).

---

## Ap√™ndice: Exemplo de Implementa√ß√£o Completa

```python
import math
import numpy as np
from typing import Dict, List, Optional

class MCTSNode:
    """N√≥ MCTS"""
    def __init__(self, prior: float = 0.0):
        self.prior = prior          # P(s,a)
        self.visit_count = 0        # N(s,a)
        self.value_sum = 0.0        # W(s,a)
        self.children: Dict[int, 'MCTSNode'] = {}

    @property
    def q_value(self) -> float:
        """Calcular Q(s,a)"""
        if self.visit_count == 0:
            return 0.0
        return self.value_sum / self.visit_count


class MCTS:
    """Buscador MCTS usando PUCT"""

    def __init__(
        self,
        policy_network,
        value_network,
        c_puct: float = 1.5,
        num_simulations: int = 800
    ):
        self.policy_network = policy_network
        self.value_network = value_network
        self.c_puct = c_puct
        self.num_simulations = num_simulations

    def search(self, root_state) -> Dict[int, float]:
        """Executar busca MCTS, retornar distribui√ß√£o de visitas das a√ß√µes"""
        root = MCTSNode()

        # Expandir n√≥ raiz
        policy = self.policy_network(root_state)
        for action, prob in enumerate(policy):
            if is_legal(root_state, action):
                root.children[action] = MCTSNode(prior=prob)

        # Executar simula√ß√µes
        for _ in range(self.num_simulations):
            self._simulate(root, root_state)

        # Retornar distribui√ß√£o de visitas
        total_visits = sum(
            child.visit_count for child in root.children.values()
        )
        return {
            action: child.visit_count / total_visits
            for action, child in root.children.items()
        }

    def _simulate(self, node: MCTSNode, state) -> float:
        """Executar uma √∫nica simula√ß√£o"""

        # Se √© estado terminal
        if is_terminal(state):
            return get_result(state)

        # Se √© n√≥ folha, expandir e avaliar
        if not node.children:
            policy = self.policy_network(state)
            value = self.value_network(state)

            for action, prob in enumerate(policy):
                if is_legal(state, action):
                    node.children[action] = MCTSNode(prior=prob)

            return value

        # Sele√ß√£o: escolher a√ß√£o com maior pontua√ß√£o PUCT
        action = self._select_action(node)
        child = node.children[action]
        next_state = apply_action(state, action)

        # Simula√ß√£o recursiva
        value = -self._simulate(child, next_state)

        # Retropropaga√ß√£o: atualizar estat√≠sticas
        child.visit_count += 1
        child.value_sum += value

        return value

    def _select_action(self, node: MCTSNode) -> int:
        """Usar f√≥rmula PUCT para selecionar a√ß√£o"""
        total_visits = sum(
            child.visit_count for child in node.children.values()
        )

        def puct_score(action: int, child: MCTSNode) -> float:
            # Q(s,a): valor m√©dio
            q = child.q_value

            # U(s,a): b√¥nus de explora√ß√£o
            u = (
                self.c_puct
                * child.prior
                * math.sqrt(total_visits)
                / (1 + child.visit_count)
            )

            return q + u

        return max(
            node.children.keys(),
            key=lambda a: puct_score(a, node.children[a])
        )


# Exemplo de uso
def play_game():
    policy_net = PolicyNetwork()
    value_net = ValueNetwork()

    mcts = MCTS(
        policy_network=policy_net,
        value_network=value_net,
        c_puct=1.5,
        num_simulations=1600
    )

    state = initial_state()

    while not is_terminal(state):
        # Executar busca MCTS
        visit_distribution = mcts.search(state)

        # Selecionar a√ß√£o com mais visitas
        action = max(visit_distribution.keys(),
                     key=lambda a: visit_distribution[a])

        # Executar a√ß√£o
        state = apply_action(state, action)
        print(f"A√ß√£o selecionada {action} com propor√ß√£o de visitas "
              f"{visit_distribution[action]:.2%}")

    print(f"Resultado do jogo: {get_result(state)}")
```

Esta implementa√ß√£o mostra o papel central da f√≥rmula PUCT no MCTS. A implementa√ß√£o real do AlphaGo tamb√©m inclui:
- Busca paralela (perda virtual)
- Avalia√ß√£o de rede neural em lote
- Reutiliza√ß√£o da √°rvore
- Ru√≠do Dirichlet
- Controle de temperatura, entre outras funcionalidades
