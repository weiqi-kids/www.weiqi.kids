---
sidebar_position: 9
title: Detalhes da Value Network
description: Compreensão aprofundada da arquitetura, desafios de treinamento e papel crítico da rede de valor do AlphaGo no MCTS
---

# Detalhes da Value Network

Se a Policy Network diz ao AlphaGo "onde deve jogar a seguir", a Value Network responde a uma questão mais fundamental:

> **"Este jogo, eu vou vencer?"**

---

## O que é a Value Network?

### Função Principal

A Value Network é uma rede neural convolucional profunda, cuja tarefa é:

> **Dado o estado atual do tabuleiro, prever a taxa de vitória final**

Em termos matemáticos:

```
v = f_θ(s)
```

Onde:
- `s`: Estado atual do tabuleiro
- `f_θ`: Value Network (θ são os parâmetros da rede)
- `v`: Um valor entre -1 e +1

### Significado da Saída

| Valor de Saída | Significado |
|----------------|-------------|
| +1 | O jogador atual vence com certeza |
| +0,5 | O jogador atual tem ~75% de taxa de vitória |
| 0 | Taxas de vitória iguais para ambos |
| -0,5 | O jogador atual tem ~25% de taxa de vitória |
| -1 | O jogador atual perde com certeza |

### Por que precisamos de um único valor?

#### Comparar diferentes escolhas

Ao jogar, frequentemente precisamos escolher entre múltiplas opções. A Value Network torna essa comparação simples:

```
Valor da posição da Opção A: 0,3
Valor da posição da Opção B: 0,5
Valor da posição da Opção C: 0,2

→ Escolher B (o maior valor)
```

Se não houvesse um único valor, como compararíamos "capturar um grupo do oponente" com "cercar um grande território"?

#### Substituir muitas simulações

Na busca em árvore de Monte Carlo tradicional, avaliar uma posição requer **simulação aleatória (rollout)**:

1. Começar da posição atual
2. Ambos os lados jogam aleatoriamente até o fim do jogo
3. Registrar vitória/derrota
4. Repetir milhares de vezes, calcular taxa de vitória

Isso é muito lento. A Value Network pode dar uma avaliação em **uma única propagação direta**, ordens de magnitude mais rápido.

| Método | Tempo de Avaliação | Precisão |
|--------|-------------------|----------|
| 1000 simulações aleatórias | ~2000 ms | Baixa |
| 15000 simulações aleatórias | ~30000 ms | Média |
| Value Network | ~3 ms | Alta (equivalente a 15000 simulações) |

---

## Arquitetura da Rede

### Similaridade com a Policy Network

A arquitetura da Value Network é muito similar à Policy Network, ambas são redes neurais convolucionais profundas:

```
Camada de Entrada → Camadas Conv ×12 → Camada Totalmente Conectada → Saída
        ↓                ↓                       ↓                    ↓
    19×19×48         19×19×192                 256-dim           Valor único
```

### Camada de Entrada

Similar à Policy Network, a entrada é um tensor de características **19×19×49**:

- **19×19**: Tamanho do tabuleiro
- **49**: 48 planos de características + 1 plano indicando de quem é a vez

O plano extra é importante: a Value Network precisa saber de quem é a vez, porque o valor da mesma posição é oposto para preto e branco.

### Camadas Convolucionais

Igual à Policy Network:
- **12 camadas convolucionais**
- **192 filtros**
- **Kernels 3×3** (5×5 na primeira camada)
- **Função de ativação ReLU**

### Diferença na Camada de Saída

Esta é a diferença chave entre Value Network e Policy Network:

#### Saída da Policy Network
```
19×19×192 → Conv 1×1 → 19×19×1 → Achatar → 361-dim → Softmax → Distribuição de probabilidade
```

#### Saída da Value Network
```
19×19×192 → Conv 1×1 → 19×19×1 → Achatar → 361-dim → FC 256 → ReLU → FC 1 → Tanh → Valor único
```

### Função de Ativação Tanh

A última camada da Value Network usa a função **Tanh** (tangente hiperbólica):

```
Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```

O intervalo de saída do Tanh é **(-1, +1)**, correspondendo exatamente a vitória/derrota.

#### Por que usar Tanh em vez de Sigmoid?

O intervalo de saída do Sigmoid é (0, 1), que também pode representar taxa de vitória. Mas Tanh tem várias vantagens:

1. **Simetria**: Centrado em 0, saída pode ser positiva ou negativa
2. **Melhor gradiente**: Gradiente próximo de 1 em torno de 0
3. **Semântica clara**: Valores positivos = vitória, valores negativos = derrota, zero = empate

### Diagrama Completo da Arquitetura

```
Entrada: 19×19×49
        ↓
    Conv 5×5, 192 filtros
        ↓
    ReLU
        ↓
    Conv 3×3, 192 filtros (×11)
        ↓
    ReLU
        ↓
    Conv 1×1, 1 filtro
        ↓
    Achatar (361-dim)
        ↓
    Totalmente Conectada (256-dim)
        ↓
    ReLU
        ↓
    Totalmente Conectada (1-dim)
        ↓
    Tanh
        ↓
Saída: [-1, +1]
```

### Contagem de Parâmetros

| Camada | Cálculo | Número de Parâmetros |
|--------|---------|----------------------|
| Camadas conv | Igual à Policy Network | ~3,9M |
| Camada FC 1 | 361×256 + 256 | 92.672 |
| Camada FC 2 | 256×1 + 1 | 257 |
| **Total** | | **~4,0M** |

Aproximadamente 4 milhões de parâmetros, ligeiramente mais que a Policy Network.

---

## Desafios do Treinamento

### Problema de Overfitting

O treinamento da Value Network é muito mais difícil que o da Policy Network. O principal problema é **overfitting**.

#### O que é overfitting?

Overfitting é quando o modelo "memoriza" os dados de treinamento em vez de aprender a generalizar. Manifesta-se como:
- Desempenho muito bom no conjunto de treinamento
- Desempenho ruim no conjunto de teste

#### Por que a Value Network é propensa a overfitting?

Considere os dados de um jogo:

```
Posição 1 → Posição 2 → Posição 3 → ... → Posição 200 → Resultado: Preto vence
```

Se treinarmos diretamente com esses dados:
- Essas 200 posições têm forte correlação
- Elas vêm do mesmo jogo, têm o mesmo resultado
- O modelo pode aprender a "reconhecer" este jogo, em vez de entender posições

O DeepMind descobriu: se usar os mesmos registros de jogos humanos para treinar Policy e Value Network, a Value Network terá sério overfitting.

### Solução: Dados de Auto-jogo

A solução do DeepMind é usar **auto-jogo** para gerar novos dados de treinamento:

```
1. Usar a RL Policy Network treinada para auto-jogo
2. Pegar apenas uma posição de cada jogo (evitar correlação)
3. O rótulo dessa posição é o resultado final do jogo
4. Gerar 30 milhões de tais amostras
```

#### Por que isso resolve o overfitting?

1. **Grande volume de dados**: 30 milhões de posições independentes
2. **Sem correlação**: Apenas uma posição de cada jogo
3. **Distribuição diferente**: A distribuição de posições do auto-jogo é diferente dos registros humanos

### Geração de Dados de Treinamento

```python
# Pseudocódigo
training_data = []

for game_id in range(30_000_000):
    # Auto-jogar um jogo
    states, result = self_play(rl_policy_network)

    # Selecionar aleatoriamente uma posição
    random_index = random.randint(0, len(states) - 1)
    state = states[random_index]

    # Registrar posição e resultado
    training_data.append((state, result))
```

---

## Objetivo e Métodos de Treinamento

### Perda de Erro Quadrático Médio

A Value Network usa **Erro Quadrático Médio (MSE)** como função de perda:

```
L(θ) = (1/n) × Σ (v_θ(s) - z)²
```

Onde:
- `v_θ(s)`: Valor previsto pelo modelo
- `z`: Resultado real (+1 ou -1)

#### Por que usar MSE em vez de entropia cruzada?

- **Entropia cruzada** é adequada para problemas de classificação (rótulos discretos)
- **MSE** é adequada para problemas de regressão (valores contínuos)

Embora o resultado seja apenas +1 ou -1, o modelo prevê valores contínuos (qualquer número entre -1 e +1). MSE faz o modelo aprender a prever valores próximos de +1 ou -1.

### Processo de Treinamento

```python
# Pseudocódigo
for epoch in range(num_epochs):
    for batch in dataloader:
        states, outcomes = batch

        # Propagação direta
        values = network(states)  # (batch, 1)

        # Calcular perda (MSE)
        loss = mse_loss(values, outcomes)

        # Retropropagação
        loss.backward()
        optimizer.step()
```

Detalhes do treinamento:
- **Otimizador**: SGD com momentum
- **Taxa de aprendizado**: 0,003
- **Tamanho do lote**: 32
- **Tempo de treinamento**: Aproximadamente 1 semana (50 GPUs)

---

## Análise de Precisão

### Comparação com Simulação Aleatória

O DeepMind realizou comparações detalhadas no artigo:

| Método de Avaliação | Erro de Previsão |
|---------------------|------------------|
| 1000 simulações aleatórias | Alto |
| 15000 simulações aleatórias | Médio |
| Value Network | Comparável a 15000 simulações |

Isso significa que uma avaliação da Value Network ≈ 15000 simulações aleatórias, mas ~1000× mais rápida.

### Precisão em Diferentes Fases

A precisão da Value Network depende do progresso do jogo:

| Fase | Jogadas Restantes | Dificuldade de Previsão | Precisão |
|------|-------------------|-------------------------|----------|
| Abertura | ~300 | Muito difícil | Baixa |
| Meio de jogo | ~150 | Difícil | Média |
| Yose | ~50 | Mais fácil | Alta |
| Final | ~10 | Simples | Muito alta |

Isso é intuitivamente razoável: quanto mais perto do fim do jogo, mais determinado é o resultado.

### Distribuição de Saída

A distribuição de saída de uma Value Network bem treinada:

```
        Frequência
          |
          |    *
          |   * *
          |  *   *
          | *     *
          |*       *
          +----+----+---- Valor de saída
         -1    0   +1

A maioria das saídas está concentrada perto de -1 e +1
(porque a maioria das posições tem uma tendência clara de vitória/derrota)
```

### Posições Incertas

Quando a saída da Value Network está próxima de 0, indica que a posição é muito complexa, com resultado incerto. Essas posições geralmente são:
- No meio de grandes batalhas
- Forças equilibradas entre ambos os lados
- Existem múltiplas variações possíveis

No MCTS, esses nós recebem mais recursos de busca (por causa da alta incerteza).

---

## Papel no MCTS

### Avaliação de Nós Folha

A Value Network desempenha um papel crítico na fase de **Avaliação** do MCTS:

```
Árvore de busca MCTS:

        Nó raiz (posição atual)
           /    \
         A        B
        /  \    /  \
       A1  A2  B1  B2 ← Nós folha
        ↓   ↓   ↓   ↓
       Aval Aval Aval Aval
```

Quando o MCTS alcança um nó folha, precisa avaliar o valor dessa posição. Existem dois métodos:

1. **Simulação aleatória (Rollout)**: Jogar aleatoriamente do nó folha até o fim do jogo
2. **Avaliação pela Value Network**: Prever diretamente com a rede neural

O AlphaGo combina ambos:

```
V(folha) = (1-λ) × V_network(folha) + λ × V_rollout(folha)
```

Onde λ = 0,5, ou seja, cada um com metade do peso.

#### Por que combinar?

- **Value Network** é mais precisa, mas pode ter viés sistemático
- **Simulação aleatória** é menos precisa, mas fornece estimativa independente
- Combinar ambos pode complementar um ao outro

### Simplificação no AlphaGo Zero

O posterior AlphaGo Zero abandonou completamente a simulação aleatória:

```
V(folha) = V_network(folha)
```

Isso simplificou muito o sistema, enquanto a força de jogo aumentou. Isso prova que a Value Network é confiável o suficiente, não precisando do "seguro" da simulação aleatória.

### Atualização por Backpropagation

Após avaliar o nó folha, esse valor é propagado de volta pelo caminho:

```
v3 = V(folha) = 0,6
      ↑
Valor Q de A2 atualizado
      ↑
Valor Q de A atualizado
      ↑
Estatísticas do nó raiz atualizadas
```

O valor Q mantido por cada nó é a média das avaliações dos nós folha que passaram por ele:

```
Q(s, a) = (1/N(s,a)) × Σ V(folha)
```

---

## Análise Visual

### Superfície de Valor

Imagine um tabuleiro 3×3 simplificado. A Value Network aprende uma "superfície de valor":

```
        Posição da pedra branca
       1   2   3
    ┌───┬───┬───┐
  1 │+0,3│-0,1│+0,2│
Posição├───┼───┼───┤
pedra  2 │-0,2│+0,5│-0,3│
preta  ├───┼───┼───┤
  3 │+0,1│-0,2│+0,4│
    └───┴───┴───┘
```

Esta superfície nos diz o valor de cada combinação de posições. Valores positivos favorecem preto, valores negativos favorecem branco.

### Evolução Durante o Treinamento

À medida que o treinamento progride, as previsões da Value Network gradualmente se tornam mais precisas:

```
       Erro de previsão
          |
     1,0  |*
          | *
     0,5  |  *
          |   *
     0,1  |    * * * * *
          +─────────────── Passos de treinamento
          0   100K  500K  1M
```

O erro cai rapidamente, depois estabiliza.

### Identificação de Posições Difíceis

A Value Network pode ajudar a identificar posições difíceis:

| Saída | Significado | Estratégia de Resposta |
|-------|-------------|------------------------|
| Próximo de +1 | Grande vantagem | Jogar solidamente |
| Próximo de -1 | Grande desvantagem | Buscar oportunidades de virar |
| Próximo de 0 | Posição complexa | Requer cálculo profundo |

O AlphaGo investe mais tempo de pensamento em posições próximas de 0.

---

## Pontos de Implementação

### Implementação em PyTorch

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ValueNetwork(nn.Module):
    def __init__(self, input_channels=49, num_filters=192, num_layers=12):
        super().__init__()

        # Primeira camada convolucional (5×5)
        self.conv1 = nn.Conv2d(input_channels, num_filters,
                               kernel_size=5, padding=2)

        # Camadas convolucionais intermediárias (3×3)×11
        self.conv_layers = nn.ModuleList([
            nn.Conv2d(num_filters, num_filters,
                     kernel_size=3, padding=1)
            for _ in range(num_layers - 1)
        ])

        # Camada convolucional de saída
        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)

        # Camadas totalmente conectadas
        self.fc1 = nn.Linear(361, 256)
        self.fc2 = nn.Linear(256, 1)

    def forward(self, x):
        # x: (batch, 49, 19, 19)

        # Camadas convolucionais
        x = F.relu(self.conv1(x))
        for conv in self.conv_layers:
            x = F.relu(conv(x))
        x = self.conv_out(x)

        # Achatar
        x = x.view(x.size(0), -1)  # (batch, 361)

        # Camadas totalmente conectadas
        x = F.relu(self.fc1(x))
        x = torch.tanh(self.fc2(x))

        return x.squeeze(-1)  # (batch,)
```

### Loop de Treinamento

```python
def train_value_network(model, optimizer, states, outcomes):
    """
    states: (batch, 49, 19, 19) - Características do tabuleiro
    outcomes: (batch,) - Resultado do jogo (+1 ou -1)
    """
    # Propagação direta
    values = model(states)  # (batch,)

    # Perda MSE
    loss = F.mse_loss(values, outcomes)

    # Retropropagação
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Calcular precisão (previsão correta de vitória/derrota)
    predictions = (values > 0).float() * 2 - 1  # Converter para +1/-1
    accuracy = (predictions == outcomes).float().mean()

    return loss.item(), accuracy.item()
```

### Técnicas para Evitar Overfitting

```python
# 1. Aumento de dados (simetria de 8 dobras)
def augment(state, outcome):
    augmented = []
    for rotation in [0, 90, 180, 270]:
        s = rotate(state, rotation)
        augmented.append((s, outcome))
        augmented.append((flip(s), outcome))
    return augmented

# 2. Dropout
class ValueNetworkWithDropout(ValueNetwork):
    def __init__(self, *args, dropout_rate=0.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        # ... camadas conv ...
        x = self.dropout(x)  # Dropout antes das camadas FC
        # ... camadas FC ...

# 3. Parada antecipada (Early Stopping)
best_val_loss = float('inf')
patience = 10
counter = 0

for epoch in range(max_epochs):
    train_loss = train_one_epoch()
    val_loss = evaluate()

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        save_model()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping!")
            break
```

---

## Colaboração com a Policy Network

### Relação Complementar

Policy Network e Value Network são complementares no AlphaGo:

| Rede | Questão Respondida | Saída | Papel no MCTS |
|------|---------------------|-------|---------------|
| Policy | Onde jogar a seguir? | Distribuição de probabilidade | Guiar direção da busca |
| Value | Vou vencer este jogo? | Valor único | Avaliar nós folha |

### Rede de Cabeça Dupla Unificada

No AlphaGo Zero, essas duas redes foram fundidas em uma **rede de cabeça dupla**:

```
       Camadas compartilhadas de extração de características
              |
       ┌──────┴──────┐
       ↓              ↓
  Cabeça Policy    Cabeça Value
       ↓              ↓
  361 probabilidades   Valor único
```

Vantagens deste design:
- **Compartilhamento de parâmetros**: Reduz computação
- **Compartilhamento de características**: Policy e Value usam as mesmas características
- **Treinamento mais estável**: Os dois objetivos se regularizam mutuamente

Veja [Rede de Cabeça Dupla e Rede Residual](../dual-head-resnet) para detalhes.

---

## Correspondência com Animações

Os conceitos principais abordados neste artigo e seus números de animação:

| Número | Conceito | Correspondência Física/Matemática |
|--------|----------|-----------------------------------|
| E2 | Value Network | Superfície de potencial |
| D4 | Função de valor | Retorno esperado |
| C6 | Avaliação de nós folha | Aproximação de função |
| H3 | Diferença temporal | Aprendizado por bootstrap |

---

## Leitura Adicional

- **Artigo anterior**: [Detalhes da Policy Network](../policy-network) — Como a rede de políticas escolhe jogadas
- **Próximo artigo**: [Design de Características de Entrada](../input-features) — Detalhes dos 48 planos de características
- **Tópico avançado**: [Combinação de MCTS com Redes Neurais](../mcts-neural-combo) — O fluxo de busca completo

---

## Pontos-Chave

1. **Value Network prevê taxa de vitória**: Saída de um único valor entre -1 e +1
2. **Saída Tanh**: Garante que a saída esteja no intervalo correto
3. **Perda MSE**: Aproxima o valor previsto do resultado real
4. **Desafio de overfitting**: Precisa usar dados de auto-jogo para evitar
5. **Substitui simulação aleatória**: Uma avaliação ≈ 15000 simulações

A Value Network é o "julgamento" do AlphaGo — ela permite que a IA avalie a qualidade de qualquer posição sem precisar esgotar todas as possibilidades.

---

## Referências

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 551, 354-359.
3. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.
4. Tesauro, G. (1995). "Temporal difference learning and TD-Gammon." *Communications of the ACM*, 38(3), 58-68.
