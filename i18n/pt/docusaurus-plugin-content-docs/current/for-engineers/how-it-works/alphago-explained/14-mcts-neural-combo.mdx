---
sidebar_position: 15
title: A CombinaÃ§Ã£o de MCTS e Redes Neurais
description: CompreensÃ£o aprofundada de como o AlphaGo combina perfeitamente a Busca em Ãrvore de Monte Carlo com redes neurais profundas
---

import { MCTSTree } from '@site/src/components/D3Charts';

# A CombinaÃ§Ã£o de MCTS e Redes Neurais

Nos artigos anteriores, apresentamos separadamente as redes neurais (Policy Network e Value Network) e os conceitos de aprendizado por reforÃ§o. Agora, vamos explorar a inovaÃ§Ã£o central do AlphaGo â€” **como combinar perfeitamente a Busca em Ãrvore de Monte Carlo (MCTS) com redes neurais**.

Esta combinaÃ§Ã£o Ã© a chave do sucesso do AlphaGo: as redes neurais fornecem "intuiÃ§Ã£o", o MCTS fornece "raciocÃ­nio", e ambos se complementam.

---

## RevisÃ£o do MCTS Tradicional

### O que Ã© MCTS?

**Monte Carlo Tree Search (MCTS)** Ã© um algoritmo de busca baseado em amostragem aleatÃ³ria, particularmente adequado para IA de jogos.

A ideia central do MCTS Ã©: **em vez de enumerar todas as jogadas possÃ­veis, Ã© melhor simular aleatoriamente um grande nÃºmero de partidas e usar estatÃ­sticas para estimar a qualidade de cada jogada**.

### Quatro Fases

O MCTS tradicional contÃ©m quatro fases que se repetem continuamente:

```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                        Ciclo MCTS                           â”‚
     â”‚                                                             â”‚
     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   â”‚ Selection â”‚ â†’  â”‚ Expansion â”‚ â†’  â”‚ Simulationâ”‚ â†’  â”‚Backprop   â”‚
     â”‚   â”‚  SeleÃ§Ã£o  â”‚    â”‚  ExpansÃ£o â”‚    â”‚  SimulaÃ§Ã£oâ”‚    â”‚  Retropropâ”‚
     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚        â”‚                                                    â”‚
     â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                           Repetir N vezes
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Vamos entender cada fase em detalhes:

### 1. Selection (SeleÃ§Ã£o)

A partir do nÃ³ raiz, descer pela Ã¡rvore, selecionando o nÃ³ filho "mais promissor" atÃ© chegar a um nÃ³ folha.

O critÃ©rio de seleÃ§Ã£o Ã© a fÃ³rmula **UCB1 (Upper Confidence Bound)**:

$$\text{UCB1}(s, a) = \bar{X}_{s,a} + c \sqrt{\frac{\ln N_s}{N_{s,a}}}$$

Onde:
- $\bar{X}_{s,a}$: Retorno mÃ©dio a partir do nÃ³ $(s, a)$ (**termo de exploraÃ§Ã£o**)
- $\sqrt{\frac{\ln N_s}{N_{s,a}}}$: BÃ´nus de exploraÃ§Ã£o (**termo de exploraÃ§Ã£o**)
- $N_s$: NÃºmero de visitas do nÃ³ pai
- $N_{s,a}$: NÃºmero de visitas do nÃ³ filho
- $c$: Constante que equilibra exploraÃ§Ã£o e aproveitamento

A sabedoria desta fÃ³rmula estÃ¡ em:
- NÃ³s com menos visitas recebem um bÃ´nus de exploraÃ§Ã£o maior
- Ã€ medida que o nÃºmero de visitas aumenta, a seleÃ§Ã£o tende cada vez mais para nÃ³s com valor real mais alto

### 2. Expansion (ExpansÃ£o)

Ao chegar a um nÃ³ folha, escolher uma aÃ§Ã£o nÃ£o explorada e criar um novo nÃ³ filho.

```
Antes da expansÃ£o:              Depois da expansÃ£o:
     â—‹ (raiz)                       â—‹ (raiz)
    /â”‚\                            /â”‚\
   â—‹ â—‹ â—‹                          â—‹ â—‹ â—‹
  /â”‚              â†’              /â”‚
 â—‹ â—‹                            â—‹ â—‹
   â†‘                               \
   nÃ³ folha                         â— (novo nÃ³)
```

### 3. Simulation (SimulaÃ§Ã£o/Rollout)

A partir do novo nÃ³, usar alguma estratÃ©gia (geralmente aleatÃ³ria ou heurÃ­stica simples) para completar rapidamente a partida e obter o resultado.

Esta Ã© a origem do nome "Monte Carlo" â€” **usar simulaÃ§Ã£o aleatÃ³ria para estimar resultados**.

A estratÃ©gia de rollout do MCTS tradicional pode ser:
- **Puramente aleatÃ³ria**: Escolher uniformemente entre as jogadas legais
- **HeurÃ­stica leve**: Usar regras simples para filtrar jogadas claramente ruins

### 4. Backpropagation (RetropropagaÃ§Ã£o)

Propagar o resultado da simulaÃ§Ã£o (vitÃ³ria/derrota) de volta pelo caminho, atualizando as estatÃ­sticas de cada nÃ³:

```
ConteÃºdo da atualizaÃ§Ã£o:
- Contagem de visitas: N(s, a) â† N(s, a) + 1
- Valor acumulado: W(s, a) â† W(s, a) + z
- Valor mÃ©dio: Q(s, a) = W(s, a) / N(s, a)
```

Onde $z$ Ã© o resultado da simulaÃ§Ã£o (+1 ou -1).

### LimitaÃ§Ãµes do MCTS Tradicional

O MCTS tradicional tem desempenho limitado no Go, os principais problemas sÃ£o:

1. **Baixa qualidade do Rollout**: A simulaÃ§Ã£o aleatÃ³ria frequentemente produz partidas irracionais
2. **Necessidade de muitas simulaÃ§Ãµes**: Cada jogada pode exigir dezenas de milhares de simulaÃ§Ãµes
3. **AvaliaÃ§Ã£o imprecisa**: Dependendo apenas de estatÃ­sticas de vitÃ³ria/derrota, a eficiÃªncia da utilizaÃ§Ã£o de informaÃ§Ã£o Ã© baixa
4. **Incapacidade de aproveitar padrÃµes**: RecomeÃ§a a busca toda vez, nÃ£o acumula experiÃªncia

Esses problemas foram elegantemente resolvidos no AlphaGo pelas redes neurais.

---

## Como as Redes Neurais Melhoram o MCTS

### Arquitetura Geral

O AlphaGo integra duas redes neurais no MCTS:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Arquitetura de Busca AlphaGo              â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    MCTS                               â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚   Selection    Expansion    Evaluation    Backup      â”‚  â”‚
â”‚  â”‚       â”‚            â”‚            â”‚            â”‚        â”‚  â”‚
â”‚  â”‚       â–¼            â–¼            â–¼            â–¼        â”‚  â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”      â”‚  â”‚
â”‚  â”‚    â”‚PUCT â”‚    â”‚Policyâ”‚    â”‚Value Netâ”‚    â”‚Atualizâ”‚    â”‚  â”‚
â”‚  â”‚    â”‚FÃ³rm.â”‚    â”‚Net   â”‚    â”‚+ Rolloutâ”‚    â”‚Stats â”‚    â”‚  â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜      â”‚  â”‚
â”‚  â”‚       â†‘            â†‘            â†‘                     â”‚  â”‚
â”‚  â”‚       â”‚            â”‚            â”‚                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â”‚            â”‚            â”‚                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                 Neural Networks                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  â”‚
â”‚  â”‚  â”‚ Policy Network  â”‚    â”‚ Value Network   â”‚           â”‚  â”‚
â”‚  â”‚  â”‚ Ï€(a|s)          â”‚    â”‚ V(s)            â”‚           â”‚  â”‚
â”‚  â”‚  â”‚ SaÃ­da: prob. de â”‚    â”‚ SaÃ­da: estimativaâ”‚           â”‚  â”‚
â”‚  â”‚  â”‚ aÃ§Ã£o            â”‚    â”‚ de vitÃ³ria      â”‚           â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### O Papel da Policy Network

**A Policy Network atua na fase de ExpansÃ£o**.

No MCTS tradicional, todas as aÃ§Ãµes nÃ£o exploradas na expansÃ£o sÃ£o consideradas igualmente importantes. Mas a Policy Network fornece **probabilidades a priori (prior probability)**:

$$P(s, a) = \pi_\theta(a|s)$$

Isso faz o MCTS explorar prioritariamente as jogadas que "parecem melhores", aumentando significativamente a eficiÃªncia da busca.

Por exemplo, em uma posiÃ§Ã£o:
- "Tengen" pode ter apenas 0.01% de probabilidade
- "Joseki de canto" pode ter 15% de probabilidade
- "Ponto grande" pode ter 10% de probabilidade

O MCTS explorarÃ¡ prioritariamente as jogadas de alta probabilidade, em vez de desperdiÃ§ar tempo em escolhas claramente ruins.

### O Papel da Value Network

**A Value Network atua na fase de AvaliaÃ§Ã£o**.

O MCTS tradicional precisa completar toda a partida para obter uma avaliaÃ§Ã£o. Mas a Value Network pode avaliar diretamente a taxa de vitÃ³ria de qualquer posiÃ§Ã£o:

$$v(s) = V_\phi(s)$$

Ã‰ como pedir a um mestre para avaliar a posiÃ§Ã£o, em vez de deixar dois iniciantes jogarem toda a partida para ver o resultado.

A versÃ£o original do AlphaGo usa uma mistura de Value Network e Rollout:

$$V(s_L) = (1 - \lambda) \cdot v_\theta(s_L) + \lambda \cdot z_L$$

Onde:
- $v_\theta(s_L)$: AvaliaÃ§Ã£o da Value Network
- $z_L$: Resultado do Rollout
- $\lambda$: Coeficiente de mistura (AlphaGo usa $\lambda = 0.5$)

### VisualizaÃ§Ã£o da Ãrvore de Busca

Vamos visualizar uma Ã¡rvore de busca MCTS:

<MCTSTree width={700} height={450} showPUCT={true} interactive={true} />

Nesta visualizaÃ§Ã£o, vocÃª pode ver:
- O tamanho do nÃ³ reflete o nÃºmero de visitas
- O caminho azul Ã© o melhor caminho selecionado pelo MCTS
- Cada nÃ³ mostra o nÃºmero de visitas N e o valor mÃ©dio Q

---

## Detalhes do Processo de Busca

### Fluxo Completo

Vamos acompanhar uma simulaÃ§Ã£o completa do MCTS:

```
Algoritmo: SimulaÃ§Ã£o Ãšnica MCTS do AlphaGo

Entrada: nÃ³ raiz s_root, Policy Network Ï€, Value Network V

1. Selection (SeleÃ§Ã£o)
   s = s_root
   caminho = []

   while s nÃ£o Ã© nÃ³ folha:
       # Usar fÃ³rmula PUCT para selecionar aÃ§Ã£o
       a* = argmax_a [Q(s,a) + U(s,a)]

       onde U(s,a) = c_puct Â· P(s,a) Â· âˆšN(s) / (1 + N(s,a))

       caminho.append((s, a*))
       s = estado apÃ³s executar aÃ§Ã£o a*

2. Expansion (ExpansÃ£o)
   Se s nÃ£o Ã© estado terminal:
       # Usar Policy Network para calcular probabilidades a priori
       P(s, Â·) = Ï€(Â·|s)

       # Criar nÃ³s filhos para todas as aÃ§Ãµes legais
       for a in aÃ§Ãµes_legais:
           criar nÃ³ filho (s, a)
           definir P(s,a), N(s,a)=0, W(s,a)=0

3. Evaluation (AvaliaÃ§Ã£o)
   # Misturar Value Network e Rollout
   v = V(s)                          # AvaliaÃ§Ã£o da Value Network
   z = rollout(s)                    # Resultado do Rollout
   valor = (1-Î»)Â·v + Î»Â·z             # Mistura

   # AlphaGo Zero simplifica para usar apenas Value Network
   # valor = V(s)

4. Backpropagation (RetropropagaÃ§Ã£o)
   for (s', a') in reverso(caminho):
       N(s', a') += 1
       W(s', a') += valor
       Q(s', a') = W(s', a') / N(s', a')
       valor = -valor                 # Alternar perspectiva
```

### Detalhes da Fase de SeleÃ§Ã£o

A fase de seleÃ§Ã£o usa a **fÃ³rmula PUCT** (que serÃ¡ discutida em detalhes no prÃ³ximo artigo):

$$a^* = \arg\max_a \left[ Q(s,a) + c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,a)} \right]$$

Esta fÃ³rmula equilibra:
- **Q(s,a)**: Valor mÃ©dio conhecido (aproveitamento)
- **U(s,a)**: BÃ´nus de exploraÃ§Ã£o, combinando probabilidade a priori e contagem de visitas (exploraÃ§Ã£o)

### Detalhes da Fase de ExpansÃ£o

Ao chegar a um nÃ³ folha, usar a Policy Network para inicializar novos nÃ³s:

```python
def expand(state, policy_network):
    # Obter probabilidades para todas as aÃ§Ãµes legais
    action_probs = policy_network(state)

    # Filtrar aÃ§Ãµes ilegais e renormalizar
    legal_actions = get_legal_actions(state)
    legal_probs = action_probs[legal_actions]
    legal_probs = legal_probs / legal_probs.sum()

    # Criar nÃ³s filhos
    for action, prob in zip(legal_actions, legal_probs):
        child = create_node(
            state=apply_action(state, action),
            prior=prob,
            visit_count=0,
            value_sum=0
        )
        add_child(current_node, action, child)
```

### Detalhes da Fase de AvaliaÃ§Ã£o

A versÃ£o original do AlphaGo usa duas avaliaÃ§Ãµes misturadas:

**AvaliaÃ§Ã£o da Value Network**:
- Entrada direta da posiÃ§Ã£o, saÃ­da da taxa de vitÃ³ria
- CÃ¡lculo rÃ¡pido (uma inferÃªncia de rede neural)
- Fornece avaliaÃ§Ã£o de perspectiva global

**AvaliaÃ§Ã£o do Rollout**:
- Usar polÃ­tica rÃ¡pida (Fast Rollout Policy) para completar a partida
- CÃ¡lculo mais lento mas fornece resultado completo da partida
- Pode descobrir algumas tÃ¡ticas que a rede neural pode ignorar

```python
def evaluate(state, value_network, rollout_policy, lambda_mix=0.5):
    # AvaliaÃ§Ã£o da Value Network
    v = value_network(state)

    # AvaliaÃ§Ã£o do Rollout
    current = state
    while not is_terminal(current):
        action = rollout_policy(current)
        current = apply_action(current, action)
    z = get_result(current)

    # Mistura
    return (1 - lambda_mix) * v + lambda_mix * z
```

O AlphaGo Zero removeu o Rollout, usando apenas a Value Network. Isso simplificou o sistema e aumentou a eficiÃªncia.

### Detalhes da RetropropagaÃ§Ã£o

Propagar os resultados da avaliaÃ§Ã£o de volta pelo caminho, atualizando estatÃ­sticas:

```python
def backpropagate(path, value):
    for state, action in reversed(path):
        # Atualizar contagem de visitas
        state.visit_count[action] += 1
        # Atualizar soma de valores
        state.value_sum[action] += value
        # Atualizar valor mÃ©dio
        state.Q[action] = state.value_sum[action] / state.visit_count[action]
        # Alternar perspectiva (o benefÃ­cio do oponente Ã© minha perda)
        value = -value
```

Note o passo `value = -value`: Go Ã© um jogo de soma zero, a vitÃ³ria de um lado Ã© a derrota do outro.

---

## AlocaÃ§Ã£o de Recursos Computacionais

### NÃºmero de SimulaÃ§Ãµes

O AlphaGo executa um grande nÃºmero de simulaÃ§Ãµes MCTS em cada jogada:

| VersÃ£o | SimulaÃ§Ãµes por jogada | Tempo de reflexÃ£o |
|--------|----------------------|-------------------|
| AlphaGo Fan | ~100.000 | Minutos |
| AlphaGo Lee | ~100.000 | Minutos |
| AlphaGo Zero (treinamento) | 1.600 | Segundos |
| AlphaGo Zero (competiÃ§Ã£o) | ~1.600 | Segundos |

O AlphaGo Zero alcanÃ§a uma forÃ§a maior com menos simulaÃ§Ãµes, resultado da melhoria da qualidade da rede neural.

### EstratÃ©gia de AlocaÃ§Ã£o de Tempo

Diferentes posiÃ§Ãµes podem exigir diferentes tempos de reflexÃ£o:

```python
def allocate_time(game_state, remaining_time):
    # AlocaÃ§Ã£o bÃ¡sica
    num_moves_remaining = estimate_remaining_moves(game_state)
    base_time = remaining_time / num_moves_remaining

    # Fatores de ajuste
    complexity = estimate_complexity(game_state)
    importance = estimate_importance(game_state)

    # Dar mais tempo para posiÃ§Ãµes complexas ou importantes
    allocated_time = base_time * complexity * importance

    # Garantir que nÃ£o exceda o tempo
    return min(allocated_time, remaining_time * 0.3)
```

Em competiÃ§Ãµes reais, o AlphaGo investe mais tempo de reflexÃ£o em posiÃ§Ãµes crÃ­ticas (como momentos prÃ³ximos Ã  fronteira entre vitÃ³ria e derrota).

### Busca Paralela

O MCTS Ã© naturalmente adequado para paralelizaÃ§Ã£o:

**TÃ©cnica de Perda Virtual (Virtual Loss)**:

```
Quando um thread estÃ¡ explorando o caminho P:
1. Temporariamente assumir que este caminho jÃ¡ perdeu (virtual loss)
2. Outros threads tenderÃ£o a explorar outros caminhos
3. Quando o resultado retornar, atualizar estatÃ­sticas reais e remover virtual loss
```

Isso garante que mÃºltiplos threads nÃ£o explorem repetidamente o mesmo caminho.

```python
def parallel_mcts_simulation(root, num_threads=8):
    virtual_losses = {}

    def simulate(thread_id):
        # Fase de seleÃ§Ã£o (com virtual loss)
        path = []
        node = root
        while not node.is_leaf():
            action = select_with_virtual_loss(node, virtual_losses)
            add_virtual_loss(node, action, virtual_losses)
            path.append((node, action))
            node = node.children[action]

        # ExpansÃ£o e avaliaÃ§Ã£o
        value = expand_and_evaluate(node)

        # RetropropagaÃ§Ã£o e remoÃ§Ã£o de virtual losses
        backpropagate(path, value)
        remove_virtual_losses(path, virtual_losses)

    # Executar mÃºltiplas simulaÃ§Ãµes em paralelo
    threads = [Thread(target=simulate, args=(i,)) for i in range(num_threads)]
    for t in threads:
        t.start()
    for t in threads:
        t.join()
```

### Processamento em Lote na GPU

A inferÃªncia de rede neural Ã© mais eficiente na GPU com processamento em lote. O AlphaGo usa **avaliaÃ§Ã£o em lote**:

```
Sem lote:
  SimulaÃ§Ã£o 1 â†’ AvaliaÃ§Ã£o 1 â†’ SimulaÃ§Ã£o 2 â†’ AvaliaÃ§Ã£o 2 â†’ ...
  Baixa utilizaÃ§Ã£o da GPU

Com lote:
  Coletar 32 posiÃ§Ãµes para avaliar
  â†’ Enviar todas de uma vez para a GPU avaliar
  â†’ Retornar 32 resultados
  Alta utilizaÃ§Ã£o da GPU
```

Isso requer agendamento mais complexo, mas aumenta significativamente o throughput.

---

## Temperatura e SeleÃ§Ã£o Final

### Temperatura Durante o Treinamento

Durante o treinamento de auto-jogo, o AlphaGo usa **temperatura** para controlar a exploraÃ§Ã£o:

$$\pi(a) = \frac{N(s,a)^{1/\tau}}{\sum_{a'} N(s,a')^{1/\tau}}$$

Onde $\tau$ Ã© o parÃ¢metro de temperatura.

- $\tau = 1$: Probabilidade proporcional Ã  contagem de visitas (manter diversidade)
- $\tau \to 0$: Selecionar a aÃ§Ã£o com mais visitas (seleÃ§Ã£o determinÃ­stica)

EstratÃ©gia do AlphaGo Zero:
- **Primeiras 30 jogadas**: $\tau = 1$, manter diversidade de abertura
- **Depois**: $\tau \to 0$, selecionar a melhor jogada

### SeleÃ§Ã£o Durante a CompetiÃ§Ã£o

Em competiÃ§Ãµes reais, a seleÃ§Ã£o Ã© geralmente determinÃ­stica:

```python
def select_move(root, temperature=0):
    if temperature == 0:
        # Selecionar aÃ§Ã£o com mais visitas
        return argmax(root.visit_counts)
    else:
        # Amostrar pela distribuiÃ§Ã£o de probabilidade ajustada pela temperatura
        probs = root.visit_counts ** (1 / temperature)
        probs = probs / probs.sum()
        return np.random.choice(actions, p=probs)
```

### Considerando Taxa de VitÃ³ria

Ã€s vezes tambÃ©m se considera o valor mÃ©dio em vez de apenas a contagem de visitas:

```python
def select_move_with_value(root, temperature=0):
    # Misturar contagem de visitas e valor
    scores = root.visit_counts * (1 + root.Q_values)
    scores = scores / scores.sum()

    if temperature == 0:
        return argmax(scores)
    else:
        probs = scores ** (1 / temperature)
        probs = probs / probs.sum()
        return np.random.choice(actions, p=probs)
```

---

## ComparaÃ§Ã£o com Rede Neural Pura

### Por que Precisamos de Busca?

Uma pergunta natural Ã©: **Se a rede neural jÃ¡ pode prever boas jogadas, por que ainda precisamos de busca?**

A resposta Ã©: **A busca pode corrigir erros da rede neural e descobrir jogadas melhores**.

| MÃ©todo | Vantagens | Desvantagens |
|--------|-----------|--------------|
| Rede neural pura | RÃ¡pida, intuitiva | Pode ter pontos cegos |
| MCTS puro | Pode analisar profundamente | Lento, precisa de avaliaÃ§Ã£o |
| Rede neural + MCTS | Combina vantagens de ambos | Alto custo computacional |

### EvidÃªncias Experimentais

Experimentos da DeepMind mostram:

```
Policy Network pura: ~3000 Elo
Policy + pouco MCTS: ~3500 Elo
Policy + Value + MCTS: ~4500 Elo
```

A busca proporciona uma melhoria significativa na forÃ§a do jogo.

### O Valor da Busca

A busca Ã© particularmente valiosa nas seguintes situaÃ§Ãµes:

1. **CÃ¡lculo tÃ¡tico**: Calcular ataques e capturas complexos
2. **CorreÃ§Ã£o de viÃ©s**: Corrigir erros sistemÃ¡ticos da rede neural
3. **Lidar com posiÃ§Ãµes raras**: A rede neural pode nÃ£o ter visto durante o treinamento
4. **Verificar intuiÃ§Ã£o**: Confirmar que uma jogada que "parece boa" realmente Ã© boa

---

## DiferenÃ§as Entre VersÃµes do AlphaGo

### AlphaGo Fan/Lee

```
Arquitetura:
- SL Policy Network (aprendizado supervisionado)
- RL Policy Network (aprendizado por reforÃ§o)
- Value Network
- Fast Rollout Policy

Durante a busca:
- Usa probabilidades a priori da SL Policy Network
- Mistura avaliaÃ§Ã£o da Value Network e Rollout
```

### AlphaGo Master

```
Arquitetura:
- Rede neural maior
- Mais dados de treinamento
- CaracterÃ­sticas melhoradas

Durante a busca:
- Similar ao AlphaGo Lee
- Rede mais forte = menos necessidade de busca
```

### AlphaGo Zero

```
Arquitetura:
- Ãšnica ResNet de cabeÃ§a dupla
- Treinamento do zero
- Sem Rollout

Durante a busca:
- CabeÃ§a de polÃ­tica fornece probabilidades a priori
- CabeÃ§a de valor avalia diretamente
- Mais simples e mais forte
```

### Resumo da EvoluÃ§Ã£o

```
AlphaGo Fan (2015)
    â”‚
    â”‚ + rede maior, mais treinamento
    â–¼
AlphaGo Lee (2016)
    â”‚
    â”‚ + mais auto-jogo
    â–¼
AlphaGo Master (2017)
    â”‚
    â”‚ + remoÃ§Ã£o de dados humanos, rede unificada, remoÃ§Ã£o de Rollout
    â–¼
AlphaGo Zero (2017)
    â”‚
    â”‚ + generalizaÃ§Ã£o para outros jogos
    â–¼
AlphaZero (2018)
```

---

## ConsideraÃ§Ãµes de ImplementaÃ§Ã£o

### Gerenciamento de MemÃ³ria

A Ã¡rvore MCTS pode ficar muito grande:

```
Suponha:
- MÃ©dia de 200 aÃ§Ãµes legais por jogada
- Profundidade de busca 10
- ExpansÃ£o completa: 200^10 â‰ˆ 10^23 nÃ³s (impossÃ­vel)

Abordagem real:
- Expandir apenas nÃ³s visitados
- Limpar periodicamente nÃ³s raramente visitados
- Reutilizar a Ã¡rvore de busca da jogada anterior
```

### ReutilizaÃ§Ã£o da Ãrvore

Quando o oponente joga, parte da Ã¡rvore de busca pode ser reutilizada:

```python
def reuse_tree(root, opponent_move):
    if opponent_move in root.children:
        new_root = root.children[opponent_move]
        # Limpar outros ramos desnecessÃ¡rios
        for action in root.children:
            if action != opponent_move:
                delete_subtree(root.children[action])
        return new_root
    else:
        # Oponente jogou uma jogada inesperada, precisa recomeÃ§ar
        return create_new_root()
```

### Cache da Rede Neural

A mesma posiÃ§Ã£o pode ser avaliada mÃºltiplas vezes, usar cache evita cÃ¡lculos repetidos:

```python
class NeuralNetworkCache:
    def __init__(self, max_size=100000):
        self.cache = LRUCache(max_size)

    def evaluate(self, state, network):
        state_hash = hash(state)
        if state_hash in self.cache:
            return self.cache[state_hash]
        else:
            result = network(state)
            self.cache[state_hash] = result
            return result
```

### UtilizaÃ§Ã£o de Simetria

O tabuleiro de Go tem 8 simetrias que podem ser usadas para melhorar a busca:

```python
def evaluate_with_symmetry(state, network):
    # Gerar todas as transformaÃ§Ãµes simÃ©tricas
    symmetries = generate_symmetries(state)  # 8 versÃµes

    # Avaliar todas as versÃµes
    values = [network(s) for s in symmetries]

    # MÃ©dia (mais estÃ¡vel)
    return np.mean(values)
```

---

## Profundidade e Largura da Busca

### Ajuste DinÃ¢mico

O MCTS equilibra automaticamente profundidade e largura:

- **Largura**: Controlada pelas probabilidades a priori da Policy Network
- **Profundidade**: Determinada pela precisÃ£o da Value Network

Quando a rede neural Ã© boa:
- Jogadas de alta confianÃ§a sÃ£o exploradas mais profundamente
- Jogadas de baixa confianÃ§a sÃ£o rapidamente descartadas
- A busca naturalmente foca nos ramos importantes

### ComparaÃ§Ã£o com Busca Tradicional

| MÃ©todo | Controle de Profundidade | Controle de Largura |
|--------|--------------------------|---------------------|
| Minimax | Profundidade fixa | Poda Alpha-Beta |
| MCTS tradicional | Determinado por simulaÃ§Ã£o | UCB1 |
| AlphaGo MCTS | Guiado por Policy + Value | PUCT + Policy |

A busca do AlphaGo Ã© mais "inteligente" â€” ela sabe quais lugares valem a pena aprofundar e quais podem ser rapidamente ignorados.

---

## CorrespondÃªncia de AnimaÃ§Ãµes

Conceitos centrais discutidos neste artigo e nÃºmeros de animaÃ§Ã£o:

| NÃºmero | Conceito | CorrespondÃªncia FÃ­sica/MatemÃ¡tica |
|--------|----------|-----------------------------------|
| ğŸ¬ C5 | Quatro fases do MCTS | Busca em Ã¡rvore |

---

## Resumo

A combinaÃ§Ã£o de MCTS com redes neurais Ã© a inovaÃ§Ã£o central do AlphaGo. Aprendemos:

1. **MCTS tradicional**: Selection, Expansion, Simulation, Backpropagation
2. **Melhorias das redes neurais**: Policy Network guia a expansÃ£o, Value Network substitui o Rollout
3. **Processo de busca**: SeleÃ§Ã£o PUCT, avaliaÃ§Ã£o em lote, retropropagaÃ§Ã£o
4. **AlocaÃ§Ã£o de recursos**: NÃºmero de simulaÃ§Ãµes, gerenciamento de tempo, busca paralela
5. **SeleÃ§Ã£o por temperatura**: Diferentes estratÃ©gias para treinamento e competiÃ§Ã£o
6. **Detalhes de implementaÃ§Ã£o**: Gerenciamento de memÃ³ria, reutilizaÃ§Ã£o da Ã¡rvore, cache

No prÃ³ximo artigo, exploraremos os detalhes matemÃ¡ticos da fÃ³rmula PUCT.

---

## Leitura Adicional

- **PrÃ³ximo artigo**: [FÃ³rmula PUCT em Detalhes](../puct-formula) â€” O princÃ­pio matemÃ¡tico da seleÃ§Ã£o MCTS
- **Artigo anterior**: [Auto-jogo](../self-play) â€” O mecanismo e efeito do auto-jogo
- **Relacionado**: [Detalhes da Policy Network](../policy-network) â€” Arquitetura da rede de polÃ­ticas

---

## ReferÃªncias

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
3. Coulom, R. (2006). "Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search." *Computers and Games*.
4. Kocsis, L., & SzepesvÃ¡ri, C. (2006). "Bandit based Monte-Carlo Planning." *ECML*.
5. Browne, C., et al. (2012). "A Survey of Monte Carlo Tree Search Methods." *IEEE TCIAIG*.
6. Rosin, C. D. (2011). "Multi-armed bandits with episode context." *Annals of Mathematics and Artificial Intelligence*.
