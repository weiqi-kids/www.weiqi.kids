---
sidebar_position: 15
title: A Combina√ß√£o de MCTS e Redes Neurais
description: Compreens√£o aprofundada de como o AlphaGo combina perfeitamente a Busca em √Årvore de Monte Carlo com redes neurais profundas
---

import { MCTSTree } from '@site/src/components/D3Charts';

# A Combina√ß√£o de MCTS e Redes Neurais

Nos artigos anteriores, apresentamos separadamente as redes neurais (Policy Network e Value Network) e os conceitos de aprendizado por refor√ßo. Agora, vamos explorar a inova√ß√£o central do AlphaGo ‚Äî **como combinar perfeitamente a Busca em √Årvore de Monte Carlo (MCTS) com redes neurais**.

Esta combina√ß√£o √© a chave do sucesso do AlphaGo: as redes neurais fornecem "intui√ß√£o", o MCTS fornece "racioc√≠nio", e ambos se complementam.

---

## Revis√£o do MCTS Tradicional

### O que √© MCTS?

**Monte Carlo Tree Search (MCTS)** √© um algoritmo de busca baseado em amostragem aleat√≥ria, particularmente adequado para IA de jogos.

A ideia central do MCTS √©: **em vez de enumerar todas as jogadas poss√≠veis, √© melhor simular aleatoriamente um grande n√∫mero de partidas e usar estat√≠sticas para estimar a qualidade de cada jogada**.

### Quatro Fases

O MCTS tradicional cont√©m quatro fases que se repetem continuamente:

```mermaid
flowchart LR
    subgraph MCTSCycle["Ciclo MCTS - Repetir N vezes"]
        Selection["Selection<br/>Sele√ß√£o"] --> Expansion["Expansion<br/>Expans√£o"]
        Expansion --> Simulation["Simulation<br/>Simula√ß√£o"]
        Simulation --> Backprop["Backprop<br/>Retroprop"]
        Backprop -.->|"Repetir"| Selection
    end
```

Vamos entender cada fase em detalhes:

### 1. Selection (Sele√ß√£o)

A partir do n√≥ raiz, descer pela √°rvore, selecionando o n√≥ filho "mais promissor" at√© chegar a um n√≥ folha.

O crit√©rio de sele√ß√£o √© a f√≥rmula **UCB1 (Upper Confidence Bound)**:

$$\text{UCB1}(s, a) = \bar{X}_{s,a} + c \sqrt{\frac{\ln N_s}{N_{s,a}}}$$

Onde:
- $\bar{X}_{s,a}$: Retorno m√©dio a partir do n√≥ $(s, a)$ (**termo de explora√ß√£o**)
- $\sqrt{\frac{\ln N_s}{N_{s,a}}}$: B√¥nus de explora√ß√£o (**termo de explora√ß√£o**)
- $N_s$: N√∫mero de visitas do n√≥ pai
- $N_{s,a}$: N√∫mero de visitas do n√≥ filho
- $c$: Constante que equilibra explora√ß√£o e aproveitamento

A sabedoria desta f√≥rmula est√° em:
- N√≥s com menos visitas recebem um b√¥nus de explora√ß√£o maior
- √Ä medida que o n√∫mero de visitas aumenta, a sele√ß√£o tende cada vez mais para n√≥s com valor real mais alto

### 2. Expansion (Expans√£o)

Ao chegar a um n√≥ folha, escolher uma a√ß√£o n√£o explorada e criar um novo n√≥ filho.

```
Antes da expans√£o:              Depois da expans√£o:
     ‚óã (raiz)                       ‚óã (raiz)
    /‚îÇ\                            /‚îÇ\
   ‚óã ‚óã ‚óã                          ‚óã ‚óã ‚óã
  /‚îÇ              ‚Üí              /‚îÇ
 ‚óã ‚óã                            ‚óã ‚óã
   ‚Üë                               \
   n√≥ folha                         ‚óè (novo n√≥)
```

### 3. Simulation (Simula√ß√£o/Rollout)

A partir do novo n√≥, usar alguma estrat√©gia (geralmente aleat√≥ria ou heur√≠stica simples) para completar rapidamente a partida e obter o resultado.

Esta √© a origem do nome "Monte Carlo" ‚Äî **usar simula√ß√£o aleat√≥ria para estimar resultados**.

A estrat√©gia de rollout do MCTS tradicional pode ser:
- **Puramente aleat√≥ria**: Escolher uniformemente entre as jogadas legais
- **Heur√≠stica leve**: Usar regras simples para filtrar jogadas claramente ruins

### 4. Backpropagation (Retropropaga√ß√£o)

Propagar o resultado da simula√ß√£o (vit√≥ria/derrota) de volta pelo caminho, atualizando as estat√≠sticas de cada n√≥:

```
Conte√∫do da atualiza√ß√£o:
- Contagem de visitas: N(s, a) ‚Üê N(s, a) + 1
- Valor acumulado: W(s, a) ‚Üê W(s, a) + z
- Valor m√©dio: Q(s, a) = W(s, a) / N(s, a)
```

Onde $z$ √© o resultado da simula√ß√£o (+1 ou -1).

### Limita√ß√µes do MCTS Tradicional

O MCTS tradicional tem desempenho limitado no Go, os principais problemas s√£o:

1. **Baixa qualidade do Rollout**: A simula√ß√£o aleat√≥ria frequentemente produz partidas irracionais
2. **Necessidade de muitas simula√ß√µes**: Cada jogada pode exigir dezenas de milhares de simula√ß√µes
3. **Avalia√ß√£o imprecisa**: Dependendo apenas de estat√≠sticas de vit√≥ria/derrota, a efici√™ncia da utiliza√ß√£o de informa√ß√£o √© baixa
4. **Incapacidade de aproveitar padr√µes**: Recome√ßa a busca toda vez, n√£o acumula experi√™ncia

Esses problemas foram elegantemente resolvidos no AlphaGo pelas redes neurais.

---

## Como as Redes Neurais Melhoram o MCTS

### Arquitetura Geral

O AlphaGo integra duas redes neurais no MCTS:

```mermaid
flowchart TB
    subgraph AlphaGoSearch["Arquitetura de Busca AlphaGo"]
        subgraph MCTS["MCTS"]
            Sel["Selection"] --> PUCT["PUCT F√≥rm."]
            Exp["Expansion"] --> PolNet["Policy Net"]
            Eval["Evaluation"] --> ValNet["Value Net<br/>+ Rollout"]
            Back["Backup"] --> Stats["Atualiz Stats"]
        end

        subgraph NN["Neural Networks"]
            PolicyNetwork["Policy Network<br/>œÄ(a|s)<br/>Sa√≠da: prob. de a√ß√£o"]
            ValueNetwork["Value Network<br/>V(s)<br/>Sa√≠da: estimativa de vit√≥ria"]
        end

        PolicyNetwork -.-> PolNet
        ValueNetwork -.-> ValNet
        PolicyNetwork -.-> PUCT
    end
```

### O Papel da Policy Network

**A Policy Network atua na fase de Expans√£o**.

No MCTS tradicional, todas as a√ß√µes n√£o exploradas na expans√£o s√£o consideradas igualmente importantes. Mas a Policy Network fornece **probabilidades a priori (prior probability)**:

$$P(s, a) = \pi_\theta(a|s)$$

Isso faz o MCTS explorar prioritariamente as jogadas que "parecem melhores", aumentando significativamente a efici√™ncia da busca.

Por exemplo, em uma posi√ß√£o:
- "Tengen" pode ter apenas 0.01% de probabilidade
- "Joseki de canto" pode ter 15% de probabilidade
- "Ponto grande" pode ter 10% de probabilidade

O MCTS explorar√° prioritariamente as jogadas de alta probabilidade, em vez de desperdi√ßar tempo em escolhas claramente ruins.

### O Papel da Value Network

**A Value Network atua na fase de Avalia√ß√£o**.

O MCTS tradicional precisa completar toda a partida para obter uma avalia√ß√£o. Mas a Value Network pode avaliar diretamente a taxa de vit√≥ria de qualquer posi√ß√£o:

$$v(s) = V_\phi(s)$$

√â como pedir a um mestre para avaliar a posi√ß√£o, em vez de deixar dois iniciantes jogarem toda a partida para ver o resultado.

A vers√£o original do AlphaGo usa uma mistura de Value Network e Rollout:

$$V(s_L) = (1 - \lambda) \cdot v_\theta(s_L) + \lambda \cdot z_L$$

Onde:
- $v_\theta(s_L)$: Avalia√ß√£o da Value Network
- $z_L$: Resultado do Rollout
- $\lambda$: Coeficiente de mistura (AlphaGo usa $\lambda = 0.5$)

### Visualiza√ß√£o da √Årvore de Busca

Vamos visualizar uma √°rvore de busca MCTS:

<MCTSTree width={700} height={450} showPUCT={true} interactive={true} />

Nesta visualiza√ß√£o, voc√™ pode ver:
- O tamanho do n√≥ reflete o n√∫mero de visitas
- O caminho azul √© o melhor caminho selecionado pelo MCTS
- Cada n√≥ mostra o n√∫mero de visitas N e o valor m√©dio Q

---

## Detalhes do Processo de Busca

### Fluxo Completo

Vamos acompanhar uma simula√ß√£o completa do MCTS:

```
Algoritmo: Simula√ß√£o √önica MCTS do AlphaGo

Entrada: n√≥ raiz s_root, Policy Network œÄ, Value Network V

1. Selection (Sele√ß√£o)
   s = s_root
   caminho = []

   while s n√£o √© n√≥ folha:
       # Usar f√≥rmula PUCT para selecionar a√ß√£o
       a* = argmax_a [Q(s,a) + U(s,a)]

       onde U(s,a) = c_puct ¬∑ P(s,a) ¬∑ ‚àöN(s) / (1 + N(s,a))

       caminho.append((s, a*))
       s = estado ap√≥s executar a√ß√£o a*

2. Expansion (Expans√£o)
   Se s n√£o √© estado terminal:
       # Usar Policy Network para calcular probabilidades a priori
       P(s, ¬∑) = œÄ(¬∑|s)

       # Criar n√≥s filhos para todas as a√ß√µes legais
       for a in a√ß√µes_legais:
           criar n√≥ filho (s, a)
           definir P(s,a), N(s,a)=0, W(s,a)=0

3. Evaluation (Avalia√ß√£o)
   # Misturar Value Network e Rollout
   v = V(s)                          # Avalia√ß√£o da Value Network
   z = rollout(s)                    # Resultado do Rollout
   valor = (1-Œª)¬∑v + Œª¬∑z             # Mistura

   # AlphaGo Zero simplifica para usar apenas Value Network
   # valor = V(s)

4. Backpropagation (Retropropaga√ß√£o)
   for (s', a') in reverso(caminho):
       N(s', a') += 1
       W(s', a') += valor
       Q(s', a') = W(s', a') / N(s', a')
       valor = -valor                 # Alternar perspectiva
```

### Detalhes da Fase de Sele√ß√£o

A fase de sele√ß√£o usa a **f√≥rmula PUCT** (que ser√° discutida em detalhes no pr√≥ximo artigo):

$$a^* = \arg\max_a \left[ Q(s,a) + c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,a)} \right]$$

Esta f√≥rmula equilibra:
- **Q(s,a)**: Valor m√©dio conhecido (aproveitamento)
- **U(s,a)**: B√¥nus de explora√ß√£o, combinando probabilidade a priori e contagem de visitas (explora√ß√£o)

### Detalhes da Fase de Expans√£o

Ao chegar a um n√≥ folha, usar a Policy Network para inicializar novos n√≥s:

```python
def expand(state, policy_network):
    # Obter probabilidades para todas as a√ß√µes legais
    action_probs = policy_network(state)

    # Filtrar a√ß√µes ilegais e renormalizar
    legal_actions = get_legal_actions(state)
    legal_probs = action_probs[legal_actions]
    legal_probs = legal_probs / legal_probs.sum()

    # Criar n√≥s filhos
    for action, prob in zip(legal_actions, legal_probs):
        child = create_node(
            state=apply_action(state, action),
            prior=prob,
            visit_count=0,
            value_sum=0
        )
        add_child(current_node, action, child)
```

### Detalhes da Fase de Avalia√ß√£o

A vers√£o original do AlphaGo usa duas avalia√ß√µes misturadas:

**Avalia√ß√£o da Value Network**:
- Entrada direta da posi√ß√£o, sa√≠da da taxa de vit√≥ria
- C√°lculo r√°pido (uma infer√™ncia de rede neural)
- Fornece avalia√ß√£o de perspectiva global

**Avalia√ß√£o do Rollout**:
- Usar pol√≠tica r√°pida (Fast Rollout Policy) para completar a partida
- C√°lculo mais lento mas fornece resultado completo da partida
- Pode descobrir algumas t√°ticas que a rede neural pode ignorar

```python
def evaluate(state, value_network, rollout_policy, lambda_mix=0.5):
    # Avalia√ß√£o da Value Network
    v = value_network(state)

    # Avalia√ß√£o do Rollout
    current = state
    while not is_terminal(current):
        action = rollout_policy(current)
        current = apply_action(current, action)
    z = get_result(current)

    # Mistura
    return (1 - lambda_mix) * v + lambda_mix * z
```

O AlphaGo Zero removeu o Rollout, usando apenas a Value Network. Isso simplificou o sistema e aumentou a efici√™ncia.

### Detalhes da Retropropaga√ß√£o

Propagar os resultados da avalia√ß√£o de volta pelo caminho, atualizando estat√≠sticas:

```python
def backpropagate(path, value):
    for state, action in reversed(path):
        # Atualizar contagem de visitas
        state.visit_count[action] += 1
        # Atualizar soma de valores
        state.value_sum[action] += value
        # Atualizar valor m√©dio
        state.Q[action] = state.value_sum[action] / state.visit_count[action]
        # Alternar perspectiva (o benef√≠cio do oponente √© minha perda)
        value = -value
```

Note o passo `value = -value`: Go √© um jogo de soma zero, a vit√≥ria de um lado √© a derrota do outro.

---

## Aloca√ß√£o de Recursos Computacionais

### N√∫mero de Simula√ß√µes

O AlphaGo executa um grande n√∫mero de simula√ß√µes MCTS em cada jogada:

| Vers√£o | Simula√ß√µes por jogada | Tempo de reflex√£o |
|--------|----------------------|-------------------|
| AlphaGo Fan | ~100.000 | Minutos |
| AlphaGo Lee | ~100.000 | Minutos |
| AlphaGo Zero (treinamento) | 1.600 | Segundos |
| AlphaGo Zero (competi√ß√£o) | ~1.600 | Segundos |

O AlphaGo Zero alcan√ßa uma for√ßa maior com menos simula√ß√µes, resultado da melhoria da qualidade da rede neural.

### Estrat√©gia de Aloca√ß√£o de Tempo

Diferentes posi√ß√µes podem exigir diferentes tempos de reflex√£o:

```python
def allocate_time(game_state, remaining_time):
    # Aloca√ß√£o b√°sica
    num_moves_remaining = estimate_remaining_moves(game_state)
    base_time = remaining_time / num_moves_remaining

    # Fatores de ajuste
    complexity = estimate_complexity(game_state)
    importance = estimate_importance(game_state)

    # Dar mais tempo para posi√ß√µes complexas ou importantes
    allocated_time = base_time * complexity * importance

    # Garantir que n√£o exceda o tempo
    return min(allocated_time, remaining_time * 0.3)
```

Em competi√ß√µes reais, o AlphaGo investe mais tempo de reflex√£o em posi√ß√µes cr√≠ticas (como momentos pr√≥ximos √† fronteira entre vit√≥ria e derrota).

### Busca Paralela

O MCTS √© naturalmente adequado para paraleliza√ß√£o:

**T√©cnica de Perda Virtual (Virtual Loss)**:

```
Quando um thread est√° explorando o caminho P:
1. Temporariamente assumir que este caminho j√° perdeu (virtual loss)
2. Outros threads tender√£o a explorar outros caminhos
3. Quando o resultado retornar, atualizar estat√≠sticas reais e remover virtual loss
```

Isso garante que m√∫ltiplos threads n√£o explorem repetidamente o mesmo caminho.

```python
def parallel_mcts_simulation(root, num_threads=8):
    virtual_losses = {}

    def simulate(thread_id):
        # Fase de sele√ß√£o (com virtual loss)
        path = []
        node = root
        while not node.is_leaf():
            action = select_with_virtual_loss(node, virtual_losses)
            add_virtual_loss(node, action, virtual_losses)
            path.append((node, action))
            node = node.children[action]

        # Expans√£o e avalia√ß√£o
        value = expand_and_evaluate(node)

        # Retropropaga√ß√£o e remo√ß√£o de virtual losses
        backpropagate(path, value)
        remove_virtual_losses(path, virtual_losses)

    # Executar m√∫ltiplas simula√ß√µes em paralelo
    threads = [Thread(target=simulate, args=(i,)) for i in range(num_threads)]
    for t in threads:
        t.start()
    for t in threads:
        t.join()
```

### Processamento em Lote na GPU

A infer√™ncia de rede neural √© mais eficiente na GPU com processamento em lote. O AlphaGo usa **avalia√ß√£o em lote**:

```
Sem lote:
  Simula√ß√£o 1 ‚Üí Avalia√ß√£o 1 ‚Üí Simula√ß√£o 2 ‚Üí Avalia√ß√£o 2 ‚Üí ...
  Baixa utiliza√ß√£o da GPU

Com lote:
  Coletar 32 posi√ß√µes para avaliar
  ‚Üí Enviar todas de uma vez para a GPU avaliar
  ‚Üí Retornar 32 resultados
  Alta utiliza√ß√£o da GPU
```

Isso requer agendamento mais complexo, mas aumenta significativamente o throughput.

---

## Temperatura e Sele√ß√£o Final

### Temperatura Durante o Treinamento

Durante o treinamento de auto-jogo, o AlphaGo usa **temperatura** para controlar a explora√ß√£o:

$$\pi(a) = \frac{N(s,a)^{1/\tau}}{\sum_{a'} N(s,a')^{1/\tau}}$$

Onde $\tau$ √© o par√¢metro de temperatura.

- $\tau = 1$: Probabilidade proporcional √† contagem de visitas (manter diversidade)
- $\tau \to 0$: Selecionar a a√ß√£o com mais visitas (sele√ß√£o determin√≠stica)

Estrat√©gia do AlphaGo Zero:
- **Primeiras 30 jogadas**: $\tau = 1$, manter diversidade de abertura
- **Depois**: $\tau \to 0$, selecionar a melhor jogada

### Sele√ß√£o Durante a Competi√ß√£o

Em competi√ß√µes reais, a sele√ß√£o √© geralmente determin√≠stica:

```python
def select_move(root, temperature=0):
    if temperature == 0:
        # Selecionar a√ß√£o com mais visitas
        return argmax(root.visit_counts)
    else:
        # Amostrar pela distribui√ß√£o de probabilidade ajustada pela temperatura
        probs = root.visit_counts ** (1 / temperature)
        probs = probs / probs.sum()
        return np.random.choice(actions, p=probs)
```

### Considerando Taxa de Vit√≥ria

√Äs vezes tamb√©m se considera o valor m√©dio em vez de apenas a contagem de visitas:

```python
def select_move_with_value(root, temperature=0):
    # Misturar contagem de visitas e valor
    scores = root.visit_counts * (1 + root.Q_values)
    scores = scores / scores.sum()

    if temperature == 0:
        return argmax(scores)
    else:
        probs = scores ** (1 / temperature)
        probs = probs / probs.sum()
        return np.random.choice(actions, p=probs)
```

---

## Compara√ß√£o com Rede Neural Pura

### Por que Precisamos de Busca?

Uma pergunta natural √©: **Se a rede neural j√° pode prever boas jogadas, por que ainda precisamos de busca?**

A resposta √©: **A busca pode corrigir erros da rede neural e descobrir jogadas melhores**.

| M√©todo | Vantagens | Desvantagens |
|--------|-----------|--------------|
| Rede neural pura | R√°pida, intuitiva | Pode ter pontos cegos |
| MCTS puro | Pode analisar profundamente | Lento, precisa de avalia√ß√£o |
| Rede neural + MCTS | Combina vantagens de ambos | Alto custo computacional |

### Evid√™ncias Experimentais

Experimentos da DeepMind mostram:

```
Policy Network pura: ~3000 Elo
Policy + pouco MCTS: ~3500 Elo
Policy + Value + MCTS: ~4500 Elo
```

A busca proporciona uma melhoria significativa na for√ßa do jogo.

### O Valor da Busca

A busca √© particularmente valiosa nas seguintes situa√ß√µes:

1. **C√°lculo t√°tico**: Calcular ataques e capturas complexos
2. **Corre√ß√£o de vi√©s**: Corrigir erros sistem√°ticos da rede neural
3. **Lidar com posi√ß√µes raras**: A rede neural pode n√£o ter visto durante o treinamento
4. **Verificar intui√ß√£o**: Confirmar que uma jogada que "parece boa" realmente √© boa

---

## Diferen√ßas Entre Vers√µes do AlphaGo

### AlphaGo Fan/Lee

```
Arquitetura:
- SL Policy Network (aprendizado supervisionado)
- RL Policy Network (aprendizado por refor√ßo)
- Value Network
- Fast Rollout Policy

Durante a busca:
- Usa probabilidades a priori da SL Policy Network
- Mistura avalia√ß√£o da Value Network e Rollout
```

### AlphaGo Master

```
Arquitetura:
- Rede neural maior
- Mais dados de treinamento
- Caracter√≠sticas melhoradas

Durante a busca:
- Similar ao AlphaGo Lee
- Rede mais forte = menos necessidade de busca
```

### AlphaGo Zero

```
Arquitetura:
- √önica ResNet de cabe√ßa dupla
- Treinamento do zero
- Sem Rollout

Durante a busca:
- Cabe√ßa de pol√≠tica fornece probabilidades a priori
- Cabe√ßa de valor avalia diretamente
- Mais simples e mais forte
```

### Resumo da Evolu√ß√£o

```
AlphaGo Fan (2015)
    ‚îÇ
    ‚îÇ + rede maior, mais treinamento
    ‚ñº
AlphaGo Lee (2016)
    ‚îÇ
    ‚îÇ + mais auto-jogo
    ‚ñº
AlphaGo Master (2017)
    ‚îÇ
    ‚îÇ + remo√ß√£o de dados humanos, rede unificada, remo√ß√£o de Rollout
    ‚ñº
AlphaGo Zero (2017)
    ‚îÇ
    ‚îÇ + generaliza√ß√£o para outros jogos
    ‚ñº
AlphaZero (2018)
```

---

## Considera√ß√µes de Implementa√ß√£o

### Gerenciamento de Mem√≥ria

A √°rvore MCTS pode ficar muito grande:

```
Suponha:
- M√©dia de 200 a√ß√µes legais por jogada
- Profundidade de busca 10
- Expans√£o completa: 200^10 ‚âà 10^23 n√≥s (imposs√≠vel)

Abordagem real:
- Expandir apenas n√≥s visitados
- Limpar periodicamente n√≥s raramente visitados
- Reutilizar a √°rvore de busca da jogada anterior
```

### Reutiliza√ß√£o da √Årvore

Quando o oponente joga, parte da √°rvore de busca pode ser reutilizada:

```python
def reuse_tree(root, opponent_move):
    if opponent_move in root.children:
        new_root = root.children[opponent_move]
        # Limpar outros ramos desnecess√°rios
        for action in root.children:
            if action != opponent_move:
                delete_subtree(root.children[action])
        return new_root
    else:
        # Oponente jogou uma jogada inesperada, precisa recome√ßar
        return create_new_root()
```

### Cache da Rede Neural

A mesma posi√ß√£o pode ser avaliada m√∫ltiplas vezes, usar cache evita c√°lculos repetidos:

```python
class NeuralNetworkCache:
    def __init__(self, max_size=100000):
        self.cache = LRUCache(max_size)

    def evaluate(self, state, network):
        state_hash = hash(state)
        if state_hash in self.cache:
            return self.cache[state_hash]
        else:
            result = network(state)
            self.cache[state_hash] = result
            return result
```

### Utiliza√ß√£o de Simetria

O tabuleiro de Go tem 8 simetrias que podem ser usadas para melhorar a busca:

```python
def evaluate_with_symmetry(state, network):
    # Gerar todas as transforma√ß√µes sim√©tricas
    symmetries = generate_symmetries(state)  # 8 vers√µes

    # Avaliar todas as vers√µes
    values = [network(s) for s in symmetries]

    # M√©dia (mais est√°vel)
    return np.mean(values)
```

---

## Profundidade e Largura da Busca

### Ajuste Din√¢mico

O MCTS equilibra automaticamente profundidade e largura:

- **Largura**: Controlada pelas probabilidades a priori da Policy Network
- **Profundidade**: Determinada pela precis√£o da Value Network

Quando a rede neural √© boa:
- Jogadas de alta confian√ßa s√£o exploradas mais profundamente
- Jogadas de baixa confian√ßa s√£o rapidamente descartadas
- A busca naturalmente foca nos ramos importantes

### Compara√ß√£o com Busca Tradicional

| M√©todo | Controle de Profundidade | Controle de Largura |
|--------|--------------------------|---------------------|
| Minimax | Profundidade fixa | Poda Alpha-Beta |
| MCTS tradicional | Determinado por simula√ß√£o | UCB1 |
| AlphaGo MCTS | Guiado por Policy + Value | PUCT + Policy |

A busca do AlphaGo √© mais "inteligente" ‚Äî ela sabe quais lugares valem a pena aprofundar e quais podem ser rapidamente ignorados.

---

## Correspond√™ncia de Anima√ß√µes

Conceitos centrais discutidos neste artigo e n√∫meros de anima√ß√£o:

| N√∫mero | Conceito | Correspond√™ncia F√≠sica/Matem√°tica |
|--------|----------|-----------------------------------|
| üé¨ C5 | Quatro fases do MCTS | Busca em √°rvore |

---

## Resumo

A combina√ß√£o de MCTS com redes neurais √© a inova√ß√£o central do AlphaGo. Aprendemos:

1. **MCTS tradicional**: Selection, Expansion, Simulation, Backpropagation
2. **Melhorias das redes neurais**: Policy Network guia a expans√£o, Value Network substitui o Rollout
3. **Processo de busca**: Sele√ß√£o PUCT, avalia√ß√£o em lote, retropropaga√ß√£o
4. **Aloca√ß√£o de recursos**: N√∫mero de simula√ß√µes, gerenciamento de tempo, busca paralela
5. **Sele√ß√£o por temperatura**: Diferentes estrat√©gias para treinamento e competi√ß√£o
6. **Detalhes de implementa√ß√£o**: Gerenciamento de mem√≥ria, reutiliza√ß√£o da √°rvore, cache

No pr√≥ximo artigo, exploraremos os detalhes matem√°ticos da f√≥rmula PUCT.

---

## Leitura Adicional

- **Pr√≥ximo artigo**: [F√≥rmula PUCT em Detalhes](../puct-formula) ‚Äî O princ√≠pio matem√°tico da sele√ß√£o MCTS
- **Artigo anterior**: [Auto-jogo](../self-play) ‚Äî O mecanismo e efeito do auto-jogo
- **Relacionado**: [Detalhes da Policy Network](../policy-network) ‚Äî Arquitetura da rede de pol√≠ticas

---

## Refer√™ncias

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
3. Coulom, R. (2006). "Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search." *Computers and Games*.
4. Kocsis, L., & Szepesv√°ri, C. (2006). "Bandit based Monte-Carlo Planning." *ECML*.
5. Browne, C., et al. (2012). "A Survey of Monte Carlo Tree Search Methods." *IEEE TCIAIG*.
6. Rosin, C. D. (2011). "Multi-armed bandits with episode context." *Annals of Mathematics and Artificial Intelligence*.
