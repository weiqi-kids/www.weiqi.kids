---
sidebar_position: 20
title: Sistemas DistribuÃ­dos e TPU
description: AnÃ¡lise profunda da arquitetura de treinamento distribuÃ­do do AlphaGo, aceleraÃ§Ã£o por TPU e MCTS paralelo em larga escala
keywords: [sistemas distribuÃ­dos, TPU, computaÃ§Ã£o paralela, MCTS, perda virtual, deep learning, aceleraÃ§Ã£o de hardware]
---

# Sistemas DistribuÃ­dos e TPU

O sucesso do AlphaGo nÃ£o foi apenas uma vitÃ³ria de algoritmos, mas tambÃ©m uma vitÃ³ria de engenharia. Para treinar uma IA de Go que supera os humanos em tempo razoÃ¡vel, foi necessÃ¡rio um sistema distribuÃ­do cuidadosamente projetado e suporte de hardware especializado.

Este artigo analisarÃ¡ profundamente a arquitetura de sistemas por trÃ¡s do AlphaGo, incluindo fluxo de treinamento, arquitetura de inferÃªncia, MCTS paralelo e o papel crucial das TPUs.

---

## VisÃ£o Geral da Arquitetura de Treinamento

### Arquitetura de Treinamento do AlphaGo Original

A arquitetura de treinamento do AlphaGo original (a versÃ£o que derrotou Lee Sedol) foi dividida em mÃºltiplas fases, cada uma usando uma configuraÃ§Ã£o de recursos diferente:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Fase 1: Aprendizado Supervisionado        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Registros   â”‚ â†’  â”‚ Cluster de  â”‚ â†’  â”‚ Policy Net  â”‚      â”‚
â”‚  â”‚ Humanos     â”‚    â”‚   GPU       â”‚    â”‚ (versÃ£o SL) â”‚      â”‚
â”‚  â”‚ (30M jogos) â”‚    â”‚ (50 GPUs)   â”‚    â”‚             â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Fase 2: Aprendizado por ReforÃ§o           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Auto-jogo   â”‚ â†’  â”‚ Cluster de  â”‚ â†’  â”‚ Policy Net  â”‚      â”‚
â”‚  â”‚ (milhÃµes    â”‚    â”‚   GPU       â”‚    â”‚ (versÃ£o RL) â”‚      â”‚
â”‚  â”‚ de jogos)   â”‚    â”‚ (50 GPUs)   â”‚    â”‚             â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Fase 3: Treinamento da Value Network      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Dados de    â”‚ â†’  â”‚ Cluster de  â”‚ â†’  â”‚ Value Net   â”‚      â”‚
â”‚  â”‚ jogos RL    â”‚    â”‚   GPU       â”‚    â”‚             â”‚      â”‚
â”‚  â”‚ (30M pos.)  â”‚    â”‚ (50 GPUs)   â”‚    â”‚             â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Arquitetura de Treinamento do AlphaGo Zero

O AlphaGo Zero simplificou significativamente o processo de treinamento, usando um Ãºnico ciclo de treinamento de ponta a ponta:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Ciclo de Treinamento AlphaGo Zero          â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚   Self-play       â”‚  â† Rede mais recente                 â”‚
â”‚  â”‚   Workers         â”‚                                      â”‚
â”‚  â”‚   (TPU Ã— N)       â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚            â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚   Replay Buffer   â”‚  (Ãºltimos 500K jogos)                â”‚
â”‚  â”‚   (RAM/SSD)       â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚            â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚   Training        â”‚                                      â”‚
â”‚  â”‚   Workers         â”‚                                      â”‚
â”‚  â”‚   (TPU Ã— M)       â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚            â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚   Network         â”‚  â†’ Atualiza rede para Self-play      â”‚
â”‚  â”‚   Checkpoint      â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

As vantagens desta arquitetura:

1. **Aprendizado contÃ­nuo**: Self-play e Training acontecem simultaneamente, sem necessidade de esperar
2. **EficiÃªncia de recursos**: Todos os recursos estÃ£o fazendo trabalho Ãºtil
3. **IteraÃ§Ã£o rÃ¡pida**: A rede atualizada Ã© imediatamente usada para gerar novos dados

---

## Workers de Auto-Jogo (Self-play Workers)

### DistribuiÃ§Ã£o de Tarefas

Os Self-play Workers sÃ£o responsÃ¡veis por realizar auto-jogo com a rede mais forte atual, gerando dados de treinamento.

| ConfiguraÃ§Ã£o | AlphaGo Zero |
|--------------|--------------|
| NÃºmero de Workers | Dezenas |
| Por Worker | 1-4 TPU |
| MCTS por jogo | 1600 simulaÃ§Ãµes |
| ProduÃ§Ã£o diÃ¡ria | ~100.000 jogos |

### Fluxo de Trabalho

O fluxo de trabalho de cada Self-play Worker:

```python
while True:
    # 1. Baixar os pesos da rede mais recente
    network = download_latest_checkpoint()

    # 2. Realizar mÃºltiplos jogos de auto-jogo
    for game in range(batch_size):
        positions = []
        board = EmptyBoard()

        while not board.is_terminal():
            # Executar MCTS
            mcts = MCTS(network, board)
            policy = mcts.search(num_simulations=1600)

            # Selecionar jogada
            action = sample(policy)

            # Registrar
            positions.append((board.state, policy))

            # Jogar
            board = board.play(action)

        # 3. Obter resultado do jogo
        result = board.get_result()

        # 4. Fazer upload dos dados
        upload_to_replay_buffer(positions, result)
```

### Balanceamento de Carga

MÃºltiplos Workers precisam de balanceamento de carga:

- **SincronizaÃ§Ã£o de rede**: Todos os Workers usam a mesma versÃ£o da rede
- **Balanceamento de dados**: Garantir que os dados de diferentes Workers sejam todos utilizados
- **Tratamento de falhas**: Falha de um Ãºnico Worker nÃ£o afeta o treinamento geral

---

## Workers de Treinamento (Training Workers)

### DistribuiÃ§Ã£o de Tarefas

Os Training Workers sÃ£o responsÃ¡veis por amostrar dados do Replay Buffer e treinar a rede neural.

| ConfiguraÃ§Ã£o | AlphaGo Zero |
|--------------|--------------|
| NÃºmero de Workers | 1-4 |
| Por Worker | 4 TPU |
| Batch Size | 2048 (512 por TPU) |
| Passos de treinamento | Dezenas de milhares por dia |

### Treinamento DistribuÃ­do

O treinamento em larga escala usa **Paralelismo de Dados (Data Parallelism)**:

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Parameter  â”‚
                    â”‚   Server    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚  TPU 0  â”‚      â”‚  TPU 1  â”‚      â”‚  TPU 2  â”‚
    â”‚ Batch 0 â”‚      â”‚ Batch 1 â”‚      â”‚ Batch 2 â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚                 â”‚                 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚  Gradient   â”‚
                    â”‚  Aggregationâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Cada TPU processa um mini-batch diferente, calcula o gradiente local e entÃ£o agrega para atualizar os parÃ¢metros globais.

### AtualizaÃ§Ã£o SÃ­ncrona vs. AssÃ­ncrona

| Modo de atualizaÃ§Ã£o | Vantagens | Desvantagens |
|---------------------|-----------|--------------|
| SÃ­ncrono | EstÃ¡vel, reproduzÃ­vel | Workers precisam esperar pelo mais lento |
| AssÃ­ncrono | Alto throughput | Gradientes podem estar desatualizados |

O AlphaGo Zero usa **atualizaÃ§Ã£o sÃ­ncrona** para garantir a estabilidade do treinamento.

---

## O Papel das TPUs

### O Que Ã© uma TPU?

**TPU (Tensor Processing Unit)** Ã© um acelerador projetado especificamente pela Google para deep learning:

| CaracterÃ­stica | TPU | GPU | CPU |
|----------------|-----|-----|-----|
| Objetivo de design | OperaÃ§Ãµes matriciais | Paralelismo geral | ComputaÃ§Ã£o geral |
| PrecisÃ£o | Otimizado para FP16/BF16 | FP32/FP16 | FP64/FP32 |
| Consumo de energia | Relativamente baixo | Mais alto | Mais alto |
| LatÃªncia | Baixa | MÃ©dia | Alta |

### Arquitetura da TPU

O nÃºcleo da TPU Ã© a **MXU (Matrix Multiply Unit)**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TPU v2/v3                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         MXU (128Ã—128)           â”‚    â”‚
â”‚  â”‚    Matrix Multiply Unit         â”‚    â”‚
â”‚  â”‚    (128Ã—128 = 16K MACs/ciclo)   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Vector   â”‚  â”‚     HBM          â”‚     â”‚
â”‚  â”‚ Unit     â”‚  â”‚   (16-32 GB)     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

A MXU pode executar 16K operaÃ§Ãµes de multiplicaÃ§Ã£o-acumulaÃ§Ã£o por ciclo, o que Ã© crucial para a multiplicaÃ§Ã£o de matrizes em redes neurais.

### Por Que o AlphaGo Precisa de TPUs?

O gargalo computacional da IA de Go estÃ¡ na **inferÃªncia da rede neural**:

| OperaÃ§Ã£o | ProporÃ§Ã£o |
|----------|-----------|
| Forward pass da rede neural | ~95% |
| OperaÃ§Ãµes da Ã¡rvore MCTS | ~4% |
| Outros | ~1% |

Cada passo do MCTS requer 1600 inferÃªncias da rede neural. O alto throughput das TPUs torna isso possÃ­vel.

### EvoluÃ§Ã£o do Uso de TPUs

| VersÃ£o | TPU para Treinamento | TPU para InferÃªncia |
|--------|---------------------|---------------------|
| AlphaGo Lee | 50 GPU | 48 TPU (v1) |
| AlphaGo Master | 4 TPU (v2) | 4 TPU (v2) |
| AlphaGo Zero | 4 TPU (v2) | 4 TPU (v2) (escalÃ¡vel) |

O nÃºmero de TPUs usadas pelo AlphaGo Zero diminuiu significativamente, graÃ§as a uma arquitetura mais eficiente e versÃµes mais recentes das TPUs.

---

## MCTS Paralelo e Perda Virtual

### O Desafio da ParalelizaÃ§Ã£o

A implementaÃ§Ã£o padrÃ£o do MCTS Ã© **serial**:

```
for i in range(num_simulations):
    1. Selection: Selecionar da raiz para baixo
    2. Expansion: Expandir nÃ³ folha
    3. Evaluation: AvaliaÃ§Ã£o da rede neural
    4. Backup: Propagar atualizaÃ§Ã£o para cima
```

Mas a avaliaÃ§Ã£o da rede neural Ã© uma **operaÃ§Ã£o em batch** amigÃ¡vel para GPU/TPU. Como fazer mÃºltiplas simulaÃ§Ãµes ocorrerem simultaneamente?

### ParalelizaÃ§Ã£o de Folhas (Leaf Parallelization)

A forma mais simples de paralelizaÃ§Ã£o: executar mÃºltiplas simulaÃ§Ãµes completas simultaneamente, depois mesclar os resultados.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Root     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
  â”‚ Sim 1   â”‚ â”‚ Sim 2  â”‚ â”‚ Sim 3  â”‚ â”‚ Sim 4  â”‚
  â”‚ (indep) â”‚ â”‚ (indep)â”‚ â”‚ (indep)â”‚ â”‚ (indep)â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Merge Trees  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Problema: Cada simulaÃ§Ã£o comeÃ§a da raiz, e pode explorar repetidamente os mesmos caminhos.

### Perda Virtual (Virtual Loss)

A DeepMind adotou a tÃ©cnica de **perda virtual** para implementar ParalelizaÃ§Ã£o de Ãrvore (Tree Parallelization).

#### Conceito BÃ¡sico

Quando uma thread estÃ¡ explorando um determinado nÃ³, reduz temporariamente o valor desse nÃ³, fazendo com que outras threads escolham outros caminhos.

```
UCB normal: Q(s,a) + c * P(s,a) * sqrt(N(s)) / (1 + N(s,a))

Com perda virtual:
(Q(s,a) * N(s,a) - v * n_virtual) / (N(s,a) + n_virtual) + c * P(s,a) * sqrt(N(s)) / (1 + N(s,a) + n_virtual)
```

Onde:
- `n_virtual` Ã© o nÃºmero de threads explorando esse nÃ³
- `v` Ã© o valor da perda virtual (geralmente 1 ou o valor correspondente Ã  taxa de vitÃ³ria)

#### Fluxo de OperaÃ§Ã£o

```
Tempo T1:
  Thread 1 seleciona caminho A â†’ B â†’ C
  NÃ³ C recebe perda virtual -1

Tempo T2:
  Thread 2 seleciona caminho A â†’ B â†’ D (porque C foi "penalizado")
  NÃ³ D recebe perda virtual -1

Tempo T3:
  Thread 1 completa avaliaÃ§Ã£o, atualiza valor real de C, remove perda virtual
  Thread 3 agora pode escolher C (se o valor real for bom o suficiente)
```

#### Efeitos da Perda Virtual

| Aspecto | Efeito |
|---------|--------|
| Diversidade de exploraÃ§Ã£o | ForÃ§a exploraÃ§Ã£o de diferentes caminhos |
| EficiÃªncia de batch | Pode avaliar mÃºltiplos nÃ³s folha simultaneamente |
| ConvergÃªncia | Perda virtual Ã© eventualmente substituÃ­da pelo valor real, nÃ£o afeta convergÃªncia |

### AvaliaÃ§Ã£o de Rede Neural em Batch

AtravÃ©s da perda virtual, mÃºltiplos nÃ³s folha a serem avaliados podem ser coletados para **inferÃªncia em batch**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Parallel MCTS                â”‚
â”‚                                         â”‚
â”‚  Thread 1 â†’ NÃ³ folha L1 â”€â”€â”             â”‚
â”‚  Thread 2 â†’ NÃ³ folha L2 â”€â”€â”¼â”€â”€â†’ Batch â”€â†’ TPU
â”‚  Thread 3 â†’ NÃ³ folha L3 â”€â”€â”¤             â”‚
â”‚  Thread 4 â†’ NÃ³ folha L4 â”€â”€â”˜             â”‚
â”‚                                         â”‚
â”‚  â† ObtÃ©m (P1,V1), (P2,V2), ... simultaneamente â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

A eficiÃªncia da inferÃªncia em batch da TPU Ã© muito maior que a inferÃªncia individual, tornando o MCTS paralelo possÃ­vel.

---

## Arquitetura de InferÃªncia

### ConfiguraÃ§Ã£o Durante Partidas

A arquitetura de inferÃªncia do AlphaGo durante partidas oficiais:

| VersÃ£o | ConfiguraÃ§Ã£o de Hardware |
|--------|--------------------------|
| AlphaGo Fan | 176 GPU |
| AlphaGo Lee | 48 TPU + mÃºltiplos servidores |
| AlphaGo Master | 4 TPU |
| AlphaGo Zero | 4 TPU (escalÃ¡vel) |

### Fluxo de InferÃªncia DistribuÃ­da

Fluxo de inferÃªncia durante partidas (usando AlphaGo Lee como exemplo):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Arquitetura de InferÃªncia DistribuÃ­da     â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   NÃ³ Master  â”‚ â† Recebe jogada do oponente, envia jogada â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   do AlphaGo                              â”‚
â”‚         â”‚                                                    â”‚
â”‚         â†“                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚              Controlador MCTS                     â”‚       â”‚
â”‚  â”‚  Gerencia Ã¡rvore de busca, distribui tarefas,     â”‚       â”‚
â”‚  â”‚  coleta resultados                                â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                                                    â”‚
â”‚         â†“                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚              Cluster de TPU (48 TPUs)             â”‚       â”‚
â”‚  â”‚                                                   â”‚       â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”     â”‚       â”‚
â”‚  â”‚   â”‚TPU â”‚ â”‚TPU â”‚ â”‚TPU â”‚ â”‚TPU â”‚ â”‚TPU â”‚ â”‚... â”‚     â”‚       â”‚
â”‚  â”‚   â”‚ 1  â”‚ â”‚ 2  â”‚ â”‚ 3  â”‚ â”‚ 4  â”‚ â”‚ 5  â”‚ â”‚ 48 â”‚     â”‚       â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜     â”‚       â”‚
â”‚  â”‚                                                   â”‚       â”‚
â”‚  â”‚   Processa requisiÃ§Ãµes de inferÃªncia de rede     â”‚       â”‚
â”‚  â”‚   neural em batch                                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Gerenciamento de Tempo de ReflexÃ£o

EstratÃ©gia de gerenciamento de tempo do AlphaGo:

| PosiÃ§Ã£o | Tempo de ReflexÃ£o | SimulaÃ§Ãµes MCTS |
|---------|-------------------|-----------------|
| Abertura (tem joseki) | Mais curto | ~10.000 |
| Meio-jogo (complexo) | Mais longo | ~100.000 |
| PosiÃ§Ã£o simples | Mais curto | ~5.000 |
| Byoyomi | Fixo | ~1.600 |

Mais simulaÃ§Ãµes MCTS geralmente significam jogadas de melhor qualidade.

---

## ComunicaÃ§Ã£o e SincronizaÃ§Ã£o

### Formato de Dados

Formato de transmissÃ£o de dados de treinamento:

```protobuf
message TrainingExample {
    // Estado do tabuleiro (17 Ã— 19 Ã— 19)
    repeated float board_planes = 1;

    // Resultado da busca MCTS (362)
    repeated float mcts_policy = 2;

    // Resultado do jogo (1 = jogador atual vence, -1 = jogador atual perde)
    float game_result = 3;
}
```

### Requisitos de Largura de Banda de Rede

| Fluxo de dados | Tamanho | FrequÃªncia |
|----------------|---------|------------|
| Amostras de treinamento | ~10 KB/amostra | Milhares de amostras/segundo |
| Pesos da rede | ~200 MB | VÃ¡rias vezes/hora |
| Mensagens de controle | < 1 KB | ContÃ­nuo |

Requisito total de largura de banda: ~100 Mbps (rede interna Ã© suficiente)

### Tratamento de Falhas

Tratamento de falhas em sistemas distribuÃ­dos:

| Tipo de falha | Forma de tratamento |
|---------------|---------------------|
| Worker caiu | Reinicia, continua usando checkpoint mais recente |
| DesconexÃ£o de rede | Buffer de dados, continua transmissÃ£o apÃ³s reconexÃ£o |
| Falha de TPU | Troca automÃ¡tica para TPU reserva |
| Dados corrompidos | Descarta apÃ³s verificaÃ§Ã£o, regenera |

---

## AnÃ¡lise de Custos

### Estimativa de Custos de Hardware

Estimativa de custos de treinamento do AlphaGo Zero baseada nos preÃ§os de TPU do Google Cloud:

| Recurso | Quantidade | PreÃ§o/hora | Total/dia |
|---------|------------|------------|-----------|
| TPU v2 Pod | 4 | ~$32 | ~$3.000 |
| VM alta memÃ³ria | VÃ¡rios | ~$5 | ~$500 |
| EspaÃ§o de armazenamento | 10 TB | ~$0.02/GB | ~$200 |
| Rede | - | IncluÃ­do | - |

**Aproximadamente $3.700/dia**, treinamento completo (40 dias) aproximadamente **$150.000**.

Nota: Esta Ã© uma estimativa de 2017, a DeepMind como subsidiÃ¡ria do Google pode ter descontos internos.

### ComparaÃ§Ã£o com Treinamento Humano

| Aspecto | AlphaGo Zero | Jogador profissional humano |
|---------|--------------|----------------------------|
| Tempo para nÃ­vel profissional | 2 dias | 10-15 anos |
| Custo de treinamento | ~$7.500 | MilhÃµes (mensalidades, custo de vida, custo de oportunidade) |
| Custos contÃ­nuos | Eletricidade | Custo de vida |
| Replicabilidade | RÃ©plica perfeita | NÃ£o replicÃ¡vel |

Claro, esta comparaÃ§Ã£o nÃ£o Ã© totalmente justa â€” os humanos aprendem muito mais do que apenas Go durante seu treinamento.

### Custos de InferÃªncia

Custos de inferÃªncia em partidas oficiais:

| ConfiguraÃ§Ã£o | Custo por jogo |
|--------------|----------------|
| 48 TPU (AlphaGo Lee) | ~$500 |
| 4 TPU (AlphaGo Zero) | ~$50 |
| GPU Ãºnica (KataGo) | ~$1 |

Os custos de inferÃªncia diminuÃ­ram significativamente com o avanÃ§o tecnolÃ³gico.

---

## EvoluÃ§Ã£o TecnolÃ³gica

### De AlphaGo a AlphaZero

| Aspecto | AlphaGo Lee | AlphaGo Zero | AlphaZero |
|---------|-------------|--------------|-----------|
| TPU de treinamento | 50+ GPU â†’ TPU | 4 TPU | 4 TPU |
| TPU de inferÃªncia | 48 TPU | 4 TPU | 4 TPU |
| MCTS/jogada | ~100.000 | ~1.600 | ~800 |
| Tempo de treinamento | Meses | 40 dias | Horas a dias |

Melhoria de eficiÃªncia de aproximadamente 100 vezes.

### Impacto na Comunidade Open Source

A arquitetura do AlphaGo inspirou mÃºltiplos projetos open source:

| Projeto | CaracterÃ­sticas |
|---------|-----------------|
| Leela Zero | Treinamento distribuÃ­do comunitÃ¡rio, replica AlphaGo Zero |
| KataGo | Treinamento eficiente em GPU Ãºnica, supera AlphaGo Zero |
| ELF OpenGo | Open source do Facebook, usa PyTorch |
| Minigo | Open source do Google, usa TensorFlow |

Esses projetos permitem que pesquisadores comuns tambÃ©m possam treinar IAs de Go poderosas.

---

## CorrespondÃªncia com AnimaÃ§Ãµes

Os conceitos principais deste artigo e nÃºmeros de animaÃ§Ã£o correspondentes:

| NÃºmero | Conceito | CorrespondÃªncia FÃ­sica/MatemÃ¡tica |
|--------|----------|-----------------------------------|
| ğŸ¬ C9 | MCTS paralelo | Problema de N-corpos |
| ğŸ¬ E9 | Treinamento distribuÃ­do | ComputaÃ§Ã£o distribuÃ­da |
| ğŸ¬ C5 | Perda virtual | Potencial de repulsÃ£o |
| ğŸ¬ D15 | InferÃªncia em batch | ComputaÃ§Ã£o vetorizada |

---

## Leitura Adicional

- **Artigo anterior**: [O Processo de Treinamento do Zero](../training-from-scratch) â€” AnÃ¡lise detalhada da curva de treinamento
- **PrÃ³ximo artigo**: [O Legado do AlphaGo](../legacy-and-impact) â€” O impacto profundo do AlphaGo no campo da IA
- **Artigo relacionado**: [A CombinaÃ§Ã£o de MCTS e Redes Neurais](../mcts-neural-combo) â€” Conhecimentos bÃ¡sicos de MCTS

---

## ReferÃªncias

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Jouppi, N., et al. (2017). "In-Datacenter Performance Analysis of a Tensor Processing Unit." *ISCA 2017*.
3. Dean, J., et al. (2012). "Large Scale Distributed Deep Networks." *NeurIPS 2012*.
4. Chaslot, G., et al. (2008). "Parallel Monte-Carlo Tree Search." *CIG 2008*.
5. Segal, R. (2010). "On the Scalability of Parallel UCT." *CIG 2010*.
