---
sidebar_position: 1
title: 배경 지식
---

# 배경 지식 개요

KataGo 실전에 들어가기 전에 바둑 AI의 발전 역사와 핵심 기술을 이해하는 것이 매우 중요합니다. 이 섹션에서는 AlphaGo부터 현대 바둑 AI까지의 기술 발전을 안내합니다.

## 왜 배경 지식이 필요한가?

바둑 AI의 발전은 인공지능 분야에서 가장 흥미로운 돌파구 중 하나입니다. 2016년 AlphaGo가 이세돌을 꺾은 대국은 바둑사의 이정표일 뿐 아니라 딥러닝과 강화학습 결합의 엄청난 성공을 표시합니다.

이러한 배경 지식을 이해하면:

- **더 나은 기술적 결정**: 다양한 방법의 장단점 이해, 프로젝트에 적합한 방안 선택
- **더 효과적인 디버깅**: 기본 원리 이해로 문제 진단 용이
- **최신 발전 따라가기**: 기초 지식을 갖추면 새 논문과 기술 이해 용이
- **오픈소스 프로젝트 기여**: KataGo 등 프로젝트 개발 참여에 설계 이념 심층 이해 필요

## 이 섹션 내용

### [AlphaGo 논문 해독](./alphago.md)

DeepMind의 고전 논문을 심층 해석합니다:

- AlphaGo의 역사적 의의와 영향
- Policy Network와 Value Network의 설계
- 몬테카를로 트리 탐색(MCTS)의 원리와 구현
- Self-play 훈련 방법의 혁신
- AlphaGo에서 AlphaGo Zero, AlphaZero로의 발전

### [KataGo 논문 해독](./katago-paper.md)

현재 가장 강한 오픈소스 바둑 AI의 기술 혁신을 이해합니다:

- KataGo의 AlphaGo 대비 개선사항
- 더 효율적인 훈련 방법과 자원 활용
- 다양한 바둑 규칙 지원의 기술 구현
- 승률과 집수 동시 예측 설계
- KataGo가 적은 자원으로 더 강한 기력 달성하는 이유

### [기타 바둑 AI 소개](./zen.md)

바둑 AI 생태계 전반 이해:

- 상용 AI: 천정(Zen), 절예(텐센트), 성진
- 오픈소스 AI: Leela Zero, ELF OpenGo, SAI
- 각 AI의 기술 특징과 적용 시나리오 비교

## 기술 발전 타임라인

| 시간 | 사건 | 중요성 |
|------|------|--------|
| 2015년 10월 | AlphaGo가 판후이 격파 | 최초 AI가 프로 기사 격파 |
| 2016년 3월 | AlphaGo가 이세돌 격파 | 세계를 놀라게 한 인간vs기계 대전 |
| 2017년 5월 | AlphaGo가 커제 격파 | AI가 인간 최정상 수준 초월 확립 |
| 2017년 10월 | AlphaGo Zero 발표 | 순수 자가대국, 인간 기보 불필요 |
| 2017년 12월 | AlphaZero 발표 | 범용 설계, 바둑/체스/장기 동시 정복 |
| 2018년 | Leela Zero 초인간 수준 도달 | 오픈소스 커뮤니티의 승리 |
| 2019년 | KataGo 발표 | 더 효율적인 훈련 방법 |
| 2020-현재 | KataGo 지속 개선 | 가장 강한 오픈소스 바둑 AI |

## 핵심 개념 미리보기

상세 섹션을 읽기 전에 몇 가지 핵심 개념을 간략히 소개합니다:

### 바둑에서 신경망의 역할

```
바둑판 상태 → 신경망 → { Policy(착점 확률), Value(승률 평가) }
```

신경망은 현재 바둑판 상태를 입력으로 받아 두 가지 정보를 출력합니다:
- **Policy**: 각 위치의 착점 확률, 탐색 방향 안내
- **Value**: 현재 국면의 승률 추정, 국면 평가용

### 몬테카를로 트리 탐색(MCTS)

MCTS는 신경망과 결합하여 최선수를 결정하는 탐색 알고리즘입니다:

1. **Selection(선택)**: 루트 노드에서 가장 유망한 경로 선택
2. **Expansion(확장)**: 리프 노드에서 새로운 가능한 수 확장
3. **Evaluation(평가)**: 신경망으로 국면 가치 평가
4. **Backpropagation(역전파)**: 평가 결과를 경로상 모든 노드에 역전파하여 업데이트

### Self-play(자가대국)

AI가 자신과 대국하여 훈련 데이터 생성:

```
초기 모델 → 자가대국 → 기보 수집 → 새 모델 훈련 → 더 강한 모델 → 반복
```

이 순환으로 AI는 인간 기보 없이도 계속 자가 향상할 수 있습니다.

## 권장 읽기 순서

1. **먼저 AlphaGo 논문 해독 읽기**: 기본 이론 프레임워크 구축
2. **다음 KataGo 논문 해독 읽기**: 최신 개선과 최적화 이해
3. **마지막으로 기타 바둑 AI 소개 읽기**: 시야 확장, 다양한 구현 방식 이해

준비되셨나요? [AlphaGo 논문 해독](./alphago.md)부터 시작합시다!

