---
sidebar_position: 14
title: 자기 대국
description: AlphaGo가 자기 대국을 통해 인간 기력의 한계를 어떻게 돌파했는지 깊이 이해하기
---

import { EloChart } from '@site/src/components/D3Charts';

# 자기 대국

이전 글에서 강화 학습의 기본 개념을 소개했습니다. 이제 AlphaGo 성공의 핵심 중 하나인 **자기 대국(Self-Play)**을 탐구해 보겠습니다.

이것은 모순처럼 보이는 개념입니다: **AI가 어떻게 자신과 바둑을 두면서 더 강해질 수 있을까요?**

답은 깊고 우아하며, 게임 이론, 진화 역학, 그리고 학습의 본질에 관련됩니다.

---

## 왜 자기 대국이 효과적인가?

### 직관적 설명

당신이 바둑 초보자이고, 무인도에서 혼자 연습한다고 상상해 보세요:

1. 한 판을 두며, 흑과 백 양쪽을 동시에 담당함
2. 대국이 끝난 후, 어떤 수가 좋았고 어떤 수가 나빴는지 분석함
3. 다음 대국에서 이전의 실수를 피하려고 시도함
4. 이 과정을 수백만 번 반복함

직관적으로 이것은 문제가 있어 보입니다:
- 수준이 매우 낮다면, 흑백 양쪽 모두 나쁜 수를 두는데, 무엇을 배울 수 있을까?
- '잘못된 균형'에 빠지지 않을까—양쪽 모두 나쁜 수를 두지만 서로 상쇄되는?

그러나 실제로 자기 대국은 지속적인 향상을 만들어낼 수 있습니다. 이유는 다음과 같습니다:

### 점진적 약점 발견

핵심 통찰은: **양쪽 모두 같은 AI라도, 각 대국의 결과에는 여전히 정보가 포함되어 있습니다**.

```
국면 A: AI가 수법 X를 선택, 최종 승리
국면 A: AI가 수법 Y를 선택, 최종 패배

→ 결론: 국면 A에서 X가 Y보다 좋음
```

많은 대국의 통계를 통해, AI는 각 국면에서 어떤 선택이 더 좋은지 학습할 수 있습니다. 이것이 **정책 경사**의 본질입니다: 좋은 선택은 강화되고, 나쁜 선택은 억제됩니다.

### 적대적 학습

자기 대국에는 특별한 특성이 있습니다: **훈련 상대가 자동으로 당신의 수준에 맞춰집니다**.

```
훈련 주기 1: AI가 효과적인 전술 T를 발견함
훈련 주기 2: 상대로서의 AI가 T에 대한 방어법을 배움
훈련 주기 3: 원래 AI가 더 좋은 전술 T'를 찾도록 강제됨
```

이것은 **군비 경쟁(Arms Race)**을 형성하며, 양쪽이 끊임없이 서로의 약점을 발견하고 극복합니다.

### 인간 기보와의 비교

| 훈련 방식 | 장점 | 단점 |
|---------|------|------|
| **인간 기보** | 인간 지혜의 결정체 학습 | 인간 수준에 제한됨 |
| **자기 대국** | 무제한 향상 잠재력 | 국소 최적에 빠질 수 있음 |
| **둘의 결합** | 빠른 시작 + 지속적 향상 | 최적의 전략 |

AlphaGo 원본은 먼저 인간 기보로 지도 학습을 하고, 그 다음 자기 대국으로 강화 학습을 했습니다. AlphaGo Zero는 자기 대국만으로도 초인간 수준에 도달할 수 있음을 증명했습니다.

---

## 게임 이론 관점

### 내시 균형

게임 이론에서 **내시 균형(Nash Equilibrium)**은 안정 상태입니다: 이 상태에서 어떤 플레이어도 일방적으로 전략을 변경할 동기가 없습니다.

바둑과 같은 **제로섬, 완전 정보 게임**에서 내시 균형은 특별한 의미가 있습니다:

$$\pi^* = \arg\max_\pi \min_{\pi'} V(\pi, \pi')$$

여기서 $V(\pi, \pi')$는 전략 $\pi$가 전략 $\pi'$와 대결할 때의 기대 가치입니다.

이것이 유명한 **Minimax 원리**입니다: 최적의 전략은 최악의 상황에서 가장 잘 수행하는 전략입니다.

### 자기 대국과 내시 균형

이론적으로, 자기 대국이 수렴한다면, 내시 균형으로 수렴해야 합니다. 바둑과 같은 결정적 게임에서 내시 균형은 **완벽한 플레이**입니다.

그러나 바둑의 상태 공간은 너무 큽니다($10^{170}$). 진정한 내시 균형을 찾는 것은 불가능합니다. 자기 대국은 실제로 이 균형을 **근사**합니다.

### 가상 대국(Fictitious Play)

자기 대국은 게임 이론의 **가상 대국** 개념과 관련이 있습니다:

1. 각 플레이어가 상대의 역사적 전략을 관찰함
2. 상대 전략의 평균 분포를 계산함
3. 이 평균 분포에 대한 최적의 응답을 선택함

어떤 조건에서 가상 대국은 내시 균형으로 수렴한다고 증명할 수 있습니다.

AlphaGo의 자기 대국은 이 개념의 신경망 구현으로 볼 수 있습니다.

---

## 자기 대국의 메커니즘

### 기본 흐름

AlphaGo의 자기 대국 흐름:

```
알고리즘: Self-Play Training

초기화: Policy Network π_θ(지도 학습 또는 무작위 초기화에서 시작 가능)

수렴할 때까지 다음 단계 반복:

1. 대국 데이터 생성
   i = 1부터 N까지(병렬 진행):
     a. 현재 정책 π_θ로 한 판 자기 대국 진행
     b. 궤적 수집: τ_i = (s_0, a_0, r_1, s_1, a_1, ...)
     c. 최종 결과 기록 z_i ∈ {-1, +1}

2. 정책 업데이트
   a. 정책 경사 계산:
      ∇J = (1/N) Σ_i Σ_t ∇_θ log π_θ(a_t|s_t) · z_i
   b. 파라미터 업데이트: θ ← θ + α · ∇J

3. 가치 네트워크 업데이트
   a. (s, z) 쌍으로 Value Network 훈련
   b. 최소화: L = E[(V_φ(s) - z)²]

4. 선택: 평가하고 체크포인트 저장
   a. 새 정책이 이전 버전과 대결
   b. 승률 > 55%이면 상대 풀 업데이트
```

### 훈련 데이터 생성

각 자기 대국은 **궤적(trajectory)**을 생성합니다:

$$\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, z)$$

여기서:
- $s_t$: 시간 단계 $t$의 바둑판 상태
- $a_t$: 시간 단계 $t$에서 선택한 행동
- $z$: 최종 결과(+1 승리, -1 패배)

200수의 대국은 200개의 훈련 샘플을 생성합니다. 매일 수십만 판의 자기 대국을 진행하면 훈련 데이터 양은 놀랍습니다.

### 정책 업데이트

정책 경사를 사용하여 Policy Network 업데이트:

$$\theta \leftarrow \theta + \alpha \cdot \nabla_\theta \mathbb{E}\left[\sum_t \log \pi_\theta(a_t|s_t) \cdot z\right]$$

이 업데이트의 효과:
- 최종적으로 승리($z = +1$)하면, 둔 모든 수의 확률을 높임
- 최종적으로 패배($z = -1$)하면, 둔 모든 수의 확률을 낮춤

이것은 조잡해 보입니다—이긴 대국에도 나쁜 수가 있을 수 있고, 진 대국에도 좋은 수가 있을 수 있습니다. 그러나 많은 대국의 통계를 통해 이러한 '잡음'은 평균화되고, 진정으로 좋은 수가 식별됩니다.

### 가치 네트워크 훈련

Value Network는 **회귀(regression)**로 훈련됩니다:

$$\phi \leftarrow \phi - \beta \cdot \nabla_\phi \mathbb{E}\left[(V_\phi(s) - z)^2\right]$$

이것은 Value Network가 예측하는 방법을 배우게 합니다: 현재 국면에서 시작하여 최종적으로 승리할 확률은 얼마인가?

Value Network의 역할은:
1. MCTS에서 리프 노드 평가 제공
2. 정책 경사의 기준선(baseline) 역할
3. 직접적인 국면 평가에 사용

---

## 무작위화의 중요성

### 결정적 순환 피하기

자기 대국이 완전히 결정적이라면, 순환에 빠질 수 있습니다:

```
전략 A가 항상 고정된 오프닝을 둠
전략 A 대 전략 A는 항상 같은 대국을 생성
한 대국만 반복적으로 학습됨
AI가 다른 가능성을 탐색할 수 없음
```

이것이 자기 대국에서 **무작위성**이 중요한 이유입니다.

### 무작위화의 원천

AlphaGo가 자기 대국에서 무작위성을 도입하는 방법:

**1. 정책 네트워크 자체가 확률적**

Policy Network는 결정적 선택이 아닌 확률 분포를 출력합니다:

$$a \sim \pi_\theta(a|s)$$

같은 국면에서 매번 다른 착수를 선택할 수 있습니다.

**2. 온도 파라미터**

훈련 시 다양성을 높이기 위해 높은 온도를 사용합니다:

$$\pi_\tau(a|s) = \frac{\pi_\theta(a|s)^{1/\tau}}{\sum_{a'} \pi_\theta(a'|s)^{1/\tau}}$$

- $\tau > 1$: 더 무작위, 더 많은 탐색
- $\tau < 1$: 더 결정적, 더 많은 활용
- $\tau = 1$: 원래 분포

**3. 디리클레 노이즈(Dirichlet Noise)**

AlphaGo Zero는 자기 대국 시 루트 노드의 사전 확률에 디리클레 노이즈를 추가합니다:

$$P(s, a) = (1 - \varepsilon) \cdot \pi_\theta(a|s) + \varepsilon \cdot \eta_a$$

여기서 $\eta \sim \text{Dir}(\alpha)$, $\varepsilon = 0.25$, $\alpha = 0.03$(바둑의 361개 행동에 대해).

이것은 매우 낮은 확률의 수법도 탐색될 기회가 있음을 보장합니다.

### 대국 풀(Population) 방법

다양성을 높이는 또 다른 방법은 **대국 풀**을 유지하는 것입니다:

```
대국 풀 = [π_1, π_2, π_3, ..., π_k](다른 버전의 전략)

각 대국:
1. 풀에서 무작위로 상대 선택
2. 해당 상대와 대국
3. 결과로 현재 정책 업데이트
4. 정기적으로 개선된 정책을 풀에 추가
```

이 방법의 장점:
- **다양성**: 다른 스타일의 상대
- **안정성**: 특정 상대에 과적합 방지
- **견고성**: 다양한 전략에 대응하는 법 학습

AlphaGo 원본과 AlphaGo Zero 모두 비슷한 기술을 사용했습니다.

---

## 기력 성장 곡선

### Elo 평점 시스템

AI 기력의 변화를 추적하기 위해 AlphaGo는 **Elo 평점 시스템**을 사용했습니다.

Elo 시스템의 기본 원리:

$$P(\text{A 승리}) = \frac{1}{1 + 10^{(R_B - R_A)/400}}$$

여기서 $R_A$와 $R_B$는 양측의 Elo 점수입니다.

- 점수 차이 200: 강자가 예상 75% 승리
- 점수 차이 400: 강자가 예상 90% 승리
- 점수 차이 800: 강자가 예상 99% 승리

### AlphaGo의 기력 성장

AlphaGo 각 버전의 기력 성장을 시각화해 보겠습니다:

<EloChart mode="zero" width={700} height={400} showMilestones={true} />

### 성장 속도 분석

곡선에서 몇 가지 흥미로운 현상을 관찰할 수 있습니다:

**1. 초기 빠른 성장**

훈련의 처음 몇 시간 동안 AI는 기본 규칙과 간단한 전술을 배웠습니다. 이것은 **저매달림 과일** 단계입니다—수정할 명백한 실수가 너무 많습니다.

**2. 중기 안정적 성장**

기본 실수가 제거됨에 따라 AI는 더 정교한 전술과 정석을 학습하기 시작합니다. 성장 속도는 느려지지만 여전히 안정적입니다.

**3. 후기 성장 둔화**

AI가 이미 매우 강할 때, 추가 향상이 어려워집니다. 실수를 수정하는 것뿐만 아니라 완전히 새로운 전략을 발견해야 할 수 있습니다.

### 인간을 초월하는 순간

AlphaGo 훈련 곡선의 주요 이정표:

| 이정표 | 해당 수준 | 달성 시간 |
|--------|--------|---------|
| 아마추어 강자 초월 | Elo ~2700 | 약 3시간 |
| Fan Hui 초월 | Elo ~3500 | 약 36시간 |
| 이세돌 초월 | Elo ~4500 | 약 60시간 |
| 원본 AlphaGo 초월 | Elo ~5000 | 약 72시간 |

이 숫자들(AlphaGo Zero에서)은 충격적입니다: **AI가 3일 만에 처음부터 인류 수천 년의 바둑 지혜를 초월했습니다**.

---

## 수렴성 분석

### 자기 대국이 수렴하는가?

이것은 중요한 이론적 문제입니다. 짧은 답은: **특정 조건에서는 그렇지만, 바둑은 너무 복잡해서 엄밀하게 증명할 수 없습니다**.

### 이론적 보장

더 단순한 게임(예: 틱택토)에서는 증명할 수 있습니다:

1. **존재성**: 내시 균형이 존재함(Minimax 정리)
2. **수렴성**: 어떤 알고리즘(예: 가상 대국)은 내시 균형으로 수렴함

바둑에 대해서는 엄밀한 수렴 보장이 없지만, 실험적 증거는 다음을 보여줍니다:
- 기력이 지속적으로 향상됨
- 명백한 진동이나 퇴화가 나타나지 않음
- 최종 기력이 모든 알려진 인간을 초월함

### 가능한 실패 모드

자기 대국이 겪을 수 있는 문제:

**1. 전략 순환(Strategy Cycling)**

```
전략 A가 전략 B를 이김
전략 B가 전략 C를 이김
전략 C가 전략 A를 이김
```

이것은 일부 게임(예: 가위바위보)에서 실제로 발생합니다. 그러나 바둑은 충분히 복잡해서 이런 순수한 순환은 발생하지 않는 것 같습니다.

**2. 자신에게 과적합**

AI가 자신의 스타일만을 위한 전략을 배워, 다른 스타일의 상대에 대응하지 못할 수 있습니다. 이것이 AlphaGo가 자신의 다른 버전과 대국하고, 최종적으로 인간 기사와 테스트하는 이유입니다.

**3. 국소 최적**

AI가 국소 최적에 빠질 수 있습니다—'나쁘지 않지만 최선은 아닌' 전략. 무작위화와 많은 대국이 이 문제를 피하는 데 도움이 됩니다.

### 실제 관찰

AlphaGo의 훈련 과정에서 관찰된 것:

1. **지속적 향상**: Elo 점수가 훈련에 따라 계속 상승
2. **퇴화 없음**: 기력이 갑자기 떨어지는 일 없음
3. **스타일 진화**: AI의 바둑 스타일이 훈련에 따라 점차 변화
4. **새로운 정석 발견**: AI가 인간이 사용한 적 없는 오프닝과 전술을 발견

이러한 관찰은 이론적 보장은 없지만, 자기 대국이 실제로 효과적임을 보여줍니다.

---

## 구현 세부사항

### 병렬 자기 대국

훈련을 가속하기 위해 AlphaGo는 대규모 병렬 자기 대국을 사용합니다:

```
아키텍처:

    ┌────────────────────────────────────────────┐
    │           Parameter Server                  │
    │    (최신 θ 저장, 기울기 업데이트 수신)        │
    └────────────────────────────────────────────┘
         ▲                              │
         │ 기울기 업데이트               │ 최신 파라미터
         │                              ▼
    ┌─────────┐  ┌─────────┐  ┌─────────┐
    │ Worker 1 │  │ Worker 2 │  │ Worker N │
    │ 자기 대국  │  │ 자기 대국  │  │ 자기 대국  │
    │ 궤적 수집  │  │ 궤적 수집  │  │ 궤적 수집  │
    └─────────┘  └─────────┘  └─────────┘
```

**주요 설계 결정**:

- **동기 vs 비동기**: AlphaGo는 비동기 업데이트 사용, Worker들이 서로 기다릴 필요 없음
- **업데이트 빈도**: N 대국 완료할 때마다 파라미터 업데이트
- **상대 선택**: 최근 몇 버전 중 하나를 무작위로 상대로 선택

### 체크포인트 전략

정기적으로 모델 체크포인트 저장, 용도:

1. **대국 풀**: 다른 버전의 상대 유지
2. **평가**: 기력 변화 추적
3. **장애 복구**: 훈련 중단 시 복구 가능

```python
# 의사 코드
def training_loop():
    for iteration in range(num_iterations):
        # 대국 데이터 생성
        trajectories = parallel_self_play(current_policy, num_games=1000)

        # 정책 업데이트
        update_policy(trajectories)

        # 정기적 평가 및 저장
        if iteration % 100 == 0:
            elo = evaluate_against_pool(current_policy)
            save_checkpoint(current_policy, elo)

            if elo > best_elo:
                add_to_pool(current_policy)
                best_elo = elo
```

### 훈련 자원 요구

AlphaGo의 훈련 규모는 인상적입니다:

| 버전 | 하드웨어 | 훈련 시간 | 자기 대국 판 수 |
|------|------|---------|-------------|
| AlphaGo Fan | 176 GPU | 수개월 | ~30M |
| AlphaGo Lee | 48 TPU | 수주 | ~30M |
| AlphaGo Zero | 4 TPU | 3일 | ~5M |
| AlphaGo Zero (40일 버전) | 4 TPU | 40일 | ~30M |

AlphaGo Zero가 더 적은 하드웨어와 더 짧은 시간으로 더 강한 기력을 달성했음을 주목하세요—이것은 알고리즘 효율성의 향상입니다.

### 하이퍼파라미터 설정

몇 가지 주요 하이퍼파라미터:

```python
# 자기 대국 설정
NUM_PARALLEL_GAMES = 5000      # 동시 진행 대국 수
GAMES_PER_ITERATION = 25000    # 각 반복의 대국 수
MCTS_SIMULATIONS = 1600        # 각 수의 MCTS 시뮬레이션 횟수

# 훈련 설정
BATCH_SIZE = 2048              # 훈련 배치 크기
LEARNING_RATE = 0.01           # 초기 학습률
L2_REGULARIZATION = 1e-4       # 가중치 감쇠

# 탐색 설정
TEMPERATURE = 1.0              # 오프닝 30수의 온도
DIRICHLET_ALPHA = 0.03         # 디리클레 노이즈 파라미터
EXPLORATION_FRACTION = 0.25    # 노이즈 비율
```

이러한 하이퍼파라미터는 많은 실험을 통해 조정되었으며, 훈련 효과에 상당한 영향을 미칩니다.

---

## 자기 대국의 변형

### AlphaGo 원본

AlphaGo 원본의 훈련 흐름:

```
1. 지도 학습 (SL): 인간 기보에서 학습
   → SL Policy Network 생성 (π_SL)

2. 강화 학습 (RL): 자기 대국
   π_RL = π_SL로 초기화
   상대 풀 = [π_SL]

   반복:
     a. π_RL이 풀의 전략과 대국
     b. 정책 경사로 π_RL 업데이트
     c. π_RL이 강해지면 풀에 추가

   → RL Policy Network 생성 (π_RL)

3. 가치 네트워크 훈련:
   π_RL로 자기 대국하여 국면 생성
   V(s)가 승률 예측하도록 훈련
```

### AlphaGo Zero

AlphaGo Zero는 이 흐름을 단순화했습니다:

```
1. 순수 자기 대국(인간 데이터 없음)
   무작위 네트워크 f_θ로 초기화

   반복:
     a. MCTS + f_θ로 자기 대국 진행
     b. 정책 헤드와 가치 헤드 동시 훈련
     c. f_θ 업데이트

   → 단일 네트워크가 정책과 가치 동시 출력
```

주요 개선:
- **인간 데이터 불필요**: 처음부터 시작
- **단일 네트워크**: 정책과 가치가 특징 공유
- **더 간결한 훈련**: 엔드투엔드 학습

### AlphaZero

AlphaZero는 더욱 일반화:

```
같은 알고리즘, 다른 게임:
- 바둑: AlphaGo Zero 수준 초과 달성
- 체스: Stockfish 초월
- 장기: Elmo 초월

유일한 게임 특정 부분: 규칙 인코딩
```

이것은 자기 대국이 바둑에 국한되지 않는 **범용 학습 패러다임**임을 증명했습니다.

---

## 인간이 배운 것

### AI가 발견한 새로운 정석

자기 대국은 인간이 사용한 적 없는 많은 수법을 만들어냈습니다:

**1. 오프닝 혁신**

AlphaGo가 선호하는 일부 오프닝:
- 3-3 침입: 초기에 귀를 침입
- 높은 수: 전통적으로 '불안정'으로 여겨짐
- 대눈사태 변화: 인간이 복잡해서 계산하기 어렵다고 생각

**2. 새로운 형세 판단**

AI의 일부 국면에 대한 평가는 인간과 크게 다릅니다:
- 어떤 '얇아' 보이는 형태가 실제로 매우 견고함
- 어떤 '두터움'의 가치가 과대평가됨
- '선수'와 '후수'에 대한 재평가

### 인간 바둑에 미친 영향

AlphaGo 이후, 프로 바둑에 상당한 변화가 있었습니다:

1. **오프닝 다양화**: 프로 기사들이 AI가 발견한 새 오프닝 사용 시작
2. **훈련 방식 변화**: AI가 프로 기사의 주요 훈련 도구가 됨
3. **바둑 이론 재고**: 많은 전통적 '바둑 이론'이 의문시되고 수정됨
4. **새로운 미학**: AI 스타일의 바둑을 감상하기 시작

커제는 AlphaGo에게 진 후 말했습니다:

> "AlphaGo가 바둑을 다시 인식하게 해주었습니다. 이전에는 인간이 바둑을 이해한다고 생각했지만, 이제는 우리가 표면만 긁었다는 것을 압니다."

---

## 철학적 사고

### 학습의 본질

자기 대국은 학습에 관한 깊은 질문을 제기합니다:

**지식은 어디서 오는가?**

- 인간 학습은 외부 정보(선생님, 책, 경험)에 의존함
- 자기 대국 AI는 규칙만 있고, 외부 지식이 없음
- 그러나 여전히 지식을 '발견'할 수 있음—이 지식은 어디서 왔는가?

답은 아마도: **지식은 게임의 규칙과 구조에 암묵적으로 포함되어 있습니다**. 바둑의 규칙이 무엇이 좋은 수이고 나쁜 수인지 정의하며, 자기 대국은 이러한 암묵적 구조를 드러낼 뿐입니다.

### 창의성과 발견

AI가 '신의 한 수'(37번째 수)를 둘 때, 이것은 창조인가 발견인가?

한 가지 관점: 그 수는 항상 바둑 규칙 안에 '존재'했고, AI는 그것을 '발견'했을 뿐입니다.
다른 관점: AI가 그 수를 '창조'했습니다. 왜냐하면 아무도(AI 자신 포함) 미리 알지 못했기 때문입니다.

이 질문에는 표준 답이 없지만, 창의성에 대한 전통적 이해에 도전합니다.

### 인간 지능의 위치

AI가 처음부터, 자기 대국을 통해 인류 수천 년의 지혜를 초월할 수 있다면, 이것이 인간에게 무엇을 의미하는가?

낙관적 견해:
- AI는 인간이 창조한 도구
- AI의 발견이 인간의 이해를 향상시킬 수 있음
- 인간이 AI와 협력하여 더 높은 수준에 도달할 수 있음

신중한 견해:
- 일부 영역에서 순수한 계산이 인간 직관을 초월할 수 있음
- '전문 기술'의 가치를 재고할 필요
- 교육과 훈련 방식이 변경될 필요가 있을 수 있음

---

## 애니메이션 대응

이 글에서 다루는 핵심 개념과 애니메이션 번호:

| 번호 | 개념 | 물리/수학 대응 |
|------|------|--------------|
| E5 | 자기 대국 순환 | 고정점 반복 |
| E6 | 전략 진화 | 진화 역학 |

---

## 요약

자기 대국은 AlphaGo 성공의 핵심 기술 중 하나입니다. 우리가 배운 것:

1. **왜 효과적인가**: 적대적 학습, 점진적 약점 발견
2. **메커니즘**: 궤적 수집, 정책 경사, 가치 네트워크 훈련
3. **무작위화**: 온도 파라미터, 디리클레 노이즈, 대국 풀
4. **기력 성장**: Elo 시스템, 성장 곡선 분석
5. **수렴성**: 이론적 보장과 실제 관찰
6. **구현 세부사항**: 병렬 훈련, 체크포인트 전략, 하이퍼파라미터

다음 글에서는 AlphaGo가 신경망과 MCTS를 결합하여 양자의 장점을 발휘하는 방법을 탐구합니다.

---

## 추가 자료

- **다음 글**: [MCTS와 신경망의 결합](../mcts-neural-combo) — 직관과 추론의 완벽한 결합
- **이전 글**: [강화 학습 입문](../reinforcement-intro) — 강화 학습의 기본 개념
- **관련**: [AlphaGo Zero 개요](../alphago-zero) — 처음부터의 돌파구

---

## 참고 자료

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
3. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
4. Heinrich, J., & Silver, D. (2016). "Deep Reinforcement Learning from Self-Play in Imperfect-Information Games." *arXiv preprint*.
5. Lanctot, M., et al. (2017). "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning." *NeurIPS*.
