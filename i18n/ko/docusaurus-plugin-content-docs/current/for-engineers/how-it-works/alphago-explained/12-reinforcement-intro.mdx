---
sidebar_position: 13
title: 강화 학습 입문
description: 강화 학습의 핵심 개념 깊이 이해하기 - Agent, Environment, Reward, MDP, 정책 경사법과 가치 함수
---

# 강화 학습 입문

앞의 글에서 AlphaGo가 지도 학습을 사용하여 인간 기보에서 학습하는 방법을 소개했습니다. 그러나 지도 학습에는 근본적인 한계가 있습니다: **인간을 모방할 수만 있고, 인간을 초월할 수 없습니다**.

AI가 인간을 초월하게 하려면, 다른 학습 방법이 필요합니다—**강화 학습(Reinforcement Learning, RL)**.

이 글에서는 강화 학습의 핵심 개념을 처음부터 이해하여, 이후의 자기 대국과 MCTS 통합의 기반을 마련합니다.

---

## 강화 학습이란?

### 다른 학습 방법과의 비교

기계 학습에는 주로 세 가지 패러다임이 있습니다:

| 패러다임 | 학습 방식 | 예시 |
|------|---------|------|
| **지도 학습** | 레이블된 데이터에서 학습 | 이미지 분류, 다음 수 예측 |
| **비지도 학습** | 레이블 없는 데이터에서 구조 발견 | 클러스터링, 차원 축소 |
| **강화 학습** | 상호작용 경험에서 학습 | 바둑, 게임, 로봇 제어 |

강화 학습의 특징은: **누구도 정답이 무엇인지 알려주지 않으며, 시행착오를 통해 스스로 발견해야 합니다**.

### 직관적인 예시

강아지에게 새로운 재주를 가르친다고 상상해 보세요:

1. 강아지가 어떤 동작을 함(무작위일 수 있음)
2. 동작이 맞으면 간식을 줌(긍정적 보상)
3. 동작이 틀리면 간식을 주지 않거나 가볍게 "아니야"라고 말함(부정적 또는 0 보상)
4. 여러 번의 시도 후, 강아지는 어떤 동작이 보상을 가져오는지 배움

이것이 강화 학습의 본질입니다: **보상 신호를 통해 어떻게 행동할지 학습**.

### 바둑에서의 강화 학습 응용

바둑에서:
- 각 수는 하나의 '행동'
- 대국이 끝날 때, 승패가 '보상'
- AI는 학습해야 함: 어떤 수법이 최종적으로 승리로 이어지는가?

하지만 여기에는 거대한 도전이 있습니다: **보상 지연**. 한 대국은 200수 이상이 될 수 있지만, 마지막에야 승패를 알 수 있습니다. 50번째 수에서 둔 한 수가 최종 결과에 얼마나 기여했는지 어떻게 알 수 있을까요?

이것이 강화 학습의 가장 핵심적인 문제 중 하나이며, 이를 **신용 할당 문제(Credit Assignment Problem)**라고 합니다.

---

## 핵심 개념

### Agent(에이전트)와 Environment(환경)

강화 학습의 기본 구조에는 두 주인공이 있습니다:

```
        ┌─────────────────────────────────────┐
        │           Environment               │
        │                                     │
        │   ┌─────────┐      ┌─────────┐     │
        │   │  State  │      │ Reward  │     │
        │   │   s_t   │      │   r_t   │     │
        │   └────┬────┘      └────┬────┘     │
        │        │                │          │
        └────────┼────────────────┼──────────┘
                 │                │
                 ▼                ▼
        ┌─────────────────────────────────────┐
        │            Agent                    │
        │                                     │
        │         ┌──────────┐               │
        │         │  Policy  │               │
        │         │   π(s)   │               │
        │         └────┬─────┘               │
        │              │                     │
        │              ▼                     │
        │         ┌──────────┐               │
        │         │  Action  │               │
        │         │   a_t    │───────────────┼───► 환경으로 보냄
        │         └──────────┘               │
        └─────────────────────────────────────┘
```

**Agent(에이전트)**:
- 결정을 내리는 주체
- 바둑에서는 바둑을 두는 AI
- '정책'(Policy)을 가지고 있어 어떤 상태에서 어떤 행동을 할지 결정

**Environment(환경)**:
- Agent가 상호작용하는 대상
- 바둑에서는 바둑판 + 상대방
- Agent의 행동을 받고, 새로운 상태와 보상을 반환

### State(상태)

**상태 s**는 환경에 대한 완전한 설명입니다. 바둑에서:
- 상태에는 포함됨: 현재 바둑판 국면, 누구 차례인지, 패 상태 등
- 상태 공간은 매우 방대함: 약 $10^{170}$가지 가능한 상태

상태는 **마르코프 특성**을 가져야 합니다: 미래는 현재 상태에만 의존하고, 과거와는 무관합니다.

### Action(행동)

**행동 a**는 Agent가 취할 수 있는 행위입니다. 바둑에서:
- 각 빈칸이 가능한 행동
- '패스'를 포함하여 총 $19 \times 19 + 1 = 362$가지 행동
- 실제로 많은 위치가 불법(자살, 패 등)

### Reward(보상)

**보상 r**은 행동에 대한 환경의 피드백입니다. 바둑에서:
- 승리: $+1$
- 패배: $-1$
- 대국 중: $0$(이것이 가장 도전적인 부분!)

보상 신호의 희소성은 바둑 강화 학습의 주요 어려움 중 하나입니다.

### Policy(정책)

**정책 π**는 Agent의 행동 지침으로, 각 상태에서 무엇을 해야 하는지 알려줍니다.

정책은 다음과 같을 수 있습니다:
- **결정적 정책**: $a = \pi(s)$, 각 상태가 유일한 행동에 대응
- **확률적 정책**: $a \sim \pi(a|s)$, 행동의 확률 분포를 제공

AlphaGo에서 Policy Network는 확률적 정책으로, 각 위치의 착수 확률을 출력합니다.

---

## 마르코프 결정 과정(MDP)

### MDP의 정의

**마르코프 결정 과정(Markov Decision Process, MDP)**은 강화 학습의 수학적 프레임워크입니다.

MDP는 다섯 개의 튜플 $(S, A, P, R, \gamma)$로 정의됩니다:

| 기호 | 의미 | 바둑에서의 대응 |
|------|------|-------------|
| $S$ | 상태 공간 | 모든 가능한 바둑판 국면 |
| $A$ | 행동 공간 | 모든 합법적인 착수 위치 |
| $P(s'|s,a)$ | 전이 확률 | 다음 수 후의 국면 변화 |
| $R(s,a,s')$ | 보상 함수 | 승패 결과 |
| $\gamma$ | 할인 계수 | 미래 보상의 중요도 |

### 마르코프 특성

MDP의 핵심 가정은 **마르코프 특성(Markov Property)**입니다:

$$P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_0) = P(s_{t+1}|s_t, a_t)$$

쉽게 말하면: **미래는 현재에만 의존하고, 과거와는 무관합니다**.

바둑이 이 특성을 만족할까요?

표면적으로 보면 그렇습니다—현재 바둑판 상태만 알면 모든 합법적인 수를 알 수 있습니다. 그러나 실제로 바둑에는 **패 규칙**이 있어 이전 수의 상태를 기억해야 합니다. AlphaGo는 이전 8수의 바둑판을 입력 특징에 인코딩하여 이 문제를 처리합니다.

### 바둑은 결정적 MDP

바둑에는 특별한 특성이 있습니다: **전이가 결정적**입니다.

체스류 게임에서 한 수를 두면 바둑판 상태의 변화는 완전히 결정적입니다(주사위 게임과 달리 무작위성이 없음). 따라서:

$$P(s'|s,a) = \begin{cases} 1 & \text{만약 } s' \text{이 } a \text{를 실행한 후의 상태라면} \\ 0 & \text{그렇지 않으면} \end{cases}$$

하지만 바둑은 **2인 게임**이라는 것을 잊지 마세요. 상대방의 수는 '불확실성'을 가져옵니다. 이것은 문제를 **적대적 MDP**로 만듭니다.

### 보상 설계

보상 함수의 설계는 강화 학습에 매우 중요합니다. 바둑에서 가장 자연스러운 설계는:

$$R(s_T) = \begin{cases} +1 & \text{만약 AI가 승리하면} \\ -1 & \text{만약 AI가 패배하면} \end{cases}$$

여기서 $T$는 대국이 끝나는 시간 단계입니다.

이러한 **희소 보상**은 거대한 도전을 가져옵니다:
- 한 대국은 200-300수가 될 수 있음
- 마지막 수에서야 승패를 알 수 있음
- 중간의 어떤 수가 좋은지 나쁜지 어떻게 판단하는가?

일부 연구에서는 **밀집 보상**을 설계하려고 시도했습니다, 예를 들어:
- 돌 잡기 보상
- 영토 추정 보상
- 형세 판단 보상

그러나 AlphaGo의 성공은 다음을 보여주었습니다: **종국 승패만을 보상으로 사용해도, 충분한 자기 대국을 통해 AI는 정교한 중반 전술을 배울 수 있습니다**.

---

## 가치 함수

### 왜 가치 함수가 필요한가?

강화 학습의 목표는 **누적 보상**을 최대화하는 것입니다. 그러나 보상은 지연되므로, '현재 상태가 얼마나 좋은지'를 평가하는 방법이 필요합니다.

이것이 **가치 함수(Value Function)**의 역할입니다.

### 상태 가치 함수 V(s)

**상태 가치 함수** $V^\pi(s)$는 다음과 같이 정의됩니다: 상태 $s$에서 시작하여 정책 $\pi$를 따를 때 예상되는 누적 보상.

$$V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t r_{t+1} \mid s_0 = s \right]$$

여기서:
- $\mathbb{E}_\pi$는 정책 $\pi$ 하에서의 기댓값
- $\gamma \in [0, 1]$은 **할인 계수**로, 근접 보상이 원거리 보상보다 더 중요하게 함
- $r_{t+1}$은 시간 단계 $t+1$에서 얻는 보상

바둑에서 $V(s)$는 다음과 같이 해석할 수 있습니다: **현재 국면에서 시작하여 AI가 승리할 확률**. AlphaGo의 Value Network가 바로 이 함수를 학습합니다.

### 행동 가치 함수 Q(s,a)

**행동 가치 함수** $Q^\pi(s,a)$는 더 나아가 상태 $s$에서 행동 $a$를 취하는 것의 가치를 평가합니다:

$$Q^\pi(s,a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t r_{t+1} \mid s_0 = s, a_0 = a \right]$$

$Q(s,a)$는 다음과 같이 해석할 수 있습니다: **현재 국면에서 이 수를 둘 때, 최종적으로 승리할 확률**.

### V와 Q의 관계

이 두 함수는 밀접한 관계가 있습니다:

$$V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s,a)$$

즉, 상태 가치 = 모든 가능한 행동의 가중 평균, 가중치는 정책에 의해 결정됩니다.

최적 정책 $\pi^*$를 알고 있다면:

$$V^*(s) = \max_a Q^*(s,a)$$

최적 상태 가치 = 최적 행동의 Q 값.

### 벨만 방정식

가치 함수는 아름다운 재귀 관계를 만족합니다—**벨만 방정식(Bellman Equation)**:

$$V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \left[ R(s,a,s') + \gamma V^\pi(s') \right]$$

쉽게 말하면: **현재 상태의 가치 = 즉시 보상 + 할인된 다음 상태 가치**.

이 방정식은 동적 프로그래밍과 많은 강화 학습 알고리즘의 이론적 기반입니다.

### AlphaGo의 Value Network

AlphaGo에서 Value Network는 $V(s)$를 학습합니다—현재 국면의 승률 평가.

```
입력: 바둑판 상태 s(19×19×17의 특징 텐서)
출력: 승률 추정 V(s) ∈ [-1, 1](tanh 활성화 사용)
```

Value Network의 훈련 목표는 최종 결과를 예측하는 것입니다:

$$L = \mathbb{E} \left[ (V_\theta(s) - z)^2 \right]$$

여기서 $z \in \{-1, +1\}$은 대국의 실제 결과입니다.

---

## 정책 경사법

### 가치에서 정책으로

전통적인 강화 학습 방법(예: Q-Learning)은 '가치 기반'입니다: 먼저 가치 함수를 학습하고, 그로부터 정책을 도출합니다.

그러나 바둑처럼 행동 공간이 거대한 문제에서는 정책을 직접 학습하는 것이 더 효과적일 수 있습니다. 이것이 **정책 경사(Policy Gradient)** 방법의 아이디어입니다.

### 정책의 매개변수화

신경망을 사용하여 정책을 표현합니다:

$$\pi_\theta(a|s)$$

여기서 $\theta$는 네트워크 파라미터입니다. 네트워크는 상태 $s$를 입력받아 각 행동의 확률을 출력합니다.

AlphaGo에서 이것이 바로 Policy Network입니다:
- 입력: 바둑판 상태
- 출력: 361개 위치의 착수 확률(패스 포함)

### 정책 경사 정리

기대 누적 보상을 최대화하는 최적 파라미터 $\theta^*$를 찾고 싶습니다:

$$J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_t r_t \right]$$

**정책 경사 정리**는 $J$에 대한 $\theta$의 기울기를 계산하는 방법을 알려줍니다:

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t \right]$$

여기서 $G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k$는 시간 $t$부터의 누적 보상입니다.

### 직관적 이해

이 공식은 다음과 같이 이해할 수 있습니다:

1. **$\nabla_\theta \log \pi_\theta(a_t|s_t)$**: 행동 $a_t$의 확률을 높이기 위해 파라미터를 어떻게 조정할지
2. **$G_t$**: 이 행동이 가져오는 총 수익

따라서:
- $G_t > 0$(좋은 결과)이면, 이 행동의 확률을 높임
- $G_t < 0$(나쁜 결과)이면, 이 행동의 확률을 낮춤

이것이 **신용 할당**의 한 가지 해결책입니다!

### REINFORCE 알고리즘

**REINFORCE**는 가장 단순한 정책 경사 알고리즘입니다:

```
알고리즘: REINFORCE

1. 정책 네트워크 파라미터 θ 초기화

2. 반복:
   a. 현재 정책 π_θ로 한 판의 대국을 완료하고, 궤적 수집:
      τ = (s_0, a_0, r_1, s_1, a_1, r_2, ..., s_T)

   b. 각 단계의 누적 수익 계산:
      G_t = r_{t+1} + γ·r_{t+2} + γ²·r_{t+3} + ...

   c. 정책 경사 계산:
      ∇J = (1/T) Σ_t ∇_θ log π_θ(a_t|s_t) · G_t

   d. 파라미터 업데이트:
      θ ← θ + α · ∇J
```

바둑에서 이것은 다음을 의미합니다:
1. AI가 스스로 한 판을 둠
2. 최종적으로 승리($G = +1$)하면, 둔 모든 수의 확률을 높임
3. 최종적으로 패배($G = -1$)하면, 둔 모든 수의 확률을 낮춤
4. 이 과정을 수백만 번 반복

### 기준선(Baseline)

REINFORCE의 문제점 중 하나는 **분산이 크다**는 것입니다. 이긴 대국을 상상해 보세요. 그 안에 일부 나쁜 수도 있을 수 있지만, 그것들의 확률도 모두 높아집니다.

해결책은 **기준선(baseline)**을 도입하는 것입니다:

$$\nabla_\theta J = \mathbb{E} \left[ \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (G_t - b(s_t)) \right]$$

일반적인 선택은 $b(s_t) = V(s_t)$이며, 이것이 **어드밴티지 함수(Advantage Function)**입니다:

$$A(s_t, a_t) = G_t - V(s_t)$$

어드밴티지 함수는 측정합니다: "이 행동이 평균보다 얼마나 좋은가?"

- $A > 0$: 이 행동이 예상보다 좋음, 확률을 높임
- $A < 0$: 이 행동이 예상보다 나쁨, 확률을 낮춤

AlphaGo는 Value Network를 사용하여 기준선을 계산하며, 이것이 Policy Network와 Value Network를 동시에 훈련해야 하는 이유입니다.

---

## 탐색과 활용

### 딜레마

강화 학습은 고전적인 딜레마에 직면합니다: **탐색과 활용(Exploration vs. Exploitation)**.

- **활용(Exploitation)**: 현재 알고 있는 것에 따라 가장 좋아 보이는 행동 선택
- **탐색(Exploration)**: 불확실한 행동을 시도하여 더 좋은 전략을 발견할 수 있음

순수한 활용은 국소 최적에 빠지고, 순수한 탐색은 명백히 나쁜 수에 시간을 낭비합니다.

### 바둑에서의 도전

바둑에서 이 문제는 특히 심각합니다:

1. **행동 공간이 거대함**: 361가지 가능한 착수
2. **보상이 희소함**: 종국에만 좋고 나쁨을 알 수 있음
3. **장기적 영향**: 한 수의 영향이 수십 수 후에야 나타날 수 있음

### ε-Greedy 전략

가장 단순한 탐색 방법:

$$\pi(a|s) = \begin{cases} 1 - \varepsilon + \frac{\varepsilon}{|A|} & \text{만약 } a = \arg\max Q(s,a) \\ \frac{\varepsilon}{|A|} & \text{그렇지 않으면} \end{cases}$$

$1-\varepsilon$의 확률로 최적 행동을 선택하고, $\varepsilon$의 확률로 무작위 선택합니다.

그러나 이것은 바둑에 너무 조잡합니다—무작위로 위치를 선택하면 대부분 나쁜 수입니다.

### Softmax 탐색

더 좋은 방법은 **softmax 분포**를 사용하는 것입니다:

$$\pi(a|s) = \frac{\exp(Q(s,a)/\tau)}{\sum_{a'} \exp(Q(s,a')/\tau)}$$

여기서 $\tau$는 **온도 파라미터**입니다:
- $\tau \to 0$: 탐욕 정책에 가까움(순수 활용)
- $\tau \to \infty$: 균등 무작위에 가까움(순수 탐색)
- $\tau = 1$: 탐색과 활용의 균형

AlphaGo는 자기 대국 훈련에서 다양성을 높이기 위해 비슷한 기술을 사용합니다.

### UCB와 PUCT

MCTS에서 탐색과 활용은 **UCB(Upper Confidence Bound)** 공식으로 처리됩니다. AlphaGo가 사용하는 것은 그 변형인 **PUCT**입니다:

$$\text{score}(s,a) = Q(s,a) + c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,a)}$$

이 공식은 [PUCT 공식 상세 설명](../puct-formula)에서 자세히 설명됩니다.

### 본질적 탐색(Intrinsic Exploration)

AlphaGo에는 또 다른 암묵적 탐색 메커니즘이 있습니다: **자기 대국 자체가 탐색입니다**.

신경망이 확정적 행동이 아닌 확률 분포를 출력하기 때문에, 각 자기 대국은 다른 대국을 생성합니다. 이것은 자연스럽게 다음을 가져옵니다:

- **전술 다양성**: 같은 국면에서 다른 수를 시도할 수 있음
- **스타일 진화**: 훈련에 따라 AI가 인간이 시도한 적 없는 정석을 '발견'할 수 있음
- **자기 수정**: 어떤 수법이 항상 지면, 확률이 점차 낮아짐

---

## 바둑 강화 학습의 특수성

### 다른 분야와의 비교

바둑 강화 학습에는 몇 가지 독특한 특성이 있습니다:

| 특성 | 바둑 | 로봇 제어 | 비디오 게임 |
|------|------|-----------|----------|
| 상태 공간 | 이산적, 매우 큼 | 연속적 | 이산적, 중간 |
| 행동 공간 | 이산적, 큼 | 연속적 | 이산적, 작음 |
| 전이 | 결정적 | 확률적 | 결정적 또는 확률적 |
| 보상 | 매우 희소 | 설계 가능 | 중간 밀도 |
| 환경 모델 | 알려짐(규칙) | 알려지지 않음 | 부분적으로 알려짐 |
| 적대성 | 완전 정보 게임 | 보통 없음 | 있을 수 있음 |

### 결정적 전이

바둑의 규칙은 완전히 알려져 있습니다. 한 수를 두면 다음 상태는 결정적입니다. 이것은 다음을 의미합니다:

- **정확하게 시뮬레이션 가능**: 환경 모델을 학습할 필요 없음
- **완벽하게 되돌릴 수 있음**: MCTS가 정확하게 검색 가능
- **환경 무작위성 처리 불필요**: 많은 문제가 단순화됨

### 완전 정보

바둑은 **완전 정보 게임**입니다—양측 모두 전체 바둑판을 볼 수 있습니다. 이것은 포커(숨겨진 정보)와 다르며, 문제를 어떤 면에서는 더 단순하게 만듭니다:

- 상대방의 숨겨진 정보를 처리할 필요 없음
- Minimax 프레임워크 사용 가능
- 상태 표현이 더 직접적

### 자기 대국의 가능성

규칙이 알려져 있고 결정적이기 때문에, AI는 실제 상대 없이 **자신과 대국**할 수 있습니다. 이것은 다음을 가져옵니다:

- **무한한 훈련 데이터**: 언제든지 새로운 대국 생성 가능
- **안정적인 상대 수준**: 상대가 바로 자신이므로 수준이 비슷함
- **점진적 향상**: 자신이 강해지면 상대도 강해짐

이것이 바로 AlphaGo 성공의 핵심이며, 다음 글 [자기 대국](../self-play)에서 자세히 논의합니다.

### 장기 신용 할당

바둑의 보상은 매우 희소하며(종국 승패만), 한 대국은 200-300수가 될 수 있습니다. 이것은 심각한 **신용 할당 문제**를 가져옵니다:

50번째 수의 좋은 수가 250번째 수에서 승리할 때, 어떻게 정확하게 공로를 분배하는가?

AlphaGo의 해결책은 여러 기술의 결합입니다:
1. **Value Network**: 중간 국면의 승률 평가, 즉시 피드백 제공
2. **MCTS**: 각 수의 좋고 나쁨을 검색으로 검증
3. **많은 대국**: 통계를 통해 신용 할당 학습

### 대칭성

바둑판에는 8중 대칭성(4개 회전 × 2개 반전)이 있습니다. AlphaGo는 이것을 활용하여 **데이터 증강**을 합니다:

- 각 훈련 국면이 8개의 변형을 생성할 수 있음
- 유효 훈련 데이터를 크게 증가
- 네트워크가 대칭성 불변 특징을 학습하도록 보장

---

## 알고리즘 비교

### 가치 기반 vs 정책 기반

| 방법 | 장점 | 단점 | 적합한 시나리오 |
|------|------|------|---------|
| **가치 기반** (Q-Learning) | 샘플 효율성 높음 | 큰 행동 공간 처리 어려움 | 행동 공간 작음 |
| **정책 기반** (REINFORCE) | 큰 행동 공간 처리 가능 | 분산 큼, 샘플 효율성 낮음 | 행동 공간 큼 |
| **Actor-Critic** | 둘의 균형 | 두 네트워크 동시 훈련 필요 | 일반적으로 강함 |

### AlphaGo의 선택

AlphaGo가 사용하는 것은 **Actor-Critic** 아키텍처의 변형입니다:

- **Policy Network**(Actor): 행동 확률 직접 출력
- **Value Network**(Critic): 상태 가치 평가

그러나 전통적인 Actor-Critic 업데이트 방식을 사용하지 않고:

1. **지도 학습**: 먼저 인간 기보에서 초기 Policy Network 학습
2. **정책 경사**: 자기 대국을 통해 Policy Network 강화
3. **회귀 학습**: 자기 대국 데이터로 Value Network 훈련
4. **MCTS 통합**: 실제 대국에서 두 네트워크 결합

이 혼합 방법은 여러 기술의 장점을 결합하며, AlphaGo 성공의 핵심 중 하나입니다.

---

## 구현 고려사항

### 훈련 안정성

정책 경사 방법은 때때로 불안정합니다. 일반적인 기술은 다음과 같습니다:

**기울기 클리핑(Gradient Clipping)**:
```python
# 기울기의 노름 제한
max_grad_norm = 0.5
torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_grad_norm)
```

**학습률 감소**:
```python
# 훈련이 진행됨에 따라 학습률 낮추기
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)
```

**PPO/TRPO 등 고급 알고리즘**:
각 업데이트의 정책 변화를 제한하여 치명적인 망각을 방지합니다.

### 메모리 관리

바둑 대국은 길어서 많은 궤적을 저장해야 합니다. 일반적인 전략:

**경험 리플레이(Experience Replay)**:
```python
# 과거 경험 저장
replay_buffer = ReplayBuffer(max_size=1000000)

# 무작위 샘플링으로 훈련
batch = replay_buffer.sample(batch_size=256)
```

**우선순위 경험 리플레이**:
'의외의' 경험(TD 오류가 큰)을 우선적으로 리플레이합니다.

### 병렬화

강화 학습은 고도로 병렬화될 수 있습니다:

- **다중 스레드 대국**: 동시에 여러 대국 진행
- **분산 훈련**: 여러 기기에서 동시에 훈련
- **비동기 업데이트**: A3C 등의 알고리즘

AlphaGo의 훈련은 수백 개의 GPU와 TPU를 사용하여 동시에 수천 판의 자기 대국을 진행했습니다.

---

## 애니메이션 대응

이 글에서 다루는 핵심 개념과 애니메이션 번호:

| 번호 | 개념 | 물리/수학 대응 |
|------|------|--------------|
| H1 | Agent-Environment 상호작용 | 마르코프 체인 |
| H4 | 정책 경사 | 확률적 최적화 |
| H6 | 탐색과 활용 | 다중 슬롯 머신 |

---

## 요약

강화 학습은 AlphaGo가 인간을 초월하는 핵심 기술입니다. 우리가 배운 것:

1. **기본 프레임워크**: Agent, Environment, State, Action, Reward
2. **MDP**: 마르코프 결정 과정, 강화 학습의 수학적 기반
3. **가치 함수**: $V(s)$와 $Q(s,a)$, 상태와 행동의 좋고 나쁨 평가
4. **정책 경사**: 정책을 직접 최적화하는 방법, REINFORCE 알고리즘
5. **탐색과 활용**: 학습 과정에서의 핵심 트레이드오프
6. **바둑 특성**: 결정성, 완전 정보, 희소 보상의 도전과 기회

다음 글에서는 AlphaGo가 **자기 대국**을 사용하여 인간을 초월하는 기력을 달성하는 방법을 깊이 탐구합니다.

---

## 추가 자료

- **다음 글**: [자기 대국](../self-play) — 왜 AI가 자신과 대국하여 강해질 수 있는가
- **관련**: [Value Network 상세 설명](../value-network) — 가치 함수의 신경망 구현
- **고급**: [PUCT 공식 상세 설명](../puct-formula) — 탐색과 활용의 수학 공식

---

## 참고 자료

1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.
2. Silver, D. (2015). ["Lectures on Reinforcement Learning"](https://www.davidsilver.uk/teaching/). University College London.
3. Schulman, J., et al. (2017). "Proximal Policy Optimization Algorithms." *arXiv preprint*.
4. Williams, R. J. (1992). "Simple statistical gradient-following algorithms for connectionist reinforcement learning." *Machine Learning*, 8(3-4), 229-256.
5. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
