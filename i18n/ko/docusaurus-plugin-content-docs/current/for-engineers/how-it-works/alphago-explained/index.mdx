---
sidebar_position: 1
title: AlphaGo 완전 해설
description: 역사적 배경부터 기술적 세부 사항까지, 20편의 글로 AlphaGo를 완벽하게 이해합니다
---

import { PolicyHeatmap, EloChart } from '@site/src/components/D3Charts';

# AlphaGo 완전 해설

2016년 3월, AlphaGo가 4:1로 세계 챔피언 이세돌을 꺾으며 전 세계를 충격에 빠뜨렸습니다. 이것은 단순한 바둑 경기의 승리가 아니라, 인공지능의 중대한 돌파구를 의미했습니다.

본 시리즈 **20편의 심층 글**에서는 역사적 배경, 기술 원리, 구현 세부 사항까지 AlphaGo의 모든 것을 완벽하게 이해할 수 있도록 안내합니다.

---

## 시리즈 안내

### 모듈 1: 역사와 돌파구

| 글 | 설명 |
|------|------|
| [AlphaGo의 탄생](./birth-of-alphago) | DeepMind 설립, Google 인수, 팀 구성 |
| [주요 대국 회고](./key-matches) | 판후이, 이세돌, 커제, Master 60연승 |
| ["신의 한 수" 심층 분석](./move-37) | 37수의 기리와 AI 관점 해석 |

### 모듈 2: 바둑의 도전

| 글 | 설명 |
|------|------|
| [바둑은 왜 어려운가?](./why-go-is-hard) | 상태 공간 10^170, 분기 계수 ~250 |
| [전통적 방법의 한계](./traditional-limits) | Minimax, Alpha-Beta, 순수 MCTS |
| [바둑판 상태 표현](./board-representation) | Zobrist Hashing, Union-Find, 특징 인코딩 |

### 모듈 3: 신경망 핵심

| 글 | 설명 |
|------|------|
| [Policy Network 상세 해설](./policy-network) | 아키텍처, Softmax 출력, 훈련 목표 |
| [Value Network 상세 해설](./value-network) | 아키텍처, Tanh 출력, 과적합 방지 |
| [입력 특징 설계](./input-features) | 48→17개 특징 평면의 진화 |
| [CNN과 바둑의 결합](./cnn-and-go) | CNN이 바둑판에 적합한 이유 |
| [지도학습 단계](./supervised-learning) | KGS 데이터셋, 57% 예측 정확도 |

### 모듈 4: 강화학습과 탐색

| 글 | 설명 |
|------|------|
| [강화학습 입문](./reinforcement-intro) | MDP, 정책 경사법, 가치 함수 |
| [자가 대국](./self-play) | 효과적인 이유, ELO 성장 곡선 |
| [MCTS와 신경망의 결합](./mcts-neural-combo) | Selection→Expansion→Evaluation→Backup |
| [PUCT 공식 상세 해설](./puct-formula) | 수학적 유도, 탐색 vs 활용 |

### 모듈 5: AlphaGo Zero 진화

| 글 | 설명 |
|------|------|
| [AlphaGo Zero 개요](./alphago-zero) | 인간 기보가 필요 없는 이유 |
| [듀얼 헤드 네트워크와 잔차 네트워크](./dual-head-resnet) | 공유 표현, 그래디언트 흐름, 40층 ResNet |
| [제로부터 훈련 과정](./training-from-scratch) | Day 0-3의 변화, 3일 만에 인간 초월 |

### 모듈 6: 기술 세부 사항과 확장

| 글 | 설명 |
|------|------|
| [분산 시스템과 TPU](./distributed-systems) | 훈련 아키텍처, 추론 아키텍처, 병렬 MCTS |
| [AlphaGo의 유산](./legacy-and-impact) | 바둑계 영향, AlphaZero, MuZero, AlphaFold |

---

## 빠른 미리보기

### Policy Network 출력 예시

Policy Network는 각 위치의 착수 확률을 출력합니다:

<PolicyHeatmap initialPosition="corner" size={400} />

### 훈련 곡선

AlphaGo Zero는 3일 만에 제로에서 시작하여 인간을 초월했습니다:

<EloChart mode="zero" width={600} height={350} />

---

## 읽기 가이드

### 배경에 따른 시작점 선택

| 당신의 배경 | 추천 시작점 |
|---------|---------|
| **완전 초보자** | [AlphaGo의 탄생](./birth-of-alphago)부터 순서대로 읽기 |
| **바둑을 아는 분** | [바둑은 왜 어려운가?](./why-go-is-hard)부터 시작 |
| **머신러닝 기초가 있는 분** | [Policy Network 상세 해설](./policy-network)부터 시작 |
| **핵심만 빠르게 이해하고 싶은 분** | [MCTS와 신경망의 결합](./mcts-neural-combo) 읽기 |
| **Zero의 돌파구를 알고 싶은 분** | [AlphaGo Zero 개요](./alphago-zero)부터 시작 |

### 예상 읽기 시간

- **전체 읽기**: 약 8-10시간
- **빠른 훑어보기**: 약 2-3시간
- **각 글당**: 약 15-25분

---

## 애니메이션 대응

본 시리즈 글은 [109개 애니메이션 개념](../concepts/)의 다음 시리즈를 참조합니다:

| 시리즈 | 주제 | 관련 글 |
|------|------|---------|
| **C 시리즈** | 몬테카를로 방법 | #5, #14, #15 |
| **D 시리즈** | 신경망 | #7, #8, #10, #11 |
| **E 시리즈** | AlphaGo 아키텍처 | #13, #16, #17, #18 |
| **H 시리즈** | 강화학습 | #12, #13 |

---

## 참고 자료

### 논문

1. Silver, D., et al. (2016). ["Mastering the game of Go with deep neural networks and tree search."](https://www.nature.com/articles/nature16961) *Nature*.
2. Silver, D., et al. (2017). ["Mastering the game of Go without human knowledge."](https://www.nature.com/articles/nature24270) *Nature*.
3. Silver, D., et al. (2018). ["A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play."](https://www.science.org/doi/10.1126/science.aar6404) *Science*.

### 추가 읽기

- [KataGo의 핵심 혁신](../katago-innovations) — 더 적은 리소스로 더 강한 기력 달성 방법
- [개념 빠른 참조표](../concepts/) — 109개 애니메이션 개념 전체 목록
- [30분 만에 첫 바둑 AI 실행하기](../../hands-on/) — 직접 실습
