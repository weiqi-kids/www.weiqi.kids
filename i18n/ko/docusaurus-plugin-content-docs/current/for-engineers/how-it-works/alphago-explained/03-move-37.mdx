---
sidebar_position: 4
title: '"신의 한 수" 심층 분석'
description: 바둑 이론, 전문가 반응, AI 관점에서 AlphaGo 대 이세돌 2국의 37수를 완전 분석
---

import { PolicyHeatmap } from '@site/src/components/D3Charts';

# "신의 한 수" 심층 분석

2016년 3월 10일, AlphaGo와 이세돌의 2국. 37수, AlphaGo가 우상단 5선에 어깨짚기를 두었습니다.

이 한 수는 나중에 "**신의 한 수**"(Divine Move)라고 불리게 되었습니다. 이 수는 AlphaGo의 승리뿐만 아니라 인류의 바둑 이해를 근본적으로 바꾸었습니다.

본 문서에서는 이 수를 여러 관점에서 심층 분석합니다: 대국 배경, 전통 바둑 이론, 전문가 반응, AI 관점, 그리고 바둑 이론에 대한 장기적 영향.

---

## 대국 국면 회고

### 2국의 포석

1국 패배 후, 이세돌은 2국에서 전략을 조정했습니다. 그는 백을 잡고 후수를 선택하여 AlphaGo의 포석 성향을 관찰한 후 전략을 수립하고자 했습니다.

포석 단계:
- **흑 1**: 우상 화점
- **백 2**: 좌하 화점
- **흑 3-백 4**: 양측 각자 귀 차지

36수까지 국면은 정상적으로 전개되었습니다. AlphaGo가 흑을 잡고 우상귀에서 국부 전투를 진행했습니다. 백(이세돌)은 우변에 세력을 구축했고, 흑은 상변에 일정한 실리 잠재력을 가지고 있었습니다.

### 36수 후의 국면

36수 후의 바둑판 상태를 살펴보겠습니다:

```
     A B C D E F G H J K L M N O P Q R S T
 19  . . . . . . . . . . . . . . . . . . .
 18  . . . . . . . . . . . . . . . . . . .
 17  . . . ○ . . . . . . . . . . . ● . . .
 16  . . . ╋ . . . . . ╋ . . . . . ╋ . . .
 15  . . . . . . . . . . . . . . . ● . . .
 14  . . . . . . . . . . . . . . ○ . . . .  ← 백 세력 범위
 13  . . . . . . . . . . . . . . . . . . .
 12  . . . . . . . . . . . . . . . . . . .
 11  . . . . . . . . . . . . . . . . . . .
 10  . . . ╋ . . . . . ╋ . . . . . ╋ . . .
  9  . . . . . . . . . . . . . . . . . . .
  8  . . . . . . . . . . . . . . . . . . .
  7  . . . . . . . . . . . . . . . . . . .
  6  . . . . . . . . . . . . . . . . . . .
  5  . . . . . . . . . . . . . . . . . . .
  4  . . . ╋ . . . . . ╋ . . . . . ╋ . . .
  3  . . . ○ . . . . . . . . . . . ● . . .
  2  . . . . . . . . . . . . . . . . . . .
  1  . . . . . . . . . . . . . . . . . . .
```

(단순화된 도해, 실제 국면은 더 복잡함)

핵심 관찰:
- 백은 우변에 외세가 있음
- 흑은 상변에 실리 잠재력이 있음
- 우상귀 전투는 일단락됨

이때, 흑(AlphaGo)의 차례였습니다.

---

## 전통 수법 분석

### 프로 기사들의 예상

37수 전, 해설실의 프로 기사들은 열띤 토론을 하고 있었습니다. 그들은 대체로 흑이 다음과 같은 수법을 선택할 것으로 예상했습니다:

**선택 A: 우하귀 걸침**

이것이 가장 "정상적인" 선택입니다. 흑은:
- 마지막 대장(우하귀) 선점
- 국면 균형 유지
- "귀는 금, 변은 은, 중앙은 풀"이라는 전통적 가치관 준수

**선택 B: 상변 집짓기**

흑은 상변에 2칸 또는 3칸 벌려두어 자신의 세력권을 공고히 할 수도 있습니다. 이렇게 하면:
- 상변의 잠재력을 실리로 전환
- 백의 발전 공간 제한

**선택 C: 중앙 분투**

일부 기사들은 흑이 중앙에 한 수 두어 백의 우변 외세를 제약할 수 있다고 생각했습니다. 가장 일반적인 선택은 아니지만 전략적으로도 타당합니다.

🎬 C3: 전통 바둑 이론의 가치 판단

### 아무도 예상하지 못한 선택

그러나 AlphaGo는 거의 아무도 생각하지 못한 위치를 선택했습니다:

**E5 (5선 어깨짚기)**

이 수는 바둑판 우반부, 중앙에 가까운 위치에 놓여 백의 우변 외세에 대한 "어깨짚기"였습니다.

---

## 37수: 5선 어깨짚기

### 이 수는 어디에?

```
     A B C D E F G H J K L M N O P Q R S T
 19  . . . . . . . . . . . . . . . . . . .
 18  . . . . . . . . . . . . . . . . . . .
 17  . . . ○ . . . . . . . . . . . ● . . .
 16  . . . ╋ . . . . . ╋ . . . . . ╋ . . .
 15  . . . . . . . . . ★ . . . . . ● . . .  ← 37수 (★)
 14  . . . . . . . . . . . . . . ○ . . . .
 13  . . . . . . . . . . . . . . . . . . .
 12  . . . . . . . . . . . . . . . . . . .
```

37수는 **K15** (또는 J5, 좌표 체계에 따라 다름) 위치에 두어졌습니다.

### "어깨짚기"란?

"어깨짚기"는 바둑의 수읽기 중 하나로, 상대 돌에 비스듬하게 접근하는 수법입니다. 그 특징은:

- **직접 접촉하지 않음**: 상대 돌과 한 칸 거리 유지
- **구조 파괴**: 상대의 예상 전개를 방해
- **대응 곤란**: 상대가 어떻게 대응하든 어떤 대가가 발생

전통적으로 어깨짚기는 보통 3선이나 4선에 둡니다. **5선 어깨짚기**는 매우 드문데, 그 이유는:

1. **위치가 너무 높음**: 5선은 중앙에 가까워 전통적으로 효율이 낮다고 여겨짐
2. **공격받기 쉬움**: 고립된 돌은 상대의 공격 대상이 되기 쉬움
3. **가치 불명확**: 귀나 변처럼 명확한 실리 가치가 없음

🎬 C5: 어깨짚기의 기하학적 특성

---

## 전문가 즉각 반응

### 해설실의 충격

37수가 놓인 순간, 해설실은 짧은 침묵에 빠졌습니다.

**한국 해설(김성룡 9단)**:

> "이...이게 뭐지? 이 수가 5선에? 이해가 안 됩니다. 이건 분명 실수일 거예요?"

**중국 해설(구리 9단)**:

> "이 수를 이해할 수 없습니다. 내 제자가 이렇게 두면 호되게 꾸짖을 겁니다."

**미국 해설(마이클 레드먼드 9단)**:

> "Very unusual move. I don't think any human would play this."
>
> (매우 이례적인 수입니다. 어떤 인간도 이렇게 두지 않을 것입니다.)

### 프로 기사들의 실시간 코멘트

각종 라이브 플랫폼에서 프로 기사들이 코멘트를 달았습니다:

**커제** (당시 세계 랭킹 1위):

> "이 수의 의도를 이해할 수 없습니다. AlphaGo가 이기면 진지하게 연구하겠습니다."

**박정환** (한국 최고 기사):

> "이 수는 너무 이상합니다. 프로그램에 문제가 생긴 건 아닐까요?"

**미위팅** (중국 세계 챔피언):

> "5선 어깨짚기? 이런 수법은 본 적이 없습니다."

🎬 C7: 전문가 직관과 AI 평가의 격차

### "만분의 일 확률"

대국 후, DeepMind 팀은 놀라운 데이터를 공개했습니다:

> "우리 분석에 따르면, 프로 기사가 같은 국면에서 37수 위치를 선택할 확률은 약 **만분의 일**입니다."

다시 말해, 인류의 바둑 지식 체계에서 이 수는 거의 "존재하지 않는" 선택지였습니다.

---

## AI 관점에서의 해석

### Policy Network의 확률 분포

AlphaGo의 Policy Network가 이 국면을 어떻게 평가했는지 살펴보겠습니다:

<PolicyHeatmap initialPosition="move37" size={450} showTopN={5} />

위 그림은 AlphaGo의 각 위치별 착점 확률 평가를 보여줍니다.

핵심 관찰:
- **37수 위치**: 확률 약 8%, 가장 높지 않음
- **전통적 선점** (우하귀 등): 확률 약 12%
- **기타 후보 위치**: 여러 영역에 분산

흥미롭게도 37수는 Policy Network 평가에서 **확률이 가장 높은 선택이 아니었습니다**. 그렇다면 AlphaGo는 왜 이것을 선택했을까요?

🎬 C9: Policy Network의 출력 분포

### MCTS의 심층 평가

답은 **몬테카를로 트리 탐색(MCTS)**에 있습니다.

Policy Network는 "직관"만 제공하고, 실제 결정은 MCTS의 심층 시뮬레이션에서 나옵니다. AlphaGo는 결정을 내리기 전에 수천 가지 가능한 미래 전개를 시뮬레이션합니다.

37수에 대한 MCTS 평가 과정:

```
위치 K15 (37수):
├── 시뮬레이션 1: 흑 승 (+0.3)
├── 시뮬레이션 2: 흑 승 (+0.5)
├── 시뮬레이션 3: 흑 승 (+0.2)
├── ...
└── 평균 승률: 58%

위치 R3 (우하귀 걸침):
├── 시뮬레이션 1: 흑 승 (+0.1)
├── 시뮬레이션 2: 백 승 (-0.2)
├── 시뮬레이션 3: 흑 승 (+0.2)
├── ...
└── 평균 승률: 52%
```

우하귀의 "직관적 확률"은 더 높았지만, 심층 시뮬레이션 후 37수의 **예상 승률이 더 높았습니다**.

🎬 C11: MCTS가 Policy Network 판단을 수정하는 방법

### Value Network의 전체 평가

Value Network는 전체 관점에서 37수의 가치를 평가했습니다:

**37수 전 승률**: 약 52% (흑 약간 우세)

**37수 후 승률**: 약 58% (흑 명확한 우세)

이는 37수가 AlphaGo의 예상 승률을 **6%포인트** 올렸다는 것을 의미합니다.

이 상승폭은 바둑에서 상당히 큰 것입니다. 보통 좋은 수가 2-3%의 승률 상승을 가져오면 이미 좋은 것입니다.

🎬 C13: Value Network의 증분 평가

---

## 바둑 이론 분석: 왜 5선 어깨짚기인가?

### 국부적 관점

표면적으로 37수는 효율이 매우 낮아 보입니다:

- **위치가 너무 높음**: 5선은 4선이나 3선보다 중앙에 더 가까움
- **실리 없음**: 귀나 변처럼 직접 실리를 만들 수 없음
- **공격받기 쉬움**: 고립된 돌은 백의 공격을 받을 수 있음

하지만 자세히 분석하면, 이 수에는 몇 가지 미묘한 이점이 있습니다:

1. **백의 외세 파괴**: 백은 원래 우변에서 발전하려 했는데, 37수가 이 계획을 방해함
2. **자신의 영향력 구축**: 이 수는 실리를 만들지 않지만 중앙에 존재감을 확립함
3. **변화 증가**: 복잡한 국면을 만들어 계산 능력이 더 강한 쪽에 유리함

### 전체적 관점

이 수의 진정한 가치는 전체 관점에서 이해해야 합니다:

**두터움과 실리의 균형**

전통 바둑 이론은 "귀는 금, 변은 은, 중앙은 풀"이라고 합니다—귀가 가장 가치 있고 중앙이 가장 가치 없다고. 하지만 37수는 이 관념에 도전했습니다.

AlphaGo의 평가에 따르면: 이 특정 국면에서 **중앙의 영향력이 귀나 변의 실리보다 더 가치 있습니다**.

그 이유는:
- 흑은 이미 충분한 실리 기반이 있음
- 백의 우변 외세가 발전하면 매우 강력해질 것임
- 자신을 확장하는 것보다 백을 제약하는 것이 더 중요함

🎬 C15: 전체 가치 함수 계산

**"선수"의 가치**

37수에는 과소평가된 또 다른 이점이 있습니다: "선수"를 유지한다는 것입니다.

바둑에서 "선수"는 주도권을 장악한다는 의미입니다. 37수를 둔 후 백은 대응해야 하고, 이로 인해 흑이 계속 국면의 흐름을 주도할 수 있습니다.

흑이 "정상적인" 우하귀 걸침을 선택했다면, 양측은 귀에서 정석을 진행하고 국면은 균형을 향했을 것입니다. 하지만 37수는 이 균형을 깨고 국면을 불확실성으로 가득 채웠습니다—그리고 이것이 바로 AlphaGo의 강점입니다.

### 이세돌의 대응 딜레마

37수 후, 이세돌은 오랫동안 생각했습니다. 그가 직면한 딜레마는:

**직접 대응하면 (예: 날일자나 비상)**:
- 37수의 가치를 인정하는 것
- 흑이 백의 외세 파괴 목적을 달성하게 함

**무시하면**:
- 흑이 중앙을 더 발전시킬 수 있음
- 백의 우변 외세가 실리로 전환되기 어려움

결국 이세돌은 대응을 선택했습니다. 하지만 무엇을 선택하든 37수는 이미 목적을 달성했습니다.

🎬 C17: 게임 이론에서의 강제 선택

---

## 후속 전개: 37수에서 승리까지

### 중반의 진행

37수 후 대국은 복잡한 중반 전투로 들어갔습니다.

**핵심 진행**:

- **40-50수**: 양측이 우변에서 격렬한 접촉전 진행
- **50-70수**: AlphaGo가 37수로 구축한 영향력을 활용해 중앙에서 우세 확보
- **70-100수**: 흑이 점차 우세를 실리로 전환

100수 무렵, AlphaGo의 리드는 이미 상당히 명확했습니다. 이세돌이 노력해서 반격했지만 국면을 뒤집을 수 없었습니다.

### 최종 결과

**AlphaGo 중반 불계승**

이 대국의 승리에 37수가 크게 기여했습니다. 대국 후 분석에 따르면, 37수가 없었다면 국면은 더 접근했을 것이고 백이 오히려 우세를 잡았을 수도 있습니다.

🎬 C19: 한 수가 전체 대국의 흐름을 바꾸는 방법

---

## 바둑 이론에 대한 영향

### 새로운 정석의 탄생

37수는 바둑계에서 "어깨짚기"라는 수법에 대한 재고를 촉발했습니다.

**전통적 관점**:
- 어깨짚기는 3선이나 4선에 두어야 함
- 5선 어깨짚기는 효율이 너무 낮음
- 고립된 돌은 공격받기 쉬움

**AlphaGo 이후**:
- 5선 어깨짚기가 특정 국면에서는 최선의 선택
- 위치의 "고저"보다 "효과"가 중요
- 매 수의 가치를 전체 관점에서 평가해야 함

### 인간 기사들의 학습

37수 이후 많은 프로 기사들이 비슷한 수법을 시도하기 시작했습니다:

**커제**는 2017년 여러 대국에서 5선 어깨짚기를 사용하여 성공했습니다:

> "AlphaGo가 가르쳐준 것은, 우리가 '나쁘다'고 생각하던 많은 수가 사실 우리가 이해하지 못한 것뿐이라는 것입니다."

**박정환**도 자신의 대국에서 이런 사고방식을 참고했습니다:

> "중요한 것은 37수라는 구체적인 위치를 기억하는 게 아니라, 새로운 시각으로 바둑판을 보는 것을 배우는 것입니다."

🎬 C21: AI가 인류의 인지 경계를 확장하는 방법

### 바둑 AI 훈련에 대한 시사점

37수는 바둑 AI 연구에도 심원한 영향을 미쳤습니다:

**Policy Network에 대한 반성**:

왜 Policy Network는 37수에 낮은 확률을 주었을까요? 인간 기보에서 학습했기 때문이고, 인간은 거의 이런 수를 두지 않기 때문입니다.

이것은 다음을 말해줍니다: **지도학습(인간에게 배우는 것)만으로는 충분하지 않습니다**. AI는 자기 탐색을 통해 인간이 모르는 좋은 수를 발견해야 합니다.

이것이 나중에 **AlphaGo Zero**가 순수 자기대국 훈련을 채택한 이유 중 하나입니다.

**MCTS에 대한 확인**:

37수는 MCTS 심층 탐색의 가치를 증명했습니다. 직관(Policy Network)이 어떤 수를 좋게 보지 않더라도, 심층 분석이 그 잠재적 가치를 발견할 수 있습니다.

이 통찰은 나중에 많은 다른 분야에 적용되었습니다.

---

## 기술적 세부사항: 37수의 의사결정 과정 재현

### Policy Network의 입력 특징

36수 후, Policy Network의 입력은 다음을 포함합니다:

| 특징 평면 | 설명 |
|----------|------|
| 1-8 | 흑돌 위치 (과거 8수) |
| 9-16 | 백돌 위치 (과거 8수) |
| 17 | 현재 누구 차례 |
| 18-48 | 기타 특징 (활로수, 단수 등) |

총 **48개의 19x19 특징 평면**이 입력 텐서를 구성합니다.

🎬 C23: AI 바둑에서 특징 공학의 중요성

### Policy Network의 출력

Policy Network는 **19x19 = 361** 차원의 확률 분포를 출력합니다.

37수 국면에 대해:

```python
# 상위 5개 후보 위치 (단순화된 예시)
{
    "R3": 0.12,   # 우하귀 걸침
    "Q17": 0.10,  # 우상귀
    "C10": 0.09,  # 좌변 대장
    "K15": 0.08,  # 37수 위치
    "D16": 0.07,  # 좌상귀
    # ... 기타 356개 위치
}
```

### MCTS의 탐색 과정

AlphaGo는 PUCT 공식을 사용하여 탐색과 활용의 균형을 맞춥니다:

```
U(s,a) = Q(s,a) + c_puct × P(s,a) × sqrt(sum_b N(s,b)) / (1 + N(s,a))
```

여기서:
- `Q(s,a)`: 위치 a의 평균 가치
- `P(s,a)`: Policy Network가 제공한 확률
- `N(s,a)`: 해당 위치가 탐색된 횟수
- `c_puct`: 탐색 상수

37수의 경우, 초기 확률 P가 낮았지만 여러 번의 시뮬레이션 후 Q 값이 계속 상승하여 결국 다른 후보 위치를 초과했습니다.

🎬 C25: PUCT 공식이 비직관적인 좋은 수를 발견하는 방법

### 시뮬레이션 횟수의 영향

DeepMind 팀은 나중에 37수의 "발견"에는 충분한 시뮬레이션 횟수가 필요하다고 분석했습니다:

| 시뮬레이션 횟수 | 최선의 선택 |
|----------------|------------|
| 100 | R3 (우하귀) |
| 1,000 | Q17 (우상귀) |
| 10,000 | K15 (37수) |
| 100,000 | K15 (더 확실) |

이것은 **심층 탐색이 얕은 탐색에서는 찾을 수 없는 좋은 수를 발견할 수 있다**는 것을 보여줍니다.

---

## 철학적 사색: 인간과 AI의 인지 차이

### 왜 인간은 37수를 생각하지 못했나?

이것은 깊은 질문입니다. 가능한 이유들:

**1. 경험의 한계**

인간 기사의 지식은 선배들의 기보를 학습한 것에서 옵니다. 선배가 어떤 수를 둔 적이 없다면 우리는 그것을 고려하지 않습니다.

**2. 직관의 편향**

인간의 직관은 유용하지만 한계도 있습니다. 우리의 직관은 어떤 선택지를 "보이지 않게" 만듭니다.

**3. 계산 능력의 차이**

37수의 가치는 심층 계산을 거쳐야 발견됩니다. 인간의 계산 능력은 제한적이어서 AI처럼 수천 가지 가능성을 시뮬레이션할 수 없습니다.

🎬 C27: 인지 편향과 AI의 초월

### 기계의 "직관"이란 무엇인가?

AlphaGo에게 "직관"이 있을까요?

어떤 의미에서 Policy Network는 AlphaGo의 "직관"입니다—밀리초 안에 각 위치의 잠재력을 평가할 수 있습니다.

하지만 이 "직관"은 인간의 직관과 다릅니다:
- **인간의 직관**: 경험과 패턴 인식에서 옴
- **AI의 직관**: 대량의 데이터 통계 학습에서 옴

흥미롭게도 37수는 **AI의 "직관"이 MCTS에 의해 수정될 수 있다**는 것을 증명했습니다. 이는 AI가 자신의 직관을 "반성"하여 더 나은 선택을 찾을 수 있다는 것을 의미합니다.

### 인간은 AI에서 무엇을 배울 수 있나?

37수가 인간 기사에게 주는 가장 큰 깨달음은 아마도:

> **경험이 족쇄가 되지 않도록 하라**

많은 "나쁜" 수는 단지 우리가 이해하지 못하는 것일 뿐입니다. 마음을 열고 비전통적인 수법을 시도하려는 의지가 새로운 가능성을 발견하게 할 수 있습니다.

이 깨달음은 바둑뿐만 아니라 인생의 많은 영역에도 적용됩니다.

---

## 애니메이션 대응

본 문서에서 다루는 핵심 개념과 애니메이션 번호:

| 번호 | 개념 | 물리/수학 대응 |
|------|------|--------------|
| 🎬 C3 | 전통 바둑 이론의 가치 판단 | 휴리스틱 함수 |
| 🎬 C5 | 어깨짚기의 기하학적 특성 | 공간 관계 |
| 🎬 C7 | 전문가 직관과 AI 평가의 격차 | 예측 오차 |
| 🎬 C9 | Policy Network의 출력 분포 | Softmax 확률 |
| 🎬 C11 | MCTS가 Policy Network를 수정하는 방법 | 베이즈 업데이트 |
| 🎬 C13 | Value Network의 증분 평가 | 가치 함수 |
| 🎬 C15 | 전체 가치 함수 계산 | 적분 근사 |
| 🎬 C17 | 게임 이론에서의 강제 선택 | 우세 전략 |
| 🎬 C19 | 한 수가 전체 대국 흐름을 바꾸는 방법 | 분기점 |
| 🎬 C21 | AI가 인류의 인지 경계를 확장하는 방법 | 탐색 공간 확장 |
| 🎬 C23 | AI 바둑에서 특징 공학의 중요성 | 표현 학습 |
| 🎬 C25 | PUCT 공식이 비직관적 좋은 수를 발견하는 방법 | 탐색-활용 균형 |
| 🎬 C27 | 인지 편향과 AI의 초월 | 불편향 추정 |

---

## 더 읽을거리

- **이전 글**: [주요 대국 회고](../key-matches) — 판후이, 이세돌, 커제의 완전한 대국 역사
- **다음 글**: [바둑은 왜 어려운가?](../why-go-is-hard) — 바둑의 계산 복잡도 이해
- **기술적 세부사항**: [Policy Network 상세 설명](../policy-network) — 직관 네트워크 깊이 이해
- **고급 읽기**: [PUCT 공식 상세 설명](../puct-formula) — 탐색과 활용의 수학

---

## 인터랙티브 탐색

### Policy Network 확률 분포

아래의 인터랙티브 시각화를 사용하여 다양한 국면에서 Policy Network의 출력을 탐색하세요:

<PolicyHeatmap initialPosition="move37" size={450} showTopN={5} interactive={true} />

다양한 프리셋 국면으로 전환하며 AI가 각 위치의 착점 확률을 어떻게 평가하는지 관찰해 보세요.

---

## 참고 자료

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. DeepMind Blog: "AlphaGo: The story so far"
3. 《AlphaGo》 다큐멘터리 (2017), 감독 Greg Kohs.
4. 이세돌 vs AlphaGo 2국 공식 기보
5. Go4Go.net 프로 기보 분석
6. 한국기원 대국 후 기술 보고서
