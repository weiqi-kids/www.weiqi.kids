---
sidebar_position: 9
title: Value Network ìƒì„¸ í•´ì„¤
description: AlphaGoì˜ ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜, í•™ìŠµ ê³¼ì œ ë° MCTSì—ì„œì˜ í•µì‹¬ ì—­í• ì— ëŒ€í•œ ì‹¬ì¸µ ì´í•´
---

# Value Network ìƒì„¸ í•´ì„¤

Policy Networkê°€ AlphaGoì—ê²Œ 'ë‹¤ìŒ ìˆ˜ë¥¼ ì–´ë””ì— ë‘ì–´ì•¼ í•˜ëŠ”ì§€' ì•Œë ¤ì¤€ë‹¤ë©´, Value NetworkëŠ” ë” ê·¼ë³¸ì ì¸ ì§ˆë¬¸ì— ë‹µí•©ë‹ˆë‹¤:

> **"ì´ ëŒ€êµ­, ë‚´ê°€ ì´ê¸¸ê¹Œ?"**

---

## Value Networkë€ ë¬´ì—‡ì¸ê°€?

### í•µì‹¬ ê¸°ëŠ¥

Value NetworkëŠ” ì‹¬ì¸µ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ìœ¼ë¡œ, ê·¸ ì„ë¬´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

> **í˜„ì¬ ë°”ë‘‘íŒ ìƒíƒœê°€ ì£¼ì–´ì§€ë©´, ìµœì¢… ìŠ¹ë¥ ì„ ì˜ˆì¸¡í•œë‹¤**

ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•˜ë©´:

```
v = f_Î¸(s)
```

ì—¬ê¸°ì„œ:
- `s`: í˜„ì¬ ë°”ë‘‘íŒ ìƒíƒœ
- `f_Î¸`: Value Network (Î¸ëŠ” ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„°)
- `v`: -1ì—ì„œ +1 ì‚¬ì´ì˜ ê°’

### ì¶œë ¥ì˜ ì˜ë¯¸

| ì¶œë ¥ ê°’ | ì˜ë¯¸ |
|--------|------|
| +1 | í˜„ì¬ í”Œë ˆì´ì–´ í•„ìŠ¹ |
| +0.5 | í˜„ì¬ í”Œë ˆì´ì–´ ì•½ 75% ìŠ¹ë¥  |
| 0 | ì–‘ì¸¡ ìŠ¹ë¥  ë™ë“± |
| -0.5 | í˜„ì¬ í”Œë ˆì´ì–´ ì•½ 25% ìŠ¹ë¥  |
| -1 | í˜„ì¬ í”Œë ˆì´ì–´ í•„íŒ¨ |

### ì™œ ë‹¨ì¼ ê°’ì´ í•„ìš”í•œê°€?

#### ë‹¤ë¥¸ ì„ íƒ ë¹„êµ

ë°”ë‘‘ì„ ë‘˜ ë•Œ, ìš°ë¦¬ëŠ” ì¢…ì¢… ì—¬ëŸ¬ ì„ íƒì§€ ì¤‘ì—ì„œ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤. Value NetworkëŠ” ì´ ë¹„êµë¥¼ ê°„ë‹¨í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤:

```
ì„ íƒ Aì˜ êµ­ë©´ ê°€ì¹˜: 0.3
ì„ íƒ Bì˜ êµ­ë©´ ê°€ì¹˜: 0.5
ì„ íƒ Cì˜ êµ­ë©´ ê°€ì¹˜: 0.2

â†’ B ì„ íƒ (ê°€ì¥ ë†’ì€ ê°€ì¹˜)
```

ë‹¨ì¼ ê°’ì´ ì—†ë‹¤ë©´, 'ìƒëŒ€ì˜ ëŒ í•œ ë©ì´ ì¡ê¸°'ì™€ 'í° ì§‘ ë‘˜ëŸ¬ì‹¸ê¸°' ì¤‘ ì–´ëŠ ê²ƒì´ ë” ì¢‹ì€ì§€ ì–´ë–»ê²Œ ë¹„êµí• ê¹Œìš”?

#### ëŒ€ëŸ‰ ì‹œë®¬ë ˆì´ì…˜ ëŒ€ì²´

ì „í†µì ì¸ ëª¬í…Œì¹´ë¥¼ë¡œ íŠ¸ë¦¬ íƒìƒ‰ì—ì„œ, êµ­ë©´ì„ í‰ê°€í•˜ë ¤ë©´ **ëœë¤ ì‹œë®¬ë ˆì´ì…˜(rollout)**ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤:

1. í˜„ì¬ êµ­ë©´ì—ì„œ ì‹œì‘
2. ì–‘ì¸¡ì´ ê²Œì„ ì¢…ë£Œê¹Œì§€ ë¬´ì‘ìœ„ë¡œ ì°©ìˆ˜
3. ìŠ¹íŒ¨ ê¸°ë¡
4. ìˆ˜ì²œ ë²ˆ ë°˜ë³µí•˜ì—¬ ìŠ¹ë¥  ê³„ì‚°

ì´ê²ƒì€ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤. Value NetworkëŠ” **í•œ ë²ˆì˜ ìˆœì „íŒŒ**ë¡œ í‰ê°€ë¥¼ ì œê³µí•˜ì—¬, ëª‡ ìë¦¿ìˆ˜ ë” ë¹ ë¦…ë‹ˆë‹¤.

| ë°©ë²• | í‰ê°€ ì‹œê°„ | ì •ë°€ë„ |
|------|---------|------|
| 1000íšŒ ëœë¤ ì‹œë®¬ë ˆì´ì…˜ | ~2000 ë°€ë¦¬ì´ˆ | ë‚®ìŒ |
| 15000íšŒ ëœë¤ ì‹œë®¬ë ˆì´ì…˜ | ~30000 ë°€ë¦¬ì´ˆ | ì¤‘ê°„ |
| Value Network | ~3 ë°€ë¦¬ì´ˆ | ë†’ìŒ (15000íšŒ ì‹œë®¬ë ˆì´ì…˜ê³¼ ë™ë“±) |

---

## ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜

### Policy Networkì™€ì˜ ìœ ì‚¬ì„±

Value Networkì˜ ì•„í‚¤í…ì²˜ëŠ” Policy Networkì™€ ë§¤ìš° ìœ ì‚¬í•˜ë©°, ë‘˜ ë‹¤ ì‹¬ì¸µ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ì…ë‹ˆë‹¤:

```
ì…ë ¥ì¸µ â†’ ì»¨ë³¼ë£¨ì…˜ì¸µ Ã—12 â†’ ì™„ì „ì—°ê²°ì¸µ â†’ ì¶œë ¥
   â†“         â†“           â†“         â†“
19Ã—19Ã—48   19Ã—19Ã—192    256ì°¨ì›     ë‹¨ì¼ ê°’
```

### ì…ë ¥ì¸µ

Policy Networkì™€ ë™ì¼í•˜ê²Œ, ì…ë ¥ì€ **19Ã—19Ã—49** íŠ¹ì„± í…ì„œì…ë‹ˆë‹¤:

- **19Ã—19**: ë°”ë‘‘íŒ í¬ê¸°
- **49**: 48ê°œ íŠ¹ì„± í‰ë©´ + 1ê°œ í˜„ì¬ ì°¨ë¡€ í‘œì‹œ í‰ë©´

ì¶”ê°€ëœ 1ê°œ í‰ë©´ì€ ì¤‘ìš”í•©ë‹ˆë‹¤: Value NetworkëŠ” ëˆ„êµ¬ì˜ ì°¨ë¡€ì¸ì§€ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤, ê°™ì€ êµ­ë©´ì´ë¼ë„ í‘ê³¼ ë°±ì˜ ê°€ì¹˜ëŠ” ë°˜ëŒ€ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

### ì»¨ë³¼ë£¨ì…˜ì¸µ

Policy Networkì™€ ë™ì¼:
- **12ê°œ ì»¨ë³¼ë£¨ì…˜ì¸µ**
- **192ê°œ í•„í„°**
- **3Ã—3 ì»¤ë„** (ì²« ë²ˆì§¸ ì¸µì€ 5Ã—5)
- **ReLU í™œì„±í™” í•¨ìˆ˜**

### ì¶œë ¥ì¸µì˜ ì°¨ì´

ì´ê²ƒì´ Value Networkì™€ Policy Networkì˜ í•µì‹¬ ì°¨ì´ì…ë‹ˆë‹¤:

#### Policy Network ì¶œë ¥
```
19Ã—19Ã—192 â†’ 1Ã—1 ì»¨ë³¼ë£¨ì…˜ â†’ 19Ã—19Ã—1 â†’ í‰íƒ„í™” â†’ 361ì°¨ì› â†’ Softmax â†’ í™•ë¥  ë¶„í¬
```

#### Value Network ì¶œë ¥
```
19Ã—19Ã—192 â†’ 1Ã—1 ì»¨ë³¼ë£¨ì…˜ â†’ 19Ã—19Ã—1 â†’ í‰íƒ„í™” â†’ 361ì°¨ì› â†’ ì™„ì „ì—°ê²°256 â†’ ReLU â†’ ì™„ì „ì—°ê²°1 â†’ Tanh â†’ ë‹¨ì¼ ê°’
```

### Tanh í™œì„±í™” í•¨ìˆ˜

Value Networkì˜ ë§ˆì§€ë§‰ ì¸µì€ **Tanh**(ìŒê³¡íƒ„ì  íŠ¸) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```
Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```

Tanhì˜ ì¶œë ¥ ë²”ìœ„ëŠ” **(-1, +1)**ë¡œ, ìŠ¹íŒ¨ì— ì •í™•íˆ ëŒ€ì‘í•©ë‹ˆë‹¤.

#### ì™œ Sigmoid ëŒ€ì‹  Tanhë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?

Sigmoidì˜ ì¶œë ¥ ë²”ìœ„ëŠ” (0, 1)ë¡œ, ìŠ¹ë¥ ì„ í‘œí˜„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ Tanhì—ëŠ” ëª‡ ê°€ì§€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤:

1. **ëŒ€ì¹­ì„±**: 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ, ì¶œë ¥ì´ ì–‘ìˆ˜ ë˜ëŠ” ìŒìˆ˜ ê°€ëŠ¥
2. **ê¸°ìš¸ê¸°ê°€ ë” ì¢‹ìŒ**: 0 ê·¼ì²˜ì—ì„œ ê¸°ìš¸ê¸°ê°€ 1ì— ê°€ê¹Œì›€
3. **ì˜ë¯¸ê°€ ëª…í™•**: ì–‘ìˆ˜ëŠ” ì´ê¹€, ìŒìˆ˜ëŠ” ì§, 0ì€ ë¬´ìŠ¹ë¶€

### ì™„ì „í•œ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```
ì…ë ¥: 19Ã—19Ã—49
        â†“
    Conv 5Ã—5, 192 filters
        â†“
    ReLU
        â†“
    Conv 3Ã—3, 192 filters (Ã—11)
        â†“
    ReLU
        â†“
    Conv 1Ã—1, 1 filter
        â†“
    í‰íƒ„í™” (361ì°¨ì›)
        â†“
    ì™„ì „ì—°ê²° (256ì°¨ì›)
        â†“
    ReLU
        â†“
    ì™„ì „ì—°ê²° (1ì°¨ì›)
        â†“
    Tanh
        â†“
ì¶œë ¥: [-1, +1]
```

### íŒŒë¼ë¯¸í„° ìˆ˜

| ì¸µ | ê³„ì‚° | íŒŒë¼ë¯¸í„° ìˆ˜ |
|---|------|---------|
| ì»¨ë³¼ë£¨ì…˜ì¸µ | Policy Networkì™€ ë™ì¼ | ~3.9M |
| ì™„ì „ì—°ê²°ì¸µ 1 | 361Ã—256 + 256 | 92,672 |
| ì™„ì „ì—°ê²°ì¸µ 2 | 256Ã—1 + 1 | 257 |
| **ì´ê³„** | | **~4.0M** |

ì•½ 400ë§Œ ê°œì˜ íŒŒë¼ë¯¸í„°ë¡œ, Policy Networkë³´ë‹¤ ì•½ê°„ ë§ìŠµë‹ˆë‹¤.

---

## í•™ìŠµì˜ ê³¼ì œ

### ê³¼ì í•© ë¬¸ì œ

Value Networkì˜ í•™ìŠµì€ Policy Networkë³´ë‹¤ í›¨ì”¬ ì–´ë µìŠµë‹ˆë‹¤. ì£¼ìš” ë¬¸ì œëŠ” **ê³¼ì í•©**ì…ë‹ˆë‹¤.

#### ê³¼ì í•©ì´ë€ ë¬´ì—‡ì¸ê°€?

ê³¼ì í•©ì€ ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ë¥¼ 'ê¸°ì–µ'í•˜ì—¬ ì¼ë°˜í™”ë¥¼ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤:
- í•™ìŠµ ì„¸íŠ¸ì—ì„œ ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ìŒ
- í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ì„±ëŠ¥ì´ ë§¤ìš° ë‚˜ì¨

#### ì™œ Value Networkê°€ ê³¼ì í•©ì— ì·¨ì•½í•œê°€?

í•œ ëŒ€êµ­ì˜ ë°ì´í„°ë¥¼ ê³ ë ¤í•´ ë´…ì‹œë‹¤:

```
êµ­ë©´ 1 â†’ êµ­ë©´ 2 â†’ êµ­ë©´ 3 â†’ ... â†’ êµ­ë©´ 200 â†’ ê²°ê³¼: í‘ ìŠ¹ë¦¬
```

ì´ ë°ì´í„°ë¡œ ì§ì ‘ í•™ìŠµí•˜ë©´:
- ì´ 200ê°œ êµ­ë©´ì€ ê°•í•œ ìƒê´€ê´€ê³„ê°€ ìˆìŒ
- ê°™ì€ ëŒ€êµ­ì—ì„œ ì™”ê³ , ê°™ì€ ê²°ê³¼ë¥¼ ê°€ì§
- ëª¨ë¸ì´ êµ­ë©´ì„ ì´í•´í•˜ê¸°ë³´ë‹¤ ì´ ëŒ€êµ­ì„ 'ì¸ì‹'í•˜ëŠ” ë²•ì„ ë°°ìš¸ ìˆ˜ ìˆìŒ

DeepMindëŠ” ë°œê²¬í–ˆìŠµë‹ˆë‹¤: ê°™ì€ ì¸ê°„ ê¸°ë³´ë¡œ Policyì™€ Value Networkë¥¼ í•™ìŠµí•˜ë©´, Value Networkê°€ ì‹¬ê°í•˜ê²Œ ê³¼ì í•©ë©ë‹ˆë‹¤.

### í•´ê²°ì±…: ì…€í”„ í”Œë ˆì´ ë°ì´í„°

DeepMindì˜ í•´ê²°ì±…ì€ **ì…€í”„ í”Œë ˆì´**ë¡œ ìƒˆë¡œìš´ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:

```
1. í•™ìŠµëœ RL Policy Networkë¡œ ì…€í”„ í”Œë ˆì´
2. ê° ëŒ€êµ­ì—ì„œ í•˜ë‚˜ì˜ êµ­ë©´ë§Œ ì·¨í•¨ (ìƒê´€ê´€ê³„ ë°©ì§€)
3. ì´ êµ­ë©´ì˜ ë ˆì´ë¸”ì€ í•´ë‹¹ ëŒ€êµ­ì˜ ìµœì¢… ê²°ê³¼
4. 3ì²œë§Œ ê°œì˜ ì´ëŸ¬í•œ ìƒ˜í”Œ ìƒì„±
```

#### ì™œ ì´ê²ƒì´ ê³¼ì í•©ì„ í•´ê²°í•˜ëŠ”ê°€?

1. **ë°ì´í„° ì–‘ì´ ë§ìŒ**: 3ì²œë§Œ ê°œì˜ ë…ë¦½ì ì¸ êµ­ë©´
2. **ìƒê´€ê´€ê³„ ì—†ìŒ**: ê° ëŒ€êµ­ì—ì„œ í•˜ë‚˜ì˜ êµ­ë©´ë§Œ ì·¨í•¨
3. **ë¶„í¬ê°€ ë‹¤ë¦„**: ì…€í”„ í”Œë ˆì´ì˜ êµ­ë©´ ë¶„í¬ëŠ” ì¸ê°„ ê¸°ë³´ì™€ ë‹¤ë¦„

### í•™ìŠµ ë°ì´í„° ìƒì„±

```python
# ì˜ì‚¬ ì½”ë“œ
training_data = []

for game_id in range(30_000_000):
    # í•œ ëŒ€êµ­ ì…€í”„ í”Œë ˆì´
    states, result = self_play(rl_policy_network)

    # ë¬´ì‘ìœ„ë¡œ í•˜ë‚˜ì˜ êµ­ë©´ ì„ íƒ
    random_index = random.randint(0, len(states) - 1)
    state = states[random_index]

    # êµ­ë©´ê³¼ ê²°ê³¼ ê¸°ë¡
    training_data.append((state, result))
```

---

## í•™ìŠµ ëª©í‘œì™€ ë°©ë²•

### í‰ê·  ì œê³± ì˜¤ì°¨ ì†ì‹¤

Value NetworkëŠ” **í‰ê·  ì œê³± ì˜¤ì°¨(MSE)**ë¥¼ ì†ì‹¤ í•¨ìˆ˜ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤:

```
L(Î¸) = (1/n) Ã— Î£ (v_Î¸(s) - z)Â²
```

ì—¬ê¸°ì„œ:
- `v_Î¸(s)`: ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°€ì¹˜
- `z`: ì‹¤ì œ ê²°ê³¼ (+1 ë˜ëŠ” -1)

#### ì™œ êµì°¨ ì—”íŠ¸ë¡œí”¼ ëŒ€ì‹  MSEë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?

- **êµì°¨ ì—”íŠ¸ë¡œí”¼**ëŠ” ë¶„ë¥˜ ë¬¸ì œì— ì í•© (ì´ì‚° ë ˆì´ë¸”)
- **MSE**ëŠ” íšŒê·€ ë¬¸ì œì— ì í•© (ì—°ì† ê°’)

ê²°ê³¼ê°€ +1 ë˜ëŠ” -1ë§Œ ìˆì§€ë§Œ, ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ ì—°ì† ê°’(-1ì—ì„œ +1 ì‚¬ì´ì˜ ëª¨ë“  ìˆ˜)ì…ë‹ˆë‹¤. MSEëŠ” ëª¨ë¸ì´ +1 ë˜ëŠ” -1ì— ê°€ê¹Œìš´ ê°’ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

### í•™ìŠµ ê³¼ì •

```python
# ì˜ì‚¬ ì½”ë“œ
for epoch in range(num_epochs):
    for batch in dataloader:
        states, outcomes = batch

        # ìˆœì „íŒŒ
        values = network(states)  # (batch, 1)

        # ì†ì‹¤ ê³„ì‚° (MSE)
        loss = mse_loss(values, outcomes)

        # ì—­ì „íŒŒ
        loss.backward()
        optimizer.step()
```

í•™ìŠµ ì„¸ë¶€ì‚¬í•­:
- **ì˜µí‹°ë§ˆì´ì €**: SGD with momentum
- **í•™ìŠµë¥ **: 0.003
- **ë°°ì¹˜ í¬ê¸°**: 32
- **í•™ìŠµ ì‹œê°„**: ì•½ 1ì£¼ (50 GPUs)

---

## ì •í™•ë„ ë¶„ì„

### ëœë¤ ì‹œë®¬ë ˆì´ì…˜ê³¼ì˜ ë¹„êµ

DeepMindëŠ” ë…¼ë¬¸ì—ì„œ ìƒì„¸í•œ ë¹„êµë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤:

| í‰ê°€ ë°©ë²• | ì˜ˆì¸¡ ì˜¤ì°¨ |
|---------|---------|
| 1000íšŒ ëœë¤ ì‹œë®¬ë ˆì´ì…˜ | ë†’ìŒ |
| 15000íšŒ ëœë¤ ì‹œë®¬ë ˆì´ì…˜ | ì¤‘ê°„ |
| Value Network | 15000íšŒ ì‹œë®¬ë ˆì´ì…˜ê³¼ ë™ë“± |

ì´ê²ƒì€ í•œ ë²ˆì˜ Value Network í‰ê°€ â‰ˆ 15000íšŒ ëœë¤ ì‹œë®¬ë ˆì´ì…˜ì´ì§€ë§Œ, ì•½ 1000ë°° ë” ë¹ ë¥´ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

### ê° ë‹¨ê³„ì˜ ì •í™•ë„

Value Networkì˜ ì •í™•ë„ëŠ” ê²Œì„ ì§„í–‰ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤:

| ë‹¨ê³„ | ë‚¨ì€ ìˆ˜ | ì˜ˆì¸¡ ë‚œì´ë„ | ì •í™•ë„ |
|------|---------|---------|--------|
| ì´ˆë°˜ | ~300 | ë§¤ìš° ì–´ë ¤ì›€ | ë‚®ìŒ |
| ì¤‘ë°˜ | ~150 | ì–´ë ¤ì›€ | ì¤‘ê°„ |
| ëë‚´ê¸° | ~50 | ë¹„êµì  ì‰¬ì›€ | ë†’ìŒ |
| ì¢…ë°˜ | ~10 | ì‰¬ì›€ | ë§¤ìš° ë†’ìŒ |

ì´ê²ƒì€ ì§ê´€ì ìœ¼ë¡œ í•©ë¦¬ì ì…ë‹ˆë‹¤: ê²Œì„ ì¢…ë£Œì— ê°€ê¹Œìš¸ìˆ˜ë¡, ê²°ê³¼ê°€ ë” í™•ì‹¤í•©ë‹ˆë‹¤.

### ì¶œë ¥ ë¶„í¬

ì˜ í•™ìŠµëœ Value Networkì˜ ì¶œë ¥ ë¶„í¬:

```
        ë¹ˆë„
          |
          |    *
          |   * *
          |  *   *
          | *     *
          |*       *
          +----+----+---- ì¶œë ¥ ê°’
         -1    0   +1

ëŒ€ë¶€ë¶„ì˜ ì¶œë ¥ì´ -1ê³¼ +1 ê·¼ì²˜ì— ì§‘ì¤‘ë¨
(ëŒ€ë¶€ë¶„ì˜ êµ­ë©´ì´ ëª…í™•í•œ ìŠ¹íŒ¨ ê²½í–¥ì„ ê°€ì§€ê¸° ë•Œë¬¸)
```

### ë¶ˆí™•ì‹¤í•œ êµ­ë©´

Value Network ì¶œë ¥ì´ 0ì— ê°€ê¹Œìš¸ ë•Œ, êµ­ë©´ì´ ë§¤ìš° ë³µì¡í•˜ê³  ìŠ¹íŒ¨ë¥¼ ì˜ˆì¸¡í•˜ê¸° ì–´ë µë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŸ¬í•œ êµ­ë©´ì€ ë³´í†µ:
- ëŒ€í˜• ì „íˆ¬ ì¤‘
- ì–‘ì¸¡ì´ ë§‰ìƒë§‰í•˜
- ì—¬ëŸ¬ ê°€ëŠ¥í•œ ë³€í™”ê°€ ì¡´ì¬

MCTSì—ì„œ ì´ëŸ¬í•œ ë…¸ë“œëŠ” ë” ë§ì€ íƒìƒ‰ ìì›ì„ ì–»ìŠµë‹ˆë‹¤ (ë¶ˆí™•ì‹¤ì„±ì´ ë†’ê¸° ë•Œë¬¸).

---

## MCTSì—ì„œì˜ ì—­í• 

### ë¦¬í”„ ë…¸ë“œ í‰ê°€

Value NetworkëŠ” MCTSì˜ **Evaluation** ë‹¨ê³„ì—ì„œ í•µì‹¬ ì—­í• ì„ í•©ë‹ˆë‹¤:

```
MCTS íƒìƒ‰ íŠ¸ë¦¬:

        ë£¨íŠ¸ ë…¸ë“œ (í˜„ì¬ êµ­ë©´)
           /    \
         A        B
        /  \    /  \
       A1  A2  B1  B2 â† ë¦¬í”„ ë…¸ë“œ
        â†“   â†“   â†“   â†“
       í‰ê°€  í‰ê°€  í‰ê°€  í‰ê°€
```

MCTSê°€ ë¦¬í”„ ë…¸ë“œì— ë„ë‹¬í•˜ë©´, ì´ êµ­ë©´ì˜ ê°€ì¹˜ë¥¼ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤. ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤:

1. **ëœë¤ ì‹œë®¬ë ˆì´ì…˜(Rollout)**: ë¦¬í”„ ë…¸ë“œì—ì„œ ê²Œì„ ì¢…ë£Œê¹Œì§€ ë¬´ì‘ìœ„ë¡œ ì°©ìˆ˜
2. **Value Network í‰ê°€**: ì‹ ê²½ë§ìœ¼ë¡œ ì§ì ‘ ì˜ˆì¸¡

AlphaGoëŠ” ë‘˜ì„ ê²°í•©í•©ë‹ˆë‹¤:

```
V(leaf) = (1-Î») Ã— V_network(leaf) + Î» Ã— V_rollout(leaf)
```

ì—¬ê¸°ì„œ Î» = 0.5, ì¦‰ ê°ê° ì ˆë°˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤.

#### ì™œ ê²°í•©í•˜ëŠ”ê°€?

- **Value Network**ëŠ” ë” ì •í™•í•˜ì§€ë§Œ, ì²´ê³„ì  í¸í–¥ì´ ìˆì„ ìˆ˜ ìˆìŒ
- **ëœë¤ ì‹œë®¬ë ˆì´ì…˜**ì€ ëœ ì •í™•í•˜ì§€ë§Œ, ë…ë¦½ì ì¸ ì¶”ì •ì„ ì œê³µ
- ë‘˜ì„ ê²°í•©í•˜ë©´ ìƒí˜¸ ë³´ì™„ ê°€ëŠ¥

### AlphaGo Zeroì˜ ë‹¨ìˆœí™”

ì´í›„ì˜ AlphaGo ZeroëŠ” ëœë¤ ì‹œë®¬ë ˆì´ì…˜ì„ ì™„ì „íˆ íê¸°í–ˆìŠµë‹ˆë‹¤:

```
V(leaf) = V_network(leaf)
```

ì´ê²ƒì€ ì‹œìŠ¤í…œì„ í¬ê²Œ ë‹¨ìˆœí™”í•˜ë©´ì„œ, ê¸°ë ¥ì´ ë” ê°•í•´ì¡ŒìŠµë‹ˆë‹¤. ì´ê²ƒì€ Value Networkê°€ ì¶©ë¶„íˆ ì‹ ë¢°í•  ìˆ˜ ìˆì–´, ëœë¤ ì‹œë®¬ë ˆì´ì…˜ì˜ 'ë³´í—˜'ì´ í•„ìš” ì—†ë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•©ë‹ˆë‹¤.

### ì—­ì „íŒŒ ì—…ë°ì´íŠ¸

ë¦¬í”„ ë…¸ë“œ í‰ê°€ í›„, ì´ ê°’ì€ ê²½ë¡œë¥¼ ë”°ë¼ ì—­ì „íŒŒë©ë‹ˆë‹¤:

```
v3 = V(leaf) = 0.6
      â†‘
A2ì˜ Q ê°’ ì—…ë°ì´íŠ¸
      â†‘
Aì˜ Q ê°’ ì—…ë°ì´íŠ¸
      â†‘
ë£¨íŠ¸ ë…¸ë“œì˜ í†µê³„ ì—…ë°ì´íŠ¸
```

ê° ë…¸ë“œê°€ ìœ ì§€í•˜ëŠ” Q ê°’ì€ ê·¸ê²ƒì„ í†µê³¼í•œ ëª¨ë“  ë¦¬í”„ ë…¸ë“œ í‰ê°€ì˜ í‰ê· ì…ë‹ˆë‹¤:

```
Q(s, a) = (1/N(s,a)) Ã— Î£ V(leaf)
```

---

## ì‹œê°í™” ë¶„ì„

### ê°€ì¹˜ ê³¡ë©´

ë‹¨ìˆœí™”ëœ 3Ã—3 ë°”ë‘‘íŒì„ ìƒìƒí•´ ë´…ì‹œë‹¤. Value Networkê°€ ë°°ìš´ ê²ƒì€ 'ê°€ì¹˜ ê³¡ë©´'ì…ë‹ˆë‹¤:

**ê°€ì¹˜ ë§¤íŠ¸ë¦­ìŠ¤ ì˜ˆì‹œ (í‘ëŒ ìœ„ì¹˜ vs ë°±ëŒ ìœ„ì¹˜)**

| í‘\ë°± | 1 | 2 | 3 |
|:---:|:---:|:---:|:---:|
| **1** | +0.3 | -0.1 | +0.2 |
| **2** | -0.2 | +0.5 | -0.3 |
| **3** | +0.1 | -0.2 | +0.4 |

ì´ ê³¡ë©´ì€ ê° ìœ„ì¹˜ ì¡°í•©ì˜ ê°€ì¹˜ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤. ì–‘ìˆ˜ëŠ” í‘ì—ê²Œ ìœ ë¦¬í•˜ê³ , ìŒìˆ˜ëŠ” ë°±ì—ê²Œ ìœ ë¦¬í•©ë‹ˆë‹¤.

### í•™ìŠµ ê³¼ì •ì—ì„œì˜ ë³€í™”

í•™ìŠµì´ ì§„í–‰ë¨ì— ë”°ë¼, Value Networkì˜ ì˜ˆì¸¡ì€ ì ì  ë” ì •í™•í•´ì§‘ë‹ˆë‹¤:

**í›ˆë ¨ ì§„í–‰ì— ë”°ë¥¸ ì˜ˆì¸¡ ì˜¤ì°¨**

| í›ˆë ¨ ìŠ¤í… | ì˜ˆì¸¡ ì˜¤ì°¨ |
|:---:|:---:|
| 0 | 1.0 |
| 100K | 0.6 |
| 500K | 0.15 |
| 1M | 0.1 |

ì˜¤ì°¨ëŠ” ë¹ ë¥´ê²Œ ê°ì†Œí•œ í›„ ì•ˆì •í™”ë©ë‹ˆë‹¤.

### ì–´ë ¤ìš´ êµ­ë©´ ì‹ë³„

Value NetworkëŠ” ì–´ë ¤ìš´ êµ­ë©´ì„ ì‹ë³„í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤:

| ì¶œë ¥ | ì˜ë¯¸ | ëŒ€ì‘ ì „ëµ |
|------|------|---------|
| +1ì— ê°€ê¹Œì›€ | í¬ê²Œ ìœ ë¦¬ | ì•ˆì •ì  ì°©ìˆ˜ |
| -1ì— ê°€ê¹Œì›€ | í¬ê²Œ ë¶ˆë¦¬ | ì—­ì „ ê¸°íšŒ ëª¨ìƒ‰ |
| 0ì— ê°€ê¹Œì›€ | ë³µì¡í•œ êµ­ë©´ | ì‹¬ì¸µ ê³„ì‚° í•„ìš” |

AlphaGoëŠ” 0ì— ê°€ê¹Œìš´ êµ­ë©´ì— ë” ë§ì€ ì‚¬ê³  ì‹œê°„ì„ íˆ¬ìí•©ë‹ˆë‹¤.

---

## êµ¬í˜„ í¬ì¸íŠ¸

### PyTorch êµ¬í˜„

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ValueNetwork(nn.Module):
    def __init__(self, input_channels=49, num_filters=192, num_layers=12):
        super().__init__()

        # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ì¸µ (5Ã—5)
        self.conv1 = nn.Conv2d(input_channels, num_filters,
                               kernel_size=5, padding=2)

        # ì¤‘ê°„ ì»¨ë³¼ë£¨ì…˜ì¸µ (3Ã—3) Ã—11
        self.conv_layers = nn.ModuleList([
            nn.Conv2d(num_filters, num_filters,
                     kernel_size=3, padding=1)
            for _ in range(num_layers - 1)
        ])

        # ì¶œë ¥ ì»¨ë³¼ë£¨ì…˜ì¸µ
        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)

        # ì™„ì „ì—°ê²°ì¸µ
        self.fc1 = nn.Linear(361, 256)
        self.fc2 = nn.Linear(256, 1)

    def forward(self, x):
        # x: (batch, 49, 19, 19)

        # ì»¨ë³¼ë£¨ì…˜ì¸µ
        x = F.relu(self.conv1(x))
        for conv in self.conv_layers:
            x = F.relu(conv(x))
        x = self.conv_out(x)

        # í‰íƒ„í™”
        x = x.view(x.size(0), -1)  # (batch, 361)

        # ì™„ì „ì—°ê²°ì¸µ
        x = F.relu(self.fc1(x))
        x = torch.tanh(self.fc2(x))

        return x.squeeze(-1)  # (batch,)
```

### í•™ìŠµ ë£¨í”„

```python
def train_value_network(model, optimizer, states, outcomes):
    """
    states: (batch, 49, 19, 19) - ë°”ë‘‘íŒ íŠ¹ì„±
    outcomes: (batch,) - ê²Œì„ ê²°ê³¼ (+1 ë˜ëŠ” -1)
    """
    # ìˆœì „íŒŒ
    values = model(states)  # (batch,)

    # MSE ì†ì‹¤
    loss = F.mse_loss(values, outcomes)

    # ì—­ì „íŒŒ
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # ì •í™•ë„ ê³„ì‚° (ìŠ¹íŒ¨ ì˜ˆì¸¡ ì •í™•ë„)
    predictions = (values > 0).float() * 2 - 1  # +1/-1ë¡œ ë³€í™˜
    accuracy = (predictions == outcomes).float().mean()

    return loss.item(), accuracy.item()
```

### ê³¼ì í•© ë°©ì§€ ê¸°ë²•

```python
# 1. ë°ì´í„° ì¦ê°• (8ì¤‘ ëŒ€ì¹­ì„±)
def augment(state, outcome):
    augmented = []
    for rotation in [0, 90, 180, 270]:
        s = rotate(state, rotation)
        augmented.append((s, outcome))
        augmented.append((flip(s), outcome))
    return augmented

# 2. Dropout
class ValueNetworkWithDropout(ValueNetwork):
    def __init__(self, *args, dropout_rate=0.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        # ... ì»¨ë³¼ë£¨ì…˜ì¸µ ...
        x = self.dropout(x)  # ì™„ì „ì—°ê²°ì¸µ ì „ì— dropout
        # ... ì™„ì „ì—°ê²°ì¸µ ...

# 3. ì¡°ê¸° ì¢…ë£Œ (Early Stopping)
best_val_loss = float('inf')
patience = 10
counter = 0

for epoch in range(max_epochs):
    train_loss = train_one_epoch()
    val_loss = evaluate()

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        save_model()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping!")
            break
```

---

## Policy Networkì™€ì˜ í˜‘ë ¥

### ìƒí˜¸ ë³´ì™„ ê´€ê³„

Policy Networkì™€ Value NetworkëŠ” AlphaGoì—ì„œ ìƒí˜¸ ë³´ì™„í•©ë‹ˆë‹¤:

| ë„¤íŠ¸ì›Œí¬ | ë‹µí•˜ëŠ” ì§ˆë¬¸ | ì¶œë ¥ | MCTS ì—­í•  |
|------|-----------|------|----------|
| Policy | ë‹¤ìŒ ìˆ˜ë¥¼ ì–´ë””ì—? | í™•ë¥  ë¶„í¬ | íƒìƒ‰ ë°©í–¥ ì•ˆë‚´ |
| Value | ì´ ëŒ€êµ­ì„ ì´ê¸¸ê¹Œ? | ë‹¨ì¼ ê°’ | ë¦¬í”„ ë…¸ë“œ í‰ê°€ |

### í†µí•©ëœ ì´ì¤‘ í—¤ë“œ ë„¤íŠ¸ì›Œí¬

AlphaGo Zeroì—ì„œëŠ” ì´ ë‘ ë„¤íŠ¸ì›Œí¬ê°€ í•˜ë‚˜ì˜ **ì´ì¤‘ í—¤ë“œ ë„¤íŠ¸ì›Œí¬**ë¡œ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤:

```mermaid
flowchart TB
    Shared["ê³µìœ  íŠ¹ì„± ì¶”ì¶œì¸µ"]
    Shared --> Policy["Policy Head"]
    Shared --> Value["Value Head"]
    Policy --> P_Out["361ê°œ í™•ë¥ "]
    Value --> V_Out["ë‹¨ì¼ ê°’"]
```

ì´ ì„¤ê³„ì˜ ì¥ì :
- **íŒŒë¼ë¯¸í„° ê³µìœ **: ê³„ì‚°ëŸ‰ ê°ì†Œ
- **íŠ¹ì„± ê³µìœ **: Policyì™€ Valueê°€ ê°™ì€ íŠ¹ì„± ì‚¬ìš©
- **í•™ìŠµì´ ë” ì•ˆì •ì **: ë‘ ëª©í‘œê°€ ì„œë¡œë¥¼ ì •ê·œí™”

ìì„¸í•œ ë‚´ìš©ì€ [ì´ì¤‘ í—¤ë“œ ë„¤íŠ¸ì›Œí¬ì™€ ì”ì°¨ ë„¤íŠ¸ì›Œí¬](../dual-head-resnet)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

---

## ì• ë‹ˆë©”ì´ì…˜ ëŒ€ì‘

ì´ ê¸€ì—ì„œ ë‹¤ë£¨ëŠ” í•µì‹¬ ê°œë…ê³¼ ì• ë‹ˆë©”ì´ì…˜ ë²ˆí˜¸:

| ë²ˆí˜¸ | ê°œë… | ë¬¼ë¦¬/ìˆ˜í•™ ëŒ€ì‘ |
|------|------|--------------|
| ğŸ¬ E2 | Value Network | í¼í…ì…œ ì—ë„ˆì§€ë©´ |
| ğŸ¬ D4 | ê°€ì¹˜ í•¨ìˆ˜ | ê¸°ëŒ€ ë³´ìƒ |
| ğŸ¬ C6 | ë¦¬í”„ ë…¸ë“œ í‰ê°€ | í•¨ìˆ˜ ê·¼ì‚¬ |
| ğŸ¬ H3 | ì‹œê°„ì°¨ í•™ìŠµ | ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ |

---

## ì¶”ê°€ ì½ê¸°

- **ì´ì „ í¸**: [Policy Network ìƒì„¸ í•´ì„¤](../policy-network) â€” ì •ì±… ë„¤íŠ¸ì›Œí¬ê°€ ì°©ìˆ˜ë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•
- **ë‹¤ìŒ í¸**: [ì…ë ¥ íŠ¹ì„± ì„¤ê³„](../input-features) â€” 48ê°œ íŠ¹ì„± í‰ë©´ ìƒì„¸
- **ì‹¬í™” ì£¼ì œ**: [MCTSì™€ ì‹ ê²½ë§ì˜ ê²°í•©](../mcts-neural-combo) â€” ì™„ì „í•œ íƒìƒ‰ ê³¼ì •

---

## í•µì‹¬ í¬ì¸íŠ¸

1. **Value NetworkëŠ” ìŠ¹ë¥ ì„ ì˜ˆì¸¡**: -1ì—ì„œ +1 ì‚¬ì´ì˜ ë‹¨ì¼ ê°’ ì¶œë ¥
2. **Tanh ì¶œë ¥**: ì¶œë ¥ì´ ì˜¬ë°”ë¥¸ ë²”ìœ„ ë‚´ì— ìˆë„ë¡ ë³´ì¥
3. **MSE ì†ì‹¤**: ì˜ˆì¸¡ ê°’ì„ ì‹¤ì œ ê²°ê³¼ì— ê·¼ì‚¬
4. **ê³¼ì í•© ê³¼ì œ**: ì…€í”„ í”Œë ˆì´ ë°ì´í„°ë¡œ ë°©ì§€ í•„ìš”
5. **ëœë¤ ì‹œë®¬ë ˆì´ì…˜ ëŒ€ì²´**: í•œ ë²ˆ í‰ê°€ â‰ˆ 15000íšŒ ì‹œë®¬ë ˆì´ì…˜

Value NetworkëŠ” AlphaGoì˜ 'íŒë‹¨ë ¥'ì…ë‹ˆë‹¤ â€” AIê°€ ëª¨ë“  ê°€ëŠ¥ì„±ì„ ë‹¤ íƒìƒ‰í•˜ì§€ ì•Šê³ ë„ ì–´ë–¤ êµ­ë©´ì˜ ì¢‹ê³  ë‚˜ì¨ì„ í‰ê°€í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

---

## ì°¸ê³  ìë£Œ

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 551, 354-359.
3. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.
4. Tesauro, G. (1995). "Temporal difference learning and TD-Gammon." *Communications of the ACM*, 38(3), 58-68.
