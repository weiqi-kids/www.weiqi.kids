---
sidebar_position: 19
title: 처음부터 훈련하는 과정
description: AlphaGo Zero가 3일 만에 무작위 착수에서 인간을 초월하고, 수천 년의 바둑 이론을 재발견하고 넘어서는 과정을 목격하세요
keywords: [AlphaGo Zero, 훈련 과정, 자기 대국, 기력 성장, 바둑 AI, 딥러닝]
---

import { EloChart } from '@site/src/components/D3Charts';

# 처음부터 훈련하는 과정

AlphaGo Zero에서 가장 놀라운 것은 최종 기력만이 아니라 **성장 과정**입니다. 완전히 무작위한 상태에서 시작하여 단 3일 만에 인류가 수천 년에 걸쳐 축적한 바둑 지식을 경험하고, 그 다음에는 인간의 모든 이해를 초월했습니다.

이 글에서는 이 놀라운 변화 과정을 단계별로 살펴보겠습니다.

---

## 훈련 곡선

먼저 AlphaGo Zero의 기력 성장 곡선을 살펴보겠습니다:

<EloChart mode="zero" width={700} height={400} />

이 곡선은 AlphaGo Zero가 72시간 동안 보여준 기력 변화를 보여줍니다. 몇 가지 주요 이정표를 확인해 보세요:

| 시간 | ELO 레이팅 | 해당 수준 |
|------|----------|--------|
| 0시간 | 0 | 무작위 착수 |
| 3시간 | ~1000 | 기본 규칙 발견 |
| 12시간 | ~3000 | 정석과 모양 발견 |
| 36시간 | ~4500 | 판후이 버전 AlphaGo 초월 |
| 60시간 | ~5200 | 이세돌 버전 AlphaGo 초월 |
| 72시간 | ~5400 | 모든 이전 버전 초월 |

**3일 만에 제로에서 인간 최정상을 초월했습니다.**

---

## Day 0: 혼돈의 시작

### 완전히 무작위한 초기 상태

훈련 시작 시 신경망의 가중치는 무작위로 초기화됩니다. 이는 다음을 의미합니다:

- **Policy Head**: 균등 분포에 가까운 출력, 각 위치의 착수 확률이 약 1/361
- **Value Head**: 0에 가까운 출력, 좋은 국면과 나쁜 국면을 구분하지 못함

이 시점의 AlphaGo Zero는 완전히 무작위로 착수합니다. 바둑판을 한 번도 본 적 없는 사람보다 더 못합니다.

### 첫 번째 자기 대국

첫 번째 자기 대국이 어떤 모습일지 상상해 보세요:

```
흑 1: 임의의 위치에 착수 (천원일 수도, 귀일 수도, 1선일 수도 있음)
백 2: 다른 임의의 위치에 착수
흑 3: 무작위...
...
200수: 바둑판 곳곳에 연결되지 않은 고립된 돌들
최종: 승패는 무작위 요소에 의해 결정됨
```

이 대국의 '품질'은 매우 낮지만, 귀중한 정보를 담고 있습니다: **최종적으로 누가 이겼는가**.

### 첫 번째 훈련 신호

양쪽 모두 무작위로 착수하지만, 승패 결과는 확정됩니다. 신경망은 학습하기 시작합니다:

> "이 국면에서 최종적으로 흑이 이겼다. 왜인지는 모르겠지만, 이 국면이 흑에게 더 좋을 수 있다."

이것은 매우 약한 신호이지만 실제적입니다. 수천 국의 이런 '쓰레기 바둑' 후에 네트워크는 몇 가지 통계적 규칙성을 발견하기 시작합니다.

---

## Hour 1-3: 게임 규칙 발견

### 규칙 인식의 출현

수만 국의 자기 대국 후, AlphaGo Zero는 바둑의 기본 규칙을 "발견"하기 시작합니다 (이 규칙들은 이미 게임 엔진에 내장되어 있지만):

#### 1. 연결의 중요성

```
관찰: 돌이 연결되어 있으면 잡히기 어려움
학습: 이미 있는 돌 옆에 우선적으로 착수하기 시작함
```

이것은 가르침을 받은 것이 아니라 승패 결과에서 배운 것입니다. 흩어진 돌은 각개격파되기 쉽고, 연결된 돌은 생존하기 쉽습니다.

#### 2. 활로의 개념

```
관찰: 돌의 인접한 빈 점이 모두 채워지면 돌이 사라짐
학습: 활로가 적은 위치를 피하기 시작하고, 상대방의 활로가 적은 돌을 공격하기 시작함
```

네트워크는 활로를 추적하는 것을 배웠습니다. 입력에 명시적인 '활로 수' 특징이 없지만, 과거 바둑판 상태에서 추론할 수 있습니다.

#### 3. 눈의 초기 형태

```
관찰: 특정 모양은 잡히기 특히 어려움
학습: 귀와 변에서 공간이 있는 모양을 형성하기 시작함
```

이것은 삶의 개념의 싹입니다. 네트워크는 내부 공간이 있는 돌 무리가 생존하기 쉽다는 것을 발견했습니다.

### 기력 평가

이 시점의 AlphaGo Zero는 대략:
- **ELO**: ~1000
- **해당 수준**: 막 규칙을 배운 초보자
- **특징**: 돌을 연결해야 한다는 것을 알고, 상대방 돌을 잡아야 한다는 것을 앎

---

## Hour 3-12: 정석과 모양 발견

### 귀의 각성

더 많은 훈련 후, 네트워크는 귀의 중요성을 발견했습니다:

```
관찰: 귀의 돌은 2개의 눈만 있으면 살 수 있음
     변에서 2개의 눈을 만들기가 더 어려움
     중앙에서 2개의 눈을 만들기가 가장 어려움
학습: 포석에서 귀를 우선 점유함
```

이것이 인간 바둑 이론에서 "금모서리 은변 풀배(金角銀邊草肚皮)"를 발견하는 과정입니다. 네트워크는 이 원칙을 알려주지 않았지만 수십만 국의 대국에서 스스로 발견했습니다.

### 정석의 출현

더욱 놀라운 것은 네트워크가 정석을 "발명"하기 시작했다는 것입니다. 정석은 귀에서 양측의 표준 착수법입니다:

#### 관찰된 현상

```
훈련 초기: 귀 착수법이 다양함
훈련 중기: 특정 착수법이 반복적으로 나타남
훈련 후기: 안정적인 귀 정석이 형성됨
```

이 정석들은 인간이 수백 년에 걸쳐 축적한 정석과 **매우 유사**하여, 이 정석들이 실제로 양측의 최적해에 근접함을 검증합니다.

### 전형적인 출현 정석

소목 정석을 예로 들면:

```
  A B C D E F G H J
9 . . . . . . . . .
8 . . . . . . . . .
7 . . . . . . . . .
6 . . . ● . . . . .   ● = 흑
5 . . . . . . . . .   ○ = 백
4 . . . ○ . ● . . .
3 . . . . . . . . .
2 . . . . . . . . .
1 . . . . . . . . .
```

흑이 소목을 점유하고, 백이 걸치고, 흑이 협공하는 이 순서가 훈련 과정에서 자연스럽게 출현했습니다.

### 모양 지식

정석 외에도 네트워크는 좋은 모양과 나쁜 모양의 차이를 배웠습니다:

| 모양 | 인간 평가 | Zero의 학습 |
|------|----------|-------------|
| 빈삼각 | 우형 | 점차 피함 |
| 호구 | 좋은 모양 | 점차 선호함 |
| 쌍비연 | 전형적인 공격 모양 | 자연 발견 |
| 진신두 | 강력한 공격 | 자연 발견 |

### 기력 평가

이 시점의 AlphaGo Zero:
- **ELO**: ~3000
- **해당 수준**: 아마추어 고단
- **특징**: 기본 정석 지식이 있고, 기본 모양을 이해함

---

## Hour 12-36: 바둑 이론의 성숙

### 전체적 시야의 형성

둘째 날에 들어서면서 네트워크는 **전체적 시야**를 보여주기 시작했습니다:

#### 세력과 실리

```
관찰: 공간을 둘러싸면 집을 얻을 수 있음
     하지만 세력도 가치가 있음 — 상대를 공격할 수 있음
학습: 집 취하기와 세력 취하기 사이에서 균형을 찾음
```

이것은 바둑에서 가장 심오한 개념 중 하나입니다. 네트워크는 "허"와 "실"의 가치를 평가하는 것을 배웠습니다.

#### 두터움과 얇음 판단

```
관찰: "두터운" 바둑은 멀리서의 전투를 지원할 수 있음
     "얇은" 바둑은 보강이 필요하며, 그렇지 않으면 공격받음
학습: 적극적으로 두터움을 구축하고, 상대방의 약점을 공격함
```

### 중반 전술

네트워크의 중반 전투 능력이 크게 향상되었습니다:

| 기술 | 설명 |
|------|------|
| 약한 돌 공격 | 상대방의 고립된 돌을 식별하고 공세를 펼침 |
| 두터움 활용 | 두터움으로 공격을 지원하고 이익을 얻음 |
| 전환 | 국부적 손실을 포기하고 전체적 우위를 얻음 |
| 타입 | 상대방의 모양을 침소함 |

### 끝내기 기술

끝내기 단계의 정확한 계산도 향상되고 있습니다:

```
관찰: 끝내기 단계에서 각 수의 가치를 정확히 계산할 수 있음
학습: 가치 크기 순서로 끝내기를 진행함
```

네트워크는 "양쪽 선수", "한쪽 선수", "후수" 등의 끝내기 개념을 배웠습니다.

### 기력 평가

이 시점의 AlphaGo Zero:
- **ELO**: ~4500
- **해당 수준**: 프로 기사 수준
- **특징**: 완전한 바둑 이해가 있고, 고품질의 대국을 둘 수 있음

---

## Hour 36-72: 인간을 초월하다

### 프로 수준 돌파

36시간쯤에 AlphaGo Zero의 기력은 프로 기사 수준에 도달했습니다. 하지만 훈련은 멈추지 않았습니다. 계속 자기 대국하고 계속 향상했습니다.

다음에 일어난 일이 더 흥미롭습니다: **인간이 생각하지 못한 착수법을 발견하기 시작했습니다**.

### 혁신적인 포석

전통 바둑 포석에는 많은 "정견"이 있습니다:

| 전통적 관점 | AlphaGo Zero의 발견 |
|----------|---------------------|
| 포석에서 먼저 귀를 점유 | 어떤 상황에서는 먼저 변을 점유하는 것이 더 좋음 |
| 소목이 가장 안정적 | 삼삼 직접 점유도 가능 |
| 정석을 잘 익혀야 함 | 적극적으로 정석에서 벗어날 수 있음 |
| 삼삼 침입은 너무 일찍 하면 탐욕스러움 | 특정 국면에서 삼삼 침입이 정확함 |

이 "발견"들은 AlphaGo 이후 인간 프로 기사들에 의해 광범위하게 연구되었으며, 많은 것들이 현대 바둑 이론에 통합되었습니다.

### 반직관적인 모양

AlphaGo Zero는 때때로 인간이 "보기 좋지 않다"고 생각하는 모양을 둡니다:

```
인간: "이것은 우형이다, 좋은 수일 리 없다"
Zero: (그 수를 둠)
분석 후: "알고 보니 이렇게 하는 것이 더 효율적이다"
```

이것은 인간 바둑 이론의 한계를 드러냅니다: 일부 "나쁜 모양"은 사실 특정 국면에서 최적해입니다.

### 과감한 버림

Zero는 인간보다 다른 이익을 위해 돌을 버리는 것에 더 적극적입니다:

```
국부적 3집 손해
전체적 주도권 획득
최종 승률 향상
```

인간 기사들은 종종 국부적 득실에 지나치게 신경 쓰지만, Zero는 항상 최종 승률을 주시합니다.

### 기력 평가

72시간 후의 AlphaGo Zero:
- **ELO**: ~5400
- **해당 수준**: 모든 인간 기사를 초월
- **특징**: 인간이 알지 못한 착수법을 발견하고, 새로운 바둑 이론을 창조함

---

## 인간 바둑 이론의 재발견

### 수천 년 vs. 3일

인간 바둑은 수천 년에 걸쳐 발전했습니다:
- 기원전 2000년경 중국에서 기원
- 당나라 때 일본에 전파되어 정밀한 바둑 이론 발전
- 20세기에 프로 시스템이 등장하고 바둑 이론이 더욱 심화됨
- 2016년, 인간은 바둑을 상당히 이해했다고 생각함

AlphaGo Zero는 3일 만에 이 여정을 완주했습니다. 더 놀라운 것은 발견한 바둑 이론이 인간의 것과 **매우 일치**한다는 것입니다.

### 검증과 초월

| 인간 지식 | Zero의 태도 |
|----------|-------------|
| 금모서리 은변 풀배 | 확인 (귀가 확실히 중요함) |
| 기본 정석 | 대부분 확인, 일부 개선 |
| 좋은 모양 나쁜 모양 | 대부분 확인, 예외 존재 |
| 버림과 전환 | 인간보다 더 과감함 |
| 두터움과 얇음 판단 | 대체로 일치, 세부 사항 다름 |

이것은 인간이 수천 년에 걸쳐 축적한 바둑 이론이 **대방향은 올바르다**는 것을 보여줍니다. 하지만 일부 영역에서는 인간의 이해가 수정되어야 합니다.

### 인간 학습에 대한 시사점

AlphaGo Zero의 훈련 과정은 인간 학습에 시사점을 줍니다:

1. **기초부터 시작**: Zero는 먼저 규칙을 배우고, 그 다음 모양을 배우고, 마지막으로 전체적 시야를 발전시킴
2. **대량 연습**: 490만 국의 자기 대국은 수십만 년의 인간 대국량에 해당
3. **승패에 집중**: "아름다운 바둑"을 추구하지 않고, 오직 이기는 것만 추구함
4. **전통에 구애받지 않음**: "불가능한" 착수를 과감히 시도함

---

## 훈련 과정의 기술적 세부사항

### 자기 대국의 메커니즘

각 자기 대국의 흐름:

```
초기화: 빈 바둑판
↓
각 수:
  1. 신경망으로 현재 국면 평가
  2. MCTS 탐색 실행 (1600회 시뮬레이션)
  3. 탐색 결과에 따라 착수 선택
  4. (국면, MCTS확률, -)을 기록
↓
게임 종료:
  1. 승패 판정 z ∈ {-1, +1}
  2. 모든 기록에 승패 보완 (국면, MCTS확률, z)
  3. 데이터를 훈련 풀에 추가
```

### 훈련의 리듬

AlphaGo Zero의 훈련은 **지속적으로 진행**됩니다:

```
Self-play Workers:       지속적으로 자기 대국 데이터 생성
Training Workers:        지속적으로 데이터 풀에서 샘플링하여 훈련
Network Updates:         정기적으로 자기 대국용 네트워크 업데이트
```

이 세 가지 과정이 동시에 진행되어 지속적인 개선 루프를 형성합니다.

### 데이터 풀 관리

훈련 데이터 풀의 관리:

| 파라미터 | 값 |
|------|-----|
| 풀 크기 | 최근 50만 국 |
| 각 대국 샘플 | ~200수 |
| 총 샘플 수 | ~1억 |
| 샘플링 방식 | 균등 무작위 |

오래된 데이터는 새 데이터로 교체되어 훈련 데이터가 현재 네트워크의 수준을 반영하도록 합니다.

### 네트워크 업데이트 전략

훈련 한 스텝마다 자기 대국의 네트워크를 업데이트하는 것이 아닙니다. 대신:

1. 일정 기간 훈련 후 후보 네트워크 생성
2. 후보 네트워크와 현재 네트워크 대전 (400국)
3. 후보 네트워크 승률 > 55%면 업데이트
4. 그렇지 않으면 훈련 계속

이것은 자기 대국이 항상 **충분히 강한** 네트워크를 사용하도록 보장합니다.

---

## 학습 속도 분석

### 왜 이렇게 빠른가?

AlphaGo Zero의 학습 속도가 놀라운 이유:

#### 1. 계산 자원

- 4개의 TPU, 초당 수만 회 추론
- 매일 수십만 국의 자기 대국 생성
- 인간의 수천 년 대국량에 해당

#### 2. 완벽한 상대

자기 대국은 다음을 의미합니다:
- 상대 수준이 항상 자신과 비슷함
- 너무 약하지도 (배울 것이 없음) 너무 강하지도 (이길 수 없음) 않음
- 이것은 이상적인 학습 조건임

#### 3. 직접적인 목표

오직 하나의 목표: 이기기. 다음은 없음:
- 선생님의 선호
- 스타일 추구
- 미학적 고려

#### 4. 효율적인 표현 학습

잔차 네트워크는 매우 추상적인 바둑판 특징을 학습할 수 있어, 수동 설계된 특징보다 더 효과적입니다.

### 인간과의 비교

| 측면 | 인간 | AlphaGo Zero |
|------|------|--------------|
| 학습 속도 | 하루 ~10국 | 하루 ~100,000국 |
| 기억 유지 | 망각이 있음 | 완벽히 유지 |
| 에너지 제한 | 휴식 필요 | 24/7 운영 |
| 혁신 능력 | 전통에 영향 받음 | 사전 제한 없음 |

---

## 훈련 과정에서의 흥미로운 현상

### 단계적 정체

훈련 곡선이 완전히 매끄럽지 않고, 때때로 **정체 기간**이 나타납니다:

```
ELO: 2000 -----> 2000 -----> 2500 ---->
          (정체)       (돌파)
```

이것은 네트워크가 어떤 새로운 개념을 학습하고 있어 "소화"하는 데 시간이 필요하기 때문일 수 있습니다.

### 전략의 출현과 소멸

어떤 전략들은 훈련 과정에서 출현했다가 다시 소멸합니다:

```
단계 1: 특정 공격 수단 발견
단계 2: 상대가 방어 학습
단계 3: 해당 수단 사용 빈도 감소
단계 4: 새로운 공격 수단 발견
```

이것은 군비 경쟁의 축소판입니다.

### "바퀴의 재발명"

훈련 과정에서 Zero는 인간이 이미 알고 있는 개념을 "재발명"합니다:

- **축**: 연속 단수가 돌을 잡을 수 있다는 것을 발견
- **환격**: 먼저 돌을 내주고 역습할 수 있다는 것을 발견
- **패**: 반복 규칙 활용 방법 발견

이 발견의 순서는 인간이 바둑을 배우는 순서와 유사합니다.

---

## 애니메이션 대응

이 글에서 다룬 핵심 개념과 애니메이션 번호:

| 번호 | 개념 | 물리/수학 대응 |
|------|------|--------------|
| 🎬 E12 | 기력 성장 곡선 | S자형 성장 (로지스틱) |
| 🎬 E7 | 제로에서 시작 | 자기 조직화 현상 |
| 🎬 E5 | 자기 대국 | 고정점 수렴 |
| 🎬 F8 | 창발 능력 | 상전이 |

---

## 추가 읽을거리

- **이전 글**: [이중 헤드 네트워크와 잔차 네트워크](../dual-head-resnet) — 이 모든 것을 지탱하는 신경망 구조
- **다음 글**: [분산 시스템과 TPU](../distributed-systems) — 이 모든 것을 가능하게 한 하드웨어
- **관련 글**: [자기 대국](../self-play) — 왜 자기 대국이 그토록 효과적인가

---

## 참고 자료

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
3. DeepMind. (2017). "AlphaGo Zero: Learning from scratch." *YouTube*.
4. Wang, F., et al. (2019). "A Survey on the Evolution of AlphaGo." *arXiv:1907.11180*.
