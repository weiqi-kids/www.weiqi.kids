---
sidebar_position: 17
title: AlphaGo Zero 개요
description: 처음부터 완전히 자기 학습하는 AlphaGo Zero가 인간 기보 없이 어떻게 모든 이전 버전을 초월했는가
keywords: [AlphaGo Zero, 자기 대국, 강화 학습, 딥러닝, 바둑 AI, 비지도 학습]
---

# AlphaGo Zero 개요

2017년 10월, DeepMind는 AI 분야를 충격에 빠뜨린 성과를 발표했습니다: **AlphaGo Zero**는 어떤 인간 기보도 사용하지 않고 완전히 무작위 상태에서 훈련을 시작하여, 단 3일 만에 이세돌을 이긴 원본 AlphaGo를 초월하고, **100:0**의 점수로 완승했습니다.

이것은 단순한 숫자상의 진보가 아닙니다. 이것은 완전히 새로운 패러다임을 대표합니다: **AI는 인간 지식 없이도 처음부터 모든 것을 발견할 수 있습니다**.

---

## 왜 인간 기보가 필요 없는가?

### 인간 기보의 한계

원본 AlphaGo의 훈련 과정은 두 단계로 나뉩니다:

1. **지도 학습**: 3천만 국의 인간 기보로 Policy Network 훈련
2. **강화 학습**: 자기 대국을 통해 더욱 향상

이 방법에는 몇 가지 근본적인 문제가 있습니다:

#### 1. 인간 기보에는 상한이 있다

인간 기사의 기력에는 극한이 있으며, 기보에는 인간의 이해뿐만 아니라 인간의 실수와 편견도 포함됩니다. AI가 인간 기보에서 배울 때, 배우는 것은:

- 인간이 좋다고 생각하는 수법(최적이 아닐 수 있음)
- 인간의 사고 방식(혁신을 제한할 수 있음)
- 인간의 실수(올바른 샘플로 학습됨)

#### 2. 지도 학습의 병목

지도 학습의 목표는 '인간 모방'—인간 기사가 어디에 둘지 예측하는 것입니다. 이는 AI의 능력 상한이 인간 기사의 능력에 의해 제한된다는 것을 의미합니다.

마치 도제가 스승만 모방할 수 있어서 영원히 스승을 초월할 수 없는 것과 같습니다.

#### 3. 데이터 수집 비용

고품질 인간 기보는 수년간의 축적이 필요하며, 바둑처럼 오랜 역사를 가진 게임에만 존재합니다. AI를 새로운 영역(예: 단백질 구조 예측)에 적용하려면 '인간 전문가 기보'가 전혀 없습니다.

### Zero의 돌파구

AlphaGo Zero는 지도 학습 단계를 완전히 건너뛰고 **무작위 초기화**에서 직접 자기 대국을 시작합니다. 이로써 위의 모든 문제가 해결됩니다:

| 문제 | 원본 AlphaGo | AlphaGo Zero |
|------|-------------|--------------|
| 인간 지식 상한 | 기보 품질에 제한됨 | 제한 없음 |
| 학습 목표 | 인간 모방 | 승률 최대화 |
| 데이터 요구량 | 3천만 국의 기보 | 0 |
| 일반화 가능성 | 바둑에만 한정 | 다른 영역으로 확장 가능 |

이것은 근본적인 패러다임 전환입니다: '인간 지식 학습'에서 '제1원리에서 지식 발견'으로.

---

## 원본 AlphaGo와의 대결: 100:0

### 압도적 승리

DeepMind는 훈련을 마친 AlphaGo Zero를 각 버전의 AlphaGo와 대국시켰습니다:

| 상대 | AlphaGo Zero 전적 |
|------|-------------------|
| AlphaGo Fan(판후이를 이긴 버전) | 100:0 |
| AlphaGo Lee(이세돌을 이긴 버전) | 100:0 |
| AlphaGo Master(60연승 버전) | 89:11 |

**100:0**—이것은 100판 경기에서 원본 AlphaGo가 한 판도 이기지 못했다는 것을 의미합니다.

### 더 적은 자원으로 더 강한 기력

이기기만 한 것이 아니라, AlphaGo Zero는 더 적은 자원으로 더 강한 기력을 달성했습니다:

| 지표 | AlphaGo Lee | AlphaGo Zero |
|------|-------------|--------------|
| 훈련 시간 | 수개월 | 40일(3일 만에 AlphaGo Lee 초월) |
| 훈련 대국 수 | 3천만 인간 기보 + 자기 대국 | 490만 국 자기 대국 |
| TPU 수(훈련) | 50+ | 4 |
| TPU 수(추론) | 48 | 4 |
| 입력 특징 | 48개 평면 | 17개 평면 |
| 신경망 | SL + RL 이중 네트워크 | 단일 이중 헤드 네트워크 |

이것은 놀라운 효율성 향상입니다: **자원 10배 이상 감소, 기력은 대폭 향상**.

### 왜 Zero가 더 강한가?

AlphaGo Zero가 더 강한 이유는 여러 관점에서 이해할 수 있습니다:

#### 1. 편견 없는 학습

원본 AlphaGo는 인간 기보에서 학습하여 인간의 편견을 계승했습니다. 예를 들어, 인간 기사는 특정 정석을 과도하게 중시하거나 특정 국면에 대해 잘못된 평가를 할 수 있습니다.

AlphaGo Zero는 이러한 짐이 없습니다. 백지에서 시작하여 승패 결과만으로 좋은 수가 무엇인지 학습합니다. 이로써 인간이 생각하지 못한 수법을 발견할 수 있습니다.

#### 2. 일관된 학습 목표

원본 AlphaGo의 훈련에는 두 가지 다른 목표가 있습니다:
- 지도 학습: 인간 착점 예측 정확도 최대화
- 강화 학습: 승률 최대화

이 두 목표는 서로 충돌할 수 있습니다. AlphaGo Zero는 단 하나의 목표만 있습니다: **승률 최대화**. 이로써 학습 과정이 더 일관되고 효과적입니다.

#### 3. 더 간결한 아키텍처

원본 AlphaGo는 분리된 Policy Network와 Value Network를 사용합니다. AlphaGo Zero는 단일 이중 헤드 네트워크를 사용하여(자세한 내용은 [다음 글: 이중 헤드 네트워크와 잔차 네트워크](../dual-head-resnet) 참조) 특징 표현이 공유되어 학습 효율이 향상됩니다.

---

## 단순화된 입력 특징: 48에서 17로

### 원본 AlphaGo의 48개 특징 평면

원본 AlphaGo의 신경망 입력은 48개의 19x19 특징 평면을 포함하며, 인간이 설계한 많은 특징을 인코딩합니다:

| 카테고리 | 특징 수 | 내용 |
|------|--------|------|
| 돌 위치 | 3 | 흑돌, 백돌, 빈 점 |
| 활로 | 8 | 1-8 활로의 돌 연결 |
| 따냄 | 8 | 1-8개 돌 따낼 수 있음 |
| 패 | 1 | 패싸움 위치 |
| 변까지 거리 | 4 | 1선에서 4선 |
| 착점 합법성 | 1 | 어떤 위치에 둘 수 있는지 |
| 히스토리 | 8 | 과거 8수의 위치 |
| 차례 | 1 | 흑 또는 백 |
| 기타 | 14 | 축, 눈 등 |

이 48개 특징은 바둑 전문가가 정성껏 설계한 것으로, 많은 도메인 지식을 포함합니다.

### AlphaGo Zero의 17개 특징 평면

AlphaGo Zero는 입력을 대폭 단순화하여 17개 특징 평면만 사용합니다:

| 평면 번호 | 내용 | 수량 |
|----------|------|------|
| 1-8 | 흑돌 위치(최근 8수) | 8 |
| 9-16 | 백돌 위치(최근 8수) | 8 |
| 17 | 현재 차례(전부 1 또는 전부 0) | 1 |

이 17개 특징은 다음만 포함합니다:
- **현재 바둑판 상태**: 각 위치에 흑돌, 백돌 또는 빈 곳
- **히스토리 정보**: 과거 8수의 바둑판 상태
- **차례 정보**: 누구 차례인지

활로 없음, 축 판단 없음, 변까지 거리 없음—이 모든 '바둑 지식'은 신경망이 스스로 학습하도록 합니다.

### 왜 단순화가 좋은가?

#### 1. 네트워크가 스스로 특징 발견

복잡한 수작업 특징은 중요한 정보를 놓치거나 잘못된 가정을 인코딩할 수 있습니다. 신경망이 원시 데이터에서 학습하면 더 나은 특징 표현을 발견할 수 있습니다.

사실 AlphaGo Zero는 인간이 설계한 모든 특징(활로, 축 등)을 학습했고, 인간이 명확히 인식하지 못한 일부 패턴도 학습했습니다.

#### 2. 더 나은 일반화 가능성

48개 특징 중 많은 것이 바둑 전용입니다(예: 축, 변까지 거리). 17개 단순화된 특징은 범용적입니다—모든 보드 게임을 유사한 방식으로 인코딩할 수 있습니다.

이것이 나중의 **AlphaZero**(범용 게임 AI)의 기초가 되었습니다.

#### 3. 인위적 오류 감소

수작업 설계 특징에는 오류나 불완전한 정의가 포함될 수 있습니다. 입력 단순화로 이러한 문제의 가능성이 제거됩니다.

---

## 단일 네트워크 아키텍처

### 원본의 이중 네트워크 설계

원본 AlphaGo는 두 개의 독립적인 신경망을 사용합니다:

```
Policy Network:  입력 → CNN → 19x19 착점 확률
Value Network:   입력 → CNN → 승률 평가(-1 ~ 1)
```

이 두 네트워크는:
- 다른 아키텍처(층 수, 채널 수 약간 다름)
- 독립적으로 훈련(먼저 Policy 훈련, 그 다음 Value 훈련)
- 어떤 매개변수도 공유하지 않음

### Zero의 이중 헤드 네트워크

AlphaGo Zero는 단일 네트워크를 사용하지만 두 개의 출력 헤드(heads)가 있습니다:

```
입력 → ResNet 공유 백본 → Policy Head → 19x19 착점 확률
                       → Value Head  → 승률 평가
```

두 Head는 동일한 ResNet 백본을 공유합니다(자세한 내용은 [다음 글: 이중 헤드 네트워크와 잔차 네트워크](../dual-head-resnet) 참조). 이로 인한 장점:

#### 1. 매개변수 효율성

공유 백본은 대부분의 매개변수가 두 작업에 공통 사용됨을 의미합니다. 총 매개변수 양이 줄고 과적합 위험이 낮아집니다.

#### 2. 특징 공유

"어디에 둬야 하는가"(Policy)와 "누가 이길 것인가"(Value)는 유사한 바둑판 패턴 이해가 필요합니다. 공유 백본으로 이러한 특징이 두 작업에 동시에 학습되고 활용됩니다.

#### 3. 훈련 안정성

공동 훈련으로 그래디언트 신호가 두 소스에서 오므로 더 풍부한 감독 신호를 제공하여 훈련이 더 안정적입니다.

### 잔차 네트워크의 위력

AlphaGo Zero의 백본은 **40층 잔차 네트워크(ResNet)**를 사용하며, 원본 AlphaGo의 13층 CNN보다 훨씬 깊습니다.

잔차 연결(skip connections)로 심층 네트워크가 효과적으로 훈련될 수 있으며, 그래디언트 소실 문제를 피합니다. 이것은 2015년 ImageNet 대회의 획기적 기술로, AlphaGo Zero에 바둑 영역에 성공적으로 적용되었습니다.

---

## 훈련 효율성 향상

### 자기 대국의 지수적 성장

AlphaGo Zero의 훈련 과정은 놀라운 효율성을 보여줍니다:

| 훈련 시간 | ELO 평점 | 해당 수준 |
|----------|----------|--------|
| 0시간 | 0 | 무작위 착점 |
| 3시간 | ~1000 | 기본 규칙 발견 |
| 12시간 | ~3000 | 정석 발견 |
| 36시간 | ~4500 | 판후이 버전 초월 |
| 60시간 | ~5200 | 이세돌 버전 초월 |
| 72시간 | ~5400 | 원본 AlphaGo 초월 |
| 40일 | ~5600 | 최강 버전 |

**3일 만에 인간 초월, 3일 만에 수개월 훈련한 이전 AI 초월**—이것은 지수적 효율성 향상입니다.

### 왜 이렇게 빠른가?

#### 1. 더 강한 탐색 유도

AlphaGo Zero의 MCTS는 완전히 신경망에 의해 유도되며, 더 이상 빠른 착점 전략(rollout)을 사용하지 않습니다. 이로써 탐색이 더 효율적이고 정확해집니다.

#### 2. 더 빠른 자기 대국

하나의 네트워크만 필요하므로(두 개가 아니라), 각 자기 대국의 계산 비용이 줄어듭니다. 같은 시간에 더 많은 훈련 데이터를 생성할 수 있습니다.

#### 3. 더 효과적인 학습

이중 헤드 네트워크의 공동 훈련으로 각 대국의 정보가 더 효과적으로 활용됩니다. Policy와 Value의 그래디언트가 서로 강화하여 수렴을 가속화합니다.

### 인간 학습과의 비교

인간 기사가 다른 수준에 도달하는 데 얼마나 걸리는가?

| 수준 | 인간 소요 시간 | AlphaGo Zero |
|------|-------------|--------------|
| 입문 | 수주 | 몇 분 |
| 아마추어 초단 | 수년 | 몇 시간 |
| 프로 수준 | 10-20년 | 1-2일 |
| 세계 챔피언 | 20년+ 전업 투입 | 3일 |
| 인간 초월 | 불가능 | 3일 |

이 비교는 인간 기사를 폄하하려는 것이 아닙니다—그들은 생물학적 뉴런을 사용하고, AlphaGo Zero는 전용 설계된 TPU와 수천 와트의 전력을 사용합니다. 하지만 올바른 학습 방법이 얼마나 효율적일 수 있는지 보여줍니다.

---

## 범용성: 체스, 장기

### AlphaZero의 탄생

2017년 12월, DeepMind는 **AlphaZero**—AlphaGo Zero의 범용 버전을 발표했습니다. 동일한 알고리즘으로 게임 규칙만 수정하면 세 가지 보드 게임에서 세계 최고 수준에 도달합니다:

| 게임 | 훈련 시간 | 상대 | 전적 |
|------|----------|------|------|
| 바둑 | 8시간 | AlphaGo Zero | 60:40 |
| 체스 | 4시간 | Stockfish 8 | 28승 72무 0패 |
| 장기 | 2시간 | Elmo | 90:8:2 |

상대에 주목하세요:
- **Stockfish**는 당시 가장 강력한 체스 엔진으로, 수십 년간의 인간 지식과 최적화 사용
- **Elmo**는 당시 가장 강력한 장기 AI

AlphaZero는 몇 시간의 훈련으로 수년간 개발된 이 전용 시스템들을 초월했습니다.

### 범용성의 의미

AlphaGo Zero / AlphaZero는 중요한 것을 증명했습니다:

> **동일한 학습 알고리즘이 다른 영역에서 초인 수준에 도달할 수 있다.**

이것은 세 개의 다른 AI가 아니라 하나의 범용 학습 프레임워크입니다:

1. **자기 대국**이 경험 생성
2. **몬테카를로 트리 탐색**이 가능성 탐색
3. **신경망**이 정책과 가치 함수 학습
4. **강화 학습**이 목표 함수 최적화

이 프레임워크는 도메인 특정 지식에 의존하지 않으며, AI의 범용화를 향한 중요한 단계입니다.

### 전통적 AI에 대한 충격

AlphaZero 이전에 체스와 장기의 가장 강력한 AI는 '전문가 시스템' 스타일이었습니다:

- **대량의 인간 지식**: 오프닝 북, 엔드게임 북, 평가 함수
- **수십 년 최적화**: 수많은 기사와 엔지니어의 노력
- **극도의 전문화**: Stockfish는 바둑을 둘 수 없고, Elmo는 체스를 둘 수 없음

AlphaZero는 하나의 범용 알고리즘으로 몇 시간 만에 이 모든 것을 초월했습니다. 이로 인해 많은 AI 연구자들이 다시 생각하게 되었습니다:

> '범용 학습 알고리즘'에 더 많은 노력을 투입해야 하는가, 아니면 '전문가 지식 인코딩'에?

답은 점점 명확해지고 있습니다: 기계가 스스로 학습하게 하는 것이 지식을 가르치는 것보다 더 효과적입니다.

---

## AlphaGo Zero의 바둑 스타일

### 인간을 초월한 미학

바둑계는 AlphaGo Zero의 수법에 대해 보편적인 평가를 내렸습니다: **더 아름답다**.

AlphaGo Lee의 수법은 때때로 '이상해' 보였습니다—37번째 수처럼 인간은 사후 분석 후에야 그 묘미를 이해할 수 있었습니다. 하지만 AlphaGo Zero의 수법은 종종 사후에 "한눈에 좋은 수라는 것을 알 수 있다"고 평가됩니다.

이것은 아마:

1. **더 강한 기력**: Zero는 더 깊이 볼 수 있어 착점이 더 여유로움
2. **인간 편견 없음**: 전통 정석에 구속받지 않음
3. **일관된 목표**: 오직 승률만 추구, 인간 모방 안 함

### 인간 바둑 이치 재발견

흥미롭게도 AlphaGo Zero는 훈련 과정에서 인간이 수천 년간 축적한 바둑 지식을 '재발견'했습니다:

- **정석**: Zero는 많은 일반적인 정석을 스스로 발견했습니다. 이것들이 실제로 쌍방의 최적해이기 때문
- **포석 원칙**: 귀, 변, 중앙의 중요성 순서
- **모양 지식**: 우형과 좋은 형의 구별

이것은 인간 바둑 이치의 합리성을 검증합니다—이 지식은 우연이 아니라 바둑 본질의 반영입니다.

### 인간을 초월한 혁신

하지만 Zero는 인간이 생각하지 못한 수법도 발견했습니다:

- **비전통적 오프닝**: 전통적 오프닝을 기반으로 한 변화
- **공격적인 버림**: 인간보다 국소를 포기하고 전체 이익을 얻으려 함
- **반직관적 모양**: 표면상 '나쁜 형'이 실제로 최적해

이러한 혁신이 인간의 바둑 이해를 바꾸고 있습니다. 많은 프로 기사들은 AlphaGo Zero의 기보 연구가 바둑에 대한 완전히 새로운 인식을 주었다고 말합니다.

---

## 기술적 세부사항 요약

### 원본 AlphaGo와의 완전한 비교

| 측면 | AlphaGo(원본) | AlphaGo Zero |
|------|----------------|--------------|
| **훈련 데이터** | 인간 기보 + 자기 대국 | 순수 자기 대국 |
| **학습 방법** | 지도 학습 + 강화 학습 | 순수 강화 학습 |
| **입력 특징** | 48개 평면 | 17개 평면 |
| **네트워크 아키텍처** | 분리된 Policy/Value | 이중 헤드 ResNet |
| **네트워크 깊이** | 13층 | 40층(또는 그 이상) |
| **MCTS 평가** | 신경망 + Rollout | 순수 신경망 |
| **탐색 횟수** | 수당 ~100,000 | 수당 ~1,600 |
| **훈련 TPU** | 50+ | 4 |
| **추론 TPU** | 48 | 4(확장 가능) |

### 핵심 알고리즘

AlphaGo Zero의 훈련 루프는 매우 간결합니다:

```
1. 자기 대국
   - 현재 네트워크로 MCTS 수행
   - MCTS 탐색 확률에 따라 착점 선택
   - 각 수의 (국면, MCTS 확률, 승패 결과) 기록

2. 네트워크 훈련
   - 경험 풀에서 샘플링
   - Policy Head: MCTS 확률과의 크로스 엔트로피 최소화
   - Value Head: 실제 승패와의 평균 제곱 오차 최소화
   - 두 목표 공동 최적화

3. 네트워크 업데이트
   - 새 네트워크로 구 네트워크 교체(대국으로 새 네트워크가 더 강함 검증)
   - 1단계로 돌아감
```

이 루프가 지속적으로 실행되며 네트워크가 계속 강해집니다. 인간 데이터 없음, 인간 지식 없음, 오직 게임 규칙과 승패 목표만.

---

## AI 연구에 대한 시사점

### 제1원리 학습

AlphaGo Zero는 '제1원리' 학습 방법을 보여줍니다:

> AI에게 어떻게 해야 하는지 말하지 말고, 목표가 무엇인지만 말하고 스스로 방법을 발견하게 하라.

이것은 전통적 전문가 시스템 방법과 선명한 대조를 이룹니다. 전문가 시스템은 인간 지식을 AI에 인코딩하려 하고, AlphaGo Zero는 AI가 스스로 지식을 발견하게 합니다.

결과: AI가 발견한 지식이 인간 지식보다 더 완전하고 더 정확할 수 있습니다.

### 자기 대국의 위력

AlphaGo Zero는 자기 대국이 무한한 훈련 데이터를 생성할 수 있음을 증명했으며, 이 데이터의 품질은 네트워크 향상에 따라 향상됩니다.

이것은 '긍정적 순환'입니다:
- 더 강한 네트워크 → 더 좋은 자기 대국 데이터
- 더 좋은 데이터 → 더 강한 네트워크

이 순환은 게임의 이론적 상한(존재한다면)에 도달할 때까지 계속 실행될 수 있습니다.

### 단순화의 중요성

AlphaGo Zero의 성공은 '단순화'의 중요성을 증명합니다:

- 입력 단순화(48 → 17)
- 아키텍처 단순화(이중 네트워크 → 단일 네트워크)
- 훈련 단순화(지도 + 강화 → 순수 강화)

매번 단순화가 시스템을 더 강하게 만들었습니다. 이것은 알려줍니다: 복잡함이 좋은 것이 아니며, 가장 간단한 해결책이 종종 가장 좋습니다.

---

## 애니메이션 대응

이 글에서 다루는 핵심 개념과 애니메이션 번호:

| 번호 | 개념 | 물리/수학 대응 |
|------|------|--------------|
| E7 | 처음부터 훈련 | 자기 조직화 현상 |
| E5 | 자기 대국 | 고정점 수렴 |
| E12 | 기력 성장 곡선 | S형 성장 |
| D12 | 잔차 네트워크 | 그래디언트 고속도로 |

---

## 더 읽을거리

- **다음 글**: [이중 헤드 네트워크와 잔차 네트워크](../dual-head-resnet) — AlphaGo Zero의 신경망 아키텍처 상세
- **관련 글**: [자기 대국](../self-play) — 왜 자기 대국이 초인 수준을 만들 수 있는가
- **기술 심화**: [처음부터의 훈련 과정](../training-from-scratch) — Day 0-3의 상세 진화

---

## 참고 자료

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
3. DeepMind. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
4. Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." *Nature*, 588, 604-609.
