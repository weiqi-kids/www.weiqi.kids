---
sidebar_position: 1
title: 심층 연구를 원하는 분들을 위해
description: "고급 주제 가이드: 신경망, MCTS, 학습, 최적화, 배포"
---

# 심층 연구를 원하는 분들을 위해

이 섹션은 바둑 AI를 깊이 연구하고자 하는 엔지니어를 위한 것으로, 기술 구현, 이론적 기초 및 실용적 응용을 다룹니다.

---

## 문서 개요

### 핵심 기술

| 문서 | 설명 |
|------|------|
| [신경망 아키텍처 상세 분석](./neural-network) | KataGo의 잔차 네트워크, 입력 특성, 다중 헤드 출력 설계 |
| [MCTS 구현 세부사항](./mcts-implementation) | PUCT 선택, 가상 손실, 배치 평가, 병렬화 |
| [KataGo 학습 메커니즘 분석](./training) | 자가 대국, 손실 함수, 학습 루프 |

### 성능 최적화

| 문서 | 설명 |
|------|------|
| [GPU 백엔드와 최적화](./gpu-optimization) | CUDA, OpenCL, Metal 백엔드 비교 및 튜닝 |
| [모델 양자화와 배포](./quantization-deploy) | FP16, INT8, TensorRT, 각 플랫폼 배포 |
| [평가 및 벤치마크 테스트](./evaluation) | Elo 레이팅, 대국 테스트, SPRT 통계 방법 |

### 고급 주제

| 문서 | 설명 |
|------|------|
| [분산 학습 아키텍처](./distributed-training) | Self-play Worker, 데이터 수집, 모델 배포 |
| [사용자 정의 규칙과 변형](./custom-rules) | 중국, 일본, AGA 규칙, 바둑판 크기 변형 |
| [핵심 논문 가이드](./papers) | AlphaGo, AlphaZero, KataGo 논문 핵심 분석 |

### 오픈소스와 구현

| 문서 | 설명 |
|------|------|
| [KataGo 소스 코드 가이드](./source-code) | 디렉토리 구조, 핵심 모듈, 코딩 스타일 |
| [오픈소스 커뮤니티 참여](./contributing) | 기여 방법, 분산 학습, 커뮤니티 참여 |
| [처음부터 바둑 AI 만들기](./build-from-scratch) | 단계별 간단한 AlphaGo Zero 구현 |

---

## 무엇을 하고 싶으신가요?

| 목표 | 추천 경로 |
|------|---------|
| 신경망 설계 이해하기 | [신경망 아키텍처 상세 분석](./neural-network) → [MCTS 구현 세부사항](./mcts-implementation) |
| 실행 성능 최적화하기 | [GPU 백엔드와 최적화](./gpu-optimization) → [모델 양자화와 배포](./quantization-deploy) |
| 학습 방법 연구하기 | [KataGo 학습 메커니즘 분석](./training) → [분산 학습 아키텍처](./distributed-training) |
| 논문 원리 이해하기 | [핵심 논문 가이드](./papers) → [신경망 아키텍처 상세 분석](./neural-network) |
| 직접 프로그래밍하기 | [처음부터 바둑 AI 만들기](./build-from-scratch) → [KataGo 소스 코드 가이드](./source-code) |
| 오픈소스 프로젝트 참여하기 | [오픈소스 커뮤니티 참여](./contributing) → [KataGo 소스 코드 가이드](./source-code) |

---

## 고급 개념 색인

심층 연구 시 다음과 같은 고급 개념을 접하게 됩니다:

### F 시리즈: 스케일링 (8개)

| 번호 | 바둑 개념 | 물리/수학 대응 |
|------|---------|--------------|
| F1 | 바둑판 크기 vs 복잡도 | 복잡도 스케일링 |
| F2 | 네트워크 크기 vs 기력 | 용량 스케일링 |
| F3 | 학습 시간 vs 수익 | 수익 체감의 법칙 |
| F4 | 데이터량 vs 일반화 | 샘플 복잡도 |
| F5 | 컴퓨팅 자원 스케일링 | 스케일링 법칙 |
| F6 | 신경 스케일링 법칙 | 이중 로그 관계 |
| F7 | 대규모 배치 학습 | 임계 배치 |
| F8 | 파라미터 효율성 | 압축 한계 |

### G 시리즈: 차원 (6개)

| 번호 | 바둑 개념 | 물리/수학 대응 |
|------|---------|--------------|
| G1 | 고차원 표현 | 벡터 공간 |
| G2 | 차원의 저주 | 고차원 딜레마 |
| G3 | 다양체 가설 | 저차원 다양체 |
| G4 | 중간 표현 | 잠재 공간 |
| G5 | 특성 분리 | 독립 성분 |
| G6 | 의미 방향 | 기하 대수 |

### H 시리즈: 강화학습 (9개)

| 번호 | 바둑 개념 | 물리/수학 대응 |
|------|---------|--------------|
| H1 | MDP | 마르코프 체인 |
| H2 | 벨만 방정식 | 동적 프로그래밍 |
| H3 | 가치 반복 | 고정점 정리 |
| H4 | 정책 그래디언트 | 확률적 최적화 |
| H5 | 경험 재생 | 중요도 샘플링 |
| H6 | 할인 인자 | 시간 선호 |
| H7 | TD 학습 | 증분 추정 |
| H8 | 어드밴티지 함수 | 베이스라인 분산 감소 |
| H9 | PPO 클리핑 | 신뢰 영역 |

### K 시리즈: 최적화 방법 (6개)

| 번호 | 바둑 개념 | 물리/수학 대응 |
|------|---------|--------------|
| K1 | SGD | 확률적 근사 |
| K2 | 모멘텀 | 관성 |
| K3 | Adam | 적응형 스텝 크기 |
| K4 | 학습률 감소 | 어닐링 |
| K5 | 그래디언트 클리핑 | 포화 제한 |
| K6 | SGD 노이즈 | 확률적 교란 |

### L 시리즈: 일반화와 안정성 (5개)

| 번호 | 바둑 개념 | 물리/수학 대응 |
|------|---------|--------------|
| L1 | 과적합 | 과잉 적응 |
| L2 | 정규화 | 제약 최적화 |
| L3 | Dropout | 희소 활성화 |
| L4 | 데이터 증강 | 대칭 깨짐 |
| L5 | 조기 종료 | 최적 정지 |

---

## 하드웨어 요구사항

### 읽기와 학습

특별한 요구사항 없음, 어떤 컴퓨터든 가능합니다.

### 모델 학습

| 규모 | 권장 하드웨어 | 학습 시간 |
|------|---------|---------|
| 미니 (b6c96) | GTX 1060 6GB | 수 시간 |
| 소형 (b10c128) | RTX 3060 12GB | 1-2일 |
| 중형 (b18c384) | RTX 4090 24GB | 1-2주 |
| 완전 (b40c256) | 다중 GPU 클러스터 | 수 주 |

### 분산 학습 기여

- GPU가 있는 모든 컴퓨터가 참여 가능
- 최소 GTX 1060 또는 동급 권장
- 안정적인 네트워크 연결 필요

---

## 시작하기

**여기서 시작하는 것을 추천합니다:**

- 원리를 이해하고 싶다면? → [신경망 아키텍처 상세 분석](./neural-network)
- 직접 구현하고 싶다면? → [처음부터 바둑 AI 만들기](./build-from-scratch)
- 논문을 읽고 싶다면? → [핵심 논문 가이드](./papers)
