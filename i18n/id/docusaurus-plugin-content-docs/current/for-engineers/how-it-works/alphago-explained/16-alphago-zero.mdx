---
sidebar_position: 17
title: Ikhtisar AlphaGo Zero
description: Mulai dari nol, belajar mandiri sepenuhnya, bagaimana AlphaGo Zero melampaui semua versi sebelumnya tanpa menggunakan catatan permainan manusia
keywords: [AlphaGo Zero, self-play, reinforcement learning, deep learning, AI Go, unsupervised learning]
---

# Ikhtisar AlphaGo Zero

Pada Oktober 2017, DeepMind mengumumkan hasil yang mengejutkan dunia AI: **AlphaGo Zero** tanpa menggunakan catatan permainan manusia sama sekali, mulai berlatih dari keadaan acak sepenuhnya, hanya dalam tiga hari melampaui AlphaGo original yang mengalahkan Lee Sedol, dan menang dengan skor **100:0**.

Ini bukan sekadar kemajuan dalam angka. Ini mewakili paradigma baru: **AI tidak membutuhkan pengetahuan manusia, bisa menemukan segalanya dari nol**.

---

## Mengapa Tidak Memerlukan Catatan Permainan Manusia?

### Keterbatasan Catatan Permainan Manusia

Proses pelatihan AlphaGo original dibagi menjadi dua tahap:

1. **Supervised Learning**: Melatih Policy Network dengan 30 juta catatan permainan manusia
2. **Reinforcement Learning**: Meningkatkan lebih lanjut melalui self-play

Metode ini memiliki beberapa masalah fundamental:

#### 1. Catatan Permainan Manusia Memiliki Batas Atas

Kekuatan bermain pemain manusia terbatas, catatan permainan berisi pemahaman manusia, juga termasuk kesalahan dan bias manusia. Ketika AI belajar dari catatan permainan manusia, yang dipelajarinya adalah:

- Langkah yang menurut manusia bagus (tapi belum tentu optimal)
- Pola berpikir manusia (tapi mungkin membatasi inovasi)
- Kesalahan manusia (akan dipelajari sebagai sampel yang benar)

#### 2. Bottleneck Supervised Learning

Tujuan supervised learning adalah "meniru manusia"—memprediksi langkah mana yang akan dimainkan pemain manusia. Ini berarti batas kemampuan AI dibatasi oleh kemampuan pemain manusia.

Seperti seorang murid yang hanya bisa meniru gurunya, tidak akan pernah bisa melampaui gurunya.

#### 3. Biaya Pengumpulan Data

Catatan permainan manusia berkualitas tinggi membutuhkan waktu bertahun-tahun untuk dikumpulkan, dan hanya ada di game seperti Go yang memiliki sejarah panjang. Jika ingin menerapkan AI ke bidang baru (seperti prediksi struktur protein), tidak ada "catatan permainan ahli manusia" yang tersedia sama sekali.

### Terobosan Zero

AlphaGo Zero sepenuhnya melewati tahap supervised learning, langsung memulai self-play dari **inisialisasi acak**. Ini menyelesaikan semua masalah di atas:

| Masalah | AlphaGo Original | AlphaGo Zero |
|------|-------------|--------------|
| Batas pengetahuan manusia | Dibatasi kualitas catatan | Tidak ada batasan ini |
| Tujuan pembelajaran | Meniru manusia | Memaksimalkan win rate |
| Kebutuhan data | 30 juta catatan | 0 |
| Kemampuan generalisasi | Hanya terbatas Go | Bisa digeneralisasi ke bidang lain |

Ini adalah perubahan paradigma fundamental: dari "mempelajari pengetahuan manusia" ke "menemukan pengetahuan dari prinsip pertama".

---

## Perbandingan dengan AlphaGo Original: 100:0

### Kemenangan Telak

DeepMind membuat AlphaGo Zero yang sudah terlatih bertanding melawan berbagai versi AlphaGo:

| Lawan | Rekor AlphaGo Zero |
|------|-------------------|
| AlphaGo Fan (versi yang mengalahkan Fan Hui) | 100:0 |
| AlphaGo Lee (versi yang mengalahkan Lee Sedol) | 100:0 |
| AlphaGo Master (versi 60 kemenangan beruntun) | 89:11 |

**100:0**—ini berarti dalam 100 pertandingan, AlphaGo original tidak bisa menang satu pun.

### Lebih Sedikit Sumber Daya, Kekuatan Bermain Lebih Tinggi

Bukan hanya menang, AlphaGo Zero juga mencapai kekuatan bermain lebih tinggi dengan sumber daya lebih sedikit:

| Metrik | AlphaGo Lee | AlphaGo Zero |
|------|-------------|--------------|
| Waktu pelatihan | Berbulan-bulan | 40 hari (3 hari melampaui AlphaGo Lee) |
| Jumlah permainan pelatihan | 30 juta catatan manusia + self-play | 4.9 juta self-play |
| Jumlah TPU (pelatihan) | 50+ | 4 |
| Jumlah TPU (inferensi) | 48 | 4 |
| Fitur input | 48 plane | 17 plane |
| Neural network | Dual network SL + RL | Single dual-head network |

Ini adalah peningkatan efisiensi yang menakjubkan: **sumber daya berkurang 10 kali lipat atau lebih, kekuatan bermain justru meningkat signifikan**.

### Mengapa Zero Lebih Kuat?

Alasan AlphaGo Zero lebih kuat bisa dipahami dari beberapa sudut pandang:

#### 1. Pembelajaran Tanpa Bias

AlphaGo original belajar dari catatan permainan manusia, mewarisi bias manusia. Misalnya, pemain manusia mungkin terlalu menekankan joseki tertentu, atau memiliki evaluasi yang salah pada posisi tertentu.

AlphaGo Zero tidak memiliki beban ini. Ia mulai dari kertas kosong, hanya belajar apa yang merupakan langkah bagus melalui hasil menang/kalah. Ini memungkinkannya menemukan langkah yang tidak pernah terpikirkan manusia.

#### 2. Tujuan Pembelajaran yang Konsisten

Pelatihan AlphaGo original memiliki dua tujuan berbeda:
- Supervised learning: Memaksimalkan akurasi prediksi langkah manusia
- Reinforcement learning: Memaksimalkan win rate

Kedua tujuan ini mungkin saling bertentangan. AlphaGo Zero hanya memiliki satu tujuan: **maksimalisasi win rate**. Ini membuat proses pembelajaran lebih konsisten dan efektif.

#### 3. Arsitektur yang Lebih Simpel

AlphaGo original menggunakan Policy Network dan Value Network yang terpisah. AlphaGo Zero menggunakan single dual-head network (lihat artikel berikutnya untuk detail), memungkinkan representasi fitur dibagi, meningkatkan efisiensi pembelajaran.

---

## Fitur Input yang Disederhanakan: Dari 48 ke 17

### 48 Plane Fitur AlphaGo Original

Input neural network AlphaGo original berisi 48 plane fitur 19x19, mengkodekan banyak fitur yang dirancang manusia:

| Kategori | Jumlah Fitur | Konten |
|------|--------|------|
| Posisi batu | 3 | Batu hitam, batu putih, titik kosong |
| Jumlah liberty | 8 | String dengan 1-8 liberty |
| Capture | 8 | Bisa capture 1-8 batu |
| Ko | 1 | Posisi ko |
| Jarak dari tepi | 4 | Baris pertama sampai keempat |
| Legalitas langkah | 1 | Posisi mana yang bisa dimainkan |
| State historis | 8 | Posisi 8 langkah terakhir |
| Giliran | 1 | Hitam atau putih |
| Lainnya | 14 | Ladder, eye, dll. |

48 fitur ini dirancang dengan cermat oleh ahli Go, berisi banyak domain knowledge.

### 17 Plane Fitur AlphaGo Zero

AlphaGo Zero sangat menyederhanakan input, hanya menggunakan 17 plane fitur:

| Nomor Plane | Konten | Jumlah |
|----------|------|------|
| 1-8 | Posisi batu hitam (8 langkah terakhir) | 8 |
| 9-16 | Posisi batu putih (8 langkah terakhir) | 8 |
| 17 | Giliran saat ini (semua 1 atau semua 0) | 1 |

17 fitur ini hanya berisi:
- **State papan saat ini**: Setiap posisi ada batu hitam, batu putih, atau kosong
- **Informasi historis**: State papan 8 langkah terakhir
- **Informasi giliran**: Giliran siapa bermain

Tidak ada jumlah liberty, tidak ada penilaian ladder, tidak ada jarak dari tepi—semua "pengetahuan Go" ini dibiarkan neural network mempelajarinya sendiri.

### Mengapa Penyederhanaan Itu Bagus?

#### 1. Biarkan Network Menemukan Fitur Sendiri

Fitur buatan tangan yang kompleks mungkin melewatkan informasi penting, atau mengkodekan asumsi yang salah. Membiarkan neural network belajar dari data mentah, ia mungkin menemukan representasi fitur yang lebih baik.

Faktanya terbukti, AlphaGo Zero mempelajari semua fitur yang dirancang manusia (jumlah liberty, ladder, dll.), dan juga mempelajari beberapa pola yang tidak disadari manusia secara eksplisit.

#### 2. Kemampuan Generalisasi Lebih Baik

Banyak dari 48 fitur yang khusus untuk Go (seperti ladder, jarak dari tepi). 17 fitur yang disederhanakan bersifat universal—game papan apa pun bisa dikodekan dengan cara serupa.

Ini meletakkan dasar untuk **AlphaZero** (AI game universal) selanjutnya.

#### 3. Mengurangi Kesalahan Buatan

Fitur yang dirancang manual mungkin berisi definisi yang salah atau tidak lengkap. Menyederhanakan input menghilangkan kemungkinan masalah semacam ini.

---

## Arsitektur Network Tunggal

### Desain Dual Network Original

AlphaGo original menggunakan dua neural network independen:

```
Policy Network:  Input → CNN → Probabilitas langkah 19x19
Value Network:   Input → CNN → Estimasi win rate (-1 sampai 1)
```

Kedua network ini:
- Memiliki arsitektur berbeda (jumlah layer, channel sedikit berbeda)
- Dilatih secara independen (latih Policy dulu, lalu Value)
- Tidak berbagi parameter apa pun

### Dual-Head Network Zero

AlphaGo Zero menggunakan network tunggal, tapi dengan dua output head:

```
Input → ResNet shared backbone → Policy Head → Probabilitas langkah 19x19
                               → Value Head  → Estimasi win rate
```

Kedua Head berbagi backbone ResNet yang sama (lihat [artikel berikutnya: Dual-Head Network dan Residual Network](./17-dual-head-resnet) untuk detail), ini membawa beberapa keuntungan:

#### 1. Efisiensi Parameter

Shared backbone berarti sebagian besar parameter digunakan bersama oleh kedua tugas. Ini mengurangi total jumlah parameter, menurunkan risiko overfitting.

#### 2. Berbagi Fitur

"Harus bermain di mana" (Policy) dan "siapa yang akan menang" (Value) membutuhkan pemahaman pola papan yang serupa. Shared backbone memungkinkan fitur-fitur ini dipelajari dan dimanfaatkan oleh kedua tugas secara bersamaan.

#### 3. Stabilitas Pelatihan

Pelatihan joint membuat sinyal gradien berasal dari dua sumber, menyediakan sinyal supervisi yang lebih kaya, membuat pelatihan lebih stabil.

### Kekuatan Residual Network

Backbone AlphaGo Zero menggunakan **40-layer Residual Network (ResNet)**, jauh lebih dalam dari 13-layer CNN AlphaGo original.

Residual connection (skip connections) memungkinkan deep network dilatih secara efektif, menghindari masalah vanishing gradient. Ini adalah teknologi terobosan dari kompetisi ImageNet 2015, berhasil diterapkan oleh AlphaGo Zero ke bidang Go.

---

## Peningkatan Efisiensi Pelatihan

### Pertumbuhan Eksponensial Self-Play

Proses pelatihan AlphaGo Zero menunjukkan efisiensi yang menakjubkan:

| Waktu Pelatihan | Rating ELO | Setara dengan |
|----------|----------|--------|
| 0 jam | 0 | Bermain acak |
| 3 jam | ~1000 | Menemukan aturan dasar |
| 12 jam | ~3000 | Menemukan joseki |
| 36 jam | ~4500 | Melampaui versi Fan Hui |
| 60 jam | ~5200 | Melampaui versi Lee Sedol |
| 72 jam | ~5400 | Melampaui AlphaGo original |
| 40 hari | ~5600 | Versi terkuat |

**Tiga hari melampaui manusia, tiga hari melampaui AI yang sebelumnya dilatih berbulan-bulan**—ini adalah peningkatan efisiensi eksponensial.

### Mengapa Secepat Ini?

#### 1. Panduan Pencarian yang Lebih Kuat

MCTS AlphaGo Zero sepenuhnya dipandu oleh neural network, tidak lagi menggunakan fast rollout policy. Ini membuat pencarian lebih efisien dan akurat.

#### 2. Self-Play Lebih Cepat

Karena hanya membutuhkan satu network (bukan dua), biaya komputasi setiap self-play berkurang. Ini berarti lebih banyak data pelatihan bisa dihasilkan dalam waktu yang sama.

#### 3. Pembelajaran Lebih Efektif

Pelatihan joint dual-head network membuat informasi setiap permainan dimanfaatkan lebih efektif. Gradien Policy dan Value saling memperkuat, mempercepat konvergensi.

### Perbandingan dengan Pembelajaran Manusia

Berapa lama waktu yang dibutuhkan pemain manusia untuk mencapai level berbeda?

| Level | Waktu yang Dibutuhkan Manusia | AlphaGo Zero |
|------|-------------|--------------|
| Pemula | Beberapa minggu | Beberapa menit |
| Amateur 1-dan | Beberapa tahun | Beberapa jam |
| Level profesional | 10-20 tahun | 1-2 hari |
| Juara dunia | 20+ tahun dedikasi penuh | 3 hari |
| Melampaui manusia | Tidak mungkin | 3 hari |

Perbandingan ini bukan untuk meremehkan pemain manusia—mereka menggunakan neuron biologis, sedangkan AlphaGo Zero menggunakan TPU yang dirancang khusus dan listrik beberapa ribu watt. Tapi ini memang menunjukkan betapa efisiennya metode pembelajaran yang tepat.

---

## Universalitas: Catur, Shogi

### Kelahiran AlphaZero

Pada Desember 2017, DeepMind mengumumkan **AlphaZero**—versi universal AlphaGo Zero. Algoritma yang sama, hanya perlu memodifikasi aturan permainan, bisa mencapai level dunia tertinggi di tiga permainan papan:

| Permainan | Waktu Pelatihan | Lawan | Rekor |
|------|----------|------|------|
| Go | 8 jam | AlphaGo Zero | 60:40 |
| Catur | 4 jam | Stockfish 8 | 28 menang 72 seri 0 kalah |
| Shogi | 2 jam | Elmo | 90:8:2 |

Perhatikan lawan-lawannya:
- **Stockfish** adalah engine catur terkuat saat itu, menggunakan puluhan tahun pengetahuan manusia dan optimisasi
- **Elmo** adalah AI shogi terkuat saat itu

AlphaZero dengan pelatihan beberapa jam, melampaui sistem khusus yang dikembangkan bertahun-tahun ini.

### Makna Universalitas

AlphaGo Zero / AlphaZero membuktikan satu hal penting:

> **Algoritma pembelajaran yang sama, bisa mencapai level superhuman di berbagai domain.**

Ini bukan tiga AI berbeda, tapi satu framework pembelajaran universal:

1. **Self-play** menghasilkan pengalaman
2. **Monte Carlo Tree Search** mengeksplorasi kemungkinan
3. **Neural Network** mempelajari fungsi policy dan value
4. **Reinforcement Learning** mengoptimalkan fungsi objektif

Framework ini tidak bergantung pada pengetahuan domain-specific, ini adalah langkah penting menuju AI yang lebih universal.

### Dampak terhadap AI Tradisional

Sebelum AlphaZero, AI terkuat untuk catur dan shogi semuanya bergaya "expert system":

- **Banyak pengetahuan manusia**: Opening book, endgame tablebase, fungsi evaluasi
- **Optimisasi puluhan tahun**: Hasil kerja keras tak terhitung pemain dan engineer
- **Sangat terspesialisasi**: Stockfish tidak bisa bermain Go, Elmo tidak bisa bermain catur

AlphaZero dengan satu algoritma universal melampaui semua ini dalam beberapa jam. Ini membuat banyak peneliti AI memikirkan ulang:

> Haruskah kita menginvestasikan lebih banyak usaha pada "algoritma pembelajaran universal", atau "pengkodean pengetahuan expert"?

Jawabannya tampaknya semakin jelas: membiarkan mesin belajar sendiri, lebih efektif daripada mengajarinya pengetahuan.

---

## Gaya Bermain AlphaGo Zero

### Melampaui Estetika Manusia

Komunitas Go memiliki evaluasi umum terhadap langkah-langkah AlphaGo Zero: **lebih indah**.

Langkah-langkah AlphaGo Lee kadang terlihat "aneh"—seperti langkah ke-37, manusia perlu analisis setelahnya baru memahami keindahannya. Tapi langkah-langkah AlphaGo Zero sering dievaluasi sebagai "langsung terlihat sebagai langkah bagus" setelahnya.

Ini mungkin karena:

1. **Kekuatan bermain lebih tinggi**: Zero bisa melihat lebih dalam, langkah lebih tenang
2. **Tanpa bias manusia**: Tidak terikat oleh joseki tradisional
3. **Tujuan yang konsisten**: Hanya mengejar win rate, tidak meniru manusia

### Menemukan Kembali Prinsip Go Manusia

Menariknya, AlphaGo Zero dalam proses pelatihan "menemukan kembali" pengetahuan Go yang diakumulasi manusia selama ribuan tahun:

- **Joseki**: Zero menemukan sendiri banyak joseki umum, karena ini memang solusi optimal untuk kedua belah pihak
- **Prinsip opening**: Urutan pentingnya sudut, sisi, tengah
- **Pengetahuan bentuk**: Perbedaan antara bentuk buruk dan bentuk bagus

Ini memvalidasi rasionalitas prinsip Go manusia—pengetahuan ini bukan kebetulan, tapi refleksi dari esensi Go.

### Inovasi Melampaui Manusia

Tapi Zero juga menemukan langkah yang tidak pernah terpikirkan manusia:

- **Opening non-konvensional**: Variasi pada dasar opening tradisional
- **Korban agresif**: Lebih bersedia daripada manusia untuk mengorbankan lokal demi keuntungan global
- **Bentuk counter-intuitif**: Bentuk yang tampak "buruk" ternyata adalah solusi optimal

Inovasi-inovasi ini sedang mengubah pemahaman manusia tentang Go. Banyak pemain profesional mengatakan, mempelajari catatan permainan AlphaGo Zero memberi mereka pemahaman baru tentang Go.

---

## Ringkasan Detail Teknis

### Perbandingan Lengkap dengan AlphaGo Original

| Aspek | AlphaGo (Original) | AlphaGo Zero |
|------|----------------|--------------|
| **Data pelatihan** | Catatan manusia + self-play | Self-play murni |
| **Metode pembelajaran** | Supervised learning + Reinforcement learning | Reinforcement learning murni |
| **Fitur input** | 48 plane | 17 plane |
| **Arsitektur network** | Policy/Value terpisah | Dual-head ResNet |
| **Kedalaman network** | 13 layer | 40 layer (atau lebih) |
| **Evaluasi MCTS** | Neural network + Rollout | Neural network murni |
| **Jumlah pencarian** | ~100,000 per langkah | ~1,600 per langkah |
| **TPU pelatihan** | 50+ | 4 |
| **TPU inferensi** | 48 | 4 (bisa diperluas) |

### Algoritma Inti

Loop pelatihan AlphaGo Zero sangat simpel:

```
1. Self-play
   - Lakukan MCTS dengan network saat ini
   - Pilih langkah berdasarkan probabilitas pencarian MCTS
   - Catat setiap langkah (posisi, probabilitas MCTS, hasil menang/kalah)

2. Latih Network
   - Ambil sampel dari experience pool
   - Policy Head: Minimalkan cross-entropy dengan probabilitas MCTS
   - Value Head: Minimalkan mean squared error dengan hasil menang/kalah aktual
   - Optimalkan kedua tujuan secara joint

3. Update Network
   - Ganti network lama dengan network baru (verifikasi network baru lebih kuat melalui pertandingan)
   - Kembali ke langkah 1
```

Loop ini terus berjalan, network terus menjadi lebih kuat. Tanpa data manusia, tanpa pengetahuan manusia, hanya aturan permainan dan tujuan menang/kalah.

---

## Implikasi untuk Penelitian AI

### Pembelajaran Prinsip Pertama

AlphaGo Zero mendemonstrasikan metode pembelajaran "prinsip pertama":

> Jangan beritahu AI cara melakukan, hanya beritahu apa tujuannya, biarkan ia menemukan caranya sendiri.

Ini membentuk kontras tajam dengan pendekatan expert system tradisional. Expert system mencoba mengkodekan pengetahuan manusia ke dalam AI, sedangkan AlphaGo Zero membiarkan AI menemukan pengetahuan sendiri.

Hasilnya: pengetahuan yang ditemukan AI mungkin lebih lengkap dan akurat daripada pengetahuan manusia.

### Kekuatan Self-Play

AlphaGo Zero membuktikan self-play bisa menghasilkan data pelatihan tak terbatas, dan kualitas data ini akan meningkat seiring peningkatan network.

Ini adalah "siklus positif":
- Network lebih kuat → Data self-play lebih baik
- Data lebih baik → Network lebih kuat

Siklus ini bisa terus berjalan, sampai mencapai batas teori permainan (jika ada).

### Pentingnya Penyederhanaan

Keberhasilan AlphaGo Zero membuktikan pentingnya "penyederhanaan":

- Sederhanakan input (48 → 17)
- Sederhanakan arsitektur (dual network → single network)
- Sederhanakan pelatihan (supervised + reinforcement → reinforcement murni)

Setiap penyederhanaan membuat sistem lebih powerful. Ini memberitahu kita: kompleks tidak berarti bagus, solusi paling sederhana seringkali yang terbaik.

---

## Korespondensi Animasi

Konsep inti yang dibahas dalam artikel ini dan nomor animasinya:

| Nomor | Konsep | Korespondensi Fisika/Matematika |
|------|------|--------------|
| E5 E7 | Pelatihan dari nol | Fenomena self-organization |
| E5 E5 | Self-play | Konvergensi fixed-point |
| E5 E12 | Kurva pertumbuhan kekuatan | Pertumbuhan S-shape |
| E5 D12 | Residual network | Highway gradien |

---

## Bacaan Lanjutan

- **Artikel Berikutnya**: [Dual-Head Network dan Residual Network](./17-dual-head-resnet) — Penjelasan detail arsitektur neural network AlphaGo Zero
- **Artikel Terkait**: [Self-Play](./13-self-play) — Mengapa self-play bisa menghasilkan level superhuman
- **Teknis Mendalam**: [Proses Pelatihan dari Nol](./18-training-from-scratch) — Evolusi detail Hari 0-3

---

## Referensi

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
3. DeepMind. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
4. Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." *Nature*, 588, 604-609.
