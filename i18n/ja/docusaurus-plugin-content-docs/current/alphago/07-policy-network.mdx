---
sidebar_position: 8
title: Policy Network 詳解
description: AlphaGo のポリシーネットワークのアーキテクチャ、学習方法、実際の応用を深く理解する。13層畳み込みから Softmax 出力まで
---

import { PolicyHeatmap } from '@site/src/components/D3Charts';

# Policy Network 詳解

囲碁のどの局面においても、合法手は平均で250種類存在します。コンピュータがランダムに選択するなら、良い手を打つことは永遠にできません。

AlphaGo のブレークスルーは、「盤面を一目見ただけで、どの位置を検討すべきかを知る」ことを学んだことにあります。

この能力は、**Policy Network（ポリシーネットワーク）**から来ています。

---

## Policy Network とは何か？

### 中心的な機能

Policy Network は深層畳み込みニューラルネットワークであり、その役割は：

> **現在の盤面状態を入力として、各位置の着手確率を出力する**

数学的に表すと：

```
p = f_θ(s)
```

ここで：
- `s`：現在の盤面状態（19×19 の盤面 + その他の特徴）
- `f_θ`：Policy Network（θ はネットワークパラメータ）
- `p`：361 個の位置の確率分布（パスを含む）

### 直感的な理解

あなたがプロ棋士だと想像してください。局面を見ると、脳内で自動的にいくつかの重要な位置が「光る」でしょう——これらは直感的に検討すべきだと思うポイントです。

Policy Network はまさにこのプロセスをシミュレートしています。

<PolicyHeatmap initialPosition="corner" size={400} />

上のヒートマップは Policy Network の出力を示しています。色が明るいほど、モデルはその位置が打つ価値があると判断しています。

### なぜ Policy Network が必要なのか？

囲碁の探索空間は膨大すぎます。すべての可能な手をフィルタリングせずに探索すると：

| 戦略 | 各手で考慮する手数 | 10手先を探索する際のノード数 |
|------|--------------|------------------|
| すべて考慮 | 361 | 361^10 ≈ 10^25 |
| Policy Network でフィルタリング | ~20 | 20^10 ≈ 10^13 |

Policy Network は探索空間を **10^12 倍**（1兆倍）削減します。

---

## ネットワークアーキテクチャ

### 全体構造

AlphaGo の Policy Network は深層畳み込みニューラルネットワーク（CNN）アーキテクチャを採用しています：

```
入力層 → 畳み込み層 ×12 → 出力畳み込み層 → Softmax
   ↓         ↓            ↓           ↓
19×19×48   19×19×192   19×19×1     362 個の確率
```

### 入力層

入力は **19×19×48** の特徴テンソルです：
- **19×19**：盤面サイズ
- **48**：48 個の特徴平面（詳細は [入力特徴の設計](../input-features) を参照）

これらの 48 個の平面には以下が含まれます：
- 黒石の位置、白石の位置
- 直近 8 手の履歴
- 呼吸点、アタリ、シチョウなどの特徴
- 合法性（どの位置に打てるか）

### 畳み込み層

ネットワークは **12 層の畳み込み層**で構成され、各層の設定は：

| パラメータ | 値 | 説明 |
|------|------|------|
| フィルター数 | 192 | 各層で 192 個の特徴マップを出力 |
| カーネルサイズ | 3×3（第1層は 5×5） | 毎回 3×3 の領域を見る |
| パディング方式 | same | 19×19 のサイズを維持 |
| 活性化関数 | ReLU | max(0, x) |

#### なぜ 192 個のフィルターなのか？

これは経験的な値です。少なすぎるとモデルの容量が制限され、多すぎると計算量と過学習のリスクが増加します。DeepMind チームは実験を通じて 192 が良いバランスポイントであることを確認しました。

#### なぜ 3×3 のカーネルなのか？

3×3 は畳み込みニューラルネットワークで最も一般的に使用されるサイズです。理由は：
1. **局所パターンを捉えるのに十分**：囲碁の眼、接ぎ、切りなどはすべて 3×3 の範囲内
2. **計算効率が高い**：大きなカーネルに比べて、3×3 はパラメータが少ない
3. **積み重ね可能**：複数層の 3×3 畳み込みで大きな受容野を実現できる

#### 第1層がなぜ 5×5 なのか？

第1層でより大きな 5×5 の畳み込みカーネルを使用するのは、入力層でやや広い範囲のパターン（コスミ、トビなど）を捉えるためです。これは設計上の選択であり、後の AlphaGo Zero では統一して 3×3 を使用しています。

### ReLU 活性化関数

各畳み込み層の後に ReLU（Rectified Linear Unit）活性化関数を適用します：

```
ReLU(x) = max(0, x)
```

なぜ ReLU を使用するのか？

1. **計算がシンプル**：最大値を取るだけで、sigmoid よりはるかに高速
2. **勾配消失の緩和**：正の領域では勾配が常に 1
3. **スパースな活性化**：負の値がゼロになり、スパースな表現を生成

### 出力層

最後の層は特殊な畳み込み層です：

```
19×19×192 → 畳み込み(1×1, 1個のフィルター) → 19×19×1 → フラット化 → 362次元ベクトル → Softmax
```

#### 1×1 畳み込み

出力層では 1×1 畳み込みを使用し、192 チャンネルを 1 に圧縮します。これは各位置の 192 次元特徴に対する線形結合と等価です。

#### Softmax 出力

362 次元ベクトル（361 個の盤面位置 + 1 個のパス）は Softmax 関数を通過します：

```
Softmax(z_i) = exp(z_i) / Σ_j exp(z_j)
```

Softmax は出力が正当な確率分布であることを保証します：
- すべての値が 0 から 1 の間
- すべての値の合計が 1

### パラメータ数

ネットワークの総パラメータ数を計算しましょう：

| 層 | 計算 | パラメータ数 |
|---|------|---------|
| 第1畳み込み層 | 5×5×48×192 + 192 | 230,592 |
| 中間畳み込み層 ×11 | (3×3×192×192 + 192) × 11 | 3,633,792 |
| 出力畳み込み層 | 1×1×192×1 + 1 | 193 |
| **合計** | | **~3.9M** |

約 **390 万個のパラメータ**で、今日の基準では小規模なネットワークです。

---

## 学習目標と方法

### 学習データ

Policy Network は**教師あり学習**を使用し、人間の棋譜から学習します。

データソース：
- **KGS Go Server**：アマチュアとプロ棋士の対局
- **約 3000 万局面**：16 万局の対局からサンプリング
- **ラベル**：各局面に対応する人間の次の手

### 交差エントロピー損失関数

学習目標は人間の着手を予測する確率を最大化することです。交差エントロピー損失関数を使用：

```
L(θ) = -Σ log p_θ(a | s)
```

ここで：
- `s`：盤面状態
- `a`：人間が実際に打った位置
- `p_θ(a | s)`：モデルがその位置を予測する確率

#### 直感的な理解

交差エントロピー損失には単純な意味があります：

> **モデルが正しい位置を予測する確率が高いほど、損失は低くなる**

人間が K10 に打ち、モデルが K10 に与えた確率が：
- 0.9 → 損失 = -log(0.9) ≈ 0.1（非常に低い、良い）
- 0.1 → 損失 = -log(0.1) ≈ 2.3（高い、悪い）
- 0.01 → 損失 = -log(0.01) ≈ 4.6（非常に高い、とても悪い）

### 学習プロセス

```python
# 疑似コード
for epoch in range(num_epochs):
    for batch in dataloader:
        states, actions = batch

        # 順伝播
        policy = network(states)  # 361 次元の確率ベクトル

        # 損失計算（交差エントロピー）
        loss = cross_entropy(policy, actions)

        # 逆伝播
        loss.backward()
        optimizer.step()
```

学習の詳細：
- **オプティマイザ**：SGD with momentum
- **学習率**：初期 0.003、徐々に減衰
- **バッチサイズ**：16
- **学習時間**：約 3 週間（50 GPUs）

### データ拡張

囲碁盤には 8 重の対称性（4 つの回転 × 2 つの反転）があります。各学習サンプルを 8 つの等価サンプルに変換できます：

```
元 → 90度回転 → 180度回転 → 270度回転
  ↓       ↓         ↓          ↓
水平反転 → ...
```

これにより有効な学習データが 8 倍に増加し、モデルが学習するパターンが方向に依存しないことを保証します。

---

## 学習結果

### 57% の正解率

学習後、Policy Network は **57% の top-1 正解率**を達成しました。

これは、任意の局面において、モデルが 57% の確率で人間の専門家が実際に打った手を予測できることを意味します。

#### この正解率は高いのか？

各局面で平均 250 の合法手があることを考えると、ランダムに推測した場合の正解率はわずか 0.4% です。

| 方法 | Top-1 正解率 |
|------|-------------|
| ランダム推測 | 0.4% |
| 以前の最強コンピュータ囲碁 | ~44% |
| AlphaGo Policy Network | **57%** |

13 ポイントの向上は少なく見えますが、その意義は大きいです。

### 棋力の向上

純粋に Policy Network のみ（探索なし）で対局すると、どの程度の棋力になるでしょうか？

| 構成 | Elo レーティング | おおよその段位 |
|------|---------|---------|
| 以前の最強プログラム（Pachi） | 2,500 | アマ 4-5 段 |
| Policy Network 単体 | 2,800 | アマ 6-7 段 |
| + MCTS 1600 シミュレーション | 3,200+ | プロレベル |

単独の Policy Network でもアマチュア高段レベルであり、MCTS を加えるとプロレベルに躍進します。

### なぜ 57% だけなのか？

人間の棋譜には以下の特性があり、正解率を制限しています：

#### 1. 複数の良い手

多くの局面で複数の手が良い手です。例えば「カカリ」と「シマリ」がどちらも正しい選択かもしれません。モデルが別の良い手を選んでも「不正解」としてカウントされます。

#### 2. スタイルの違い

棋士によってスタイルが異なります。攻撃的な棋士と堅実な棋士は同じ局面で異なる手を打つかもしれません。モデルが学習するのは「平均的な」スタイルです。

#### 3. 人間もミスをする

KGS のデータにはアマチュア棋士の対局が含まれており、彼らの選択は必ずしも最善ではありません。モデルがいくつかの「間違い」を学習するのは正常です。

---

## MCTS における役割

Policy Network は AlphaGo の MCTS において 2 つの重要な役割を果たします：

### 1. 探索方向のガイド

MCTS の **Selection** フェーズで、Policy Network の出力は UCB（Upper Confidence Bound）の計算に使用されます：

```
UCB(s, a) = Q(s, a) + c_puct × P(s, a) × √(N(s)) / (1 + N(s, a))
```

ここで `P(s, a)` は Policy Network が出力する確率です。

これは以下を意味します：
- **高確率の手は優先的に探索される**
- **低確率の手も探索される機会がある**（探索項があるため）

### 2. ノード展開の事前確率

MCTS が新しいノードを展開する際、Policy Network はすべての子ノードの**事前確率**を提供します。

```
ノード s を展開：
  for each action a:
    child = Node()
    child.prior = policy_network(s)[a]  # 事前確率
    child.value = 0
    child.visits = 0
```

これらの事前確率により、MCTS はまだ訪問されていない子ノードでも、どれがより探索に値するかを「知る」ことができます。

---

## 軽量版 vs 完全版

AlphaGo には実際には 2 つの Policy Network があります：

### 完全版（SL Policy Network）

- **アーキテクチャ**：13 層 CNN、192 filters
- **正解率**：57%
- **推論時間**：約 3 ミリ秒/局面
- **用途**：MCTS の Selection と Expansion

### 軽量版（Rollout Policy Network）

- **アーキテクチャ**：線形モデル + 手作り特徴
- **正解率**：24%
- **推論時間**：約 2 マイクロ秒/局面（1500 倍高速）
- **用途**：高速シミュレーション（rollout）

### なぜ軽量版が必要なのか？

MCTS の **Simulation** フェーズでは、現在のノードからゲーム終了まで打ち続ける必要があり、100 手以上かかる可能性があります。毎手で完全版 Policy Network を使用すると、遅すぎます。

軽量版は正解率が 24% しかありませんが、速度は 1500 倍高速です。rollout では、精度よりも速度が重要です。

### 軽量版の特徴

軽量版は手作りの特徴を使用し、以下が含まれます：

| 特徴タイプ | 例 |
|---------|------|
| 局所パターン | 3×3 領域の石の配置 |
| グローバル特徴 | 隅にあるか、大場か |
| 戦術特徴 | アタリ、シチョウ、応接 |

これらの特徴は線形モデル（隠れ層なし）に入力され、計算速度は非常に高速です。

### AlphaGo Zero の改良

後の AlphaGo Zero では軽量版と rollout を完全に廃止しました。Value Network で葉ノードを直接評価し、高速シミュレーションは不要になりました。これは重大な簡略化です。

---

## 強化学習によるファインチューニング（RL Policy Network）

### 教師あり学習の限界

教師あり学習で学習された Policy Network には根本的な問題があります：

> **学習しているのは「人間を模倣すること」であって「勝つこと」ではない**

これは人間の悪い癖を学習することや、人間が遭遇したことのない局面でパフォーマンスが悪化することを意味します。

### 自己対局による強化

DeepMind の解決策は**ポリシー勾配**（Policy Gradient）法を用いた強化学習です：

```
1. Policy Network に自己対局させる
2. 各対局のすべての着手を記録
3. 勝敗に基づいてパラメータを調整：
   - 勝った → これらの着手の確率を増加
   - 負けた → これらの着手の確率を減少
```

### REINFORCE アルゴリズム

具体的には REINFORCE アルゴリズムを使用：

```
∇J(θ) = E[Σ_t ∇log π_θ(a_t | s_t) × z]
```

ここで：
- `z`：この対局の結果（+1 勝ち、-1 負け）
- `π_θ(a_t | s_t)`：状態 `s_t` でアクション `a_t` を選択する確率

### 結果

約 1 日の自己対局学習（128 万局）後、RL Policy Network は：

| 指標 | SL Policy | RL Policy |
|------|-----------|-----------|
| SL Policy との対戦 | 50% | **80%** |
| Elo 向上 | - | +100 |

正解率はわずかに低下する可能性がありますが（人間を完全に模倣しなくなるため）、実際の対戦勝率は大幅に向上します。

### 「模倣」から「革新」へ

強化学習により Policy Network は人間が考えたことのない着手を学習しました。これらの着手は学習データに存在しませんでしたが、効果的です。

これが AlphaGo が「神の一手」を打てる理由です——人間の経験に制限されないのです。

---

## 可視化分析

### 異なる局面での確率分布

異なる局面での Policy Network の出力を見てみましょう：

#### 序盤（布石段階）

<PolicyHeatmap initialPosition="opening" size={400} />

序盤では、確率は主に以下に集中しています：
- 隅（隅を占める）
- 辺（カカリ、シマリ）
- 「大場」の位置

これは囲碁の基本原則に沿っています：隅は金、辺は銀、中央は草。

#### 戦いの局面

<PolicyHeatmap initialPosition="fighting" size={400} />

戦いの際、確率は以下に集中：
- 重要な切断点
- アタリ、応接
- 眼作り、眼潰し

これはモデルが局所戦術を学習したことを示しています。

#### ヨセ段階

<PolicyHeatmap initialPosition="endgame" size={400} />

ヨセ時、確率は各ヨセポイントに分散し、目数の正確な計算が必要です。

### 隠れ層は何を学習するのか？

畳み込み層の出力を可視化することで、モデルが学習した「特徴」を見ることができます：

- **低層**：基本形状（眼、切断点）
- **中層**：戦術パターン（アタリ、シチョウ）
- **高層**：グローバル概念（勢力、厚み）

これは人間が囲碁を認識する階層構造と非常に似ています。

---

## 実装のポイント

### PyTorch 実装

以下は簡略化された Policy Network の実装です：

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class PolicyNetwork(nn.Module):
    def __init__(self, input_channels=48, num_filters=192, num_layers=12):
        super().__init__()

        # 第1畳み込み層（5×5）
        self.conv1 = nn.Conv2d(input_channels, num_filters,
                               kernel_size=5, padding=2)

        # 中間畳み込み層（3×3）×11
        self.conv_layers = nn.ModuleList([
            nn.Conv2d(num_filters, num_filters,
                     kernel_size=3, padding=1)
            for _ in range(num_layers - 1)
        ])

        # 出力畳み込み層（1×1）
        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)

    def forward(self, x):
        # x: (batch, 48, 19, 19)

        # 第1層
        x = F.relu(self.conv1(x))

        # 中間層
        for conv in self.conv_layers:
            x = F.relu(conv(x))

        # 出力層
        x = self.conv_out(x)  # (batch, 1, 19, 19)

        # フラット化 + Softmax
        x = x.view(x.size(0), -1)  # (batch, 361)
        x = F.softmax(x, dim=1)

        return x
```

### 学習ループ

```python
def train_step(model, optimizer, states, actions):
    """
    states: (batch, 48, 19, 19) - 盤面特徴
    actions: (batch,) - 人間が打った位置 (0-360)
    """
    # 順伝播
    policy = model(states)  # (batch, 361)

    # 交差エントロピー損失
    loss = F.cross_entropy(
        torch.log(policy + 1e-8),  # log(0) を防ぐ
        actions
    )

    # 逆伝播
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # 正解率の計算
    predictions = policy.argmax(dim=1)
    accuracy = (predictions == actions).float().mean()

    return loss.item(), accuracy.item()
```

### 推論時の注意点

実際の対局時には、以下に注意が必要です：

1. **非合法手のフィルタリング**：非合法位置の確率を 0 に設定し、再正規化
2. **温度調整**：温度パラメータで確率分布の「シャープさ」を制御
3. **バッチ推論**：MCTS では複数の局面をバッチ処理可能

```python
def get_move_probabilities(model, state, legal_moves, temperature=1.0):
    """合法手の確率分布を取得"""
    policy = model(state)  # (361,)

    # 合法手のみ保持
    mask = torch.zeros(361)
    mask[legal_moves] = 1
    policy = policy * mask

    # 温度調整
    if temperature != 1.0:
        policy = policy ** (1 / temperature)

    # 再正規化
    policy = policy / policy.sum()

    return policy
```

---

## アニメーション対応

本記事で扱う中心的な概念とアニメーション番号：

| 番号 | 概念 | 物理/数学対応 |
|------|------|--------------|
| E1 | Policy Network | 確率場 |
| D9 | CNN 特徴抽出 | フィルター応答 |
| D3 | 教師あり学習 | 最尤推定 |
| H4 | ポリシー勾配 | 確率的最適化 |

---

## 関連記事

- **次の記事**：[Value Network 詳解](../value-network) — AlphaGo は局面をどう評価するか
- **関連トピック**：[入力特徴の設計](../input-features) — 48 個の特徴平面の詳解
- **原理の深掘り**：[CNN と囲碁の融合](../cnn-and-go) — なぜ畳み込みニューラルネットワークが盤面に適しているのか

---

## キーポイント

1. **Policy Network は確率分布生成器**：盤面を入力し、361 個の位置の確率を出力
2. **13 層 CNN + Softmax**：深層畳み込みで特徴を抽出し、Softmax で確率を出力
3. **57% の正解率**：以前のコンピュータ囲碁プログラムをはるかに上回る
4. **2 つのバージョン**：完全版は MCTS の決定に、軽量版は高速シミュレーションに使用
5. **強化学習によるファインチューニング**：「人間を模倣」から「勝利を追求」へ進化

Policy Network は AlphaGo の「直感」です——これにより AI は人間のように、検討すべき着手を素早く識別できるようになります。

---

## 参考文献

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Maddison, C. J., et al. (2014). "Move Evaluation in Go Using Deep Convolutional Neural Networks." *arXiv:1412.6564*.
3. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.
4. LeCun, Y., Bengio, Y., & Hinton, G. (2015). "Deep learning." *Nature*, 521, 436-444.
