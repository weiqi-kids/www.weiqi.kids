---
sidebar_position: 19
title: ゼロからの学習過程
description: AlphaGo Zero がわずか3日間でランダムな打ち手から人類を超越し、千年の棋理を再発見・超越する過程を目撃する
keywords: [AlphaGo Zero, 学習過程, 自己対局, 棋力成長, 囲碁 AI, 深層学習]
---

import { EloChart } from '@site/src/components/D3Charts';

# ゼロからの学習過程

AlphaGo Zero の最も驚くべき点は、最終的な棋力だけではなく、その**成長過程**にあります。完全にランダムな状態から始まり、わずか3日間で人類が数千年かけて蓄積した囲碁の知識を経験し、そして人類の理解をすべて超越しました。

本記事では、この驚異的な変貌の過程を一歩ずつ見ていきます。

---

## 学習曲線

まず、AlphaGo Zero の棋力成長曲線を見てみましょう：

<EloChart mode="zero" width={700} height={400} />

この曲線は、AlphaGo Zero の72時間における棋力の変化を示しています。いくつかの重要なマイルストーンに注目してください：

| 時間 | ELO レーティング | 相当するレベル |
|------|----------|--------|
| 0 時間 | 0 | ランダムな打ち手 |
| 3 時間 | ~1000 | 基本ルールの発見 |
| 12 時間 | ~3000 | 定石と石の形の発見 |
| 36 時間 | ~4500 | 樊麾版 AlphaGo を超越 |
| 60 時間 | ~5200 | 李世乭版 AlphaGo を超越 |
| 72 時間 | ~5400 | すべての先行バージョンを超越 |

**3日間で、ゼロから人類の頂点を超越。**

---

## Day 0：混沌の始まり

### 完全にランダムな初期状態

学習開始時、ニューラルネットワークの重みはランダムに初期化されています。これは以下を意味します：

- **Policy Head**：出力はほぼ均一分布で、各位置の着手確率は約 1/361
- **Value Head**：出力は約 0 で、良い局面と悪い局面を区別できない

この時点の AlphaGo Zero の着手は完全にランダムで、碁盤を見たことがない人よりもひどい状態です。

### 最初の自己対局

最初の自己対局がどのようなものか想像してみてください：

```
黒 1：ランダムな場所に着手（天元かもしれないし、隅かもしれないし、一線かもしれない）
白 2：別のランダムな場所に着手
黒 3：ランダム...
...
第 200 手：盤面上には孤立した石が散在し、連結がない
最終：勝敗はランダムな要因で決定
```

この対局の「質」は極めて低いですが、貴重な情報が含まれています：**最終的に誰が勝ったか**。

### 最初の学習シグナル

両者ともランダムに打っていますが、勝敗の結果は確定しています。ニューラルネットワークは学習を始めます：

> 「この局面で、最終的に黒が勝った。なぜかは分からないが、この局面は黒にとって有利かもしれない。」

これは非常に弱いシグナルですが、真実です。このような「ゴミのような対局」を数千局経た後、ネットワークはいくつかの統計的パターンを発見し始めます。

---

## Hour 1-3：ゲームルールの発見

### 出現するルール認識

数万局の自己対局の後、AlphaGo Zero は囲碁の基本ルールを「発見」し始めます（これらのルールは既にゲームエンジンに組み込まれていますが）：

#### 1. 連結の重要性

```
観察：石が連結していると、取られにくい
学習：既存の石の隣に着手することを優先し始める
```

これは教えられたものではなく、勝敗の結果から学んだものです。散在する石は各個撃破されやすく、連結した石は生存しやすいのです。

#### 2. 呼吸点（ダメ）の概念

```
観察：石の隣接する空点がすべて占められると、石が消える
学習：ダメの少ない位置を避け始め、相手のダメの少ない石を攻撃し始める
```

ネットワークはダメの数を追跡することを学びました。入力には明示的な「ダメの数」特徴はありませんが、過去の盤面状態から推論できます。

#### 3. 眼の萌芽

```
観察：ある形は特に取られにくい
学習：隅や辺で空間のある形を作り始める
```

これは活き石の概念の萌芽です。ネットワークは、内部に空間を持つ石のグループが生存しやすいことを発見しました。

### 棋力評価

この時点の AlphaGo Zero はおよそ：
- **ELO**：~1000
- **相当するレベル**：ルールを覚えたばかりの初心者
- **特徴**：石を連結すること、相手の石を取ることを知っている

---

## Hour 3-12：定石と石の形の発見

### 隅部の覚醒

さらなる学習を経て、ネットワークは隅部の重要性を発見しました：

```
観察：隅の石は2つの眼を作りやすい
     辺では2つの眼を作るのが難しい
     中央では2つの眼を作るのが最も難しい
学習：序盤で隅を優先的に占める
```

これは人間の棋理における「金隅銀辺草腹」（隅は金、辺は銀、中央は草の価値）の発見過程です。ネットワークはこの原則を教えられたのではなく、数十万局の対局から自分で発見したのです。

### 定石の出現

さらに驚くべきことに、ネットワークは定石、つまり隅における双方の標準的な応酬を「発明」し始めました：

#### 観察された現象

```
学習初期：隅での着手は多種多様
学習中期：特定の着手が繰り返し出現
学習後期：安定した隅の定石が形成
```

これらの定石は、人間が数百年かけて蓄積した定石と**高度に類似**しており、これらの定石が確かに双方の最適解に近いことを検証しています。

### 典型的に出現した定石

小目定石を例にすると：

```
  A B C D E F G H J
9 . . . . . . . . .
8 . . . . . . . . .
7 . . . . . . . . .
6 . . . ● . . . . .   ● = 黒
5 . . . . . . . . .   ○ = 白
4 . . . ○ . ● . . .
3 . . . . . . . . .
2 . . . . . . . . .
1 . . . . . . . . .
```

黒が小目を占め、白がカカリ、黒がハサミ——このシーケンスは学習過程で自然に出現しました。

### 石の形の知識

定石の他に、ネットワークは良い形と悪い形の違いも学びました：

| 形 | 人間の評価 | Zero の学習 |
|------|----------|-------------|
| 空き三角 | 愚形 | 徐々に避ける |
| トラ口 | 好形 | 徐々に好む |
| 双飛燕 | 典型的な攻撃形 | 自然に発見 |
| 鎮神頭 | 強力な攻撃 | 自然に発見 |

### 棋力評価

この時点の AlphaGo Zero：
- **ELO**：~3000
- **相当するレベル**：アマチュア高段者
- **特徴**：基本的な定石の知識を持ち、基本的な石の形を理解

---

## Hour 12-36：棋理の成熟

### 全局観の形成

2日目に入り、ネットワークは**全局観**を示し始めます：

#### 勢力と地

```
観察：空間を囲うと目数が得られる
     しかし勢力にも価値がある——相手を攻撃できる
学習：地取りと勢力取りのバランスを探る
```

これは囲碁で最も深遠な概念の一つです。ネットワークは「虚」と「実」の価値を評価することを学びました。

#### 厚薄の判断

```
観察：「厚い」石は遠くの戦いを支援できる
     「薄い」石は補強が必要で、そうしないと攻撃される
学習：主動的に厚みを築き、相手の薄みを攻撃する
```

### 中盤の戦術

ネットワークの中盤戦闘能力は大幅に向上しました：

| 技術 | 説明 |
|------|------|
| 弱い石の攻撃 | 相手の孤立した石を識別し、攻勢を仕掛ける |
| 厚みの利用 | 厚みで攻撃を支援し、利益を得る |
| 振り替わり | 局部的な損失を諦め、全局的な優位を得る |
| 打ち込み | 相手の模様を消す |

### ヨセの技術

収官段階の精確な計算も向上しています：

```
観察：ヨセ段階では各手の価値を精確に計算できる
学習：価値の大きさ順にヨセを打つ
```

ネットワークは「双方先手」「片先手」「後手」などのヨセの概念を学びました。

### 棋力評価

この時点の AlphaGo Zero：
- **ELO**：~4500
- **相当するレベル**：プロ棋士レベル
- **特徴**：完全な囲碁の理解を持ち、高品質な対局を打てる

---

## Hour 36-72：人類を超越

### プロレベルの突破

36時間頃、AlphaGo Zero の棋力はプロ棋士レベルに達しました。しかし学習は止まりません——自己対局を続け、向上し続けます。

次に起こったことはさらに興味深いものです：**人間が考えたこともない着手を発見し始めました**。

### 革新的な序盤

伝統的な囲碁の序盤には多くの「定見」があります：

| 伝統的見解 | AlphaGo Zero の発見 |
|----------|---------------------|
| 序盤はまず隅を占める | 場合によっては先に辺を占める方が良い |
| 小目が最も堅実 | 三々に直接打つのも可能 |
| 定石は覚えておくべき | 主動的に定石から外れることもできる |
| 早い三々入りは貪欲 | 特定の局面では三々入りが正しい |

これらの「発見」は AlphaGo 以後、人間のプロ棋士によって広く研究され、多くは現代の棋理に取り入れられました。

### 直感に反する石の形

AlphaGo Zero は時として人間が「見栄えが悪い」と考える形を打ちます：

```
人間：「これは愚形だ、良い手であるはずがない」
Zero：（その手を打つ）
分析後：「実はこの方が効率的だった」
```

これは人間の棋理の限界を明らかにしています：一部の「悪形」は実は特定の局面での最適解なのです。

### 積極的な捨て石

Zero は人間より積極的に石を捨てて他の利益と交換します：

```
局部で3目の損失
全局的に主導権を獲得
最終的に勝率が向上
```

人間の棋士は往々にして局部的な得失に過度にこだわりますが、Zero は常に最終的な勝率を見据えています。

### 棋力評価

72時間後の AlphaGo Zero：
- **ELO**：~5400
- **相当するレベル**：すべての人間棋士を超越
- **特徴**：人間未知の着手を発見し、新しい棋理を創造

---

## 人類の棋理の再発見

### 数千年 vs. 3日

人間の囲碁は数千年かけて発展しました：
- 紀元前2000年頃に中国で起源
- 唐代に日本に伝わり、精密な棋理が発展
- 20世紀にプロ制度が確立し、棋理がさらに深化
- 2016年、人間は囲碁をかなり理解していると考えていた

AlphaGo Zero は3日でこの道程を歩みました。さらに驚くべきことに、発見した棋理は人間のものと**高度に一致**していました。

### 検証と超越

| 人間の知識 | Zero の態度 |
|----------|-------------|
| 金隅銀辺草腹 | 確認（隅は確かに重要） |
| 基本定石 | 大部分確認、一部改善 |
| 好形悪形 | 大部分確認、例外あり |
| 捨て石の振り替わり | 人間より積極的 |
| 厚薄判断 | おおむね一致、細部に違い |

これは人間が数千年蓄積した棋理の**大方向が正しい**ことを示しています。しかし一部の領域では、人間の理解に修正が必要です。

### 人間の学習への示唆

AlphaGo Zero の学習過程は人間の学習に示唆を与えます：

1. **基礎から始める**：Zero はまずルールを学び、次に石の形を学び、最後に全局観を発展させた
2. **大量の練習**：490万局の自己対局は、人間の数十万年分の対局量に相当
3. **勝敗に集中**：「美しい碁」を追求せず、勝つことだけを追求
4. **伝統に縛られない**：「不可能」と思われる着手を試す勇気

---

## 学習過程の技術的詳細

### 自己対局のメカニズム

各自己対局の流れ：

```
初期化：空の碁盤
↓
各手：
  1. ニューラルネットワークで現在の局面を評価
  2. MCTS 探索を実行（1600回のシミュレーション）
  3. 探索結果に基づいて着手を選択
  4. (局面, MCTS確率, -) を記録
↓
ゲーム終了：
  1. 勝敗を判定 z ∈ {-1, +1}
  2. すべての記録に勝敗を補完 (局面, MCTS確率, z)
  3. データを学習プールに追加
```

### 学習のリズム

AlphaGo Zero の学習は**継続的に進行**します：

```
Self-play Workers:       継続的に自己対局データを生成
Training Workers:        継続的にデータプールからサンプリングして学習
Network Updates:         定期的に自己対局用のネットワークを更新
```

これら3つのプロセスが同時に進行し、継続的に改善するサイクルを形成します。

### データプール管理

学習データプールの管理：

| パラメータ | 値 |
|------|-----|
| プールサイズ | 直近50万局 |
| 各局のサンプル | ~200手 |
| 総サンプル数 | ~1億 |
| サンプリング方式 | 均一ランダム |

古いデータは新しいデータに置き換えられ、学習データが現在のネットワークのレベルを反映することを保証します。

### ネットワーク更新戦略

学習1ステップごとに自己対局のネットワークを更新するのではありません。代わりに：

1. 一定期間学習後、候補ネットワークを生成
2. 候補ネットワークと現在のネットワークを対戦させる（400局）
3. 候補ネットワークの勝率 > 55% なら更新
4. そうでなければ学習を継続

これにより、自己対局では常に**十分に強い**ネットワークが使用されます。

---

## 学習速度の分析

### なぜこれほど速いのか？

AlphaGo Zero の学習速度が驚異的な理由：

#### 1. 計算リソース

- 4つの TPU、毎秒数万回の推論
- 毎日数十万局の自己対局を生成
- 人間の数千年分の対局量に相当

#### 2. 完璧な対戦相手

自己対局は以下を意味します：
- 対戦相手のレベルは常に自分と同等
- 弱すぎない（学ぶものがない）、強すぎない（勝てない）
- これは理想的な学習条件

#### 3. 直接的な目標

目標は一つだけ：勝つこと。以下はありません：
- 先生の好み
- スタイルの追求
- 美学的考慮

#### 4. 効率的な表現学習

残差ネットワークは非常に抽象的な盤面特徴を学習でき、手作りの特徴よりも効果的です。

### 人間との比較

| 側面 | 人間 | AlphaGo Zero |
|------|------|--------------|
| 学習速度 | 毎日 ~10 局 | 毎日 ~100,000 局 |
| 記憶保持 | 忘却あり | 完璧に保持 |
| 体力の制限 | 休息が必要 | 24/7 稼働 |
| 創造性 | 伝統に影響される | 先入観なし |

---

## 学習過程での興味深い現象

### 段階的な停滞

学習曲線は完全に滑らかではなく、時に**停滞期**が現れます：

```
ELO: 2000 -----> 2000 -----> 2500 ---->
          (停滞)       (突破)
```

これはおそらく、ネットワークが新しい概念を学習中で、「消化」する時間が必要なためです。

### 戦略の出現と消失

特定の戦略が学習過程で出現し、その後消失することがあります：

```
段階 1：ある攻撃手段を発見
段階 2：相手が防御を学ぶ
段階 3：その手段の使用頻度が低下
段階 4：新しい攻撃手段を発見
```

これは軍拡競争の縮図です。

### 「車輪の再発明」

学習過程で、Zero は人間が既に知っている概念を「再発明」します：

- **シチョウ**：連続アタリで石を取れることを発見
- **ウッテガエシ**：先に石を捨てて後で反撃できることを発見
- **コウ**：着手禁止ルールの利用方法を発見

これらの発見の順序は、人間が囲碁を学ぶ順序と類似しています。

---

## アニメーション対応

本記事で扱った核心概念とアニメーション番号：

| 番号 | 概念 | 物理/数学対応 |
|------|------|--------------|
| :clapper: E12 | 棋力成長曲線 | S字成長（ロジスティック） |
| :clapper: E7 | ゼロから始める | 自己組織化現象 |
| :clapper: E5 | 自己対局 | 不動点収束 |
| :clapper: F8 | 創発能力 | 相転移 |

---

## 関連記事

- **前の記事**：[双頭ネットワークと残差ネットワーク](../dual-head-resnet) — これらすべてを支えるニューラルネットワークアーキテクチャ
- **次の記事**：[分散システムと TPU](../distributed-systems) — これらすべてを可能にしたハードウェア
- **関連記事**：[自己対局](../self-play) — なぜ自己対局がこれほど効果的なのか

---

## 参考文献

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
3. DeepMind. (2017). "AlphaGo Zero: Learning from scratch." *YouTube*.
4. Wang, F., et al. (2019). "A Survey on the Evolution of AlphaGo." *arXiv:1907.11180*.
