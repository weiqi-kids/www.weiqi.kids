---
sidebar_position: 17
title: AlphaGo Zero 概要
description: ゼロから始まり、完全に自己学習。AlphaGo Zero は人間の棋譜なしでどのようにして全ての前バージョンを超えたのか
keywords: [AlphaGo Zero, 自己対戦, 強化学習, 深層学習, 囲碁AI, 教師なし学習]
---

# AlphaGo Zero 概要

2017年10月、DeepMind は AI 界を震撼させる成果を発表しました。**AlphaGo Zero** は人間の棋譜を一切使用せず、完全にランダムな状態から訓練を開始し、わずか3日でイ・セドル九段を破った元祖 AlphaGo を超え、**100:0** のスコアで完勝しました。

これは単なる数字上の進歩ではありません。これは全く新しいパラダイムを意味します：**AI は人間の知識なしで、ゼロから全てを発見できる**。

---

## なぜ人間の棋譜は不要なのか？

### 人間の棋譜の限界

元祖 AlphaGo の訓練プロセスは2つの段階に分かれていました：

1. **教師あり学習**：3000万局の人間の棋譜で Policy Network を訓練
2. **強化学習**：自己対戦を通じてさらに向上

この方法にはいくつかの根本的な問題があります：

#### 1. 人間の棋譜には上限がある

人間の棋士の棋力には限界があり、棋譜に含まれるのは人間の理解であると同時に、人間の誤りや偏見も含まれます。AI が人間の棋譜から学習する時、学ぶのは：

- 人間が良いと考える手法（しかし最適とは限らない）
- 人間の思考パターン（しかしイノベーションを制限する可能性がある）
- 人間の誤り（正しいサンプルとして学習されてしまう）

#### 2. 教師あり学習のボトルネック

教師あり学習の目標は「人間を模倣する」こと——人間の棋士がどこに打つかを予測することです。これは AI の能力上限が人間の棋士の能力によって制限されることを意味します。

師匠を模倣するだけの弟子が、決して師匠を超えられないのと同じです。

#### 3. データ収集コスト

高品質な人間の棋譜は何年もの蓄積が必要であり、囲碁のような長い歴史を持つゲームにのみ存在します。AI を新しい分野（例えばタンパク質構造予測）に応用したい場合、「人間の専門家の棋譜」はそもそも存在しません。

### Zero の突破

AlphaGo Zero は教師あり学習の段階を完全にスキップし、**ランダム初期化**から直接自己対戦を開始しました。これは上記の全ての問題を解決します：

| 問題 | 元祖 AlphaGo | AlphaGo Zero |
|------|-------------|--------------|
| 人間の知識の上限 | 棋譜の品質に制限される | この制限なし |
| 学習目標 | 人間を模倣 | 勝率を最大化 |
| データ要件 | 3000万局の棋譜 | 0 |
| 汎用性 | 囲碁のみ | 他の分野に拡張可能 |

これは根本的なパラダイムシフトです：「人間の知識を学習する」から「第一原理から知識を発見する」へ。

---

## 元祖 AlphaGo との対比：100:0

### 圧倒的な勝利

DeepMind は訓練完了した AlphaGo Zero を各バージョンの AlphaGo と対戦させました：

| 対戦相手 | AlphaGo Zero の戦績 |
|------|-------------------|
| AlphaGo Fan（ファン・フイに勝利したバージョン） | 100:0 |
| AlphaGo Lee（イ・セドルに勝利したバージョン） | 100:0 |
| AlphaGo Master（60連勝バージョン） | 89:11 |

**100:0**——これは100局の対戦で、元祖 AlphaGo が1局も勝てなかったことを意味します。

### より少ないリソースで、より強い棋力

勝つだけでなく、AlphaGo Zero はより少ないリソースでより強い棋力を達成しました：

| 指標 | AlphaGo Lee | AlphaGo Zero |
|------|-------------|--------------|
| 訓練時間 | 数ヶ月 | 40日（3日で AlphaGo Lee を超越） |
| 訓練局数 | 3000万人間棋譜 + 自己対戦 | 490万局の自己対戦 |
| TPU数（訓練） | 50+ | 4 |
| TPU数（推論） | 48 | 4 |
| 入力特徴 | 48プレーン | 17プレーン |
| ニューラルネットワーク | SL + RL デュアルネットワーク | 単一デュアルヘッドネットワーク |

これは驚くべき効率向上です：**リソースは10倍以上削減されたのに、棋力は大幅に向上**。

### なぜ Zero はより強いのか？

AlphaGo Zero がより強い理由はいくつかの角度から理解できます：

#### 1. バイアスのない学習

元祖 AlphaGo は人間の棋譜から学習し、人間のバイアスを引き継ぎました。例えば、人間の棋士は特定の定石を過度に重視したり、特定の局面に対して誤った評価をしていることがあります。

AlphaGo Zero にはこれらの荷物がありません。白紙の状態から、勝敗の結果だけを通じて何が良い手かを学習します。これにより、人間が考えたこともない手法を発見できます。

#### 2. 一貫した学習目標

元祖 AlphaGo の訓練には2つの異なる目標がありました：
- 教師あり学習：人間の着手予測精度を最大化
- 強化学習：勝率を最大化

この2つの目標は互いに矛盾する可能性があります。AlphaGo Zero は1つの目標のみ：**勝率の最大化**。これにより学習プロセスがより一貫し効果的になります。

#### 3. よりシンプルなアーキテクチャ

元祖 AlphaGo は分離した Policy Network と Value Network を使用していました。AlphaGo Zero は単一のデュアルヘッドネットワークを使用し（詳細は[次の記事：デュアルヘッドネットワークと残差ネットワーク](../dual-head-resnet)を参照）、特徴表現を共有できるようにし、学習効率を向上させました。

---

## 簡素化された入力特徴：48から17へ

### 元祖 AlphaGo の48特徴プレーン

元祖 AlphaGo のニューラルネットワーク入力には、48個の19x19特徴プレーンが含まれ、多くの人間が設計した特徴がエンコードされていました：

| カテゴリ | 特徴数 | 内容 |
|------|--------|------|
| 石の位置 | 3 | 黒石、白石、空点 |
| 呼吸点 | 8 | 1-8呼吸点の連 |
| 取り | 8 | 1-8個取れる |
| コウ | 1 | コウの位置 |
| 辺までの距離 | 4 | 一線から四線 |
| 着手の合法性 | 1 | どの位置に打てるか |
| 履歴状態 | 8 | 過去8手の位置 |
| 手番 | 1 | 黒番または白番 |
| その他 | 14 | シチョウ、眼形など |

これら48の特徴は囲碁の専門家が慎重に設計したもので、多くのドメイン知識を含んでいます。

### AlphaGo Zero の17特徴プレーン

AlphaGo Zero は入力を大幅に簡素化し、17個の特徴プレーンのみを使用します：

| プレーン番号 | 内容 | 数量 |
|----------|------|------|
| 1-8 | 黒石の位置（直近8手） | 8 |
| 9-16 | 白石の位置（直近8手） | 8 |
| 17 | 現在の手番（全て1または全て0） | 1 |

これら17の特徴には以下のみが含まれます：
- **現在の盤面状態**：各位置に黒石、白石、または空
- **履歴情報**：過去8手の盤面状態
- **手番情報**：どちらの番か

呼吸点も、シチョウ判定も、辺までの距離もありません——これら全ての「囲碁知識」はニューラルネットワーク自身に学習させます。

### なぜ簡素化が良いのか？

#### 1. ネットワーク自身に特徴を発見させる

複雑な手作業の特徴は重要な情報を見落としたり、誤った仮定をエンコードする可能性があります。ニューラルネットワークに生データから学習させることで、より良い特徴表現を発見できる可能性があります。

実際、AlphaGo Zero は人間が設計した全ての特徴（呼吸点、シチョウなど）を学習し、さらに人間が明確に意識していなかったパターンも学習しました。

#### 2. より良い汎用性

48の特徴の多くは囲碁専用のもの（シチョウ、辺までの距離など）でした。17の簡素化された特徴は汎用的で——どのボードゲームでも同様の方法でエンコードできます。

これは後の **AlphaZero**（汎用ゲームAI）の基礎を築きました。

#### 3. 人為的エラーの削減

手作業で設計された特徴には、誤りや不完全な定義が含まれる可能性があります。入力の簡素化により、このような問題の可能性が排除されます。

---

## 単一ネットワークアーキテクチャ

### 元祖のデュアルネットワーク設計

元祖 AlphaGo は2つの独立したニューラルネットワークを使用していました：

```
Policy Network:  入力 → CNN → 19x19 着手確率
Value Network:   入力 → CNN → 勝率評価（-1から1）
```

これら2つのネットワーク：
- 異なるアーキテクチャを持つ（層数、チャネル数がやや異なる）
- 独立して訓練（まず Policy を訓練し、次に Value を訓練）
- パラメータを一切共有しない

### Zero のデュアルヘッドネットワーク

AlphaGo Zero は単一のネットワークを使用しますが、2つの出力ヘッド（heads）があります：

```
入力 → ResNet 共有バックボーン → Policy Head → 19x19 着手確率
                              → Value Head  → 勝率評価
```

2つの Head は同じ ResNet バックボーンを共有し（詳細は[次の記事：デュアルヘッドネットワークと残差ネットワーク](../dual-head-resnet)を参照）、これにはいくつかの利点があります：

#### 1. パラメータ効率

バックボーンの共有は、大部分のパラメータが2つのタスクで共用されることを意味します。これにより総パラメータ量が削減され、過学習のリスクが低下します。

#### 2. 特徴共有

「どこに打つべきか」（Policy）と「誰が勝つか」（Value）は、類似した盤面パターンを理解する必要があります。バックボーンの共有により、これらの特徴を2つのタスクが同時に学習し利用できます。

#### 3. 訓練の安定性

共同訓練により、勾配信号が2つのソースから来るため、より豊富な監督信号が提供され、訓練がより安定します。

### 残差ネットワークの威力

AlphaGo Zero のバックボーンは **40層の残差ネットワーク（ResNet）** を使用しており、元祖 AlphaGo の13層 CNN よりもはるかに深いです。

残差接続（skip connections）により、深層ネットワークの効果的な訓練が可能になり、勾配消失問題を回避できます。これは2015年の ImageNet コンペティションでの画期的な技術であり、AlphaGo Zero で囲碁分野に成功裏に適用されました。

---

## 訓練効率の向上

### 自己対戦の指数関数的成長

AlphaGo Zero の訓練プロセスは驚くべき効率を示しました：

| 訓練時間 | ELO レーティング | 相当するレベル |
|----------|----------|--------|
| 0時間 | 0 | ランダムな手 |
| 3時間 | ~1000 | 基本ルールを発見 |
| 12時間 | ~3000 | 定石を発見 |
| 36時間 | ~4500 | ファン・フイ版を超越 |
| 60時間 | ~5200 | イ・セドル版を超越 |
| 72時間 | ~5400 | 元祖 AlphaGo を超越 |
| 40日 | ~5600 | 最強バージョン |

**3日で人間を超え、3日で数ヶ月かけて訓練した以前の AI を超越**——これは指数関数的な効率向上です。

### なぜこれほど速いのか？

#### 1. より強力な探索ガイダンス

AlphaGo Zero の MCTS は完全にニューラルネットワークによってガイドされ、高速プレイアウト戦略（rollout）をもはや使用しません。これにより探索がより効率的かつ正確になります。

#### 2. より速い自己対戦

ネットワークが1つだけで済む（2つではなく）ため、各自己対戦の計算コストが削減されます。これは同じ時間でより多くの訓練データを生成できることを意味します。

#### 3. より効果的な学習

デュアルヘッドネットワークの共同訓練により、各対局の情報がより効果的に利用されます。Policy と Value の勾配が相互に強化し合い、収束を加速します。

### 人間の学習との比較

人間の棋士は異なるレベルに達するまでにどれくらいの時間が必要でしょうか？

| レベル | 人間の所要時間 | AlphaGo Zero |
|------|-------------|--------------|
| 入門 | 数週間 | 数分 |
| アマチュア初段 | 数年 | 数時間 |
| プロレベル | 10-20年 | 1-2日 |
| 世界チャンピオン | 20年以上のフルタイム投入 | 3日 |
| 人間を超越 | 不可能 | 3日 |

この比較は人間の棋士を貶めるためではありません——彼らは生物学的ニューロンを使用しており、AlphaGo Zero は専用設計の TPU と数キロワットの電力を使用しています。しかし、正しい学習方法がいかに効率的であり得るかを確実に示しています。

---

## 汎用性：チェス、将棋

### AlphaZero の誕生

2017年12月、DeepMind は **AlphaZero** を発表しました——AlphaGo Zero の汎用版です。同じアルゴリズムで、ゲームルールを変更するだけで、3つのボードゲームで世界トップレベルに到達できます：

| ゲーム | 訓練時間 | 対戦相手 | 戦績 |
|------|----------|------|------|
| 囲碁 | 8時間 | AlphaGo Zero | 60:40 |
| チェス | 4時間 | Stockfish 8 | 28勝72引き分け0敗 |
| 将棋 | 2時間 | Elmo | 90:8:2 |

ここでの対戦相手に注目してください：
- **Stockfish** は当時最強のチェスエンジンで、数十年の人間の知識と最適化を使用
- **Elmo** は当時最強の将棋 AI

AlphaZero は数時間の訓練で、何年もかけて開発されたこれらの専用システムを超越しました。

### 汎用性の意義

AlphaGo Zero / AlphaZero は重要なことを証明しました：

> **同じ学習アルゴリズムが、異なる分野で超人的レベルに到達できる。**

これは3つの異なる AI ではなく、1つの汎用的な学習フレームワークです：

1. **自己対戦**で経験を生成
2. **モンテカルロ木探索**で可能性を探索
3. **ニューラルネットワーク**で戦略と価値関数を学習
4. **強化学習**で目的関数を最適化

このフレームワークはドメイン固有の知識に依存せず、AI の汎用化に向けた重要な一歩を踏み出しました。

### 従来の AI への衝撃

AlphaZero 以前、チェスと将棋の最強 AI は全て「エキスパートシステム」スタイルでした：

- **大量の人間の知識**：オープニングブック、エンドゲームデータベース、評価関数
- **数十年の最適化**：無数の棋士とエンジニアの心血
- **極度の専門化**：Stockfish は囲碁を打てず、Elmo はチェスを打てない

AlphaZero は1つの汎用アルゴリズムで数時間でこれら全てを超越しました。これにより多くの AI 研究者が再考しました：

> 「汎用学習アルゴリズム」にもっと努力を注ぐべきか、それとも「専門家の知識のエンコード」にか？

答えはますます明らかになっているようです：機械に自ら学習させることは、知識を教えるよりも効果的です。

---

## AlphaGo Zero の棋風

### 人間を超えた美学

囲碁界は AlphaGo Zero の着手について共通の評価を持っています：**より美しい**。

AlphaGo Lee の着手は時として「奇妙」に見えました——第37手のような着手は、人間が事後分析してようやくその妙味を理解できました。しかし AlphaGo Zero の着手は、しばしば事後に「一目で良い手だとわかる」と評価されます。

これはおそらく以下の理由によります：

1. **より強い棋力**：Zero はより深く読めるため、着手がより余裕がある
2. **人間のバイアスがない**：従来の定石に縛られない
3. **一貫した目標**：勝率のみを追求し、人間を模倣しない

### 人間の棋理の再発見

興味深いことに、AlphaGo Zero は訓練プロセスで人間が数千年かけて蓄積した囲碁の知識を「再発見」しました：

- **定石**：Zero は多くの一般的な定石を自ら発見しました。なぜならこれらは確かに双方の最適解だからです
- **布石の原則**：隅、辺、中央の重要性の順序
- **石形の知識**：愚形と好形の違い

これは人間の棋理の妥当性を検証しました——これらの知識は偶然ではなく、囲碁の本質の反映です。

### 人間を超えたイノベーション

しかし Zero は人間が考えたこともない着手も発見しました：

- **非伝統的な序盤**：従来の序盤を基礎とした変化
- **積極的な捨て石**：人間よりも局部を放棄して全局の優勢を得ることを厭わない
- **反直感的な形**：表面上の「悪形」が実は最適解

これらのイノベーションは人間の囲碁理解を変えつつあります。多くのプロ棋士は、AlphaGo Zero の棋譜を研究することで、囲碁に対する全く新しい認識を得たと述べています。

---

## 技術詳細まとめ

### 元祖 AlphaGo との完全比較

| 側面 | AlphaGo（元祖） | AlphaGo Zero |
|------|----------------|--------------|
| **訓練データ** | 人間の棋譜 + 自己対戦 | 純粋な自己対戦 |
| **学習方法** | 教師あり学習 + 強化学習 | 純粋な強化学習 |
| **入力特徴** | 48プレーン | 17プレーン |
| **ネットワークアーキテクチャ** | 分離した Policy/Value | デュアルヘッド ResNet |
| **ネットワーク深度** | 13層 | 40層（またはそれ以上） |
| **MCTS 評価** | ニューラルネットワーク + Rollout | 純粋なニューラルネットワーク |
| **探索回数** | 1手あたり約100,000 | 1手あたり約1,600 |
| **訓練 TPU** | 50+ | 4 |
| **推論 TPU** | 48 | 4（スケーラブル） |

### コアアルゴリズム

AlphaGo Zero の訓練ループは非常にシンプルです：

```
1. 自己対戦
   - 現在のネットワークで MCTS を実行
   - MCTS 探索確率に従って着手を選択
   - 各手の（局面, MCTS確率, 勝敗結果）を記録

2. ネットワーク訓練
   - 経験プールからサンプリング
   - Policy Head：MCTS 確率との交差エントロピーを最小化
   - Value Head：実際の勝敗との平均二乗誤差を最小化
   - 2つの目標を共同最適化

3. ネットワーク更新
   - 新しいネットワークで古いネットワークを置き換え（対戦で新ネットワークがより強いことを検証）
   - ステップ1に戻る
```

このループは継続的に実行され、ネットワークは絶えず強くなります。人間のデータも、人間の知識もなく、ゲームルールと勝敗目標のみ。

---

## AI 研究への示唆

### 第一原理からの学習

AlphaGo Zero は「第一原理」からの学習方法を示しました：

> AI に方法を教えるのではなく、目標が何かを伝えて、方法を自ら発見させる。

これは従来のエキスパートシステムアプローチと鮮明なコントラストを成します。エキスパートシステムは人間の知識を AI にエンコードしようとしましたが、AlphaGo Zero は AI に自ら知識を発見させます。

結果として：AI が発見した知識は人間の知識よりも完全で正確である可能性があります。

### 自己対戦の威力

AlphaGo Zero は、自己対戦が無限の訓練データを生成でき、そのデータの品質はネットワークの向上とともに向上することを証明しました。

これは「正のサイクル」です：
- より強いネットワーク → より良い自己対戦データ
- より良いデータ → より強いネットワーク

このサイクルはゲームの理論的上限（存在する場合）に達するまで継続的に実行できます。

### 簡素化の重要性

AlphaGo Zero の成功は「簡素化」の重要性を証明しました：

- 入力の簡素化（48 → 17）
- アーキテクチャの簡素化（デュアルネットワーク → 単一ネットワーク）
- 訓練の簡素化（教師あり + 強化 → 純粋な強化）

各簡素化がシステムをより強力にしました。これは私たちに教えています：複雑さは良さではない、最もシンプルな解決策がしばしば最良です。

---

## アニメーション対応

本記事に関連するコア概念とアニメーション番号：

| 番号 | 概念 | 物理/数学対応 |
|------|------|--------------|
| 🎬 E7 | ゼロからの訓練 | 自己組織化現象 |
| 🎬 E5 | 自己対戦 | 不動点収束 |
| 🎬 E12 | 棋力成長曲線 | S字型成長 |
| 🎬 D12 | 残差ネットワーク | 勾配ハイウェイ |

---

## 関連記事

- **次の記事**：[デュアルヘッドネットワークと残差ネットワーク](../dual-head-resnet) — AlphaGo Zero のニューラルネットワークアーキテクチャの詳細解説
- **関連記事**：[自己対戦](../self-play) — なぜ自己対戦が超人的レベルを生み出せるのか
- **技術深掘り**：[ゼロからの訓練プロセス](../training-from-scratch) — Day 0-3 の詳細な進化

---

## 参考文献

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
3. DeepMind. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
4. Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." *Nature*, 588, 604-609.
