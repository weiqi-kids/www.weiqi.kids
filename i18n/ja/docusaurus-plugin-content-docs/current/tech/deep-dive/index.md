---
sidebar_position: 1
title: 深く研究したい方へ
description: 上級トピックガイド：ニューラルネットワーク、MCTS、訓練、最適化、デプロイメント
---

# 深く研究したい方へ

このセクションは、囲碁AIを深く研究したいエンジニア向けで、技術実装、理論基礎、実用応用をカバーしています。

---

## 記事一覧

### コア技術

| 記事 | 説明 |
|------|------|
| [ニューラルネットワークアーキテクチャ詳解](./neural-network) | KataGoの残差ネットワーク、入力特徴、マルチヘッド出力設計 |
| [MCTS実装詳細](./mcts-implementation) | PUCT選択、仮想損失、バッチ評価、並列化 |
| [KataGo訓練メカニズム解析](./training) | 自己対局、損失関数、訓練ループ |

### パフォーマンス最適化

| 記事 | 説明 |
|------|------|
| [GPUバックエンドと最適化](./gpu-optimization) | CUDA、OpenCL、Metalバックエンドの比較とチューニング |
| [モデル量子化とデプロイメント](./quantization-deploy) | FP16、INT8、TensorRT、各プラットフォームへのデプロイ |
| [評価とベンチマーク](./evaluation) | Eloレーティング、対局テスト、SPRT統計手法 |

### 上級トピック

| 記事 | 説明 |
|------|------|
| [分散訓練アーキテクチャ](./distributed-training) | Self-play Worker、データ収集、モデル公開 |
| [カスタムルールと変則](./custom-rules) | 中国、日本、AGAルール、盤面サイズバリエーション |
| [重要論文ガイド](./papers) | AlphaGo、AlphaZero、KataGo論文の重要ポイント解説 |

### オープンソースと実装

| 記事 | 説明 |
|------|------|
| [KataGoソースコード解説](./source-code) | ディレクトリ構造、コアモジュール、コーディングスタイル |
| [オープンソースコミュニティへの参加](./contributing) | 貢献方法、分散訓練、コミュニティ参加 |
| [ゼロから囲碁AIを作る](./build-from-scratch) | AlphaGo Zeroのシンプル版を一歩ずつ実装 |

---

## 目的別ガイド

| 目標 | 推奨パス |
|------|---------|
| ニューラルネットワーク設計を理解したい | [ニューラルネットワークアーキテクチャ詳解](./neural-network) → [MCTS実装詳細](./mcts-implementation) |
| 実行パフォーマンスを最適化したい | [GPUバックエンドと最適化](./gpu-optimization) → [モデル量子化とデプロイメント](./quantization-deploy) |
| 訓練方法を研究したい | [KataGo訓練メカニズム解析](./training) → [分散訓練アーキテクチャ](./distributed-training) |
| 論文の原理を理解したい | [重要論文ガイド](./papers) → [ニューラルネットワークアーキテクチャ詳解](./neural-network) |
| 実際にコードを書きたい | [ゼロから囲碁AIを作る](./build-from-scratch) → [KataGoソースコード解説](./source-code) |
| オープンソースプロジェクトに参加したい | [オープンソースコミュニティへの参加](./contributing) → [KataGoソースコード解説](./source-code) |

---

## 上級概念索引

深く研究する際に、以下の上級概念に触れることになります：

### Fシリーズ：スケーリング（8個）

| 番号 | 囲碁の概念 | 物理/数学対応 |
|------|---------|--------------|
| F1 | 盤面サイズ vs 複雑度 | 複雑度スケーリング |
| F2 | ネットワークサイズ vs 棋力 | 容量スケーリング |
| F3 | 訓練時間 vs 収益 | 収益逓減の法則 |
| F4 | データ量 vs 汎化 | サンプル複雑度 |
| F5 | 計算資源スケーリング | スケーリング則 |
| F6 | ニューラルスケーリング則 | 両対数関係 |
| F7 | 大バッチ訓練 | 臨界バッチ |
| F8 | パラメータ効率 | 圧縮限界 |

### Gシリーズ：次元（6個）

| 番号 | 囲碁の概念 | 物理/数学対応 |
|------|---------|--------------|
| G1 | 高次元表現 | ベクトル空間 |
| G2 | 次元の呪い | 高次元の困難 |
| G3 | 多様体仮説 | 低次元多様体 |
| G4 | 中間表現 | 隠れ空間 |
| G5 | 特徴分離 | 独立成分 |
| G6 | 意味方向 | 幾何代数 |

### Hシリーズ：強化学習（9個）

| 番号 | 囲碁の概念 | 物理/数学対応 |
|------|---------|--------------|
| H1 | MDP | マルコフ連鎖 |
| H2 | ベルマン方程式 | 動的計画法 |
| H3 | 価値反復 | 不動点定理 |
| H4 | 方策勾配 | 確率的最適化 |
| H5 | 経験再生 | 重要度サンプリング |
| H6 | 割引率 | 時間選好 |
| H7 | TD学習 | 逐次推定 |
| H8 | アドバンテージ関数 | ベースライン分散削減 |
| H9 | PPOクリッピング | 信頼領域 |

### Kシリーズ：最適化手法（6個）

| 番号 | 囲碁の概念 | 物理/数学対応 |
|------|---------|--------------|
| K1 | SGD | 確率的近似 |
| K2 | モメンタム | 慣性 |
| K3 | Adam | 適応ステップサイズ |
| K4 | 学習率減衰 | アニーリング |
| K5 | 勾配クリッピング | 飽和制限 |
| K6 | SGDノイズ | 確率的摂動 |

### Lシリーズ：汎化と安定性（5個）

| 番号 | 囲碁の概念 | 物理/数学対応 |
|------|---------|--------------|
| L1 | 過学習 | 過適合 |
| L2 | 正則化 | 制約最適化 |
| L3 | Dropout | スパース活性化 |
| L4 | データ拡張 | 対称性破れ |
| L5 | 早期終了 | 最適停止 |

---

## ハードウェア要件

### 読書と学習

特別な要件なし、どのコンピュータでも可能です。

### モデル訓練

| 規模 | 推奨ハードウェア | 訓練時間 |
|------|---------|---------|
| ミニ（b6c96） | GTX 1060 6GB | 数時間 |
| 小型（b10c128） | RTX 3060 12GB | 1-2日 |
| 中型（b18c384） | RTX 4090 24GB | 1-2週間 |
| フル（b40c256） | マルチGPUクラスター | 数週間 |

### 分散訓練への貢献

- GPUを持つどのコンピュータでも参加可能
- 最低GTX 1060または同等以上を推奨
- 安定したネットワーク接続が必要

---

## 読み始める

**ここから始めることをお勧めします：**

- 原理を理解したい方 → [ニューラルネットワークアーキテクチャ詳解](./neural-network)
- 実際に実装したい方 → [ゼロから囲碁AIを作る](./build-from-scratch)
- 論文を読みたい方 → [重要論文ガイド](./papers)
