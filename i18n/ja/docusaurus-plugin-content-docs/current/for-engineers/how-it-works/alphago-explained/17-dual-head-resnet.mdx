---
sidebar_position: 18
title: デュアルヘッドネットワークと残差ネットワーク
description: AlphaGo Zero のニューラルネットワークアーキテクチャの詳細解説 - 共有バックボーン、Policy Head、Value Head、40層 ResNet
keywords: [デュアルヘッドネットワーク, 残差ネットワーク, ResNet, Policy Head, Value Head, 深層学習, ニューラルネットワークアーキテクチャ]
---

# デュアルヘッドネットワークと残差ネットワーク

AlphaGo Zero の最も重要なアーキテクチャ革新の1つは、元祖 AlphaGo のデュアルネットワーク設計を**デュアルヘッドネットワーク**（Dual-Head Network）に置き換えたことです。この一見シンプルな変更が、顕著な性能向上とよりエレガントな学習プロセスをもたらしました。

本記事では、このアーキテクチャの設計原理、数学的基礎、そしてなぜこれほど効果的なのかを詳しく解説します。

---

## デュアルヘッドネットワーク設計

### 全体アーキテクチャ

AlphaGo Zero のニューラルネットワークは3つの部分に分けられます：

```
入力（17 × 19 × 19）
       ↓
┌─────────────────────────────────────────┐
│            共有バックボーン（ResNet）      │
│        40個の残差ブロック、256チャネル     │
└─────────────────────────────────────────┘
       ↓                    ↓
┌─────────────┐      ┌─────────────┐
│  Policy Head │      │  Value Head  │
│  (戦略ヘッド) │      │  (価値ヘッド) │
└─────────────┘      └─────────────┘
       ↓                    ↓
  19×19 確率分布          勝率 [-1, 1]
  + 1 Pass 確率
```

各部分を順に解説します。

### 共有バックボーン（Shared Backbone）

共有バックボーンは深い**残差ネットワーク（ResNet）**であり、盤面状態から特徴を抽出する役割を担います。

#### アーキテクチャ詳細

| コンポーネント | 仕様 |
|------|------|
| 入力層 | 3×3 畳み込み、256チャネル |
| 残差ブロック | 40個（または20個の軽量版） |
| 各残差ブロック | 2層 3×3 畳み込み、256チャネル |
| 活性化関数 | ReLU |
| 正規化 | Batch Normalization |

#### 数学的表現

入力を x（次元 17 x 19 x 19）とすると、共有バックボーンの出力は：

```
f(x) = ResNet_40(Conv_3x3(x))
```

ここで f(x)（次元 256 x 19 x 19）は高次元特徴表現です。

### Policy Head（戦略ヘッド）

Policy Head は各位置の着手確率を予測する役割を担います。

#### アーキテクチャ詳細

```
共有バックボーン出力（256 × 19 × 19）
       ↓
1×1 畳み込み（2チャネル）
       ↓
Batch Normalization
       ↓
ReLU
       ↓
フラット化（2 × 19 × 19 = 722）
       ↓
全結合層（362）
       ↓
Softmax
       ↓
出力：362個の確率（361位置 + Pass）
```

#### 数学的表現

```
π = Softmax(FC(Flatten(ReLU(BN(Conv_1x1(f(x)))))))
```

出力 π は362次元ベクトルで、全要素が非負かつ和が1を満たします。

### Value Head（価値ヘッド）

Value Head は現在の局面の勝率を予測する役割を担います。

#### アーキテクチャ詳細

```
共有バックボーン出力（256 × 19 × 19）
       ↓
1×1 畳み込み（1チャネル）
       ↓
Batch Normalization
       ↓
ReLU
       ↓
フラット化（1 × 19 × 19 = 361）
       ↓
全結合層（256）
       ↓
ReLU
       ↓
全結合層（1）
       ↓
Tanh
       ↓
出力：勝率 [-1, 1]
```

#### 数学的表現

```
v = Tanh(FC_1(ReLU(FC_2(Flatten(ReLU(BN(Conv_1x1(f(x)))))))))
```

出力 v は [-1, 1] の範囲内：
- v = 1：現在の手番が必勝
- v = -1：現在の手番が必敗
- v = 0：互角

---

## なぜバックボーンを共有するのか？

### 直感的理解

「次にどこに打つべきか」（Policy）と「誰が勝つか」（Value）という2つの問題は、実際には同じ盤面パターンを理解する必要があります：

- **石形**：どの形が良く、どの形が悪いか
- **勢力**：どちらが大きいか、どこにまだスペースがあるか
- **死活**：どの石が生きていて、どの石がまだコウ争い中か
- **戦い**：どこに攻め合いがあるか、局部の勝敗はどうか

2つの独立したネットワークを使用すると、これらの特徴を2回学習する必要があります。共有バックボーンにより、これらの基礎特徴は1回だけ学習すれば、両方のタスクで使用できます。

### マルチタスク学習の視点

機械学習の観点から、これは**マルチタスク学習（Multi-task Learning）**の一種です：

```
L = L_policy + L_value
```

2つのタスクが基礎表現を共有することで、いくつかの利点が生まれます：

#### 1. 正則化効果

パラメータの共有は暗黙の正則化に相当します。ある特徴が Policy にのみ有用で Value には無用（またはその逆）の場合、過度に増幅されにくくなります。

有効パラメータ量は2つの独立したネットワークのパラメータ量よりも少なくなります。

#### 2. データ効率

各対局は同時に Policy ラベル（MCTS 探索確率）と Value ラベル（最終的な勝敗）を生成します。共有バックボーンにより、両方のラベルが共有特徴の訓練に使用され、データ利用効率が向上します。

#### 3. 豊富な勾配信号

両方のタスクの勾配が共有バックボーンに流れます：

```
∂L/∂θ_shared = ∂L_policy/∂θ_shared + ∂L_value/∂θ_shared
```

これにより、より豊富な監督信号が提供され、共有特徴がよりロバストになります。

### 実験的証拠

DeepMind のアブレーション実験は、デュアルヘッドネットワークが分離したデュアルネットワークを大幅に上回ることを示しました：

| 構成 | ELO レーティング | 相対差 |
|------|----------|----------|
| 分離した Policy + Value ネットワーク | 基準 | - |
| デュアルヘッドネットワーク（共有バックボーン） | +300 ELO | 約65%の勝率差 |

300 ELO の差は、デュアルヘッドネットワークが分離ネットワークに対して約65%の勝率を持つことを意味します。これは顕著な向上です。

---

## 残差ネットワークの原理

### 深層ネットワークのジレンマ

ResNet が発明される前、深層ニューラルネットワークはパラドックスに直面していました：

> 理論的には、より深いネットワークは少なくとも浅いネットワークと同等以上であるべき（最悪の場合、追加の層は恒等写像を学習できる）。しかし実際には、より深いネットワークはしばしばより悪い性能を示す。

これが**劣化問題（Degradation Problem）**です：

- 訓練誤差が深さとともに増加する（過学習ではなく、最適化の困難）
- 勾配が逆伝播時に徐々に消失する（Vanishing Gradient）
- 深い層のパラメータはほとんど効果的に更新されない

### 残差ブロックの設計

何愷明らは2015年に、シンプルでエレガントな解決策を提案しました：**残差接続（Skip Connection）**。

```
入力 x
   ↓
┌─────────────┐
│  畳み込み層   │
│  BN + ReLU  │
│  畳み込み層   │
│  BN        │
└─────────────┘
   ↓ F(x)
   ↓←────────────── x（スキップ接続）
   +
   ↓
 ReLU
   ↓
出力 x + F(x)
```

#### 数学的表現

従来のネットワーク：目標写像 H(x) を学習

```
y = H(x)
```

残差ネットワーク：**残差写像** F(x) = H(x) - x を学習

```
y = F(x) + x
```

### なぜ残差接続は効果的なのか？

#### 1. 勾配ハイウェイ

逆伝播の勾配を考えます：

```
∂L/∂x = ∂L/∂y × ∂y/∂x = ∂L/∂y × (1 + ∂F(x)/∂x)
```

鍵となるのは **+1** です。∂F(x)/∂x が非常に小さいかゼロであっても、勾配は +1 を通じて直接伝播できます。

これは「勾配ハイウェイ」を建設したようなもので、勾配が出力層から入力層へ障害なく伝播できます。

#### 2. 恒等写像の学習が容易

最適解が恒等写像に近い（H(x) が約 x）場合：
- 従来のネットワーク：H(x) = x を学習する必要があり、難しい可能性がある
- 残差ネットワーク：F(x) が約 0 を学習するだけでよく、比較的容易

重みをゼロまたはゼロに近い値に初期化すると、残差ブロックは自然に恒等写像に向かいます。

#### 3. アンサンブル効果

深い ResNet は多くの浅いネットワークの**暗黙的なアンサンブル**と見なせます。n 個の残差ブロックがある場合、情報は 2^n 種類の異なるパスを通じて流れることができます。

このアンサンブル効果はモデルのロバスト性を高めます。

### ImageNet での ResNet の突破

ResNet は2015年の ImageNet コンペティションで驚くべき成績を収めました：

| 深さ | Top-5 エラー率 |
|------|-------------|
| VGG-19（残差なし） | 7.3% |
| ResNet-34 | 5.7% |
| ResNet-152 | 4.5% |
| 人間レベル | 約5.1% |

**152層**の ResNet は訓練可能であるだけでなく、19層の VGG よりもはるかに優れていました。これは残差接続が深層ネットワークの訓練問題を確かに解決したことを証明しています。

---

## AlphaGo Zero の40層 ResNet

### なぜ40層を選択したのか？

DeepMind は異なる深さの ResNet をテストしました：

| 残差ブロック数 | 総層数 | ELO レーティング |
|------------|--------|----------|
| 5 | 11 | 基準 |
| 10 | 21 | +200 |
| 20 | 41 | +400 |
| 40 | 81 | +500 |

より深いネットワークは確かにより強いですが、限界効用は逓減します。AlphaGo Zero は20または40個の残差ブロックを使用します：

- **AlphaGo Zero（論文版）**：40個の残差ブロック、256チャネル
- **軽量版**：20個の残差ブロック、256チャネル

40層の構成は棋力と訓練コストの間で良いバランスを取っています。

### 具体的な構成

AlphaGo Zero の ResNet 構成は以下の通りです：

```
入力：17 × 19 × 19
↓
畳み込み層：3×3, 256チャネル, BN, ReLU
↓
残差ブロック ×40：
  ├─ 畳み込み層：3×3, 256チャネル, BN, ReLU
  ├─ 畳み込み層：3×3, 256チャネル, BN
  └─ スキップ接続 + ReLU
↓
Policy Head / Value Head
```

#### パラメータ数の推定

| コンポーネント | パラメータ数（約） |
|------|-------------|
| 入力畳み込み | 17 × 3 × 3 × 256 ≈ 39K |
| 各残差ブロック | 2 × 256 × 3 × 3 × 256 ≈ 1.2M |
| 40個の残差ブロック | 40 × 1.2M ≈ 47M |
| Policy Head | 約1M |
| Value Head | 約0.2M |
| **合計** | **約48M** |

約4800万パラメータで、現代の基準では中規模のニューラルネットワークです。

### Batch Normalization の役割

各畳み込み層の後には **Batch Normalization（BN）** があり、これは訓練の安定性に不可欠です：

#### 1. 活性値の正規化

BN は各層の活性値を平均0、分散1に正規化します：

```
x_hat = (x - μ_B) / sqrt(σ_B² + ε)
y = γ × x_hat + β
```

ここで γ と β は学習可能なパラメータです。

#### 2. 内部共変量シフトの緩和

深層ネットワークでは、各層の入力分布が前の層のパラメータ更新とともに変化します。BN により各層の入力分布が安定し、訓練の収束が加速されます。

#### 3. 正則化効果

BN は訓練時に mini-batch の統計量を使用するため、ランダム性が導入され、軽度の正則化効果があります。

---

## 他のアーキテクチャとの比較

### vs. 元祖 AlphaGo の CNN

| 特性 | AlphaGo 元祖 | AlphaGo Zero |
|------|-------------|--------------|
| アーキテクチャタイプ | 標準 CNN | ResNet |
| 深さ | 13層 | 41-81層 |
| 残差接続 | なし | あり |
| ネットワーク数 | 2（分離） | 1（共有） |
| BN | なし | あり |

### vs. VGG スタイルネットワーク

VGG は2014年 ImageNet 準優勝のアーキテクチャで、積み重ねた 3×3 畳み込みを使用：

| 特性 | VGG | ResNet |
|------|-----|--------|
| 訓練可能な最大深さ | 約19層 | 152層以上 |
| 勾配の流れ | 層ごとに減衰 | ハイウェイあり |
| 訓練の難易度 | 深層は困難 | 深層も訓練可能 |

### vs. Inception / GoogLeNet

Inception はマルチスケール畳み込みを並列に使用：

| 特性 | Inception | ResNet |
|------|-----------|--------|
| 特徴 | マルチスケール特徴 | 深さの積み重ね |
| 複雑さ | 比較的高い | シンプル |
| 囲碁への適用性 | 一般的 | 優秀 |

ResNet のシンプルな設計は、深い推論が必要な囲碁のようなタスクにより適しています。

### vs. Transformer

2017年に提案された Transformer アーキテクチャは NLP 分野で大きな成功を収めました。Transformer を囲碁に適用する試みもあります：

| 特性 | ResNet | Transformer |
|------|--------|-------------|
| 帰納バイアス | 局所性（畳み込み） | グローバルアテンション |
| 位置エンコーディング | 暗黙的（畳み込み） | 明示的 |
| 囲碁での性能 | 優秀 | 可能だが ResNet を上回らない |
| 計算効率 | 比較的高い | 比較的低い（O(n²)） |

明確な空間構造を持つ囲碁のような問題には、CNN/ResNet の帰納バイアスがより適切です。

---

## 設計選択の詳細分析

### なぜ 3×3 畳み込みを使うのか？

AlphaGo Zero は全体を通じて 3×3 畳み込みを使用し、より大きな畳み込みカーネルは使用しません：

1. **パラメータ効率**：2つの 3×3 畳み込みの受容野は1つの 5×5 と同等ですが、パラメータ数は少ない（18 vs 25）
2. **より深いネットワーク**：同じパラメータ量でより多くの層を積み重ねられる
3. **より多くの非線形性**：層間に ReLU があり、表現力が増加

### なぜ256チャネルなのか？

256チャネルは経験的な選択です：

- **少なすぎる**（例：64）：表現力が不足し、複雑なパターンを捉えられない
- **多すぎる**（例：512）：パラメータ量が倍増し、訓練コストが大幅に増加するが、棋力向上は限定的

後の KataGo の実験では、チャネル数は訓練リソースに応じて調整できることが示されました：
- 低リソース：128チャネル、20ブロック
- 高リソース：256チャネル、40ブロック
- より高リソース：384チャネル、60ブロック

### なぜ Policy Head は Softmax、Value Head は Tanh なのか？

#### Policy Head：Softmax

着手は**分類問題**です——361位置（+ Pass）から1つを選択します。Softmax 出力は以下を満たします：
- 全確率が非負：π_i >= 0
- 確率の和が1：Σπ_i = 1

これは確率分布の定義と一致します。

#### Value Head：Tanh

勝率は**回帰問題**です——連続値を予測します。Tanh の出力範囲は [-1, 1]：
- 有界：極端な値を生成しない
- 対称：勝ちと負けを対称に処理
- 微分可能：勾配計算に便利

Tanh を使用し、非有界出力（線形層など）を使用しないことで、訓練の不安定性を防ぎます。

---

## 訓練の詳細

### 損失関数

AlphaGo Zero の総損失は3項の和です：

```
L = L_policy + L_value + L_reg
```

#### Policy Loss

**交差エントロピー損失**を使用し、ネットワーク出力を MCTS 探索確率に近づけます：

```
L_policy = -Σ π_MCTS(a) × log(π_net(a))
```

ここで：
- π_MCTS(a) は行動 a に対する MCTS の探索確率
- π_net(a) はネットワーク出力の確率

#### Value Loss

**平均二乗誤差（MSE）** を使用し、ネットワーク出力を実際の勝敗に近づけます：

```
L_value = (v_net - z)²
```

ここで：
- v_net はネットワークが予測する勝率
- z は実際の対局結果（+1 または -1）

#### Regularization Loss

過学習を防ぐために **L2 正則化**を使用：

```
L_reg = c × ||θ||²
```

ここで c は正則化係数、θ はネットワークパラメータです。

### 最適化器の設定

| パラメータ | 値 |
|------|-----|
| 最適化器 | SGD + Momentum |
| モメンタム | 0.9 |
| 初期学習率 | 0.01 |
| 学習率減衰 | X ステップごとに半減 |
| Batch Size | 32 × 2048 = 64K（分散） |
| L2 正則化係数 | 1e-4 |

### データ拡張

囲碁盤には8つの対称性があります（4回の回転 × 2回の反転）。訓練時、各局面から8つの等価な訓練サンプルを生成できます。

これにより有効訓練データが8倍に増加し、追加の自己対戦は不要です。

---

## 実装上の考慮事項

### メモリ最適化

40層 ResNet の訓練には大量のメモリが必要です：
- **順伝播**：各層の活性値を保存する必要がある（逆伝播用）
- **逆伝播**：勾配を保存する必要がある

最適化戦略：
1. **勾配チェックポイント（Gradient Checkpointing）**：一部の活性値のみを保存し、必要時に再計算
2. **混合精度訓練**：FP16 を使用してメモリ使用量を削減
3. **分散訓練**：batch を複数の GPU/TPU に分散

### 推論の最適化

推論時には BN の mini-batch 統計量は不要で、訓練時に蓄積した移動平均を使用できます：

```
x_hat = (x - μ_moving) / sqrt(σ_moving² + ε)
```

これにより推論速度が向上し、結果が決定論的になります。

### 量子化と圧縮

デプロイ時にはネットワークをさらに圧縮できます：
- **重み量子化**：FP32 → INT8、メモリが4分の1に削減
- **プルーニング**：小さな重みの接続を除去
- **知識蒸留**：大きなネットワークで小さなネットワークを訓練

---

## アニメーション対応

本記事に関連するコア概念とアニメーション番号：

| 番号 | 概念 | 物理/数学対応 |
|------|------|--------------|
| 🎬 E3 | デュアルヘッドネットワーク | マルチタスク学習 |
| 🎬 D12 | 残差接続 | 勾配ハイウェイ |
| 🎬 D8 | 畳み込みニューラルネットワーク | 局所受容野 |
| 🎬 D10 | Batch Normalization | 分布正規化 |

---

## 関連記事

- **前の記事**：[AlphaGo Zero 概要](../alphago-zero) — なぜ人間の棋譜は不要なのか
- **次の記事**：[ゼロからの訓練プロセス](../training-from-scratch) — Day 0-3 の詳細な進化
- **技術深掘り**：[CNN と囲碁の融合](../cnn-and-go) — なぜ CNN は盤面に適しているのか

---

## 参考文献

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. He, K., et al. (2016). "Deep Residual Learning for Image Recognition." *CVPR 2016*.
3. Ioffe, S., & Szegedy, C. (2015). "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift." *ICML 2015*.
4. Caruana, R. (1997). "Multitask Learning." *Machine Learning*, 28(1), 41-75.
5. Veit, A., et al. (2016). "Residual Networks Behave Like Ensembles of Relatively Shallow Networks." *NeurIPS 2016*.
