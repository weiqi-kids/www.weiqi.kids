---
sidebar_position: 17
title: AlphaGo Zero अवलोकन
description: शून्य से शुरू, पूरी तरह स्व-शिक्षित, AlphaGo Zero ने बिना मानव शतरंज रिकॉर्ड के सभी पूर्ववर्ती संस्करणों को कैसे पछाड़ा
keywords: [AlphaGo Zero, सेल्फ-प्ले, रीइन्फोर्समेंट लर्निंग, डीप लर्निंग, Go AI, अनसुपरवाइज्ड लर्निंग]
---

# AlphaGo Zero अवलोकन

2017 के अक्टूबर में, DeepMind ने AI जगत को चौंकाने वाला एक परिणाम प्रकाशित किया: **AlphaGo Zero** ने बिना किसी मानव शतरंज रिकॉर्ड के, पूरी तरह यादृच्छिक स्थिति से प्रशिक्षण शुरू करके, केवल तीन दिनों में ली से-दोल को हराने वाले मूल AlphaGo को पार कर लिया, और **100:0** के स्कोर से जीता।

यह केवल संख्यात्मक प्रगति नहीं है। यह एक पूरी तरह नई प्रतिमान का प्रतिनिधित्व करता है: **AI को मानव ज्ञान की आवश्यकता नहीं है, यह शून्य से सब कुछ खोज सकता है**।

---

## मानव शतरंज रिकॉर्ड की आवश्यकता क्यों नहीं?

### मानव शतरंज रिकॉर्ड की सीमाएं

मूल AlphaGo की प्रशिक्षण प्रक्रिया दो चरणों में विभाजित थी:

1. **सुपरवाइज्ड लर्निंग**: 3 करोड़ मानव खेलों से Policy Network को प्रशिक्षित किया
2. **रीइन्फोर्समेंट लर्निंग**: सेल्फ-प्ले के माध्यम से और सुधार

इस विधि में कई मौलिक समस्याएं हैं:

#### 1. मानव शतरंज रिकॉर्ड की सीमा है

मानव खिलाड़ियों की क्षमता की एक सीमा है, शतरंज रिकॉर्ड में मानव की समझ शामिल है, साथ ही मानव की गलतियां और पूर्वाग्रह भी। जब AI मानव शतरंज रिकॉर्ड से सीखता है, वह सीखता है:

- मानव जो अच्छा मानते हैं (लेकिन जरूरी नहीं कि सर्वोत्तम हो)
- मानव की सोच के पैटर्न (लेकिन नवाचार को सीमित कर सकते हैं)
- मानव की गलतियां (सही नमूनों के रूप में सीखी जाती हैं)

#### 2. सुपरवाइज्ड लर्निंग की बाधा

सुपरवाइज्ड लर्निंग का लक्ष्य "मानव की नकल करना" है—यह भविष्यवाणी करना कि मानव खिलाड़ी कौन सी चाल चलेगा। इसका मतलब है कि AI की क्षमता की ऊपरी सीमा मानव खिलाड़ी की क्षमता द्वारा सीमित है।

जैसे एक शिष्य केवल गुरु की नकल कर सकता है, गुरु से आगे कभी नहीं जा सकता।

#### 3. डेटा संग्रह की लागत

उच्च गुणवत्ता वाले मानव शतरंज रिकॉर्ड को संचित करने में वर्षों लगते हैं, और केवल Go जैसे लंबे इतिहास वाले खेलों में ही मौजूद हैं। यदि AI को नए क्षेत्रों में लागू करना है (जैसे प्रोटीन संरचना भविष्यवाणी), "मानव विशेषज्ञ शतरंज रिकॉर्ड" उपलब्ध नहीं हैं।

### Zero की सफलता

AlphaGo Zero ने सुपरवाइज्ड लर्निंग चरण को पूरी तरह छोड़ दिया, सीधे **यादृच्छिक आरंभीकरण** से सेल्फ-प्ले शुरू किया। इसने उपरोक्त सभी समस्याओं का समाधान किया:

| समस्या | मूल AlphaGo | AlphaGo Zero |
|------|-------------|--------------|
| मानव ज्ञान की सीमा | शतरंज रिकॉर्ड गुणवत्ता से सीमित | कोई सीमा नहीं |
| सीखने का लक्ष्य | मानव की नकल | जीत दर अधिकतम करना |
| डेटा आवश्यकता | 3 करोड़ खेल | 0 |
| विस्तारणीयता | केवल Go | अन्य क्षेत्रों में विस्तार योग्य |

यह एक मौलिक प्रतिमान परिवर्तन है: "मानव ज्ञान सीखना" से "प्रथम सिद्धांतों से ज्ञान खोजना"।

---

## मूल AlphaGo से तुलना: 100:0

### कुचलने वाली जीत

DeepMind ने प्रशिक्षित AlphaGo Zero को AlphaGo के विभिन्न संस्करणों से खेलाया:

| प्रतिद्वंद्वी | AlphaGo Zero का रिकॉर्ड |
|------|-------------------|
| AlphaGo Fan (फान हुई को हराने वाला संस्करण) | 100:0 |
| AlphaGo Lee (ली से-दोल को हराने वाला संस्करण) | 100:0 |
| AlphaGo Master (60 लगातार जीत वाला संस्करण) | 89:11 |

**100:0**—इसका मतलब है कि 100 खेलों में, मूल AlphaGo एक भी नहीं जीत सका।

### कम संसाधन, मजबूत खेल

केवल जीतना नहीं, AlphaGo Zero ने कम संसाधनों के साथ मजबूत खेल हासिल किया:

| सूचक | AlphaGo Lee | AlphaGo Zero |
|------|-------------|--------------|
| प्रशिक्षण समय | कई महीने | 40 दिन (3 दिन में AlphaGo Lee को पार) |
| प्रशिक्षण खेल | 3 करोड़ मानव शतरंज रिकॉर्ड + सेल्फ-प्ले | 49 लाख सेल्फ-प्ले खेल |
| TPU संख्या (प्रशिक्षण) | 50+ | 4 |
| TPU संख्या (इन्फरेंस) | 48 | 4 |
| इनपुट विशेषताएं | 48 प्लेन | 17 प्लेन |
| न्यूरल नेटवर्क | SL + RL दोहरा नेटवर्क | एकल डुअल-हेड नेटवर्क |

यह एक आश्चर्यजनक दक्षता सुधार है: **संसाधन 10 गुना से अधिक कम, लेकिन खेल क्षमता में काफी वृद्धि**।

### Zero इतना मजबूत क्यों है?

AlphaGo Zero के मजबूत होने के कारणों को कई दृष्टिकोणों से समझा जा सकता है:

#### 1. पूर्वाग्रह-मुक्त सीखना

मूल AlphaGo मानव शतरंज रिकॉर्ड से सीखा, मानव के पूर्वाग्रहों को विरासत में लिया। उदाहरण के लिए, मानव खिलाड़ी कुछ जोसेकी को अधिक महत्व दे सकते हैं, या कुछ स्थितियों का गलत मूल्यांकन कर सकते हैं।

AlphaGo Zero के पास यह बोझ नहीं है। यह खाली स्लेट से शुरू करता है, केवल जीत-हार परिणाम से सीखता है कि अच्छी चाल क्या है। इससे यह उन चालों की खोज कर सका जो मानव ने कभी नहीं सोची थीं।

#### 2. एकसमान सीखने का लक्ष्य

मूल AlphaGo के प्रशिक्षण में दो अलग-अलग लक्ष्य थे:
- सुपरवाइज्ड लर्निंग: मानव चाल भविष्यवाणी सटीकता अधिकतम करना
- रीइन्फोर्समेंट लर्निंग: जीत दर अधिकतम करना

ये दो लक्ष्य परस्पर विरोधी हो सकते हैं। AlphaGo Zero का केवल एक लक्ष्य है: **जीत दर अधिकतम करना**। इससे सीखने की प्रक्रिया अधिक एकसमान और प्रभावी है।

#### 3. सरल आर्किटेक्चर

मूल AlphaGo अलग Policy Network और Value Network का उपयोग करता था। AlphaGo Zero एकल डुअल-हेड नेटवर्क का उपयोग करता है (अगले लेख में विस्तार से), जिससे फीचर प्रतिनिधित्व साझा होता है, सीखने की दक्षता बढ़ती है।

---

## सरलीकृत इनपुट विशेषताएं: 48 से 17

### मूल AlphaGo के 48 फीचर प्लेन

मूल AlphaGo के न्यूरल नेटवर्क इनपुट में 48 19x19 फीचर प्लेन थे, जिसमें मानव द्वारा डिज़ाइन की गई कई विशेषताएं एनकोड थीं:

| श्रेणी | फीचर संख्या | सामग्री |
|------|--------|------|
| पत्थर की स्थिति | 3 | काला पत्थर, सफेद पत्थर, खाली बिंदु |
| लिबर्टी | 8 | 1-8 लिबर्टी वाले स्ट्रिंग्स |
| कैप्चर | 8 | 1-8 पत्थर कैप्चर करने योग्य |
| को | 1 | को स्थिति |
| किनारे की दूरी | 4 | पहली से चौथी लाइन |
| चाल वैधता | 1 | कहां चल सकते हैं |
| ऐतिहासिक स्थिति | 8 | पिछली 8 चालों की स्थिति |
| बारी | 1 | काले या सफेद की बारी |
| अन्य | 14 | लैडर, आई आदि |

ये 48 विशेषताएं Go विशेषज्ञों द्वारा सावधानीपूर्वक डिज़ाइन की गई थीं, जिसमें बहुत सारा डोमेन ज्ञान था।

### AlphaGo Zero के 17 फीचर प्लेन

AlphaGo Zero ने इनपुट को काफी सरल किया, केवल 17 फीचर प्लेन:

| प्लेन नंबर | सामग्री | संख्या |
|----------|------|------|
| 1-8 | काले पत्थर की स्थिति (पिछली 8 चालें) | 8 |
| 9-16 | सफेद पत्थर की स्थिति (पिछली 8 चालें) | 8 |
| 17 | वर्तमान बारी (सब 1 या सब 0) | 1 |

इन 17 विशेषताओं में केवल शामिल है:
- **वर्तमान बोर्ड स्थिति**: प्रत्येक स्थान पर काला पत्थर, सफेद पत्थर या खाली
- **ऐतिहासिक जानकारी**: पिछली 8 चालों की बोर्ड स्थिति
- **बारी जानकारी**: किसकी बारी है

कोई लिबर्टी नहीं, कोई लैडर निर्णय नहीं, कोई किनारे की दूरी नहीं—यह सारा "Go ज्ञान" न्यूरल नेटवर्क को स्वयं सीखने दिया गया।

### सरलीकरण अच्छा क्यों है?

#### 1. नेटवर्क को स्वयं विशेषताएं खोजने दें

जटिल हाथ से बनाई गई विशेषताएं महत्वपूर्ण जानकारी छोड़ सकती हैं, या गलत धारणाएं एनकोड कर सकती हैं। न्यूरल नेटवर्क को कच्चे डेटा से सीखने दें, यह बेहतर फीचर प्रतिनिधित्व खोज सकता है।

वास्तव में, AlphaGo Zero ने मानव द्वारा डिज़ाइन की गई सभी विशेषताएं (लिबर्टी, लैडर आदि) सीखीं, और कुछ ऐसे पैटर्न भी सीखे जिनके बारे में मानव को स्पष्ट जागरूकता नहीं थी।

#### 2. बेहतर विस्तारणीयता

48 विशेषताओं में से कई Go-विशिष्ट हैं (जैसे लैडर, किनारे की दूरी)। 17 सरलीकृत विशेषताएं सार्वभौमिक हैं—किसी भी बोर्ड गेम को समान तरीके से एनकोड किया जा सकता है।

इसने बाद के **AlphaZero** (सार्वभौमिक गेम AI) की नींव रखी।

#### 3. मानवीय त्रुटियों को कम करना

हाथ से डिज़ाइन की गई विशेषताओं में गलतियां या अधूरी परिभाषाएं हो सकती हैं। इनपुट का सरलीकरण इस प्रकार की समस्याओं की संभावना को समाप्त करता है।

---

## एकल नेटवर्क आर्किटेक्चर

### मूल का दोहरा नेटवर्क डिज़ाइन

मूल AlphaGo दो स्वतंत्र न्यूरल नेटवर्क का उपयोग करता था:

```
Policy Network:  इनपुट → CNN → 19x19 चाल संभाव्यता
Value Network:   इनपुट → CNN → जीत दर मूल्यांकन (-1 से 1)
```

ये दो नेटवर्क:
- अलग-अलग आर्किटेक्चर थे (लेयर संख्या, चैनल संख्या थोड़ी अलग)
- स्वतंत्र रूप से प्रशिक्षित (पहले Policy, फिर Value)
- कोई पैरामीटर साझा नहीं

### Zero का डुअल-हेड नेटवर्क

AlphaGo Zero एक नेटवर्क का उपयोग करता है, लेकिन दो आउटपुट हेड्स के साथ:

```
इनपुट → ResNet साझा बैकबोन → Policy Head → 19x19 चाल संभाव्यता
                           → Value Head  → जीत दर मूल्यांकन
```

दोनों Head एक ही ResNet बैकबोन साझा करते हैं (विस्तार के लिए [अगला लेख: डुअल-हेड नेटवर्क और रेसिड्युअल नेटवर्क](../dual-head-resnet) देखें), जिसके कई फायदे हैं:

#### 1. पैरामीटर दक्षता

साझा बैकबोन का मतलब है कि अधिकांश पैरामीटर दोनों कार्यों द्वारा साझा किए जाते हैं। इससे कुल पैरामीटर संख्या कम होती है, ओवरफिटिंग का जोखिम कम होता है।

#### 2. फीचर साझाकरण

"कहां चलना चाहिए" (Policy) और "कौन जीतेगा" (Value) को समान बोर्ड पैटर्न समझने की आवश्यकता है। साझा बैकबोन इन विशेषताओं को दोनों कार्यों द्वारा एक साथ सीखने और उपयोग करने देता है।

#### 3. प्रशिक्षण स्थिरता

संयुक्त प्रशिक्षण ग्रेडिएंट सिग्नल को दो स्रोतों से लाता है, समृद्ध पर्यवेक्षण सिग्नल प्रदान करता है, प्रशिक्षण को अधिक स्थिर बनाता है।

### रेसिड्युअल नेटवर्क की शक्ति

AlphaGo Zero का बैकबोन **40 लेयर रेसिड्युअल नेटवर्क (ResNet)** का उपयोग करता है, जो मूल AlphaGo के 13 लेयर CNN से बहुत गहरा है।

रेसिड्युअल कनेक्शन (skip connections) गहरे नेटवर्क को प्रभावी ढंग से प्रशिक्षित करने देते हैं, ग्रेडिएंट विलुप्ति की समस्या से बचाते हैं। यह 2015 ImageNet प्रतियोगिता की सफलता वाली तकनीक है, जिसे AlphaGo Zero ने Go क्षेत्र में सफलतापूर्वक लागू किया।

---

## प्रशिक्षण दक्षता में सुधार

### सेल्फ-प्ले की घातांकीय वृद्धि

AlphaGo Zero की प्रशिक्षण प्रक्रिया आश्चर्यजनक दक्षता दिखाती है:

| प्रशिक्षण समय | ELO रेटिंग | के समकक्ष |
|----------|----------|--------|
| 0 घंटे | 0 | यादृच्छिक चालें |
| 3 घंटे | ~1000 | बुनियादी नियम खोजे |
| 12 घंटे | ~3000 | जोसेकी खोजे |
| 36 घंटे | ~4500 | फान हुई संस्करण को पार |
| 60 घंटे | ~5200 | ली से-दोल संस्करण को पार |
| 72 घंटे | ~5400 | मूल AlphaGo को पार |
| 40 दिन | ~5600 | सबसे मजबूत संस्करण |

**तीन दिन में मानव को पार, तीन दिन में महीनों के प्रशिक्षण वाले AI को पार**—यह घातांकीय दक्षता सुधार है।

### इतना तेज़ क्यों?

#### 1. मजबूत खोज मार्गदर्शन

AlphaGo Zero का MCTS पूरी तरह न्यूरल नेटवर्क द्वारा मार्गदर्शित है, अब तेज़ चाल रणनीति (rollout) का उपयोग नहीं करता। इससे खोज अधिक कुशल और सटीक है।

#### 2. तेज़ सेल्फ-प्ले

केवल एक नेटवर्क की आवश्यकता (दो के बजाय) होने से, प्रत्येक सेल्फ-प्ले खेल की गणना लागत कम होती है। इसका मतलब है कि समान समय में अधिक प्रशिक्षण डेटा उत्पन्न हो सकता है।

#### 3. अधिक प्रभावी सीखना

डुअल-हेड नेटवर्क का संयुक्त प्रशिक्षण प्रत्येक खेल की जानकारी का अधिक प्रभावी उपयोग करता है। Policy और Value के ग्रेडिएंट एक-दूसरे को मजबूत करते हैं, कन्वर्जेंस को तेज़ करते हैं।

### मानव सीखने से तुलना

मानव खिलाड़ियों को विभिन्न स्तरों तक पहुंचने में कितना समय लगता है?

| स्तर | मानव के लिए समय | AlphaGo Zero |
|------|-------------|--------------|
| शुरुआती | कुछ सप्ताह | कुछ मिनट |
| शौकिया 1 दान | कई वर्ष | कुछ घंटे |
| पेशेवर स्तर | 10-20 वर्ष | 1-2 दिन |
| विश्व चैंपियन | 20+ वर्ष पूर्णकालिक | 3 दिन |
| मानव से परे | असंभव | 3 दिन |

यह तुलना मानव खिलाड़ियों को कम आंकने के लिए नहीं है—वे जैविक न्यूरॉन्स का उपयोग करते हैं, जबकि AlphaGo Zero विशेष रूप से डिज़ाइन किए गए TPU और कई किलोवाट बिजली का उपयोग करता है। लेकिन यह वास्तव में दिखाता है कि सही सीखने की विधि कितनी कुशल हो सकती है।

---

## सार्वभौमिकता: शतरंज, शोगी

### AlphaZero का जन्म

2017 के दिसंबर में, DeepMind ने **AlphaZero** प्रकाशित किया—AlphaGo Zero का सार्वभौमिक संस्करण। एक ही एल्गोरिथम, केवल खेल नियमों को बदलकर, तीन बोर्ड खेलों में विश्व स्तरीय क्षमता प्राप्त कर सकता है:

| खेल | प्रशिक्षण समय | प्रतिद्वंद्वी | रिकॉर्ड |
|------|----------|------|------|
| Go | 8 घंटे | AlphaGo Zero | 60:40 |
| शतरंज | 4 घंटे | Stockfish 8 | 28 जीत 72 ड्रॉ 0 हार |
| शोगी | 2 घंटे | Elmo | 90:8:2 |

इन प्रतिद्वंद्वियों पर ध्यान दें:
- **Stockfish** उस समय का सबसे मजबूत शतरंज इंजन था, दशकों के मानव ज्ञान और अनुकूलन का उपयोग
- **Elmo** उस समय का सबसे मजबूत शोगी AI था

AlphaZero ने कुछ घंटों के प्रशिक्षण से इन वर्षों में विकसित विशेष प्रणालियों को पार कर लिया।

### सार्वभौमिकता का महत्व

AlphaGo Zero / AlphaZero ने एक महत्वपूर्ण बात साबित की:

> **एक ही सीखने का एल्गोरिथम विभिन्न क्षेत्रों में अतिमानवीय स्तर प्राप्त कर सकता है।**

यह तीन अलग-अलग AI नहीं हैं, बल्कि एक सार्वभौमिक सीखने की रूपरेखा है:

1. **सेल्फ-प्ले** अनुभव उत्पन्न करता है
2. **मोंटे कार्लो ट्री सर्च** संभावनाओं का अन्वेषण करता है
3. **न्यूरल नेटवर्क** रणनीति और मूल्य फ़ंक्शन सीखता है
4. **रीइन्फोर्समेंट लर्निंग** उद्देश्य फ़ंक्शन को अनुकूलित करता है

यह रूपरेखा डोमेन-विशिष्ट ज्ञान पर निर्भर नहीं करती, AI के सार्वभौमिकरण की दिशा में यह एक महत्वपूर्ण कदम है।

### पारंपरिक AI पर प्रभाव

AlphaZero से पहले, शतरंज और शोगी के सबसे मजबूत AI "विशेषज्ञ प्रणाली" शैली के थे:

- **बहुत सारा मानव ज्ञान**: ओपनिंग बुक, एंडगेम बुक, मूल्यांकन फ़ंक्शन
- **दशकों का अनुकूलन**: अनगिनत खिलाड़ियों और इंजीनियरों का परिश्रम
- **अत्यधिक विशेषज्ञता**: Stockfish Go नहीं खेल सकता, Elmo शतरंज नहीं खेल सकता

AlphaZero ने एक सार्वभौमिक एल्गोरिथम से कुछ घंटों में यह सब पार कर लिया। इसने कई AI शोधकर्ताओं को पुनर्विचार कराया:

> क्या हमें "सार्वभौमिक सीखने के एल्गोरिथम" में अधिक प्रयास लगाना चाहिए, या "विशेषज्ञ ज्ञान एन्कोडिंग" में?

उत्तर तेजी से स्पष्ट होता जा रहा है: मशीन को स्वयं सीखने देना उसे ज्ञान सिखाने से अधिक प्रभावी है।

---

## AlphaGo Zero की खेल शैली

### मानव सौंदर्यबोध से परे

Go समुदाय में AlphaGo Zero की चालों का एक सामान्य मूल्यांकन है: **अधिक सुंदर**।

AlphaGo Lee की चालें कभी-कभी "अजीब" लगती थीं—जैसे 37वीं चाल, मानव को इसकी सुंदरता समझने के लिए बाद में विश्लेषण की आवश्यकता थी। लेकिन AlphaGo Zero की चालों को अक्सर "पहली नज़र में अच्छी चाल" के रूप में आंका जाता है।

यह शायद इसलिए है:

1. **मजबूत खेल**: Zero अधिक गहराई से देख सकता है, चालें अधिक आत्मविश्वास से
2. **कोई मानव पूर्वाग्रह नहीं**: पारंपरिक जोसेकी से बंधा नहीं
3. **एकसमान लक्ष्य**: केवल जीत दर का पीछा, मानव की नकल नहीं

### मानव Go सिद्धांतों की पुनः खोज

दिलचस्प बात यह है कि AlphaGo Zero ने प्रशिक्षण प्रक्रिया में मानव के हज़ारों वर्षों के संचित Go ज्ञान को "पुनः खोजा":

- **जोसेकी**: Zero ने स्वयं कई सामान्य जोसेकी खोजे, क्योंकि ये वास्तव में दोनों पक्षों के लिए सर्वोत्तम हैं
- **लेआउट सिद्धांत**: कॉर्नर, साइड, सेंटर का महत्व क्रम
- **आकार ज्ञान**: बुरे और अच्छे आकारों में अंतर

इसने मानव Go सिद्धांतों की तर्कसंगतता को प्रमाणित किया—यह ज्ञान आकस्मिक नहीं है, बल्कि Go के सार का प्रतिबिंब है।

### मानव से परे नवाचार

लेकिन Zero ने ऐसी चालें भी खोजीं जो मानव ने कभी नहीं सोची थीं:

- **अपरंपरागत ओपनिंग**: पारंपरिक ओपनिंग पर विविधताएं
- **आक्रामक त्याग**: स्थानीय को छोड़ने के लिए मानव से अधिक तैयार, वैश्विक लाभ के लिए
- **प्रतिकूल आकार**: सतह पर "बुरा आकार" वास्तव में सर्वोत्तम है

ये नवाचार Go की मानव समझ को बदल रहे हैं। कई पेशेवर खिलाड़ी कहते हैं कि AlphaGo Zero के शतरंज रिकॉर्ड का अध्ययन करने से उन्हें Go की पूरी तरह नई समझ मिली।

---

## तकनीकी विवरण सारांश

### मूल AlphaGo से पूर्ण तुलना

| पहलू | AlphaGo (मूल) | AlphaGo Zero |
|------|----------------|--------------|
| **प्रशिक्षण डेटा** | मानव शतरंज रिकॉर्ड + सेल्फ-प्ले | शुद्ध सेल्फ-प्ले |
| **सीखने की विधि** | सुपरवाइज्ड + रीइन्फोर्समेंट | शुद्ध रीइन्फोर्समेंट |
| **इनपुट विशेषताएं** | 48 प्लेन | 17 प्लेन |
| **नेटवर्क आर्किटेक्चर** | अलग Policy/Value | डुअल-हेड ResNet |
| **नेटवर्क गहराई** | 13 लेयर | 40 लेयर (या अधिक) |
| **MCTS मूल्यांकन** | न्यूरल नेटवर्क + Rollout | शुद्ध न्यूरल नेटवर्क |
| **खोज संख्या** | प्रति चाल ~100,000 | प्रति चाल ~1,600 |
| **प्रशिक्षण TPU** | 50+ | 4 |
| **इन्फरेंस TPU** | 48 | 4 (स्केलेबल) |

### मुख्य एल्गोरिथम

AlphaGo Zero का प्रशिक्षण चक्र बहुत संक्षिप्त है:

```
1. सेल्फ-प्ले
   - वर्तमान नेटवर्क से MCTS
   - MCTS खोज संभाव्यता के अनुसार चाल चुनें
   - प्रत्येक चाल का (स्थिति, MCTS संभाव्यता, जीत-हार परिणाम) रिकॉर्ड करें

2. नेटवर्क प्रशिक्षण
   - अनुभव पूल से नमूने लें
   - Policy Head: MCTS संभाव्यता के साथ क्रॉस-एंट्रॉपी न्यूनतम करें
   - Value Head: वास्तविक जीत-हार के साथ MSE न्यूनतम करें
   - दोनों लक्ष्यों का संयुक्त अनुकूलन

3. नेटवर्क अपडेट
   - नए नेटवर्क से पुराने को बदलें (खेल द्वारा सत्यापित कि नया मजबूत है)
   - चरण 1 पर वापस जाएं
```

यह चक्र लगातार चलता है, नेटवर्क लगातार मजबूत होता है। कोई मानव डेटा नहीं, कोई मानव ज्ञान नहीं, केवल खेल नियम और जीत-हार लक्ष्य।

---

## AI अनुसंधान के लिए प्रेरणा

### प्रथम सिद्धांत सीखना

AlphaGo Zero ने "प्रथम सिद्धांत" सीखने की विधि दिखाई:

> AI को कैसे करना है मत बताओ, केवल लक्ष्य बताओ, इसे स्वयं विधि खोजने दो।

यह पारंपरिक विशेषज्ञ प्रणाली विधि से बिल्कुल अलग है। विशेषज्ञ प्रणालियां मानव ज्ञान को AI में एन्कोड करने का प्रयास करती हैं, जबकि AlphaGo Zero AI को स्वयं ज्ञान खोजने देता है।

परिणाम है: AI द्वारा खोजा गया ज्ञान मानव ज्ञान से अधिक पूर्ण और सटीक हो सकता है।

### सेल्फ-प्ले की शक्ति

AlphaGo Zero ने साबित किया कि सेल्फ-प्ले असीमित प्रशिक्षण डेटा उत्पन्न कर सकता है, और इस डेटा की गुणवत्ता नेटवर्क के सुधार के साथ सुधरती है।

यह एक "सकारात्मक चक्र" है:
- मजबूत नेटवर्क → बेहतर सेल्फ-प्ले डेटा
- बेहतर डेटा → मजबूत नेटवर्क

यह चक्र तब तक चल सकता है जब तक खेल की सैद्धांतिक सीमा (यदि मौजूद हो) तक नहीं पहुंच जाता।

### सरलीकरण का महत्व

AlphaGo Zero की सफलता ने "सरलीकरण" का महत्व साबित किया:

- इनपुट का सरलीकरण (48 → 17)
- आर्किटेक्चर का सरलीकरण (दोहरा नेटवर्क → एकल नेटवर्क)
- प्रशिक्षण का सरलीकरण (सुपरवाइज्ड + रीइन्फोर्समेंट → शुद्ध रीइन्फोर्समेंट)

प्रत्येक सरलीकरण ने सिस्टम को अधिक शक्तिशाली बनाया। यह हमें बताता है: जटिल अच्छा नहीं है, सबसे सरल समाधान अक्सर सबसे अच्छा होता है।

---

## एनिमेशन संदर्भ

इस लेख में शामिल मुख्य अवधारणाएं और एनिमेशन नंबर:

| नंबर | अवधारणा | भौतिकी/गणित समकक्ष |
|------|------|--------------|
| E7 | शून्य से प्रशिक्षण | स्व-संगठन घटना |
| E5 | सेल्फ-प्ले | स्थिर बिंदु कन्वर्जेंस |
| E12 | खेल क्षमता वृद्धि वक्र | S-आकार वृद्धि |
| D12 | रेसिड्युअल नेटवर्क | ग्रेडिएंट हाईवे |

---

## आगे पढ़ने के लिए

- **अगला लेख**: [डुअल-हेड नेटवर्क और रेसिड्युअल नेटवर्क](../dual-head-resnet) — AlphaGo Zero के न्यूरल नेटवर्क आर्किटेक्चर का विस्तृत विवरण
- **संबंधित लेख**: [सेल्फ-प्ले](../self-play) — सेल्फ-प्ले अतिमानवीय स्तर क्यों उत्पन्न कर सकता है
- **तकनीकी गहराई**: [शून्य से प्रशिक्षण प्रक्रिया](../training-from-scratch) — Day 0-3 का विस्तृत विकास

---

## संदर्भ

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
3. DeepMind. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
4. Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." *Nature*, 588, 604-609.
