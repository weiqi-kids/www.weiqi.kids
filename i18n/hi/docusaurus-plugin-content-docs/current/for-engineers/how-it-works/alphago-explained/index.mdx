---
sidebar_position: 1
title: AlphaGo संपूर्ण विश्लेषण
description: ऐतिहासिक पृष्ठभूमि से तकनीकी विवरण तक, 20 लेखों में AlphaGo को पूरी तरह समझें
---

import { PolicyHeatmap, EloChart } from '@site/src/components/D3Charts';

# AlphaGo संपूर्ण विश्लेषण

मार्च 2016 में, AlphaGo ने विश्व चैंपियन ली सेडोल को 4:1 से हराकर पूरी दुनिया को चौंका दिया। यह सिर्फ एक गो खेल की जीत नहीं थी, बल्कि यह कृत्रिम बुद्धिमत्ता में एक बड़ी सफलता का प्रतीक था।

यह श्रृंखला **20 गहन लेखों** में आपको ऐतिहासिक पृष्ठभूमि, तकनीकी सिद्धांतों और कार्यान्वयन विवरणों के माध्यम से AlphaGo की हर चीज़ को पूरी तरह से समझाएगी।

---

## श्रृंखला नेविगेशन

### मॉड्यूल 1: इतिहास और सफलताएं

| लेख | विवरण |
|-----|-------|
| [AlphaGo का जन्म](./birth-of-alphago) | DeepMind की स्थापना, Google द्वारा अधिग्रहण, टीम की संरचना |
| [प्रमुख मैचों की समीक्षा](./key-matches) | फैन हुई, ली सेडोल, के जी, Master की 60 लगातार जीत |
| ["दैवीय चाल" का गहन विश्लेषण](./move-37) | 37वीं चाल का खेल तर्क और AI दृष्टिकोण |

### मॉड्यूल 2: गो की चुनौती

| लेख | विवरण |
|-----|-------|
| [गो कठिन क्यों है?](./why-go-is-hard) | स्टेट स्पेस 10^170, ब्रांचिंग फैक्टर ~250 |
| [पारंपरिक तरीकों की सीमाएं](./traditional-limits) | Minimax, Alpha-Beta, शुद्ध MCTS |
| [बोर्ड स्टेट प्रतिनिधित्व](./board-representation) | Zobrist Hashing, Union-Find, फीचर एन्कोडिंग |

### मॉड्यूल 3: न्यूरल नेटवर्क कोर

| लेख | विवरण |
|-----|-------|
| [Policy Network विस्तार से](./policy-network) | आर्किटेक्चर, Softmax आउटपुट, प्रशिक्षण उद्देश्य |
| [Value Network विस्तार से](./value-network) | आर्किटेक्चर, Tanh आउटपुट, ओवरफिटिंग से बचाव |
| [इनपुट फीचर डिज़ाइन](./input-features) | 48 से 17 फीचर प्लेन का विकास |
| [CNN और गो का संयोजन](./cnn-and-go) | CNN बोर्ड के लिए उपयुक्त क्यों है |
| [सुपरवाइज्ड लर्निंग चरण](./supervised-learning) | KGS डेटासेट, 57% पूर्वानुमान सटीकता |

### मॉड्यूल 4: रीइन्फोर्समेंट लर्निंग और सर्च

| लेख | विवरण |
|-----|-------|
| [रीइन्फोर्समेंट लर्निंग परिचय](./reinforcement-intro) | MDP, पॉलिसी ग्रेडिएंट, वैल्यू फंक्शन |
| [सेल्फ-प्ले](./self-play) | यह प्रभावी क्यों है, ELO वृद्धि वक्र |
| [MCTS और न्यूरल नेटवर्क का संयोजन](./mcts-neural-combo) | Selection→Expansion→Evaluation→Backup |
| [PUCT फॉर्मूला विस्तार से](./puct-formula) | गणितीय व्युत्पत्ति, अन्वेषण बनाम दोहन |

### मॉड्यूल 5: AlphaGo Zero का विकास

| लेख | विवरण |
|-----|-------|
| [AlphaGo Zero अवलोकन](./alphago-zero) | मानव खेलों की आवश्यकता क्यों नहीं |
| [ड्यूअल-हेड नेटवर्क और ResNet](./dual-head-resnet) | साझा प्रतिनिधित्व, ग्रेडिएंट प्रवाह, 40-लेयर ResNet |
| [शून्य से प्रशिक्षण प्रक्रिया](./training-from-scratch) | दिन 0-3 के परिवर्तन, 3 दिनों में मनुष्यों से आगे |

### मॉड्यूल 6: तकनीकी विवरण और विस्तार

| लेख | विवरण |
|-----|-------|
| [वितरित प्रणालियां और TPU](./distributed-systems) | प्रशिक्षण आर्किटेक्चर, इन्फरेंस आर्किटेक्चर, समानांतर MCTS |
| [AlphaGo की विरासत](./legacy-and-impact) | गो जगत पर प्रभाव, AlphaZero, MuZero, AlphaFold |

---

## त्वरित पूर्वावलोकन

### Policy Network आउटपुट उदाहरण

Policy Network प्रत्येक स्थिति पर चाल की संभावना आउटपुट करती है:

<PolicyHeatmap initialPosition="corner" size={400} />

### प्रशिक्षण वक्र

AlphaGo Zero ने शून्य से शुरू करके 3 दिनों में मनुष्यों को पार कर लिया:

<EloChart mode="zero" width={600} height={350} />

---

## पढ़ने के सुझाव

### अपनी पृष्ठभूमि के आधार पर शुरुआती बिंदु चुनें

| आपकी पृष्ठभूमि | सुझाया गया शुरुआती बिंदु |
|---------------|-------------------------|
| **पूर्ण नौसिखिया** | [AlphaGo का जन्म](./birth-of-alphago) से शुरू करें, क्रम में पढ़ें |
| **गो जानते हैं** | [गो कठिन क्यों है?](./why-go-is-hard) से शुरू करें |
| **मशीन लर्निंग की समझ है** | [Policy Network विस्तार से](./policy-network) से शुरू करें |
| **जल्दी सार समझना चाहते हैं** | [MCTS और न्यूरल नेटवर्क का संयोजन](./mcts-neural-combo) पढ़ें |
| **Zero की सफलता समझना चाहते हैं** | [AlphaGo Zero अवलोकन](./alphago-zero) से शुरू करें |

### अनुमानित पढ़ने का समय

- **पूर्ण पढ़ाई**: लगभग 8-10 घंटे
- **त्वरित ब्राउज़िंग**: लगभग 2-3 घंटे
- **प्रत्येक लेख**: लगभग 15-25 मिनट

---

## एनिमेशन पत्राचार

इस श्रृंखला में [109 एनिमेशन अवधारणाओं](../concepts/) की निम्नलिखित श्रृंखलाओं का संदर्भ है:

| श्रृंखला | विषय | संबंधित लेख |
|---------|------|------------|
| **C श्रृंखला** | मोंटे कार्लो विधियां | #5, #14, #15 |
| **D श्रृंखला** | न्यूरल नेटवर्क | #7, #8, #10, #11 |
| **E श्रृंखला** | AlphaGo आर्किटेक्चर | #13, #16, #17, #18 |
| **H श्रृंखला** | रीइन्फोर्समेंट लर्निंग | #12, #13 |

---

## संदर्भ

### पेपर

1. Silver, D., et al. (2016). ["Mastering the game of Go with deep neural networks and tree search."](https://www.nature.com/articles/nature16961) *Nature*.
2. Silver, D., et al. (2017). ["Mastering the game of Go without human knowledge."](https://www.nature.com/articles/nature24270) *Nature*.
3. Silver, D., et al. (2018). ["A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play."](https://www.science.org/doi/10.1126/science.aar6404) *Science*.

### आगे पढ़ें

- [KataGo की प्रमुख नवाचार](../katago-innovations) — कम संसाधनों के साथ मजबूत खेल कैसे प्राप्त करें
- [अवधारणा त्वरित संदर्भ तालिका](../concepts/) — 109 एनिमेशन अवधारणाओं की पूरी सूची
- [30 मिनट में अपना पहला गो AI चलाएं](../../hands-on/) — व्यावहारिक अनुभव
