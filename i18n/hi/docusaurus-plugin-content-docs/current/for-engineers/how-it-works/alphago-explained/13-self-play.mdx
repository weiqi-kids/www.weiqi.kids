---
sidebar_position: 14
title: सेल्फ-प्ले
description: AlphaGo ने सेल्फ-प्ले के माध्यम से मानव स्किल की सीमाओं को कैसे पार किया, इसकी गहन समझ
---

import { EloChart } from '@site/src/components/D3Charts';

# सेल्फ-प्ले

पिछले लेख में, हमने रीइन्फोर्समेंट लर्निंग की बुनियादी अवधारणाएं पेश कीं। अब, आइए AlphaGo की सफलता की एक कुंजी—**सेल्फ-प्ले (Self-Play)** का पता लगाएं।

यह एक विरोधाभासी सा लगने वाला कॉन्सेप्ट है: **AI खुद से खेलकर मजबूत कैसे हो सकता है?**

इसका जवाब गहरा और सुंदर है, जिसमें गेम थ्योरी, इवोल्यूशनरी डायनामिक्स, और लर्निंग का सार शामिल है।

---

## सेल्फ-प्ले क्यों काम करता है?

### सहज व्याख्या

कल्पना करें आप गो के बिगिनर हैं, किसी सुनसान टापू पर अकेले प्रैक्टिस कर रहे हैं:

1. आप एक गेम खेलते हैं, खुद दोनों पक्ष (काला और सफेद) खेलते हैं
2. गेम खत्म होने के बाद, आप विश्लेषण करते हैं कौन सी चालें अच्छी थीं, कौन सी बुरी
3. अगले गेम में, आप पिछली गलतियों से बचने की कोशिश करते हैं
4. आप इस प्रक्रिया को लाखों बार दोहराते हैं

सहज रूप से, यह समस्याग्रस्त लगता है:
- यदि आपका स्तर बहुत कम है, दोनों पक्ष बुरी चालें चलते हैं, तो क्या सीख सकते हैं?
- क्या "गलत संतुलन" में फंस जाएंगे—दोनों पक्ष गलत चालें चलते हैं लेकिन एक-दूसरे को कैंसल कर देते हैं?

लेकिन वास्तव में, सेल्फ-प्ले लगातार प्रगति ला सकता है। कारण इस प्रकार हैं:

### कमज़ोरियों की क्रमिक खोज

मुख्य इनसाइट यह है: **भले ही दोनों पक्ष एक ही AI हों, हर गेम के परिणाम में जानकारी होती है**।

```
पोज़िशन A: AI ने चाल X चुनी, अंततः जीता
पोज़िशन A: AI ने चाल Y चुनी, अंततः हारा

→ निष्कर्ष: पोज़िशन A में, X, Y से बेहतर है
```

बड़ी संख्या में गेम के स्टैटिस्टिक्स से, AI हर पोज़िशन में सीख सकता है कौन से चयन बेहतर हैं। यही **पॉलिसी ग्रेडिएंट** का सार है: अच्छे चयन को स्ट्रेंथन किया जाता है, बुरे चयन को सप्रेस किया जाता है।

### एडवर्सेरियल लर्निंग

सेल्फ-प्ले की एक विशेष प्रॉपर्टी है: **ट्रेनिंग ऑपोनेंट ऑटोमैटिकली आपके लेवल के अनुकूल हो जाता है**।

```
ट्रेनिंग साइकल 1: AI ने एक प्रभावी टैक्टिक T खोजा
ट्रेनिंग साइकल 2: ऑपोनेंट के रूप में AI ने T को डिफेंड करना सीखा
ट्रेनिंग साइकल 3: ओरिजिनल AI को बेहतर टैक्टिक T' खोजना पड़ा
```

यह एक **आर्म्स रेस (Arms Race)** बनाता है, दोनों पक्ष लगातार एक-दूसरे की कमज़ोरियां खोजते और दूर करते हैं।

### मानव गेम रिकॉर्ड से तुलना

| ट्रेनिंग मेथड | फायदे | नुकसान |
|---------|------|------|
| **मानव गेम रिकॉर्ड** | मानव ज्ञान का सार सीखना | मानव स्तर तक सीमित |
| **सेल्फ-प्ले** | अनलिमिटेड इम्प्रूवमेंट पोटेंशियल | लोकल ऑप्टिमम में फंस सकता है |
| **दोनों का कॉम्बिनेशन** | तेज़ स्टार्ट + लगातार इम्प्रूवमेंट | बेस्ट स्ट्रैटेजी |

ओरिजिनल AlphaGo ने पहले मानव गेम से सुपरवाइज़्ड लर्निंग की, फिर सेल्फ-प्ले से रीइन्फोर्समेंट लर्निंग की। AlphaGo Zero ने साबित किया कि केवल सेल्फ-प्ले से भी सुपरह्यूमन लेवल तक पहुंचा जा सकता है।

---

## गेम थ्योरी पर्सपेक्टिव

### नैश इक्विलिब्रियम

गेम थ्योरी में, **नैश इक्विलिब्रियम (Nash Equilibrium)** एक स्थिर स्टेट है: इस स्टेट में, किसी भी प्लेयर के पास एकतरफा स्ट्रैटेजी बदलने की प्रेरणा नहीं है।

गो जैसे **ज़ीरो-सम, परफेक्ट इन्फॉर्मेशन गेम** के लिए, नैश इक्विलिब्रियम का विशेष अर्थ है:

$$\pi^* = \arg\max_\pi \min_{\pi'} V(\pi, \pi')$$

जहां $V(\pi, \pi')$ स्ट्रैटेजी $\pi$ का स्ट्रैटेजी $\pi'$ के खिलाफ एक्सपेक्टेड वैल्यू है।

यही प्रसिद्ध **Minimax प्रिंसिपल** है: बेस्ट स्ट्रैटेजी वह है जो वर्स्ट केस में भी बेस्ट परफॉर्म करे।

### सेल्फ-प्ले और नैश इक्विलिब्रियम

थ्योरेटिकली, यदि सेल्फ-प्ले कन्वर्ज करता है, तो इसे नैश इक्विलिब्रियम की ओर कन्वर्ज करना चाहिए। गो जैसे डिटरमिनिस्टिक गेम के लिए, नैश इक्विलिब्रियम **परफेक्ट प्ले** है।

लेकिन गो का स्टेट स्पेस बहुत बड़ा है ($10^{170}$), हम सच्चे नैश इक्विलिब्रियम तक नहीं पहुंच सकते। सेल्फ-प्ले वास्तव में इस इक्विलिब्रियम को **अप्रॉक्सिमेट** कर रहा है।

### फिक्टिशस प्ले (Fictitious Play)

सेल्फ-प्ले गेम थ्योरी में **फिक्टिशस प्ले** कॉन्सेप्ट से संबंधित है:

1. प्रत्येक प्लेयर ऑपोनेंट की हिस्टोरिकल स्ट्रैटेजी ऑब्ज़र्व करता है
2. ऑपोनेंट स्ट्रैटेजी का एवरेज डिस्ट्रीब्यूशन कैलकुलेट करता है
3. इस एवरेज डिस्ट्रीब्यूशन के खिलाफ बेस्ट रिस्पॉन्स चुनता है

कुछ कंडीशंस में, फिक्टिशस प्ले को नैश इक्विलिब्रियम की ओर कन्वर्ज करना प्रूव किया जा सकता है।

AlphaGo का सेल्फ-प्ले इस कॉन्सेप्ट का न्यूरल नेटवर्क इम्प्लीमेंटेशन माना जा सकता है।

---

## सेल्फ-प्ले मेकेनिज़्म

### बेसिक फ्लो

AlphaGo का सेल्फ-प्ले फ्लो:

```
एल्गोरिथम: Self-Play Training

इनिशियलाइज़ेशन: Policy Network π_θ (सुपरवाइज़्ड लर्निंग या रैंडम से शुरू)

कन्वर्जेंस तक निम्न स्टेप्स दोहराएं:

1. गेम डेटा जेनरेट करें
   i = 1 से N के लिए (पैरेलल में):
     a. वर्तमान पॉलिसी π_θ से सेल्फ-प्ले गेम खेलें
     b. ट्रैजेक्टरी कलेक्ट करें: τ_i = (s_0, a_0, r_1, s_1, a_1, ...)
     c. फाइनल रिजल्ट रिकॉर्ड करें z_i ∈ {-1, +1}

2. पॉलिसी अपडेट करें
   a. पॉलिसी ग्रेडिएंट कैलकुलेट करें:
      ∇J = (1/N) Σ_i Σ_t ∇_θ log π_θ(a_t|s_t) · z_i
   b. पैरामीटर अपडेट करें: θ ← θ + α · ∇J

3. Value Network अपडेट करें
   a. (s, z) पेयर से Value Network ट्रेन करें
   b. मिनिमाइज़ करें: L = E[(V_φ(s) - z)²]

4. ऑप्शनल: इवैल्यूएट और चेकपॉइंट सेव करें
   a. नई पॉलिसी को पुराने वर्ज़न से खिलाएं
   b. यदि विन रेट > 55%, ऑपोनेंट पूल अपडेट करें
```

### ट्रेनिंग डेटा का जेनरेशन

हर सेल्फ-प्ले गेम एक **ट्रैजेक्टरी (trajectory)** प्रोड्यूस करता है:

$$\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, z)$$

जहां:
- $s_t$: टाइम स्टेप $t$ पर बोर्ड स्टेट
- $a_t$: टाइम स्टेप $t$ पर चुनी गई चाल
- $z$: फाइनल रिजल्ट (+1 जीत, -1 हार)

एक 200 चालों का गेम 200 ट्रेनिंग सैंपल प्रोड्यूस करता है। रोज़ाना लाखों सेल्फ-प्ले गेम से, ट्रेनिंग डेटा की मात्रा विशाल है।

### पॉलिसी अपडेट

पॉलिसी ग्रेडिएंट से Policy Network अपडेट करें:

$$\theta \leftarrow \theta + \alpha \cdot \nabla_\theta \mathbb{E}\left[\sum_t \log \pi_\theta(a_t|s_t) \cdot z\right]$$

इस अपडेट का प्रभाव:
- यदि अंत में जीत ($z = +1$), सभी चालों की प्रोबेबिलिटी बढ़ाएं
- यदि अंत में हार ($z = -1$), सभी चालों की प्रोबेबिलिटी कम करें

यह क्रूड लगता है—जीते गेम में भी कुछ बुरी चालें हो सकती हैं, हारे गेम में भी कुछ अच्छी। लेकिन बड़ी संख्या में गेम के स्टैटिस्टिक्स से, यह "नॉइज़" एवरेज आउट हो जाता है, सच्ची अच्छी चालें पहचानी जाती हैं।

### Value Network ट्रेनिंग

Value Network **रिग्रेशन** से ट्रेन होती है:

$$\phi \leftarrow \phi - \beta \cdot \nabla_\phi \mathbb{E}\left[(V_\phi(s) - z)^2\right]$$

यह Value Network को सिखाता है: वर्तमान पोज़िशन से शुरू करके, अंततः जीतने की प्रोबेबिलिटी क्या है?

Value Network का काम:
1. MCTS में लीफ नोड इवैल्यूएशन देना
2. पॉलिसी ग्रेडिएंट के लिए बेसलाइन होना
3. सीधे पोज़िशन इवैल्यूएशन के लिए उपयोग

---

## रैंडमाइज़ेशन का महत्व

### डिटरमिनिस्टिक साइकल से बचना

यदि सेल्फ-प्ले पूरी तरह डिटरमिनिस्टिक है, तो साइकल में फंस सकता है:

```
पॉलिसी A हमेशा एक फिक्स्ड ओपनिंग चलती है
पॉलिसी A vs पॉलिसी A हमेशा एक ही गेम प्रोड्यूस करती है
केवल एक गेम बार-बार सीखा जाता है
AI अन्य संभावनाएं एक्सप्लोर नहीं कर पाता
```

इसीलिए सेल्फ-प्ले में **रैंडमनेस** महत्वपूर्ण है।

### रैंडमाइज़ेशन के स्रोत

AlphaGo सेल्फ-प्ले में रैंडमनेस के तरीके:

**1. Policy Network खुद स्टोकैस्टिक है**

Policy Network प्रोबेबिलिटी डिस्ट्रीब्यूशन आउटपुट करती है, डिटरमिनिस्टिक चयन नहीं:

$$a \sim \pi_\theta(a|s)$$

समान पोज़िशन में, हर बार अलग चाल चुनी जा सकती है।

**2. टेम्परेचर पैरामीटर**

ट्रेनिंग में हायर टेम्परेचर डायवर्सिटी बढ़ाने के लिए:

$$\pi_\tau(a|s) = \frac{\pi_\theta(a|s)^{1/\tau}}{\sum_{a'} \pi_\theta(a'|s)^{1/\tau}}$$

- $\tau > 1$: अधिक रैंडम, अधिक एक्सप्लोरेशन
- $\tau < 1$: अधिक डिटरमिनिस्टिक, अधिक एक्सप्लॉइटेशन
- $\tau = 1$: ओरिजिनल डिस्ट्रीब्यूशन

**3. डिरिक्ले नॉइज़ (Dirichlet Noise)**

AlphaGo Zero सेल्फ-प्ले में, रूट नोड की प्रायर प्रोबेबिलिटी पर डिरिक्ले नॉइज़ जोड़ता है:

$$P(s, a) = (1 - \varepsilon) \cdot \pi_\theta(a|s) + \varepsilon \cdot \eta_a$$

जहां $\eta \sim \text{Dir}(\alpha)$, $\varepsilon = 0.25$, $\alpha = 0.03$ (गो के 361 एक्शन के लिए)।

यह सुनिश्चित करता है कि बहुत कम प्रोबेबिलिटी वाली चालें भी एक्सप्लोर होने का मौका पाती हैं।

### पॉपुलेशन मेथड

डायवर्सिटी बढ़ाने का एक और तरीका **ऑपोनेंट पूल** मेंटेन करना है:

```
ऑपोनेंट पूल = [π_1, π_2, π_3, ..., π_k] (अलग-अलग वर्ज़न की पॉलिसी)

हर गेम के लिए:
1. पूल से रैंडमली एक ऑपोनेंट चुनें
2. उस ऑपोनेंट से खेलें
3. रिजल्ट से वर्तमान पॉलिसी अपडेट करें
4. समय-समय पर इम्प्रूव्ड पॉलिसी पूल में जोड़ें
```

इस मेथड के फायदे:
- **डायवर्सिटी**: अलग-अलग स्टाइल के ऑपोनेंट
- **स्टेबिलिटी**: किसी विशेष ऑपोनेंट पर ओवरफिटिंग से बचना
- **रोबस्टनेस**: विभिन्न स्ट्रैटेजी से डील करना सीखना

ओरिजिनल AlphaGo और AlphaGo Zero दोनों ने समान तकनीक इस्तेमाल की।

---

## स्किल ग्रोथ कर्व

### Elo रेटिंग सिस्टम

AI स्किल चेंज ट्रैक करने के लिए, AlphaGo ने **Elo रेटिंग सिस्टम** इस्तेमाल किया।

Elo सिस्टम का बेसिक प्रिंसिपल:

$$P(\text{A जीत}) = \frac{1}{1 + 10^{(R_B - R_A)/400}}$$

जहां $R_A$ और $R_B$ दोनों पक्षों के Elo स्कोर हैं।

- 200 पॉइंट डिफरेंस: स्ट्रॉन्ग प्लेयर 75% जीतने की उम्मीद
- 400 पॉइंट डिफरेंस: स्ट्रॉन्ग प्लेयर 90% जीतने की उम्मीद
- 800 पॉइंट डिफरेंस: स्ट्रॉन्ग प्लेयर 99% जीतने की उम्मीद

### AlphaGo की स्किल ग्रोथ

आइए AlphaGo के विभिन्न वर्ज़न की स्किल ग्रोथ विज़ुअलाइज़ करें:

<EloChart mode="zero" width={700} height={400} showMilestones={true} />

### ग्रोथ स्पीड एनालिसिस

कर्व से कुछ दिलचस्प अवलोकन:

**1. इनिशियल रैपिड ग्रोथ**

ट्रेनिंग के पहले कुछ घंटों में, AI बेसिक रूल्स और सिंपल टैक्टिक्स सीखता है। यह **लो-हैंगिंग फ्रूट** फेज़ है—बहुत सारी स्पष्ट गलतियां सुधारने के लिए हैं।

**2. मिड-फेज़ स्टेडी ग्रोथ**

बेसिक गलतियां दूर होने के बाद, AI अधिक परिष्कृत टैक्टिक्स और जोसेकी सीखना शुरू करता है। ग्रोथ स्पीड कम होती है, लेकिन स्थिर रहती है।

**3. लेट-फेज़ ग्रोथ स्लोडाउन**

जब AI पहले से बहुत स्ट्रॉन्ग है, और इम्प्रूवमेंट कठिन हो जाता है। शायद पूरी तरह नई स्ट्रैटेजी खोजनी होगी, केवल गलतियां सुधारना काफी नहीं।

### मानव से आगे निकलने का क्षण

AlphaGo ट्रेनिंग कर्व के मुख्य माइलस्टोन:

| माइलस्टोन | समकक्ष | समय |
|--------|--------|---------|
| स्ट्रॉन्ग एमेच्योर से आगे | Elo ~2700 | लगभग 3 घंटे |
| Fan Hui से आगे | Elo ~3500 | लगभग 36 घंटे |
| Lee Sedol से आगे | Elo ~4500 | लगभग 60 घंटे |
| ओरिजिनल AlphaGo से आगे | Elo ~5000 | लगभग 72 घंटे |

ये नंबर (AlphaGo Zero से) चौंकाने वाले हैं: **AI ने 3 दिनों में शून्य से मानव के हजारों वर्षों के गो ज्ञान को पार कर लिया**।

---

## कन्वर्जेंस एनालिसिस

### क्या सेल्फ-प्ले कन्वर्ज करता है?

यह एक महत्वपूर्ण थ्योरेटिकल प्रश्न है। संक्षिप्त उत्तर: **कुछ कंडीशंस में हां, लेकिन गो बहुत कॉम्प्लेक्स है, हम स्ट्रिक्टली प्रूव नहीं कर सकते**।

### थ्योरेटिकल गारंटी

सिंपल गेम (जैसे टिक-टैक-टो) के लिए, प्रूव किया जा सकता है:

1. **एग्ज़िस्टेंस**: नैश इक्विलिब्रियम एग्ज़िस्ट करता है (Minimax थ्योरम)
2. **कन्वर्जेंस**: कुछ एल्गोरिथम (जैसे फिक्टिशस प्ले) नैश इक्विलिब्रियम की ओर कन्वर्ज करते हैं

गो के लिए, हमारे पास स्ट्रिक्ट कन्वर्जेंस गारंटी नहीं है, लेकिन एक्सपेरिमेंटल एविडेंस दिखाता है:
- स्किल लगातार इम्प्रूव होती है
- कोई स्पष्ट ऑसिलेशन या डिग्रेडेशन नहीं
- फाइनल स्किल सभी ज्ञात मानवों से आगे

### संभावित फेलियर मोड

सेल्फ-प्ले में आ सकने वाली समस्याएं:

**1. स्ट्रैटेजी साइक्लिंग (Strategy Cycling)**

```
स्ट्रैटेजी A, स्ट्रैटेजी B को हराती है
स्ट्रैटेजी B, स्ट्रैटेजी C को हराती है
स्ट्रैटेजी C, स्ट्रैटेजी A को हराती है
```

यह कुछ गेम में वाकई होता है (जैसे रॉक-पेपर-सीज़र्स)। लेकिन गो में पर्याप्त कॉम्प्लेक्सिटी है, ऐसी प्योर साइक्लिंग नहीं होती।

**2. खुद पर ओवरफिटिंग**

AI शायद केवल अपनी स्टाइल के खिलाफ स्ट्रैटेजी सीखे, अन्य स्टाइल के ऑपोनेंट से डील न कर पाए। इसीलिए AlphaGo अपने अलग-अलग वर्ज़न से खेलता है, और अंत में मानव से भी टेस्ट करता है।

**3. लोकल ऑप्टिमम**

AI लोकल ऑप्टिमम में फंस सकता है—एक "ठीक-ठाक लेकिन बेस्ट नहीं" स्ट्रैटेजी। रैंडमाइज़ेशन और बड़ी संख्या में गेम इस समस्या से बचने में मदद करते हैं।

### वास्तविक अवलोकन

AlphaGo की ट्रेनिंग प्रोसेस से अवलोकन:

1. **लगातार प्रगति**: Elo स्कोर ट्रेनिंग के साथ बढ़ता रहा
2. **कोई डिग्रेडेशन नहीं**: स्किल में अचानक गिरावट नहीं हुई
3. **स्टाइल इवोल्यूशन**: AI की प्लेइंग स्टाइल ट्रेनिंग के साथ बदली
4. **नए जोसेकी की खोज**: AI ने वो ओपनिंग और टैक्टिक्स खोजे जो मानव ने कभी इस्तेमाल नहीं किए

ये अवलोकन बताते हैं कि भले ही थ्योरेटिकल गारंटी नहीं है, प्रैक्टिस में सेल्फ-प्ले वाकई काम करता है।

---

## इम्प्लीमेंटेशन डिटेल्स

### पैरेलल सेल्फ-प्ले

ट्रेनिंग स्पीड बढ़ाने के लिए, AlphaGo लार्ज-स्केल पैरेलल सेल्फ-प्ले इस्तेमाल करता है:

```
आर्किटेक्चर:

    ┌────────────────────────────────────────────┐
    │           Parameter Server                  │
    │    (लेटेस्ट θ स्टोर, ग्रेडिएंट अपडेट रिसीव)    │
    └────────────────────────────────────────────┘
         ▲                              │
         │ ग्रेडिएंट अपडेट               │ लेटेस्ट पैरामीटर
         │                              ▼
    ┌─────────┐  ┌─────────┐  ┌─────────┐
    │ Worker 1 │  │ Worker 2 │  │ Worker N │
    │ सेल्फ-प्ले │  │ सेल्फ-प्ले │  │ सेल्फ-प्ले │
    │ ट्रैजेक्टरी │  │ ट्रैजेक्टरी │  │ ट्रैजेक्टरी │
    │ कलेक्ट   │  │ कलेक्ट   │  │ कलेक्ट   │
    └─────────┘  └─────────┘  └─────────┘
```

**मुख्य डिज़ाइन डिसीज़न**:

- **सिंक्रोनस vs एसिंक्रोनस**: AlphaGo एसिंक्रोनस अपडेट इस्तेमाल करता है, Workers को एक-दूसरे का इंतज़ार नहीं करना पड़ता
- **अपडेट फ्रीक्वेंसी**: हर N गेम कंप्लीट होने पर पैरामीटर अपडेट
- **ऑपोनेंट सेलेक्शन**: हाल के कुछ वर्ज़न में से रैंडमली ऑपोनेंट चुनना

### चेकपॉइंट स्ट्रैटेजी

समय-समय पर मॉडल चेकपॉइंट सेव करना:

1. **ऑपोनेंट पूल**: अलग-अलग वर्ज़न के ऑपोनेंट मेंटेन करना
2. **इवैल्यूएशन**: स्किल चेंज ट्रैक करना
3. **फॉल्ट रिकवरी**: ट्रेनिंग इंटरप्ट होने पर रिकवरी

```python
# स्यूडोकोड
def training_loop():
    for iteration in range(num_iterations):
        # गेम डेटा जेनरेट करें
        trajectories = parallel_self_play(current_policy, num_games=1000)

        # पॉलिसी अपडेट करें
        update_policy(trajectories)

        # समय-समय पर इवैल्यूएट और सेव करें
        if iteration % 100 == 0:
            elo = evaluate_against_pool(current_policy)
            save_checkpoint(current_policy, elo)

            if elo > best_elo:
                add_to_pool(current_policy)
                best_elo = elo
```

### ट्रेनिंग रिसोर्स रिक्वायरमेंट

AlphaGo की ट्रेनिंग स्केल प्रभावशाली है:

| वर्ज़न | हार्डवेयर | ट्रेनिंग टाइम | सेल्फ-प्ले गेम |
|------|------|---------|-------------|
| AlphaGo Fan | 176 GPU | कई महीने | ~30M |
| AlphaGo Lee | 48 TPU | कई सप्ताह | ~30M |
| AlphaGo Zero | 4 TPU | 3 दिन | ~5M |
| AlphaGo Zero (40-दिन वर्ज़न) | 4 TPU | 40 दिन | ~30M |

नोट करें AlphaGo Zero ने कम हार्डवेयर और कम समय में और स्ट्रॉन्ग स्किल हासिल की—यह एल्गोरिथम एफिशिएंसी में सुधार है।

### हाइपरपैरामीटर सेटिंग्स

कुछ मुख्य हाइपरपैरामीटर:

```python
# सेल्फ-प्ले सेटिंग्स
NUM_PARALLEL_GAMES = 5000      # एक साथ चलने वाले गेम
GAMES_PER_ITERATION = 25000    # प्रति इटरेशन गेम
MCTS_SIMULATIONS = 1600        # प्रति चाल MCTS सिमुलेशन

# ट्रेनिंग सेटिंग्स
BATCH_SIZE = 2048              # ट्रेनिंग बैच साइज़
LEARNING_RATE = 0.01           # इनिशियल लर्निंग रेट
L2_REGULARIZATION = 1e-4       # वेट डिके

# एक्सप्लोरेशन सेटिंग्स
TEMPERATURE = 1.0              # पहली 30 चालों का टेम्परेचर
DIRICHLET_ALPHA = 0.03         # डिरिक्ले नॉइज़ पैरामीटर
EXPLORATION_FRACTION = 0.25    # नॉइज़ फ्रैक्शन
```

ये हाइपरपैरामीटर बहुत एक्सपेरिमेंट से ट्यून किए गए हैं, ट्रेनिंग इफेक्ट पर बड़ा प्रभाव है।

---

## सेल्फ-प्ले के वेरिएंट

### ओरिजिनल AlphaGo

ओरिजिनल AlphaGo का ट्रेनिंग फ्लो:

```
1. सुपरवाइज़्ड लर्निंग (SL): मानव गेम से सीखना
   → SL Policy Network (π_SL) प्रोड्यूस

2. रीइन्फोर्समेंट लर्निंग (RL): सेल्फ-प्ले
   इनिशियलाइज़ π_RL = π_SL
   ऑपोनेंट पूल = [π_SL]

   दोहराएं:
     a. π_RL, पूल की पॉलिसी से खेले
     b. पॉलिसी ग्रेडिएंट से π_RL अपडेट
     c. यदि π_RL स्ट्रॉन्ग हो, पूल में जोड़े

   → RL Policy Network (π_RL) प्रोड्यूस

3. Value Network ट्रेनिंग:
   π_RL से सेल्फ-प्ले से पोज़िशन जेनरेट
   V(s) को विन रेट प्रेडिक्ट करने के लिए ट्रेन
```

### AlphaGo Zero

AlphaGo Zero ने इस फ्लो को सिंप्लिफाई किया:

```
1. प्योर सेल्फ-प्ले (कोई ह्यूमन डेटा नहीं)
   रैंडम नेटवर्क f_θ इनिशियलाइज़

   दोहराएं:
     a. MCTS + f_θ से सेल्फ-प्ले गेम
     b. पॉलिसी हेड और वैल्यू हेड दोनों साथ ट्रेन
     c. f_θ अपडेट

   → सिंगल नेटवर्क पॉलिसी और वैल्यू दोनों आउटपुट
```

मुख्य सुधार:
- **कोई ह्यूमन डेटा नहीं**: शून्य से शुरू
- **सिंगल नेटवर्क**: पॉलिसी और वैल्यू शेयर्ड फीचर
- **सिंपलर ट्रेनिंग**: एंड-टू-एंड लर्निंग

### AlphaZero

AlphaZero ने और जनरलाइज़ किया:

```
एक ही एल्गोरिथम, अलग-अलग गेम:
- गो: AlphaGo Zero से आगे का लेवल
- चेस: Stockfish से आगे
- शोगी: Elmo से आगे

केवल गेम-स्पेसिफिक पार्ट: रूल एनकोडिंग
```

यह साबित करता है कि सेल्फ-प्ले एक **जनरल लर्निंग पैराडाइम** है, गो तक सीमित नहीं।

---

## मानव ने इससे क्या सीखा?

### AI द्वारा खोजे गए नए जोसेकी

सेल्फ-प्ले ने कई ऐसी चालें प्रोड्यूस कीं जो मानव ने कभी इस्तेमाल नहीं कीं:

**1. ओपनिंग इनोवेशन**

AlphaGo की कुछ पसंदीदा ओपनिंग:
- 3-3 इनवेज़न: शुरुआती चरण में ही कॉर्नर इनवेज़न
- हाई पोज़िशन प्ले: ट्रेडिशनली "अनस्टेबल" माना जाता था
- बिग अवलांच वेरिएशन: मानव को लगता था गणना करना कठिन है

**2. नई पोज़िशन इवैल्यूएशन**

कुछ पोज़िशन पर AI का मूल्यांकन मानव से बहुत अलग था:
- कुछ "कमज़ोर" दिखने वाले शेप वास्तव में ठोस हैं
- कुछ "इन्फ्लुएंस" का मूल्य ओवरएस्टिमेट था
- "सेंटे" और "गोटे" का पुनर्मूल्यांकन

### मानव गो पर प्रभाव

AlphaGo के बाद, प्रोफेशनल गो में महत्वपूर्ण बदलाव:

1. **ओपनिंग डायवर्सिफिकेशन**: प्रोफेशनल खिलाड़ियों ने AI-डिस्कवर्ड ओपनिंग इस्तेमाल शुरू कीं
2. **ट्रेनिंग मेथड चेंज**: AI प्रोफेशनल खिलाड़ियों का मुख्य ट्रेनिंग टूल बन गया
3. **गो थ्योरी पर पुनर्विचार**: कई ट्रेडिशनल "प्रिंसिपल" क्वेश्चन और करेक्ट किए गए
4. **नई एस्थेटिक्स**: AI स्टाइल के गो की सराहना शुरू हुई

Ke Jie ने AlphaGo से हारने के बाद कहा:

> "AlphaGo ने मुझे गो को फिर से समझाया। पहले मुझे लगता था मानव गो समझते हैं, अब मुझे पता है हमने केवल सतह छुई है।"

---

## दार्शनिक विचार

### लर्निंग का सार

सेल्फ-प्ले लर्निंग के बारे में गहरे प्रश्न उठाता है:

**ज्ञान कहां से आता है?**

- मानव लर्निंग बाहरी जानकारी (टीचर, किताबें, एक्सपीरियंस) पर निर्भर है
- सेल्फ-प्ले AI के पास केवल नियम हैं, कोई बाहरी ज्ञान नहीं
- फिर भी यह ज्ञान "खोज" सकता है—यह ज्ञान कहां से आया?

जवाब शायद है: **ज्ञान गेम के नियमों और स्ट्रक्चर में इंप्लिसिट है**। गो के नियम डिफाइन करते हैं क्या अच्छी चाल है, क्या बुरी, सेल्फ-प्ले बस इन इंप्लिसिट स्ट्रक्चर को रिवील करता है।

### क्रिएटिविटी और डिस्कवरी

जब AI "मूव 37" जैसी "गॉड मूव" चलता है, यह क्रिएशन है या डिस्कवरी?

एक व्यू: वह चाल हमेशा गो के नियमों में "एग्ज़िस्ट" करती थी, AI ने बस "डिस्कवर" किया।
दूसरा व्यू: AI ने वह चाल "क्रिएट" की, क्योंकि किसी को (AI सहित) पहले से नहीं पता था।

इस प्रश्न का स्टैंडर्ड उत्तर नहीं है, लेकिन यह क्रिएटिविटी की हमारी ट्रेडिशनल समझ को चैलेंज करता है।

### मानव इंटेलिजेंस की पोज़िशन

यदि AI शून्य से शुरू करके, सेल्फ-प्ले से मानव के हजारों वर्षों के ज्ञान को पार कर सकता है, तो मानव के लिए इसका क्या अर्थ है?

ऑप्टिमिस्टिक व्यू:
- AI मानव द्वारा बनाया टूल है
- AI की डिस्कवरी मानव की समझ बढ़ा सकती है
- मानव AI के साथ कोलैबोरेट कर सकते हैं, हायर लेवल तक पहुंच सकते हैं

कॉशस व्यू:
- कुछ डोमेन में, प्योर कम्प्यूटेशन मानव इंट्यूशन से आगे जा सकता है
- "एक्सपर्ट स्किल" के वैल्यू पर पुनर्विचार जरूरी
- एजुकेशन और ट्रेनिंग मेथड बदलने की जरूरत हो सकती है

---

## एनिमेशन संदर्भ

इस लेख में शामिल मुख्य अवधारणाएं और एनिमेशन नंबर:

| नंबर | अवधारणा | भौतिकी/गणित संदर्भ |
|------|------|--------------|
| E5 | सेल्फ-प्ले साइकल | फिक्स्ड पॉइंट इटरेशन |
| E6 | पॉलिसी इवोल्यूशन | इवोल्यूशनरी डायनामिक्स |

---

## सारांश

सेल्फ-प्ले AlphaGo की सफलता की एक मुख्य तकनीक है। हमने सीखा:

1. **यह क्यों काम करता है**: एडवर्सेरियल लर्निंग, कमज़ोरियों की क्रमिक खोज
2. **मेकेनिज़्म**: ट्रैजेक्टरी कलेक्शन, पॉलिसी ग्रेडिएंट, Value Network ट्रेनिंग
3. **रैंडमाइज़ेशन**: टेम्परेचर पैरामीटर, डिरिक्ले नॉइज़, ऑपोनेंट पूल
4. **स्किल ग्रोथ**: Elo सिस्टम, ग्रोथ कर्व एनालिसिस
5. **कन्वर्जेंस**: थ्योरेटिकल गारंटी और वास्तविक अवलोकन
6. **इम्प्लीमेंटेशन डिटेल्स**: पैरेलल ट्रेनिंग, चेकपॉइंट स्ट्रैटेजी, हाइपरपैरामीटर

अगले लेख में, हम देखेंगे AlphaGo न्यूरल नेटवर्क और MCTS को कैसे कंबाइन करता है, दोनों की स्ट्रेंथ का फायदा उठाता है।

---

## आगे पढ़ने के लिए

- **अगला लेख**: [MCTS और न्यूरल नेटवर्क का कॉम्बिनेशन](../mcts-neural-combo) — इंट्यूशन और रीज़निंग का परफेक्ट मेल
- **पिछला लेख**: [रीइन्फोर्समेंट लर्निंग परिचय](../reinforcement-intro) — रीइन्फोर्समेंट लर्निंग की बेसिक कॉन्सेप्ट्स
- **संबंधित**: [AlphaGo Zero ओवरव्यू](../alphago-zero) — शून्य से शुरू की ब्रेकथ्रू

---

## संदर्भ सामग्री

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
3. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
4. Heinrich, J., & Silver, D. (2016). "Deep Reinforcement Learning from Self-Play in Imperfect-Information Games." *arXiv preprint*.
5. Lanctot, M., et al. (2017). "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning." *NeurIPS*.
