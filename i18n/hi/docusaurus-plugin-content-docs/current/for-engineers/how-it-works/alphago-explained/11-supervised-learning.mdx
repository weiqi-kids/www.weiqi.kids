---
sidebar_position: 12
title: सुपरवाइज़्ड लर्निंग चरण
description: AlphaGo ने 3 करोड़ मानव गेम पोज़िशन से कैसे सीखा और 57% प्रेडिक्शन एक्यूरेसी हासिल की
---

import { EloChart } from '@site/src/components/D3Charts';

# सुपरवाइज़्ड लर्निंग चरण

AlphaGo के सेल्फ-प्ले करने से पहले, इसे बड़ी मात्रा में मानव गेम रिकॉर्ड "देखने" की जरूरत थी। इस प्रक्रिया को **सुपरवाइज़्ड लर्निंग** कहते हैं।

3 करोड़ मानव गेम पोज़िशन का विश्लेषण करके, AlphaGo की Policy Network ने **57% प्रेडिक्शन एक्यूरेसी** हासिल की—आधे से अधिक मामलों में मानव विशेषज्ञ की अगली चाल का सही अनुमान लगा सकी।

यह शायद बहुत प्रभावशाली न लगे, लेकिन यह देखते हुए कि प्रत्येक पोज़िशन में औसतन 250 वैध चालें होती हैं, यह एक आश्चर्यजनक उपलब्धि है।

---

## मानव गेम रिकॉर्ड से क्यों शुरू करें?

### सीखने का शुरुआती बिंदु

कल्पना करें कि आपको एक ऐसे व्यक्ति को गो खेलना सिखाना है जो बिल्कुल नहीं जानता। आप क्या करेंगे?

**विकल्प A: रैंडम एक्सप्लोरेशन**
```
उसे बेतरतीब चालें चलने दें, धीरे-धीरे पता लगाएं क्या अच्छी चाल है
→ बहुत कम दक्षता, शायद कभी न सीखे
```

**विकल्प B: विशेषज्ञों को देखें**
```
उसे बड़ी संख्या में प्रोफेशनल गेम देखने दें, उनकी चालों की नकल करें
→ बेसिक्स के बाद, खुद एक्सप्लोर करें
```

AlphaGo ने विकल्प B चुना। सुपरवाइज़्ड लर्निंग "विशेषज्ञों को देखने" का गणितीय संस्करण है।

### मानव गेम रिकॉर्ड का मूल्य

मानवों ने हजारों वर्षों में गो सिद्धांत विकसित किया है। यह ज्ञान गेम रिकॉर्ड में एनकोड है:

- **ओपनिंग जोसेकी**: लंबे समय से सत्यापित ओपनिंग चालें
- **मिडगेम टैक्टिक्स**: अटैक-डिफेंस ट्रांज़िशन की बुद्धिमत्ता
- **एंडगेम तकनीक**: टेरिटरी काउंटिंग की बारीकियां
- **बिग पिक्चर व्यू**: ग्लोबल जजमेंट की इंट्यूशन

सुपरवाइज़्ड लर्निंग ने AlphaGo को इस मानव ज्ञान को "इनहेरिट" करने दिया, शून्य से शुरू किए बिना।

---

## ट्रेनिंग डेटा स्रोत

### KGS Go Server

AlphaGo का ट्रेनिंग डेटा मुख्य रूप से **KGS Go Server** (Kiseido Go Server भी कहा जाता है) से आया, जो एक प्रसिद्ध ऑनलाइन गो प्लेटफॉर्म है।

#### KGS की विशेषताएं

| विशेषता | विवरण |
|------|------|
| यूजर | मुख्य रूप से एमेच्योर, कुछ प्रोफेशनल भी |
| स्किल रेंज | बिगिनर से प्रोफेशनल 9-दान तक |
| गेम रिकॉर्ड | पूर्ण SGF फॉर्मेट में सेव |
| एक्टिव पीरियड | 2000 से अब तक |

#### KGS क्यों चुना?

1. **बड़ी डेटा मात्रा**: लाखों गेम रिकॉर्ड
2. **यूनिफॉर्म फॉर्मेट**: SGF फॉर्मेट पार्स करने में आसान
3. **स्किल लेबल**: प्रत्येक यूजर का रेटिंग है
4. **विविधता**: अलग-अलग स्टाइल के खिलाड़ी

### 3 करोड़ पोज़िशन

KGS के गेम रिकॉर्ड से, DeepMind ने लगभग **3 करोड़ पोज़िशन** निकाले:

```
रॉ डेटा:
- लगभग 1.6 लाख गेम
- प्रति गेम लगभग 200 चालें
- कुल ~3.2 करोड़ पोज़िशन

डेटा फिल्टरिंग:
- कम रैंक गेम फिल्टर
- बीच में सरेंडर की गई पोज़िशन फिल्टर
- अंतिम लगभग 3 करोड़ हाई-क्वालिटी पोज़िशन
```

### डेटा फॉर्मेट

प्रत्येक ट्रेनिंग सैंपल में शामिल:

```python
{
    "board_state": [[0, 1, 2, ...], ...],  # 19×19 बोर्ड
    "features": [...],                      # 48 फीचर प्लेन
    "next_move": 123,                       # मानव की चाल (0-360)
    "game_result": 1,                       # 1=काला जीता, -1=सफेद जीता
    "player_rank": "5d",                    # इस चाल चलने वाले का रैंक
}
```

---

## डेटा प्रीप्रोसेसिंग

### SGF पार्सिंग

SGF (Smart Game Format) गो गेम रिकॉर्ड का स्टैंडर्ड फॉर्मेट है:

```
(;GM[1]FF[4]CA[UTF-8]AP[CGoban:3]ST[2]
RU[Japanese]SZ[19]KM[6.50]
PW[White]PB[Black]
;B[pd];W[dd];B[pq];W[dp];B[qk];W[nc]...
)
```

पार्स करने की जरूरत:
- बोर्ड साइज (SZ[19])
- प्रत्येक चाल (B[pd], W[dd]...)
- गेम रिजल्ट (RE[B+2.5])

```python
def parse_sgf(sgf_string):
    """SGF गेम रिकॉर्ड पार्स करें"""
    moves = []
    # सभी चालें निकालें
    pattern = r';([BW])\[([a-s]{2})\]'
    for match in re.finditer(pattern, sgf_string):
        color = match.group(1)  # 'B' या 'W'
        coord = match.group(2)  # 'pd', 'dd', आदि

        # कोऑर्डिनेट कन्वर्ट करें
        x = ord(coord[0]) - ord('a')
        y = ord(coord[1]) - ord('a')

        moves.append((color, x, y))

    return moves
```

### फीचर एक्सट्रैक्शन

प्रत्येक पोज़िशन के लिए, 48 फीचर प्लेन निकालें (विस्तार के लिए देखें [इनपुट फीचर डिज़ाइन](../input-features)):

```python
def extract_features(board, history, current_player):
    """48 फीचर प्लेन निकालें"""
    features = np.zeros((48, 19, 19))

    # पत्थरों की पोज़िशन
    features[0] = (board == 1)  # काले पत्थर
    features[1] = (board == 2)  # सफेद पत्थर
    features[2] = (board == 0)  # खाली पॉइंट

    # हिस्ट्री रिकॉर्ड
    for i, hist in enumerate(history[:8]):
        features[3+i] = (hist == 1)
        features[11+i] = (hist == 2)

    # लिबर्टी, अटारी, लैडर आदि...
    # (विस्तृत इम्प्लीमेंटेशन छोड़ा गया)

    return features
```

### डेटा ऑगमेंटेशन

गो बोर्ड में **8 गुना सिमेट्री** है (4 रोटेशन × 2 मिरर)। प्रत्येक ओरिजिनल सैंपल 8 में बदल सकता है:

```
ओरिजिनल:     90° रोटेट:   180° रोटेट:  270° रोटेट:
┌───┐    ┌───┐      ┌───┐      ┌───┐
│ ● │    │   │      │   │      │   │
│   │ → │   │  →   │   │  →   │   │
│   │    │ ● │      │ ● │      │ ● │
└───┘    └───┘      └───┘      └───┘

प्रत्येक को होरिज़ॉन्टल फ्लिप करें, 8 समतुल्य ट्रेनिंग सैंपल मिलते हैं
```

यह इफेक्टिव ट्रेनिंग डेटा को 8 गुना बढ़ाता है, साथ ही सुनिश्चित करता है कि मॉडल जो पैटर्न सीखे वे किसी विशेष दिशा पर निर्भर न हों।

```python
def augment(state, action):
    """8 गुना सिमेट्री ऑगमेंटेशन"""
    augmented = []

    for rotation in [0, 1, 2, 3]:  # 0, 90, 180, 270 डिग्री
        rotated_state = np.rot90(state, rotation, axes=(1, 2))
        rotated_action = rotate_action(action, rotation)
        augmented.append((rotated_state, rotated_action))

        # होरिज़ॉन्टल फ्लिप
        flipped_state = np.flip(rotated_state, axis=2)
        flipped_action = flip_action(rotated_action)
        augmented.append((flipped_state, flipped_action))

    return augmented
```

---

## लॉस फंक्शन

### क्रॉस-एंट्रॉपी लॉस

सुपरवाइज़्ड लर्निंग Policy Network को ट्रेन करने के लिए **क्रॉस-एंट्रॉपी लॉस (Cross-Entropy Loss)** का उपयोग करती है:

```
L(θ) = -Σ log p_θ(a | s)
```

जहां:
- `s`: बोर्ड स्टेट
- `a`: मानव की वास्तविक चाल (लेबल)
- `p_θ(a | s)`: मॉडल द्वारा उस पोज़िशन की प्रेडिक्टेड प्रोबेबिलिटी

### इंट्यूटिव समझ

क्रॉस-एंट्रॉपी लॉस "मॉडल प्रेडिक्शन और लेबल के बीच अंतर" मापता है:

| सिचुएशन | मॉडल प्रेडिक्शन | लॉस | विवरण |
|------|---------|------|------|
| परफेक्ट प्रेडिक्शन | a की प्रोबेबिलिटी = 1.0 | 0 | सबसे अच्छा |
| कॉन्फिडेंट और सही | a की प्रोबेबिलिटी = 0.9 | 0.1 | बहुत अच्छा |
| अनिश्चित लेकिन सही | a की प्रोबेबिलिटी = 0.5 | 0.7 | ठीक है |
| गलत प्रेडिक्शन | a की प्रोबेबिलिटी = 0.1 | 2.3 | बहुत खराब |
| पूरी तरह गलत | a की प्रोबेबिलिटी = 0.01 | 4.6 | सबसे खराब |

लॉस फंक्शन मॉडल को सही पोज़िशन की प्रोबेबिलिटी बढ़ाने के लिए प्रेरित करता है।

### MSE से तुलना

मीन स्क्वेयर्ड एरर (MSE) क्यों नहीं?

```python
# MSE:
loss_mse = (prediction - target)^2

# Cross-Entropy:
loss_ce = -log(prediction[target])
```

| विशेषता | MSE | Cross-Entropy |
|------|-----|---------------|
| टारगेट टाइप | रिग्रेशन (कंटीन्यूअस वैल्यू) | क्लासिफिकेशन (प्रोबेबिलिटी डिस्ट्रीब्यूशन) |
| ग्रेडिएंट बिहेवियर | एरर जितना बड़ा, ग्रेडिएंट उतना बड़ा | कॉन्फिडेंट गलती पर, ग्रेडिएंट और बड़ा |
| उपयुक्त सिचुएशन | Value Network | Policy Network |

Policy Network 361 क्लास की प्रोबेबिलिटी डिस्ट्रीब्यूशन आउटपुट करती है, क्रॉस-एंट्रॉपी स्वाभाविक चयन है।

---

## ट्रेनिंग प्रक्रिया

### हार्डवेयर कॉन्फ़िगरेशन

DeepMind ने बड़ी मात्रा में कंप्यूट रिसोर्स का उपयोग किया:

| रिसोर्स | संख्या |
|------|------|
| GPU | 50 |
| ट्रेनिंग समय | लगभग 3 सप्ताह |
| बैच साइज | 16 |
| कुल ट्रेनिंग स्टेप्स | ~340M |

### ऑप्टिमाइज़र

**स्टोकैस्टिक ग्रेडिएंट डिसेंट (SGD) + मोमेंटम** का उपयोग:

```python
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.003,         # इनिशियल लर्निंग रेट
    momentum=0.9,     # मोमेंटम कोएफिशिएंट
    weight_decay=1e-4 # L2 रेगुलराइज़ेशन
)
```

#### Adam की जगह SGD क्यों?

2016 में, SGD + momentum अभी भी इमेज टास्क के लिए मुख्य चयन था। वास्तव में, बाद के रिसर्च (KataGo सहित) ने पाया कि Adam जैसे ऑप्टिमाइज़र बेहतर हो सकते हैं।

### लर्निंग रेट शेड्यूल

लर्निंग रेट ट्रेनिंग के दौरान धीरे-धीरे कम होता है:

```python
scheduler = torch.optim.lr_scheduler.StepLR(
    optimizer,
    step_size=80_000_000,  # हर 80M स्टेप
    gamma=0.1              # लर्निंग रेट × 0.1
)
```

```
लर्निंग रेट:
0.003  ────────┐
              └─────── 0.0003 ────────┐
                                      └─────── 0.00003
         80M स्टेप        160M स्टेप        240M स्टेप
```

### ट्रेनिंग लूप

```python
def train_epoch(model, dataloader, optimizer):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for batch in dataloader:
        states, actions = batch

        # फॉरवर्ड पास
        policy = model(states)  # (batch, 361)

        # लॉस कैलकुलेट करें
        loss = F.cross_entropy(policy, actions)

        # बैकवर्ड पास
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # स्टैटिस्टिक्स
        total_loss += loss.item()
        predictions = policy.argmax(dim=1)
        correct += (predictions == actions).sum().item()
        total += actions.size(0)

    accuracy = correct / total
    avg_loss = total_loss / len(dataloader)

    return avg_loss, accuracy
```

### ट्रेनिंग कर्व

टिपिकल ट्रेनिंग प्रोसेस:

```
एक्यूरेसी
60% |                    ......**********
    |              ......*
50% |        ......*
    |    ....*
40% |  ..*
    |..*
30% |*
    +───────────────────────────────────── ट्रेनिंग स्टेप्स
    0       100M     200M     300M     340M
```

लॉस और एक्यूरेसी तेजी से सुधरते हैं, फिर स्थिर हो जाते हैं।

---

## परिणाम विश्लेषण

### 57% एक्यूरेसी

पूर्ण ट्रेनिंग के बाद, Policy Network ने **57.0% टॉप-1 एक्यूरेसी** हासिल की।

#### टॉप-1 एक्यूरेसी क्या है?

```
प्रेडिक्शन: मॉडल 361 प्रोबेबिलिटी आउटपुट करता है
Top-1: सबसे ज्यादा प्रोबेबिलिटी वाली पोज़िशन
एक्यूरेसी: यह पोज़िशन मानव की वास्तविक चाल के बराबर होने का अनुपात
```

57% का अर्थ है: मॉडल के पास आधे से अधिक मामलों में मानव विशेषज्ञ की अगली चाल का सही अनुमान है।

### अन्य प्रोग्राम से तुलना

| प्रोग्राम | Top-1 एक्यूरेसी | विवरण |
|------|-------------|------|
| रैंडम चयन | 0.4% | बेसलाइन |
| ट्रेडिशनल फीचर + लीनियर मॉडल | ~24% | 2008 लेवल |
| शैलो CNN | ~44% | 2014 लेवल |
| **AlphaGo Policy Network** | **57%** | 2016 ब्रेकथ्रू |
| AlphaGo Zero | ~60% | 2017 |

DeepMind का डीप CNN पिछली सबसे अच्छी विधि से 13 प्रतिशत अंक आगे था।

### स्किल मूल्यांकन

केवल Policy Network (बिना सर्च के) से खेलने पर स्किल:

<EloChart mode="training" width={600} height={350} />

| कॉन्फ़िगरेशन | Elo रेटिंग | अनुमानित लेवल |
|------|---------|---------|
| ट्रेडिशनल बेस्ट (Pachi) | ~2500 | एमेच्योर 4-5 दान |
| SL Policy Network | ~2800 | एमेच्योर 6-7 दान |

प्योर सुपरवाइज़्ड लर्निंग ने ही एमेच्योर हाई-दान लेवल हासिल कर लिया, जो 2016 में एक बड़ी ब्रेकथ्रू थी।

### एक्यूरेसी vs स्किल

दिलचस्प बात यह है कि एक्यूरेसी और स्किल लीनियर रिलेशन में नहीं हैं:

```
एक्यूरेसी:  44% → 57% (13% इंप्रूवमेंट)
Elo:    ~2500 → ~2800 (~300 इंप्रूवमेंट)

एक्यूरेसी इंप्रूवमेंट अनुपात: 13% / 44% ≈ 30%
Elo इंप्रूवमेंट अनुपात: 300 / 2500 ≈ 12%
```

एक्यूरेसी में छोटा सुधार भी स्किल में महत्वपूर्ण सुधार ला सकता है, क्योंकि:
- क्रिटिकल पोज़िशन में सही चयन अधिक महत्वपूर्ण है
- स्पष्ट गलतियों से बचना अधिक अच्छी चालों से ज्यादा महत्वपूर्ण है

---

## सुपरवाइज़्ड लर्निंग की सीमाएं

### समस्या 1: सीलिंग इफेक्ट

सुपरवाइज़्ड लर्निंग केवल "मानव स्तर" तक पहुंच सकती है, उससे आगे नहीं:

```
SL Policy का लक्ष्य: मानव की नकल करना
          ↓
यदि मानव में गलत आदतें हैं
          ↓
SL Policy भी वे गलतियां सीखेगा
```

उदाहरण के लिए, यदि ट्रेनिंग डेटा के खिलाड़ी "मूव 37" जैसी नॉन-ट्रेडिशनल चालें नहीं चलते, तो SL Policy भी नहीं सीखेगी।

### समस्या 2: अच्छी और बुरी चाल में अंतर नहीं

सुपरवाइज़्ड लर्निंग केवल "मानव ने क्या चला" देखती है, चाल अच्छी है या बुरी, इसकी परवाह नहीं:

```
पोज़िशन A: मानव ने K10 चला (वास्तव में बुरी चाल)
पोज़िशन B: मानव ने Q4 चला (अच्छी चाल)

SL Policy दोनों को समान मानती है, दोनों सीखती है
```

ट्रेनिंग डेटा में एमेच्योर खिलाड़ियों के गेम शामिल हैं, जिनमें कई गलतियां हैं। SL Policy इन गलतियों को सीखेगी।

### समस्या 3: अपर्याप्त एक्सप्लोरेशन

SL Policy केवल मानव की ज्ञात चालें सीखती है:

```
मानव चाल सेट: {A, B, C, D, E}
           ↓
SL Policy केवल इन चालों में से चुनेगी
           ↓
शायद बेहतर चाल F है, लेकिन कभी खोजी नहीं गई
```

यह सुपरवाइज़्ड लर्निंग की मूलभूत सीमा है: यह केवल वही सीख सकती है जो ट्रेनिंग डेटा में है।

### समाधान: रीइन्फोर्समेंट लर्निंग

मानव से आगे जाने के लिए, AlphaGo ने सुपरवाइज़्ड लर्निंग के बाद **रीइन्फोर्समेंट लर्निंग** की:

```
SL Policy (मानव स्तर)
      ↓ सेल्फ-प्ले
RL Policy (मानव से आगे)
```

विस्तार के लिए देखें [रीइन्फोर्समेंट लर्निंग परिचय](../reinforcement-intro) और [सेल्फ-प्ले](../self-play)।

---

## इम्प्लीमेंटेशन पॉइंट्स

### संपूर्ण ट्रेनिंग कोड

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset

class GoDataset(Dataset):
    def __init__(self, data_path):
        # प्रीप्रोसेस्ड डेटा लोड करें
        self.states = np.load(f"{data_path}/states.npy")
        self.actions = np.load(f"{data_path}/actions.npy")

    def __len__(self):
        return len(self.states)

    def __getitem__(self, idx):
        state = torch.FloatTensor(self.states[idx])
        action = torch.LongTensor([self.actions[idx]])[0]
        return state, action

def train_policy_network():
    # मॉडल
    model = PolicyNetwork(input_channels=48, num_filters=192, num_layers=12)
    model = model.cuda()

    # डेटा
    dataset = GoDataset("data/kgs")
    dataloader = DataLoader(
        dataset, batch_size=16, shuffle=True, num_workers=4
    )

    # ऑप्टिमाइज़र
    optimizer = optim.SGD(
        model.parameters(),
        lr=0.003,
        momentum=0.9,
        weight_decay=1e-4
    )
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=80_000_000, gamma=0.1)

    # ट्रेनिंग लूप
    best_accuracy = 0

    for epoch in range(100):
        model.train()
        total_loss = 0
        correct = 0
        total = 0

        for states, actions in dataloader:
            states = states.cuda()
            actions = actions.cuda()

            # फॉरवर्ड पास
            policy = model(states)
            loss = nn.functional.cross_entropy(policy, actions)

            # बैकवर्ड पास
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()

            # स्टैटिस्टिक्स
            total_loss += loss.item()
            predictions = policy.argmax(dim=1)
            correct += (predictions == actions).sum().item()
            total += actions.size(0)

        accuracy = correct / total
        print(f"Epoch {epoch}: Loss={total_loss/len(dataloader):.4f}, Acc={accuracy:.4f}")

        # बेस्ट मॉडल सेव करें
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            torch.save(model.state_dict(), "best_policy.pth")

    print(f"Best accuracy: {best_accuracy:.4f}")
```

### इवैल्यूएशन कोड

```python
def evaluate_policy(model, test_dataloader):
    model.eval()

    correct_top1 = 0
    correct_top5 = 0
    total = 0

    with torch.no_grad():
        for states, actions in test_dataloader:
            states = states.cuda()
            actions = actions.cuda()

            policy = model(states)

            # Top-1 एक्यूरेसी
            top1_pred = policy.argmax(dim=1)
            correct_top1 += (top1_pred == actions).sum().item()

            # Top-5 एक्यूरेसी
            top5_pred = policy.topk(5, dim=1)[1]
            for i, action in enumerate(actions):
                if action in top5_pred[i]:
                    correct_top5 += 1

            total += actions.size(0)

    print(f"Top-1 Accuracy: {correct_top1/total:.4f}")
    print(f"Top-5 Accuracy: {correct_top5/total:.4f}")
```

### सामान्य समस्याएं और समाधान

| समस्या | लक्षण | समाधान |
|------|------|---------|
| ओवरफिटिंग | ट्रेनिंग एक्यूरेसी हाई, टेस्ट एक्यूरेसी लो | डेटा ऑगमेंटेशन बढ़ाएं, Dropout |
| अस्थिर ट्रेनिंग | लॉस में तेज उतार-चढ़ाव | लर्निंग रेट कम करें, बैच साइज बढ़ाएं |
| धीमा कन्वर्जेंस | एक्यूरेसी रुकी हुई | लर्निंग रेट एडजस्ट करें, डेटा चेक करें |
| मेमोरी अपर्याप्त | OOM एरर | बैच साइज कम करें, मिक्स्ड प्रिसिजन |

---

## एनिमेशन संदर्भ

इस लेख में शामिल मुख्य अवधारणाएं और एनिमेशन नंबर:

| नंबर | अवधारणा | भौतिकी/गणित संदर्भ |
|------|------|--------------|
| D3 | सुपरवाइज़्ड लर्निंग | मैक्सिमम लाइकलीहुड एस्टिमेशन |
| D5 | क्रॉस-एंट्रॉपी लॉस | KL डाइवर्जेंस |
| D6 | ग्रेडिएंट डिसेंट | ऑप्टिमाइज़ेशन |
| A6 | डेटा प्रीप्रोसेसिंग | स्टैंडर्डाइज़ेशन |

---

## आगे पढ़ने के लिए

- **पिछला लेख**: [CNN और गो का संयोजन](../cnn-and-go) — कन्वोल्यूशनल न्यूरल नेटवर्क बोर्ड को कैसे प्रोसेस करता है
- **अगला लेख**: [रीइन्फोर्समेंट लर्निंग परिचय](../reinforcement-intro) — मानव से आगे जाने की कुंजी
- **संबंधित विषय**: [Policy Network विस्तार](../policy-network) — नेटवर्क आर्किटेक्चर की डिटेल

---

## मुख्य बिंदु

1. **KGS गेम रिकॉर्ड ट्रेनिंग डेटा स्रोत हैं**: लगभग 3 करोड़ हाई-क्वालिटी पोज़िशन
2. **क्रॉस-एंट्रॉपी लॉस लर्निंग चलाता है**: मॉडल को सही पोज़िशन की प्रोबेबिलिटी बढ़ाने के लिए प्रेरित करता है
3. **57% एक्यूरेसी बड़ी ब्रेकथ्रू थी**: पिछली सबसे अच्छी विधि से 13 प्रतिशत अंक आगे
4. **8 गुना सिमेट्री ऑगमेंटेशन**: ट्रेनिंग डेटा को प्रभावी ढंग से बढ़ाता है
5. **सुपरवाइज़्ड लर्निंग की सीलिंग है**: ट्रेनिंग डेटा के स्तर से आगे नहीं जा सकती

सुपरवाइज़्ड लर्निंग AlphaGo का "शुरुआती बिंदु" है—इसने मानव के हजारों वर्षों के गो ज्ञान को इनहेरिट किया, बाद की रीइन्फोर्समेंट लर्निंग के लिए नींव रखी।

---

## संदर्भ सामग्री

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Maddison, C. J., et al. (2014). "Move Evaluation in Go Using Deep Convolutional Neural Networks." *arXiv:1412.6564*.
3. Clark, C., & Storkey, A. (2015). "Training Deep Convolutional Neural Networks to Play Go." *ICML*.
4. KGS Game Archives: [https://www.gokgs.com/archives.jsp](https://www.gokgs.com/archives.jsp)
