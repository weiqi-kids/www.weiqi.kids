---
sidebar_position: 17
title: AlphaGo Zero 概述
description: 从零开始、完全自学，AlphaGo Zero 如何在没有人类棋谱的情况下超越所有前代版本
keywords: [AlphaGo Zero, 自我对弈, 强化学习, 深度学习, 围棋 AI, 无监督学习]
---

# AlphaGo Zero 概述

2017 年 10 月，DeepMind 发表了一个震惊 AI 界的成果：**AlphaGo Zero** 在没有使用任何人类棋谱的情况下，从完全随机的状态开始训练，仅仅三天就超越了击败李世石的原版 AlphaGo，并以 **100:0** 的比分完胜。

这不只是数字上的进步。这代表一个全新的范式：**AI 不需要人类知识，可以从零发现一切**。

---

## 为什么不需要人类棋谱？

### 人类棋谱的限制

原版 AlphaGo 的训练过程分为两个阶段：

1. **监督学习**：用 3000 万局人类棋谱训练 Policy Network
2. **强化学习**：通过自我对弈进一步提升

这个方法有几个根本性的问题：

#### 1. 人类棋谱有上限

人类棋手的棋力有极限，棋谱中包含的是人类的理解，也包含人类的错误和偏见。当 AI 从人类棋谱学习时，它学到的是：

- 人类认为好的下法（但不一定是最优的）
- 人类的思维模式（但可能限制创新）
- 人类的错误（会被当作正确的样本学习）

#### 2. 监督学习的瓶颈

监督学习的目标是「模仿人类」——预测人类棋手会下哪一步。这意味着 AI 的能力上限被人类棋手的能力所限制。

就像一个学徒只能模仿师傅，永远无法超越师傅一样。

#### 3. 数据收集成本

高品质的人类棋谱需要多年累积，而且只存在于围棋这类有悠久历史的游戏中。如果要将 AI 应用到新领域（如蛋白质结构预测），根本没有「人类专家棋谱」可用。

### Zero 的突破

AlphaGo Zero 完全跳过监督学习阶段，直接从**随机初始化**开始自我对弈。这解决了上述所有问题：

| 问题 | 原版 AlphaGo | AlphaGo Zero |
|------|-------------|--------------|
| 人类知识上限 | 受限于棋谱品质 | 无此限制 |
| 学习目标 | 模仿人类 | 最大化胜率 |
| 数据需求 | 3000 万局棋谱 | 0 |
| 可推广性 | 仅限围棋 | 可推广至其他领域 |

这是一个根本性的范式转变：从「学习人类知识」转向「从第一性原理发现知识」。

---

## 与原版 AlphaGo 的对比：100:0

### 碾压性的胜利

DeepMind 让训练完成的 AlphaGo Zero 与各个版本的 AlphaGo 对弈：

| 对手 | AlphaGo Zero 战绩 |
|------|-------------------|
| AlphaGo Fan（击败樊麾版本） | 100:0 |
| AlphaGo Lee（击败李世石版本） | 100:0 |
| AlphaGo Master（60 连胜版本） | 89:11 |

**100:0**——这意味着在 100 盘比赛中，原版 AlphaGo 连一盘都赢不了。

### 更少的资源，更强的棋力

不只是赢，AlphaGo Zero 还用更少的资源达成更强的棋力：

| 指标 | AlphaGo Lee | AlphaGo Zero |
|------|-------------|--------------|
| 训练时间 | 数月 | 40 天（3 天超越 AlphaGo Lee） |
| 训练局数 | 3000 万人类棋谱 + 自我对弈 | 490 万局自我对弈 |
| TPU 数量（训练） | 50+ | 4 |
| TPU 数量（推理） | 48 | 4 |
| 输入特征 | 48 个平面 | 17 个平面 |
| 神经网络 | SL + RL 双网络 | 单一双头网络 |

这是一个惊人的效率提升：**资源减少 10 倍以上，棋力却大幅提升**。

### 为什么 Zero 更强？

AlphaGo Zero 更强的原因可以从几个角度理解：

#### 1. 无偏见的学习

原版 AlphaGo 从人类棋谱学习，继承了人类的偏见。例如，人类棋手可能过度重视某些定式，或对某些局面有错误的评估。

AlphaGo Zero 没有这些包袱。它从白纸开始，只通过胜负结果来学习什么是好棋。这让它能够发现人类从未想过的下法。

#### 2. 一致的学习目标

原版 AlphaGo 的训练有两个不同的目标：
- 监督学习：最大化对人类落子的预测准确率
- 强化学习：最大化胜率

这两个目标可能互相冲突。AlphaGo Zero 只有一个目标：**胜率最大化**。这让学习过程更加一致和有效。

#### 3. 更简洁的架构

原版 AlphaGo 使用分离的 Policy Network 和 Value Network。AlphaGo Zero 使用单一的双头网络（详见下一篇），让特征表示能够被共享，提高了学习效率。

---

## 简化的输入特征：从 48 到 17

### 原版 AlphaGo 的 48 个特征平面

原版 AlphaGo 的神经网络输入包含 48 个 19x19 的特征平面，编码了大量人类设计的特征：

| 类别 | 特征数 | 内容 |
|------|--------|------|
| 棋子位置 | 3 | 黑子、白子、空点 |
| 气数 | 8 | 1-8 气的棋串 |
| 提子 | 8 | 能提 1-8 颗子 |
| 打劫 | 1 | 劫争位置 |
| 边线距离 | 4 | 一线到四线 |
| 落子合法性 | 1 | 哪些位置可以下 |
| 历史状态 | 8 | 过去 8 手的位置 |
| 轮次 | 1 | 黑方或白方 |
| 其他 | 14 | 征子、眼位等 |

这 48 个特征是围棋专家精心设计的，包含了大量领域知识。

### AlphaGo Zero 的 17 个特征平面

AlphaGo Zero 大幅简化了输入，只使用 17 个特征平面：

| 平面编号 | 内容 | 数量 |
|----------|------|------|
| 1-8 | 黑子位置（最近 8 步） | 8 |
| 9-16 | 白子位置（最近 8 步） | 8 |
| 17 | 当前轮次（全 1 或全 0） | 1 |

这 17 个特征只包含：
- **当前棋盘状态**：每个位置有黑子、白子或空
- **历史信息**：过去 8 步的棋盘状态
- **轮次信息**：轮到谁下

没有气数、没有征子判断、没有边线距离——所有这些「围棋知识」都让神经网络自己学习。

### 为什么简化是好的？

#### 1. 让网络自己发现特征

复杂的手工特征可能遗漏重要信息，或编码错误的假设。让神经网络从原始数据学习，它可能发现更好的特征表示。

事实证明，AlphaGo Zero 学会了人类设计的所有特征（气数、征子等），还学到了一些人类没有明确意识到的模式。

#### 2. 更好的可推广性

48 个特征中的许多是围棋专用的（如征子、边线距离）。17 个简化特征则是通用的——任何棋盘游戏都可以用类似的方式编码。

这为后来的 **AlphaZero**（通用游戏 AI）奠定了基础。

#### 3. 减少人为错误

手工设计的特征可能包含错误或不完整的定义。简化输入消除了这类问题的可能性。

---

## 单一网络架构

### 原版的双网络设计

原版 AlphaGo 使用两个独立的神经网络：

```
Policy Network:  输入 → CNN → 19x19 落子概率
Value Network:   输入 → CNN → 胜率评估（-1 到 1）
```

这两个网络：
- 有不同的架构（层数、通道数略有不同）
- 独立训练（先训练 Policy，再训练 Value）
- 不共享任何参数

### Zero 的双头网络

AlphaGo Zero 使用单一网络，但有两个输出头（heads）：

```
输入 → ResNet 共享主干 → Policy Head → 19x19 落子概率
                       → Value Head  → 胜率评估
```

两个 Head 共享同一个 ResNet 主干（详见[下一篇：双头网络与残差网络](../dual-head-resnet)），这带来几个好处：

#### 1. 参数效率

共享主干意味着大部分参数被两个任务共用。这减少了总参数量，降低了过拟合风险。

#### 2. 特征共享

「应该下哪里」（Policy）和「谁会赢」（Value）需要理解类似的棋盘模式。共享主干让这些特征能被两个任务同时学习和利用。

#### 3. 训练稳定性

联合训练让梯度信号来自两个来源，提供了更丰富的监督信号，让训练更加稳定。

### 残差网络的威力

AlphaGo Zero 的主干使用 **40 层残差网络（ResNet）**，比原版 AlphaGo 的 13 层 CNN 深得多。

残差连接（skip connections）让深层网络得以有效训练，避免了梯度消失问题。这是 2015 年 ImageNet 竞赛的突破性技术，被 AlphaGo Zero 成功应用到围棋领域。

---

## 训练效率的提升

### 自我对弈的指数增长

AlphaGo Zero 的训练过程展示了令人惊叹的效率：

| 训练时间 | ELO 评分 | 相当于 |
|----------|----------|--------|
| 0 小时 | 0 | 随机乱下 |
| 3 小时 | ~1000 | 发现基本规则 |
| 12 小时 | ~3000 | 发现定式 |
| 36 小时 | ~4500 | 超越樊麾版 |
| 60 小时 | ~5200 | 超越李世石版 |
| 72 小时 | ~5400 | 超越原版 AlphaGo |
| 40 天 | ~5600 | 最强版本 |

**三天超越人类、三天超越之前花费数月训练的 AI**——这是指数级的效率提升。

### 为什么这么快？

#### 1. 更强的搜索引导

AlphaGo Zero 的 MCTS 完全由神经网络引导，不再使用快速走子策略（rollout）。这让搜索更加高效和准确。

#### 2. 更快的自我对弈

由于只需要一个网络（而非两个），每局自我对弈的计算成本降低。这意味着在相同时间内可以产生更多训练数据。

#### 3. 更有效的学习

双头网络的联合训练让每一局棋的信息被更有效地利用。Policy 和 Value 的梯度相互强化，加速了收敛。

### 与人类学习的对比

人类棋手需要多长时间达到不同水平？

| 水平 | 人类所需时间 | AlphaGo Zero |
|------|-------------|--------------|
| 入门 | 数周 | 几分钟 |
| 业余初段 | 数年 | 数小时 |
| 职业水平 | 10-20 年 | 1-2 天 |
| 世界冠军 | 20+ 年全职投入 | 3 天 |
| 超越人类 | 不可能 | 3 天 |

这个对比不是要贬低人类棋手——他们用的是生物神经元，而 AlphaGo Zero 用的是专门设计的 TPU 和几千瓦的电力。但它确实展示了正确的学习方法可以多么高效。

---

## 通用性：西洋棋、将棋

### AlphaZero 的诞生

2017 年 12 月，DeepMind 发表了 **AlphaZero**——AlphaGo Zero 的通用版本。同一套算法，只需修改游戏规则，就能在三种棋类游戏中达到世界顶级水平：

| 游戏 | 训练时间 | 对手 | 战绩 |
|------|----------|------|------|
| 围棋 | 8 小时 | AlphaGo Zero | 60:40 |
| 西洋棋 | 4 小时 | Stockfish 8 | 28 胜 72 和 0 负 |
| 将棋 | 2 小时 | Elmo | 90:8:2 |

注意这里的对手：
- **Stockfish** 是当时最强的西洋棋引擎，使用几十年人类知识和优化
- **Elmo** 是当时最强的将棋 AI

AlphaZero 用几小时训练，就超越了这些耗费多年开发的专用系统。

### 通用性的意义

AlphaGo Zero / AlphaZero 证明了一件重要的事：

> **同一套学习算法，可以在不同领域达到超人水平。**

这不是三个不同的 AI，而是一个通用的学习框架：

1. **自我对弈**产生经验
2. **蒙特卡洛树搜索**探索可能性
3. **神经网络**学习策略和价值函数
4. **强化学习**优化目标函数

这个框架不依赖领域特定的知识，这为 AI 的通用化迈出了重要一步。

### 对传统 AI 的冲击

在 AlphaZero 之前，西洋棋和将棋的最强 AI 都是「专家系统」风格的：

- **大量人类知识**：开局库、残局库、评估函数
- **数十年优化**：无数棋手和工程师的心血
- **极度专业化**：Stockfish 不能下围棋，Elmo 不能下西洋棋

AlphaZero 用一个通用算法在几小时内超越了这一切。这让许多 AI 研究者重新思考：

> 我们应该投入更多精力在「通用学习算法」，还是「专家知识编码」？

答案似乎越来越清楚：让机器自己学习，比教它知识更有效。

---

## AlphaGo Zero 的下棋风格

### 超越人类的审美

围棋界对 AlphaGo Zero 的下法有一个普遍评价：**更加优美**。

AlphaGo Lee 的下法有时显得「怪异」——像第 37 手那样的落子，人类需要事后分析才能理解其妙处。但 AlphaGo Zero 的下法常常在事后被评价为「一眼就知道是好棋」。

这可能是因为：

1. **更强的棋力**：Zero 能看得更深，落子更加从容
2. **无人类偏见**：不受传统定式的束缚
3. **一致的目标**：只追求胜率，不模仿人类

### 重新发现人类棋理

有趣的是，AlphaGo Zero 在训练过程中「重新发现」了人类数千年累积的围棋知识：

- **定式**：Zero 自己发现了许多常见定式，因为这些确实是双方最优解
- **布局原则**：角、边、中央的重要性顺序
- **棋形知识**：愚形与好形的区别

这验证了人类棋理的合理性——这些知识不是偶然的，而是围棋本质的反映。

### 超越人类的创新

但 Zero 也发现了人类从未想过的下法：

- **非常规开局**：在传统开局基础上的变化
- **激进的弃子**：比人类更愿意放弃局部换取全局优势
- **反直觉的形状**：表面上的「坏形」其实是最优解

这些创新正在改变人类对围棋的理解。许多职业棋手表示，研究 AlphaGo Zero 的棋谱让他们对围棋有了全新的认识。

---

## 技术细节总结

### 与原版 AlphaGo 的完整对比

| 方面 | AlphaGo（原版） | AlphaGo Zero |
|------|----------------|--------------|
| **训练数据** | 人类棋谱 + 自我对弈 | 纯自我对弈 |
| **学习方法** | 监督学习 + 强化学习 | 纯强化学习 |
| **输入特征** | 48 个平面 | 17 个平面 |
| **网络架构** | 分离的 Policy/Value | 双头 ResNet |
| **网络深度** | 13 层 | 40 层（或更多） |
| **MCTS 评估** | 神经网络 + Rollout | 纯神经网络 |
| **搜索次数** | 每步 ~100,000 | 每步 ~1,600 |
| **训练 TPU** | 50+ | 4 |
| **推理 TPU** | 48 | 4（可扩展） |

### 核心算法

AlphaGo Zero 的训练循环非常简洁：

```
1. 自我对弈
   - 用当前网络进行 MCTS
   - 按 MCTS 搜索概率选择落子
   - 记录每一步的 (局面, MCTS概率, 胜负结果)

2. 训练网络
   - 从经验池中取样
   - Policy Head：最小化与 MCTS 概率的交叉熵
   - Value Head：最小化与实际胜负的均方误差
   - 联合优化两个目标

3. 更新网络
   - 用新网络替换旧网络（通过对弈验证新网络更强）
   - 回到步骤 1
```

这个循环持续运行，网络不断变强。没有人类数据、没有人类知识，只有游戏规则和胜负目标。

---

## 对 AI 研究的启示

### 第一性原理学习

AlphaGo Zero 展示了一种「第一性原理」的学习方法：

> 不要告诉 AI 怎么做，只告诉它目标是什么，让它自己发现方法。

这与传统的专家系统方法形成鲜明对比。专家系统试图将人类知识编码进 AI，而 AlphaGo Zero 让 AI 自己发现知识。

结果是：AI 发现的知识可能比人类知识更完整、更准确。

### 自我对弈的威力

AlphaGo Zero 证明了自我对弈可以产生无限的训练数据，而且这些数据的品质会随着网络的提升而提升。

这是一个「正向循环」：
- 更强的网络 → 更好的自我对弈数据
- 更好的数据 → 更强的网络

这个循环可以持续运行，直到达到游戏的理论上限（如果存在的话）。

### 简化的重要性

AlphaGo Zero 的成功证明了「简化」的重要性：

- 简化输入（48 → 17）
- 简化架构（双网络 → 单网络）
- 简化训练（监督 + 强化 → 纯强化）

每一次简化都让系统更加强大。这告诉我们：复杂不等于好，最简单的解决方案往往是最好的。

---

## 动画对应

本文涉及的核心概念与动画编号：

| 编号 | 概念 | 物理/数学对应 |
|------|------|--------------|
| 🎬 E7 | 从零开始训练 | 自组织现象 |
| 🎬 E5 | 自我对弈 | 不动点收敛 |
| 🎬 E12 | 棋力成长曲线 | S 型增长 |
| 🎬 D12 | 残差网络 | 梯度高速公路 |

---

## 延伸阅读

- **下一篇**：[双头网络与残差网络](../dual-head-resnet) — 详解 AlphaGo Zero 的神经网络架构
- **相关文章**：[自我对弈](../self-play) — 为什么自我对弈能产生超人水平
- **技术深入**：[从零训练的过程](../training-from-scratch) — Day 0-3 的详细演进

---

## 参考资料

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
3. DeepMind. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
4. Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." *Nature*, 588, 604-609.
