---
sidebar_position: 14
title: 自我对弈
description: 深入理解 AlphaGo 如何透过自我对弈突破人类棋力的极限
---

import { EloChart } from '@site/src/components/D3Charts';

# 自我对弈

在前一篇文章中，我们介绍了强化学习的基本概念。现在，让我们探讨 AlphaGo 成功的关键之一——**自我对弈（Self-Play）**。

这是一个看似矛盾的概念：**AI 怎么能透过和自己下棋变得更强？**

答案既深刻又优雅，涉及博弈论、演化动力学、以及学习的本质。

---

## 为什么自我对弈有效？

### 直觉解释

想象你是一位围棋初学者，在一个荒岛上独自练习：

1. 你下了一盘棋，自己同时扮演黑白双方
2. 对局结束后，你分析哪些棋下得好、哪些下得差
3. 下一盘棋时，你尝试避免之前的错误
4. 你重复这个过程数百万次

直觉上，这似乎有问题：
- 如果你的水平很差，黑白双方都下差棋，能学到什么？
- 会不会陷入「错误的平衡」——双方都下错棋但能互相抵消？

但实际上，自我对弈能够产生持续的进步。原因如下：

### 渐进式发现弱点

关键洞见是：**即使双方都是同一个 AI，每盘棋的结果仍然包含信息**。

```
局面 A：AI 选择了走法 X，最终获胜
局面 A：AI 选择了走法 Y，最终失败

→ 结论：在局面 A 中，X 比 Y 好
```

透过统计大量对局，AI 能够学习到每个局面下哪些选择更优。这就是**策略梯度**的本质：好的选择会被强化，差的选择会被抑制。

### 对抗性学习

自我对弈有一个特殊的性质：**训练对手会自动适应你的水平**。

```
训练周期 1：AI 发现了一个有效的战术 T
训练周期 2：作为对手的 AI 学会了如何防守 T
训练周期 3：原版 AI 被迫寻找更好的战术 T'
```

这形成了一个**军备竞赛（Arms Race）**，双方不断发现并克服彼此的弱点。

### 与人类棋谱的比较

| 训练方式 | 优点 | 缺点 |
|---------|------|------|
| **人类棋谱** | 学习人类智慧的结晶 | 受限于人类水平 |
| **自我对弈** | 无上限的提升潜力 | 可能陷入局部最优 |
| **两者结合** | 快速起步 + 持续提升 | 最佳策略 |

AlphaGo 原版先用人类棋谱做监督学习，再用自我对弈做强化学习。AlphaGo Zero 则证明了只用自我对弈也能达到超人水平。

---

## 博弈论视角

### 纳什均衡

在博弈论中，**纳什均衡（Nash Equilibrium）** 是一种稳定状态：在这个状态下，没有任何玩家有动机单方面改变策略。

对于围棋这样的**零和、完美信息博弈**，纳什均衡有特殊的意义：

$$\pi^* = \arg\max_\pi \min_{\pi'} V(\pi, \pi')$$

其中 $V(\pi, \pi')$ 是当策略 $\pi$ 对上策略 $\pi'$ 时的预期价值。

这就是著名的 **Minimax 原则**：最佳策略是那个能在最坏情况下表现最好的策略。

### 自我对弈与纳什均衡

理论上，如果自我对弈能够收敛，它应该收敛到纳什均衡。对于围棋这样的确定性博弈，纳什均衡就是**完美下法**。

但围棋的状态空间太大了（$10^{170}$），我们不可能找到真正的纳什均衡。自我对弈实际上是在**近似**这个均衡。

### 虚拟对弈（Fictitious Play）

自我对弈与博弈论中的**虚拟对弈**概念相关：

1. 每个玩家观察对手的历史策略
2. 计算对手策略的平均分布
3. 选择对抗这个平均分布的最佳回应

在某些条件下，虚拟对弈可以证明会收敛到纳什均衡。

AlphaGo 的自我对弈可以看作是这个概念的神经网络实现。

---

## 自我对弈的机制

### 基本流程

AlphaGo 的自我对弈流程：

```
算法：Self-Play Training

初始化：Policy Network π_θ（可从监督学习或随机初始化开始）

重复以下步骤直到收敛：

1. 产生对弈数据
   对于 i = 1 到 N（并行进行）：
     a. 用当前策略 π_θ 进行一局自我对弈
     b. 收集轨迹：τ_i = (s_0, a_0, r_1, s_1, a_1, ...)
     c. 记录最终结果 z_i ∈ {-1, +1}

2. 更新策略
   a. 计算策略梯度：
      ∇J = (1/N) Σ_i Σ_t ∇_θ log π_θ(a_t|s_t) · z_i
   b. 更新参数：θ ← θ + α · ∇J

3. 更新价值网络
   a. 用 (s, z) 对训练 Value Network
   b. 最小化：L = E[(V_φ(s) - z)²]

4. 可选：评估并保存检查点
   a. 让新策略对抗旧版本
   b. 如果胜率 > 55%，更新对手池
```

### 训练数据的产生

每局自我对弈产生一个**轨迹（trajectory）**：

$$\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, z)$$

其中：
- $s_t$：时间步 $t$ 的棋盘状态
- $a_t$：时间步 $t$ 选择的动作
- $z$：最终结果（+1 胜利，-1 失败）

一局 200 手的对弈就产生了 200 个训练样本。每天进行数十万局自我对弈，训练数据量是惊人的。

### 策略更新

使用策略梯度更新 Policy Network：

$$\theta \leftarrow \theta + \alpha \cdot \nabla_\theta \mathbb{E}\left[\sum_t \log \pi_\theta(a_t|s_t) \cdot z\right]$$

这个更新的效果：
- 如果最终获胜（$z = +1$），增加所有落子的概率
- 如果最终失败（$z = -1$），减少所有落子的概率

这看起来很粗糙——赢棋时可能也有些差棋，输棋时可能也有些好棋。但透过大量对局的统计，这些「杂讯」会被平均掉，真正的好棋会被识别出来。

### 价值网络训练

Value Network 使用**回归（regression）** 训练：

$$\phi \leftarrow \phi - \beta \cdot \nabla_\phi \mathbb{E}\left[(V_\phi(s) - z)^2\right]$$

这让 Value Network 学会预测：从当前局面开始，最终获胜的概率是多少？

Value Network 的作用是：
1. 在 MCTS 中提供叶节点评估
2. 作为策略梯度的基准线（baseline）
3. 直接用于局面评估

---

## 随机化的重要性

### 避免确定性循环

如果自我对弈是完全确定性的，可能会陷入循环：

```
策略 A 总是下固定的开局
策略 A 对上策略 A 总是产生相同的棋局
只有一局棋被反复学习
AI 无法探索其他可能性
```

这就是为什么**随机性**在自我对弈中至关重要。

### 随机化的来源

AlphaGo 在自我对弈中引入随机性的方式：

**1. 策略网络本身是随机的**

Policy Network 输出的是概率分布，而非确定性选择：

$$a \sim \pi_\theta(a|s)$$

同样的局面，每次可能选择不同的落子。

**2. 温度参数**

在训练时使用较高的温度（temperature）来增加多样性：

$$\pi_\tau(a|s) = \frac{\pi_\theta(a|s)^{1/\tau}}{\sum_{a'} \pi_\theta(a'|s)^{1/\tau}}$$

- $\tau > 1$：更随机，更多探索
- $\tau < 1$：更确定，更多利用
- $\tau = 1$：原始分布

**3. 狄利克雷噪音（Dirichlet Noise）**

AlphaGo Zero 在自我对弈时，在根节点的先验概率上加入狄利克雷噪音：

$$P(s, a) = (1 - \varepsilon) \cdot \pi_\theta(a|s) + \varepsilon \cdot \eta_a$$

其中 $\eta \sim \text{Dir}(\alpha)$，$\varepsilon = 0.25$，$\alpha = 0.03$（针对围棋的 361 个动作）。

这确保了即使是非常低概率的走法，也有机会被探索。

### 对弈池（Population）方法

另一种增加多样性的方法是维护一个**对弈池**：

```
对弈池 = [π_1, π_2, π_3, ..., π_k]（不同版本的策略）

每局对弈：
1. 从池中随机选择一个对手
2. 与该对手进行对弈
3. 用结果更新当前策略
4. 定期将改进的策略加入池中
```

这种方法的好处：
- **多样性**：不同风格的对手
- **稳定性**：避免对特定对手过拟合
- **鲁棒性**：学会应对各种策略

AlphaGo 原版和 AlphaGo Zero 都使用了类似的技术。

---

## 棋力成长曲线

### Elo 评分系统

为了追踪 AI 棋力的变化，AlphaGo 使用了 **Elo 评分系统**。

Elo 系统的基本原理：

$$P(\text{A 胜}) = \frac{1}{1 + 10^{(R_B - R_A)/400}}$$

其中 $R_A$ 和 $R_B$ 是双方的 Elo 分数。

- 分差 200：强者预期赢 75%
- 分差 400：强者预期赢 90%
- 分差 800：强者预期赢 99%

### AlphaGo 的棋力成长

让我们可视化 AlphaGo 各版本的棋力成长：

<EloChart mode="zero" width={700} height={400} showMilestones={true} />

### 成长速度分析

从曲线可以观察到几个有趣的现象：

**1. 初期快速增长**

在训练的最初几个小时，AI 学会了基本规则和简单战术。这是**低垂果实**阶段——有太多明显的错误可以修正。

**2. 中期稳定增长**

随着基本错误被消除，AI 开始学习更精妙的战术和定式。增长速度变慢，但仍然稳定。

**3. 后期增长放缓**

当 AI 已经很强时，进一步提升变得困难。可能需要发现全新的策略，而不只是修正错误。

### 超越人类的时刻

AlphaGo 训练曲线中的关键里程碑：

| 里程碑 | 相当于 | 达成时间 |
|--------|--------|---------|
| 超越业余强豪 | Elo ~2700 | 约 3 小时 |
| 超越 Fan Hui | Elo ~3500 | 约 36 小时 |
| 超越 Lee Sedol | Elo ~4500 | 约 60 小时 |
| 超越原版 AlphaGo | Elo ~5000 | 约 72 小时 |

这些数字（来自 AlphaGo Zero）令人震惊：**AI 在 3 天内从零开始超越了人类数千年的围棋智慧**。

---

## 收敛性分析

### 自我对弈会收敛吗？

这是一个重要的理论问题。简短的答案是：**在某些条件下会，但围棋太复杂了，我们无法严格证明**。

### 理论保证

对于较简单的游戏（如井字棋），可以证明：

1. **存在性**：存在纳什均衡（Minimax 定理）
2. **收敛性**：某些算法（如虚拟对弈）会收敛到纳什均衡

对于围棋，我们没有严格的收敛保证，但实验证据显示：
- 棋力持续提升
- 没有出现明显的振荡或退化
- 最终棋力超越所有已知人类

### 可能的失败模式

自我对弈可能遇到的问题：

**1. 策略循环（Strategy Cycling）**

```
策略 A 打败策略 B
策略 B 打败策略 C
策略 C 打败策略 A
```

这在某些游戏中确实会发生（如剪刀石头布）。但围棋有足够的复杂性，这种纯粹的循环似乎不会发生。

**2. 过拟合到自己**

AI 可能学会了只针对自己风格的策略，而无法应对其他风格的对手。这是为什么 AlphaGo 会与不同版本的自己对弈，以及最终与人类棋手测试。

**3. 局部最优**

AI 可能陷入局部最优——一种「还不错但不是最好」的策略。随机化和大量对弈有助于避免这个问题。

### 实际观察

从 AlphaGo 的训练过程观察到：

1. **持续进步**：Elo 分数随着训练持续上升
2. **没有退化**：没有出现棋力突然下降的情况
3. **风格演化**：AI 的下棋风格随着训练逐渐变化
4. **发现新定式**：AI 发现了人类从未使用过的开局和战术

这些观察表明，虽然我们没有理论保证，但自我对弈在实践中确实有效。

---

## 实作细节

### 并行自我对弈

为了加速训练，AlphaGo 使用大规模并行自我对弈：

```
架构：

    ┌────────────────────────────────────────────┐
    │           Parameter Server                  │
    │    (储存最新的 θ，接收梯度更新)              │
    └────────────────────────────────────────────┘
         ▲                              │
         │ 梯度更新                     │ 最新参数
         │                              ▼
    ┌─────────┐  ┌─────────┐  ┌─────────┐
    │ Worker 1 │  │ Worker 2 │  │ Worker N │
    │ 自我对弈  │  │ 自我对弈  │  │ 自我对弈  │
    │ 收集轨迹  │  │ 收集轨迹  │  │ 收集轨迹  │
    └─────────┘  └─────────┘  └─────────┘
```

**关键设计决策**：

- **同步 vs 非同步**：AlphaGo 使用非同步更新，Worker 不需要等待彼此
- **更新频率**：每完成 N 局对弈就更新一次参数
- **对手选择**：随机选择最近几个版本中的一个作为对手

### 检查点策略

定期保存模型检查点，用于：

1. **对弈池**：维护不同版本的对手
2. **评估**：追踪棋力变化
3. **故障恢复**：训练中断时可以恢复

```python
# 伪代码
def training_loop():
    for iteration in range(num_iterations):
        # 产生对弈数据
        trajectories = parallel_self_play(current_policy, num_games=1000)

        # 更新策略
        update_policy(trajectories)

        # 定期评估和保存
        if iteration % 100 == 0:
            elo = evaluate_against_pool(current_policy)
            save_checkpoint(current_policy, elo)

            if elo > best_elo:
                add_to_pool(current_policy)
                best_elo = elo
```

### 训练资源需求

AlphaGo 的训练规模令人印象深刻：

| 版本 | 硬件 | 训练时间 | 自我对弈局数 |
|------|------|---------|-------------|
| AlphaGo Fan | 176 GPU | 数月 | ~30M |
| AlphaGo Lee | 48 TPU | 数周 | ~30M |
| AlphaGo Zero | 4 TPU | 3 天 | ~5M |
| AlphaGo Zero (40天版) | 4 TPU | 40 天 | ~30M |

注意 AlphaGo Zero 用更少的硬件和更短的时间达到了更强的棋力——这是算法效率的提升。

### 超参数设置

一些关键的超参数：

```python
# 自我对弈设置
NUM_PARALLEL_GAMES = 5000      # 同时进行的对弈数
GAMES_PER_ITERATION = 25000    # 每次迭代的对弈数
MCTS_SIMULATIONS = 1600        # 每步棋的 MCTS 模拟次数

# 训练设置
BATCH_SIZE = 2048              # 训练批次大小
LEARNING_RATE = 0.01           # 初始学习率
L2_REGULARIZATION = 1e-4       # 权重衰减

# 探索设置
TEMPERATURE = 1.0              # 开局 30 手的温度
DIRICHLET_ALPHA = 0.03         # 狄利克雷噪音参数
EXPLORATION_FRACTION = 0.25    # 噪音比例
```

这些超参数是经过大量实验调整的，对训练效果有显著影响。

---

## 自我对弈的变体

### AlphaGo 原版

AlphaGo 原版的训练流程：

```
1. 监督学习 (SL)：从人类棋谱学习
   → 产生 SL Policy Network (π_SL)

2. 强化学习 (RL)：自我对弈
   初始化 π_RL = π_SL
   对手池 = [π_SL]

   重复：
     a. π_RL 与池中策略对弈
     b. 用策略梯度更新 π_RL
     c. 如果 π_RL 变强，加入池中

   → 产生 RL Policy Network (π_RL)

3. 价值网络训练：
   用 π_RL 自我对弈产生局面
   训练 V(s) 预测胜率
```

### AlphaGo Zero

AlphaGo Zero 简化了这个流程：

```
1. 纯自我对弈（无人类数据）
   初始化随机网络 f_θ

   重复：
     a. 用 MCTS + f_θ 进行自我对弈
     b. 同时训练策略头和价值头
     c. 更新 f_θ

   → 单一网络同时输出策略和价值
```

关键改进：
- **无需人类数据**：从零开始
- **单一网络**：策略和价值共享特征
- **更简洁的训练**：端到端学习

### AlphaZero

AlphaZero 进一步泛化：

```
同样的算法，不同的游戏：
- 围棋：达到超越 AlphaGo Zero 的水平
- 西洋棋：超越 Stockfish
- 将棋：超越 Elmo

唯一的游戏特定部分：规则编码
```

这证明了自我对弈是一种**通用的学习范式**，不限于围棋。

---

## 人类从中学到什么？

### AI 发现的新定式

自我对弈产生了许多人类从未使用过的下法：

**1. 开局创新**

AlphaGo 偏好的一些开局：
- 3-3 侵入：在早期就侵入角部
- 高位下法：传统上被认为「不稳定」
- 大雪崩变化：人类认为复杂难以计算

**2. 新的形势判断**

AI 对某些局面的评估与人类大相径庭：
- 某些看似「薄弱」的棋形其实很坚实
- 某些「厚势」的价值被高估
- 对「先手」和「后手」的重新评估

### 对人类围棋的影响

AlphaGo 之后，职业围棋发生了显著变化：

1. **开局多样化**：职业棋手开始使用 AI 发现的新开局
2. **训练方式改变**：AI 成为职业棋手的主要训练工具
3. **棋理重新思考**：许多传统「棋理」被质疑和修正
4. **新的美学**：开始欣赏 AI 风格的棋

柯洁在输给 AlphaGo 后说：

> 「AlphaGo 让我重新认识围棋。我以前认为人类理解围棋，现在我知道我们只是触及皮毛。」

---

## 哲学思考

### 学习的本质

自我对弈提出了关于学习的深刻问题：

**知识从哪里来？**

- 人类学习依赖于外部信息（老师、书本、经验）
- 自我对弈的 AI 只有规则，没有外部知识
- 但它仍然能「发现」知识——这些知识是从哪里来的？

答案可能是：**知识隐含在游戏规则和结构中**。围棋的规则定义了什么是好棋、什么是坏棋，自我对弈只是揭示了这些隐含的结构。

### 创造力与发现

当 AI 下出「神之一手」（Move 37），这算是创造还是发现？

一种观点是：那步棋一直「存在」于围棋的规则中，AI 只是「发现」了它。
另一种观点是：AI 「创造」了这步棋，因为没有人（包括 AI 自己）事先知道它。

这个问题没有标准答案，但它挑战了我们对创造力的传统理解。

### 人类智慧的位置

如果 AI 可以从零开始，透过自我对弈超越人类数千年的智慧，这对人类意味着什么？

乐观的看法：
- AI 是人类创造的工具
- AI 的发现可以增强人类的理解
- 人类可以与 AI 合作，达到更高的水平

谨慎的看法：
- 某些领域，纯粹的计算可能超越人类直觉
- 需要重新思考「专业技能」的价值
- 教育和训练方式可能需要改变

---

## 动画对应

本文涉及的核心概念与动画编号：

| 编号 | 概念 | 物理/数学对应 |
|------|------|--------------|
| 🎬 E5 | 自我对弈循环 | 不动点迭代 |
| 🎬 E6 | 策略演化 | 进化动力学 |

---

## 总结

自我对弈是 AlphaGo 成功的关键技术之一。我们学习了：

1. **为什么有效**：对抗性学习、渐进式发现弱点
2. **机制**：轨迹收集、策略梯度、价值网络训练
3. **随机化**：温度参数、狄利克雷噪音、对弈池
4. **棋力成长**：Elo 系统、成长曲线分析
5. **收敛性**：理论保证与实际观察
6. **实作细节**：并行训练、检查点策略、超参数

下一篇，我们将探讨 AlphaGo 如何将神经网络与 MCTS 结合，发挥两者的优势。

---

## 延伸阅读

- **下一篇**：[MCTS 与神经网络的结合](../mcts-neural-combo) — 直觉与推理的完美结合
- **上一篇**：[强化学习入门](../reinforcement-intro) — 强化学习的基本概念
- **相关**：[AlphaGo Zero 概述](../alphago-zero) — 从零开始的突破

---

## 参考资料

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
3. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
4. Heinrich, J., & Silver, D. (2016). "Deep Reinforcement Learning from Self-Play in Imperfect-Information Games." *arXiv preprint*.
5. Lanctot, M., et al. (2017). "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning." *NeurIPS*.
