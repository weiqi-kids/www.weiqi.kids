---
sidebar_position: 4
title: 「神之一手」深度分析
description: 从棋理、专家反应到 AI 视角，完整解读 AlphaGo 对李世乭第二盘的第 37 手
---

import { PolicyHeatmap } from '@site/src/components/D3Charts';

# 「神之一手」深度分析

2016 年 3 月 10 日，AlphaGo 与李世乭的第二盘对局。第 37 手，AlphaGo 在右上方五路下出一步「肩冲」。

这一手棋，后来被称为「**神之一手**」（Divine Move）。它不只让 AlphaGo 赢得比赛，更改变了人类对围棋的理解。

本文将从多个角度深度分析这步棋：对局背景、传统棋理、专家反应、AI 视角，以及它对围棋理论的长远影响。

---

## 对局局面回顾

### 第二盘的开局

第一盘失利后，李世乭在第二盘做出调整。他选择执白后手，希望观察 AlphaGo 的开局倾向后再制定策略。

开局阶段：
- **黑 1**：右上角星位
- **白 2**：左下角星位
- **黑 3-白 4**：双方各占一角

到第 36 手为止，局面发展正常。AlphaGo 执黑，在右上角进行了一场局部战斗。白棋（李世乭）在右边建立了势力，黑棋则在上边有一定实地。

### 第 36 手后的局面

让我们看看第 36 手后的棋盘状态：

```
     A B C D E F G H J K L M N O P Q R S T
 19  . . . . . . . . . . . . . . . . . . .
 18  . . . . . . . . . . . . . . . . . . .
 17  . . . ○ . . . . . . . . . . . ● . . .
 16  . . . ╋ . . . . . ╋ . . . . . ╋ . . .
 15  . . . . . . . . . . . . . . . ● . . .
 14  . . . . . . . . . . . . . . ○ . . . .  ← 白棋势力范围
 13  . . . . . . . . . . . . . . . . . . .
 12  . . . . . . . . . . . . . . . . . . .
 11  . . . . . . . . . . . . . . . . . . .
 10  . . . ╋ . . . . . ╋ . . . . . ╋ . . .
  9  . . . . . . . . . . . . . . . . . . .
  8  . . . . . . . . . . . . . . . . . . .
  7  . . . . . . . . . . . . . . . . . . .
  6  . . . . . . . . . . . . . . . . . . .
  5  . . . . . . . . . . . . . . . . . . .
  4  . . . ╋ . . . . . ╋ . . . . . ╋ . . .
  3  . . . ○ . . . . . . . . . . . ● . . .
  2  . . . . . . . . . . . . . . . . . . .
  1  . . . . . . . . . . . . . . . . . . .
```

（简化示意图，实际局面更复杂）

关键观察：
- 白棋在右边有外势
- 黑棋在上边有实地潜力
- 右上角的战斗告一段落

此时，轮到黑棋（AlphaGo）落子。

---

## 传统下法分析

### 职业棋手的预期

在第 37 手之前，解说室里的职业棋手正在热烈讨论。他们普遍预期黑棋会选择以下几种下法：

**选项 A：右下角挂角**

这是最「正常」的选择。黑棋可以：
- 抢占最后的大场（右下角）
- 保持局面平衡
- 遵循「金角银边草肚皮」的传统价值观

**选项 B：上边围空**

黑棋也可以在上边拆二或拆三，巩固自己的势力范围。这样可以：
- 将上边的潜力转化为实地
- 限制白棋的发展空间

**选项 C：中央分投**

一些棋手认为黑棋可能会在中央下一手，制约白棋的右边外势。这虽然不是最常见的选择，但在战略上也说得通。

🎬 C3：传统棋理的价值判断

### 没有人预料到的选择

然而，AlphaGo 选择了一个几乎没有人想到的位置：

**E5（五路肩冲）**

这一手落在了棋盘右半部、靠近中央的位置，是对白棋右边外势的一步「肩冲」。

---

## 第 37 手：五路肩冲

### 这步棋在哪里？

```
     A B C D E F G H J K L M N O P Q R S T
 19  . . . . . . . . . . . . . . . . . . .
 18  . . . . . . . . . . . . . . . . . . .
 17  . . . ○ . . . . . . . . . . . ● . . .
 16  . . . ╋ . . . . . ╋ . . . . . ╋ . . .
 15  . . . . . . . . . ★ . . . . . ● . . .  ← 第 37 手（★）
 14  . . . . . . . . . . . . . . ○ . . . .
 13  . . . . . . . . . . . . . . . . . . .
 12  . . . . . . . . . . . . . . . . . . .
```

第 37 手下在了 **K15**（或称 J5，坐标系统因来源而异）位置。

### 什么是「肩冲」？

「肩冲」是围棋中的一个手筋，指的是斜向靠近对方棋子的下法。它的特点是：

- **不直接接触**：与对方棋子保持一步距离
- **破坏结构**：打乱对方的预期发展
- **难以应对**：不管对方如何应对，都会产生某种代价

传统上，肩冲通常下在三路或四路。**五路肩冲**极为罕见，因为：

1. **位置太高**：五路靠近中央，传统认为效率较低
2. **容易被攻击**：孤立的棋子容易成为对方攻击目标
3. **价值不明确**：不像边角那样有明确的实地价值

🎬 C5：肩冲的几何特性

---

## 专家即时反应

### 解说室的震惊

第 37 手落下的瞬间，解说室陷入了短暂的沉默。

**韩国解说（金成龙九段）**：

> 「这...这是什么？这步棋下在五路？我不理解。这一定是失误吧？」

**中国解说（古力九段）**：

> 「我看不懂这步棋。如果是我的学生这样下，我会狠狠批评他。」

**美国解说（Michael Redmond 九段）**：

> 「Very unusual move. I don't think any human would play this.」
>
> （非常不寻常的一手。我不认为有人类会这样下。）

### 职业棋手的实时评论

在各种直播平台上，职业棋手纷纷发表评论：

**柯洁**（当时世界排名第一）：

> 「我无法理解这步棋的意图。如果 AlphaGo 赢了，我会认真研究。」

**朴廷桓**（韩国顶尖棋手）：

> 「这步棋太奇怪了。是不是程序出了问题？」

**芈昱廷**（中国世界冠军）：

> 「五路肩冲？从来没见过这种下法。」

🎬 C7：专家直觉与 AI 评估的差距

### 「万分之一的概率」

赛后，DeepMind 团队透露了一个惊人的数据：

> 「根据我们的分析，如果一位职业棋手面对同样的局面，选择第 37 手这个位置的概率大约是 **万分之一**。」

换句话说，在人类的围棋知识体系中，这步棋几乎是「不存在」的选项。

---

## AI 视角的解读

### Policy Network 的概率分布

让我们看看 AlphaGo 的 Policy Network 如何评估这个局面：

<PolicyHeatmap initialPosition="move37" size={450} showTopN={5} />

上图展示了 AlphaGo 对各个位置的落子概率评估。

关键观察：
- **第 37 手的位置**：概率约 8%，并非最高
- **传统选点**（如右下角）：概率约 12%
- **其他候选位置**：分散在不同区域

有趣的是，第 37 手在 Policy Network 的评估中**并非概率最高的选择**。那为什么 AlphaGo 选择了它？

🎬 C9：Policy Network 的输出分布

### MCTS 的深度评估

答案在于 **蒙地卡罗树搜索（MCTS）**。

Policy Network 只提供「直觉」，真正的决策来自 MCTS 的深度模拟。AlphaGo 在做出决定前，会模拟数千种可能的未来走向。

对于第 37 手，MCTS 的评估过程如下：

```
位置 K15（第 37 手）:
├── 模拟 1: 黑胜（+0.3）
├── 模拟 2: 黑胜（+0.5）
├── 模拟 3: 黑胜（+0.2）
├── ...
└── 平均胜率: 58%

位置 R3（右下角挂角）:
├── 模拟 1: 黑胜（+0.1）
├── 模拟 2: 白胜（-0.2）
├── 模拟 3: 黑胜（+0.2）
├── ...
└── 平均胜率: 52%
```

虽然右下角的「直觉概率」更高，但经过深度模拟后，第 37 手的**预期胜率更高**。

🎬 C11：MCTS 如何修正 Policy Network 的判断

### Value Network 的全局评估

Value Network 从全局角度评估了第 37 手的价值：

**下第 37 手前的胜率**：约 52%（黑棋略优）

**下第 37 手后的胜率**：约 58%（黑棋明显优势）

这意味着，第 37 手让 AlphaGo 的预期胜率提升了 **6 个百分点**。

这个提升幅度在围棋中是相当显著的。通常，一步好棋能带来 2-3% 的胜率提升就已经很好了。

🎬 C13：Value Network 的增量评估

---

## 棋理分析：为什么是五路肩冲？

### 从局部看

表面上，第 37 手似乎效率很低：

- **位置太高**：五路比四路或三路更靠近中央
- **没有实地**：不像边角那样能直接围取实地
- **容易被攻击**：孤立的棋子可能被白棋攻击

但如果我们仔细分析，这步棋有几个微妙的好处：

1. **破坏白棋的外势**：白棋原本计划在右边发展，第 37 手打乱了这个计划
2. **建立自己的影响力**：这步棋虽然不围实地，但在中央建立了存在感
3. **增加变化**：创造了复杂的局面，有利于计算能力更强的一方

### 从全局看

这步棋的真正价值需要从全局角度来理解：

**厚势与实地的权衡**

传统围棋理论认为「金角银边草肚皮」——角最有价值，中央最没价值。但第 37 手挑战了这个观念。

AlphaGo 的评估显示：在这个特定局面下，**中央的影响力比边角的实地更有价值**。

这是因为：
- 黑棋已经有足够的实地基础
- 白棋的右边外势如果发展起来会很强大
- 制约白棋比扩张自己更重要

🎬 C15：全局价值函数的计算

**「先手」的价值**

第 37 手还有一个被低估的好处：它保持了「先手」。

在围棋中，「先手」意味着掌握主动权。第 37 手下完后，白棋不得不应对，这让黑棋可以继续主导局面走向。

如果黑棋选择「正常」的右下角挂角，双方可能会在角部进行定式，然后局面趋于平衡。但第 37 手打破了这种平衡，让局面充满不确定性——而这正是 AlphaGo 所擅长的。

### 李世乭的应对困境

第 37 手之后，李世乭思考了很长时间。他面临的困境是：

**如果直接应对（例如跳或飞）**：
- 等于承认第 37 手的价值
- 让黑棋达到了破坏白棋外势的目的

**如果不理会**：
- 黑棋可能会进一步发展中央
- 白棋的右边外势难以成为实地

最终，李世乭选择了应对。但无论他选择什么，第 37 手已经达到了它的目的。

🎬 C17：博弈论中的强制选择

---

## 后续发展：从第 37 手到胜利

### 中盘的演变

第 37 手之后，对局进入了复杂的中盘战斗。

**关键进展**：

- **手数 40-50**：双方在右边进行了激烈的接触战
- **手数 50-70**：AlphaGo 利用第 37 手建立的影响力，在中央取得优势
- **手数 70-100**：黑棋逐渐将优势转化为实地

到了第 100 手左右，AlphaGo 的领先已经相当明显。李世乭虽然努力反击，但无法扭转局势。

### 最终结果

**AlphaGo 中盘胜**

这盘棋的胜利，第 37 手居功至伟。赛后分析显示，如果没有第 37 手，局面会更加接近，白棋甚至可能取得优势。

🎬 C19：一步棋如何改变整盘棋的走向

---

## 对围棋理论的影响

### 新定式的诞生

第 37 手引发了围棋界对「肩冲」这个手筋的重新思考。

**传统观点**：
- 肩冲应该下在三路或四路
- 五路肩冲效率太低
- 孤立的棋子容易被攻击

**AlphaGo 之后**：
- 五路肩冲在特定局面下是最佳选择
- 位置的「高低」不如「效果」重要
- 需要从全局角度评估每一步棋的价值

### 人类棋手的学习

第 37 手之后，许多职业棋手开始尝试类似的下法：

**柯洁**在 2017 年的几盘对局中使用了五路肩冲，并取得成功：

> 「AlphaGo 教会我，很多我们认为『不好』的棋，其实只是我们不理解。」

**朴廷桓**也在自己的对局中借鉴了这种思维方式：

> 「重要的不是记住第 37 手这个具体的位置，而是学会用新的眼光看待棋盘。」

🎬 C21：AI 如何拓展人类的认知边界

### 围棋 AI 训练的启示

第 37 手对围棋 AI 的研究也有深远影响：

**对 Policy Network 的反思**：

为什么 Policy Network 给第 37 手的概率较低？因为它是从人类棋谱中学习的，而人类几乎不会下这种棋。

这说明：**仅靠监督学习（从人类学习）是不够的**。AI 需要自我探索，才能发现人类未知的好棋。

这也是后来 **AlphaGo Zero** 采用纯自我对弈训练的原因之一。

**对 MCTS 的肯定**：

第 37 手证明了 MCTS 深度搜索的价值。即使直觉（Policy Network）不看好一步棋，深度分析也能发现它的潜在价值。

这个洞见后来被应用到许多其他领域。

---

## 技术细节：重现第 37 手的决策过程

### Policy Network 的输入特征

在第 36 手后，Policy Network 的输入包括：

| 特征平面 | 描述 |
|----------|------|
| 1-8 | 黑棋位置（过去 8 步） |
| 9-16 | 白棋位置（过去 8 步） |
| 17 | 当前该谁下 |
| 18-48 | 其他特征（气数、叫吃等） |

总计 **48 个 19x19 的特征平面**，构成输入张量。

🎬 C23：特征工程在 AI 围棋中的重要性

### Policy Network 的输出

Policy Network 输出一个 **19x19 = 361** 维的概率分布。

对于第 37 手的局面：

```python
# 前 5 名候选位置（简化示意）
{
    "R3": 0.12,   # 右下角挂角
    "Q17": 0.10,  # 右上角
    "C10": 0.09,  # 左边大场
    "K15": 0.08,  # 第 37 手的位置
    "D16": 0.07,  # 左上角
    # ... 其他 356 个位置
}
```

### MCTS 的探索过程

AlphaGo 使用 PUCT 公式来平衡探索与利用：

```
U(s,a) = Q(s,a) + c_puct × P(s,a) × sqrt(sum_b N(s,b)) / (1 + N(s,a))
```

其中：
- `Q(s,a)`：位置 a 的平均价值
- `P(s,a)`：Policy Network 给出的概率
- `N(s,a)`：该位置被探索的次数
- `c_puct`：探索常数

对于第 37 手，虽然初始概率 P 较低，但经过多次模拟后，Q 值不断提高，最终超过了其他候选位置。

🎬 C25：PUCT 公式如何发现非直觉的好棋

### 模拟次数的影响

DeepMind 团队后来分析，第 37 手的「发现」需要足够的模拟次数：

| 模拟次数 | 最佳选择 |
|----------|---------|
| 100 | R3（右下角） |
| 1,000 | Q17（右上角） |
| 10,000 | K15（第 37 手） |
| 100,000 | K15（更确定） |

这说明：**深度搜索能够发现浅层搜索无法找到的好棋**。

---

## 哲学思考：人类与 AI 的认知差异

### 为什么人类想不到第 37 手？

这是一个深刻的问题。可能的原因包括：

**1. 经验的局限**

人类棋手的知识来自学习前人的棋谱。如果前人从来没有下过某种棋，我们就不会去考虑它。

**2. 直觉的偏见**

人类的直觉是有用的，但也是有局限的。我们的直觉会让我们「看不见」某些选项。

**3. 计算能力的差异**

第 37 手的价值需要经过深度计算才能发现。人类的计算能力有限，无法像 AI 那样模拟数千种可能。

🎬 C27：认知偏见与 AI 的超越

### 机器的「直觉」是什么？

AlphaGo 有「直觉」吗？

从某种意义上说，Policy Network 就是 AlphaGo 的「直觉」——它可以在毫秒内评估每个位置的潜力。

但这种「直觉」与人类的直觉不同：
- **人类的直觉**：来自经验和模式识别
- **AI 的直觉**：来自大量数据的统计学习

有趣的是，第 37 手证明了：**AI 的「直觉」可以被 MCTS 修正**。这意味着 AI 能够「反思」自己的直觉，找到更好的选择。

### 人类能从 AI 学到什么？

第 37 手给人类棋手的最大启示可能是：

> **不要让经验成为枷锁**

很多「不好」的棋，可能只是我们不理解。打开心态，愿意尝试非传统的下法，可能会发现新的可能性。

这个启示不只适用于围棋，也适用于人生的许多领域。

---

## 动画对应

本文涉及的核心概念与动画编号：

| 编号 | 概念 | 物理/数学对应 |
|------|------|--------------|
| 🎬 C3 | 传统棋理的价值判断 | 启发式函数 |
| 🎬 C5 | 肩冲的几何特性 | 空间关系 |
| 🎬 C7 | 专家直觉与 AI 评估的差距 | 预测误差 |
| 🎬 C9 | Policy Network 的输出分布 | Softmax 概率 |
| 🎬 C11 | MCTS 如何修正 Policy Network | 贝氏更新 |
| 🎬 C13 | Value Network 的增量评估 | 价值函数 |
| 🎬 C15 | 全局价值函数的计算 | 积分近似 |
| 🎬 C17 | 博弈论中的强制选择 | 优势策略 |
| 🎬 C19 | 一步棋改变整盘棋的走向 | 分岔点 |
| 🎬 C21 | AI 如何拓展人类的认知边界 | 搜索空间扩展 |
| 🎬 C23 | 特征工程在 AI 围棋中的重要性 | 表示学习 |
| 🎬 C25 | PUCT 公式如何发现非直觉的好棋 | 探索-利用权衡 |
| 🎬 C27 | 认知偏见与 AI 的超越 | 无偏估计 |

---

## 延伸阅读

- **上一篇**：[关键对局回顾](../key-matches) — 樊麾、李世乭、柯洁的完整对局历史
- **下一篇**：[围棋为什么难？](../why-go-is-hard) — 理解围棋的计算复杂度
- **技术细节**：[Policy Network 详解](../policy-network) — 深入理解直觉网络
- **进阶阅读**：[PUCT 公式详解](../puct-formula) — 探索与利用的数学

---

## 互动探索

### Policy Network 概率分布

使用下方的互动视觉化，探索不同局面下 Policy Network 的输出：

<PolicyHeatmap initialPosition="move37" size={450} showTopN={5} interactive={true} />

尝试切换不同的预设局面，观察 AI 如何评估各个位置的落子概率。

---

## 参考资料

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. DeepMind Blog: "AlphaGo: The story so far"
3. 《AlphaGo》纪录片 (2017)，导演 Greg Kohs。
4. 李世乭 vs AlphaGo 第二盘官方棋谱
5. Go4Go.net 专业棋谱分析
6. 韩国棋院赛后技术报告
