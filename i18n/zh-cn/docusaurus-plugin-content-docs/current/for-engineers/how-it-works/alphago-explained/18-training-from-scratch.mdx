---
sidebar_position: 19
title: 从零训练的过程
description: 见证 AlphaGo Zero 如何在三天内从随机乱下到超越人类，重新发现并超越千年棋理
keywords: [AlphaGo Zero, 训练过程, 自我对弈, 棋力成长, 围棋 AI, 深度学习]
---

import { EloChart } from '@site/src/components/D3Charts';

# 从零训练的过程

AlphaGo Zero 最令人惊叹的不只是最终的棋力，而是它的**成长过程**——从完全随机的状态开始，在短短三天内经历了人类数千年才完成的围棋知识积累，然后超越人类所有的理解。

本文将带你一步步见证这个惊人的蜕变过程。

---

## 训练曲线

首先，让我们看看 AlphaGo Zero 的棋力成长曲线：

<EloChart mode="zero" width={700} height={400} />

这条曲线展示了 AlphaGo Zero 在 72 小时内的棋力变化。注意几个关键里程碑：

| 时间 | ELO 评分 | 相当于 |
|------|----------|--------|
| 0 小时 | 0 | 随机乱下 |
| 3 小时 | ~1000 | 发现基本规则 |
| 12 小时 | ~3000 | 发现定式和棋形 |
| 36 小时 | ~4500 | 超越樊麾版 AlphaGo |
| 60 小时 | ~5200 | 超越李世石版 AlphaGo |
| 72 小时 | ~5400 | 超越所有先前版本 |

**三天，从零到超越人类顶峰。**

---

## Day 0：混沌的开始

### 完全随机的初始状态

训练开始时，神经网络的权重是随机初始化的。这意味着：

- **Policy Head**：输出接近均匀分布，每个位置的落子概率约为 1/361
- **Value Head**：输出接近 0，无法区分好局面和坏局面

此时的 AlphaGo Zero 下棋完全是乱下——比一个从未见过棋盘的人还要差。

### 第一局自我对弈

想象一下第一局自我对弈是什么样子：

```
黑 1：随机落在某处（可能是天元，可能是角上，可能是一线）
白 2：随机落在另一处
黑 3：随机...
...
第 200 手：棋盘上到处是孤立的棋子，没有任何连接
最终：胜负由随机因素决定
```

这局棋的「品质」极低，但它包含了宝贵的信息：**最终谁赢了**。

### 第一个训练信号

虽然双方都在乱下，但胜负结果是确定的。神经网络开始学习：

> 「在这个局面下，最终黑方赢了。虽然我不知道为什么，但这个局面对黑方可能比较好。」

这是一个非常弱的信号，但它是真实的。经过数千局这样的「垃圾棋」之后，网络开始发现一些统计规律。

---

## Hour 1-3：发现游戏规则

### 涌现的规则意识

经过数万局自我对弈后，AlphaGo Zero 开始「发现」围棋的基本规则（虽然这些规则早就内建在游戏引擎中）：

#### 1. 连接的重要性

```
观察：当棋子相连时，比较不容易被吃掉
学习：开始优先在已有棋子旁边落子
```

这不是被教导的，而是从胜负结果中学到的。散落的棋子容易被各个击破，连成一片的棋子更容易存活。

#### 2. 气的概念

```
观察：当棋子的邻接空点都被占据时，棋子会消失
学习：开始避免气很少的位置，开始攻击对手气少的棋子
```

网络学会了追踪气数——虽然输入中没有明确的「气数」特征，但从历史棋盘状态中可以推断出来。

#### 3. 眼的雏形

```
观察：某些形状特别难被吃掉
学习：开始在角落和边上形成有空间的形状
```

这是活棋概念的萌芽。网络发现，有内部空间的棋子群更容易存活。

### 棋力评估

此时的 AlphaGo Zero 大约是：
- **ELO**：~1000
- **相当于**：刚学会规则的初学者
- **特征**：知道要连接棋子，知道要吃对方的棋

---

## Hour 3-12：发现定式与棋形

### 角部的觉醒

经过更多训练，网络发现了角部的重要性：

```
观察：角部的棋子只需要 2 个眼就能活
     边上需要 2 个眼较难
     中央需要 2 个眼最难
学习：开局时优先占角
```

这就是人类棋理中「金角银边草肚皮」的发现过程。网络没有被告知这个原则，而是从数十万局对弈中自己发现的。

### 定式的涌现

更令人惊奇的是，网络开始「发明」定式——双方在角部的标准下法：

#### 观察到的现象

```
训练初期：角部下法五花八门
训练中期：某些下法反复出现
训练后期：形成稳定的角部定式
```

这些定式与人类数百年累积的定式**高度相似**，验证了这些定式确实是双方最优解的近似。

### 典型的涌现定式

以小目定式为例：

```
  A B C D E F G H J
9 . . . . . . . . .
8 . . . . . . . . .
7 . . . . . . . . .
6 . . . ● . . . . .   ● = 黑
5 . . . . . . . . .   ○ = 白
4 . . . ○ . ● . . .
3 . . . . . . . . .
2 . . . . . . . . .
1 . . . . . . . . .
```

黑方占据小目，白方挂角，黑方夹击——这个序列在训练过程中自然涌现。

### 棋形知识

除了定式，网络也学会了好形与坏形的区别：

| 形状 | 人类评价 | Zero 的学习 |
|------|----------|-------------|
| 空三角 | 愚形 | 逐渐避免 |
| 虎口 | 好形 | 逐渐偏好 |
| 双飞燕 | 经典攻击形 | 自然发现 |
| 镇神头 | 强力攻击 | 自然发现 |

### 棋力评估

此时的 AlphaGo Zero：
- **ELO**：~3000
- **相当于**：业余高段
- **特征**：有基本的定式知识，懂得基本棋形

---

## Hour 12-36：棋理的成熟

### 全局观的形成

进入第二天，网络开始展现出**全局观**：

#### 势力与实地

```
观察：围住空间可以得到目数
     但势力也有价值——可以攻击对方
学习：在取地和取势之间寻找平衡
```

这是围棋中最深奥的概念之一。网络学会了评估「虚」和「实」的价值。

#### 厚薄判断

```
观察：「厚」的棋可以支援远处的战斗
     「薄」的棋需要补强，否则会被攻击
学习：主动建立厚势，攻击对方的薄弱
```

### 中盘战术

网络的中盘战斗能力大幅提升：

| 技术 | 描述 |
|------|------|
| 攻击弱棋 | 识别对方的孤棋，发动攻势 |
| 利用厚味 | 用厚势支援攻击，获得利益 |
| 转换 | 放弃局部损失，换取全局优势 |
| 打入 | 侵消对方的模样 |

### 官子技巧

收官阶段的精确计算也在提升：

```
观察：官子阶段每一手的价值可以精确计算
学习：按照价值大小依序收官
```

网络学会了「双方先手」「单方先手」「后手」等官子概念。

### 棋力评估

此时的 AlphaGo Zero：
- **ELO**：~4500
- **相当于**：职业棋手水平
- **特征**：有完整的围棋理解，能下出高品质的对局

---

## Hour 36-72：超越人类

### 突破职业水平

在 36 小时左右，AlphaGo Zero 的棋力达到了职业棋手水平。但训练并没有停止——它继续自我对弈，继续提升。

接下来发生的事情更加有趣：**它开始发现人类从未想过的下法**。

### 颠覆性的开局

传统围棋开局有许多「定见」：

| 传统观点 | AlphaGo Zero 的发现 |
|----------|---------------------|
| 开局先占角 | 某些情况下先占边更好 |
| 小目最稳健 | 三三直接占角可行 |
| 定式要记熟 | 可以主动偏离定式 |
| 点三三太早贪 | 某些局面下点三三正确 |

这些「发现」在 AlphaGo 之后被人类职业棋手广泛研究，许多已经被纳入现代棋理。

### 反直觉的棋形

AlphaGo Zero 有时会下出人类认为「不好看」的形状：

```
人类：「这是愚形，不可能是好棋」
Zero：（下了那步棋）
分析后：「原来这样更有效率」
```

这揭示了人类棋理的局限：有些「坏形」其实是特定局面下的最优解。

### 激进的弃子

Zero 比人类更愿意弃子换取其他利益：

```
局部亏损 3 目
全局获得主动权
最终胜率提升
```

人类棋手往往过度在意局部得失，而 Zero 始终盯着最终胜率。

### 棋力评估

72 小时后的 AlphaGo Zero：
- **ELO**：~5400
- **相当于**：超越所有人类棋手
- **特征**：发现人类未知的下法，创造新的棋理

---

## 重新发现人类棋理

### 数千年 vs. 三天

人类围棋发展了数千年：
- 公元前 2000 年左右起源于中国
- 唐朝传入日本，发展出精密的棋理
- 20 世纪出现职业体系，棋理进一步深化
- 2016 年，人类认为已经相当理解围棋

AlphaGo Zero 用三天走完了这段路程。更惊人的是，它发现的棋理与人类的**高度一致**。

### 验证与超越

| 人类知识 | Zero 的态度 |
|----------|-------------|
| 金角银边草肚皮 | 确认（角落确实重要） |
| 基本定式 | 大部分确认，少数改进 |
| 好形坏形 | 大部分确认，特例存在 |
| 弃子转换 | 比人类更激进 |
| 厚薄判断 | 大致一致，细节不同 |

这说明人类数千年累积的棋理**大方向是正确的**。但也有一些领域，人类的理解需要修正。

### 对人类学习的启示

AlphaGo Zero 的训练过程给人类学习带来启示：

1. **从基础开始**：Zero 先学会规则，再学会棋形，最后发展全局观
2. **大量练习**：490 万局自我对弈相当于数十万年的人类对局量
3. **专注胜负**：不追求「漂亮的棋」，只追求赢
4. **不受传统束缚**：敢于尝试「不可能」的下法

---

## 训练过程的技术细节

### 自我对弈的机制

每一局自我对弈的流程：

```
初始化：空棋盘
↓
每一步：
  1. 用神经网络评估当前局面
  2. 执行 MCTS 搜索（1600 次模拟）
  3. 根据搜索结果选择落子
  4. 记录 (局面, MCTS概率, -)
↓
游戏结束：
  1. 判定胜负 z ∈ {-1, +1}
  2. 为所有记录补上胜负 (局面, MCTS概率, z)
  3. 将数据加入训练池
```

### 训练的节奏

AlphaGo Zero 的训练是**持续进行**的：

```
Self-play Workers:       不断产生自我对弈数据
Training Workers:        不断从数据池取样训练
Network Updates:         定期更新自我对弈用的网络
```

这三个过程同时进行，形成一个持续改进的循环。

### 数据池管理

训练数据池的管理：

| 参数 | 值 |
|------|-----|
| 池大小 | 最近 50 万局 |
| 每局样本 | ~200 步 |
| 总样本数 | ~1 亿 |
| 取样方式 | 均匀随机 |

旧的数据会被新数据替换，确保训练数据反映当前网络的水平。

### 网络更新策略

不是每训练一步就更新自我对弈的网络。而是：

1. 训练一段时间后，产生候选网络
2. 用候选网络对战当前网络（400 局）
3. 如果候选网络胜率 > 55%，更新
4. 否则继续训练

这确保了自我对弈始终使用**足够强**的网络。

---

## 学习速度的分析

### 为什么这么快？

AlphaGo Zero 学习速度惊人的原因：

#### 1. 计算资源

- 4 个 TPU，每秒数万次推理
- 每天产生数十万局自我对弈
- 相当于人类数千年的对局量

#### 2. 完美的对手

自我对弈意味着：
- 对手水平始终与自己相当
- 不会太弱（学不到东西）也不会太强（无法获胜）
- 这是理想的学习条件

#### 3. 直接的目标

只有一个目标：赢。没有：
- 老师的偏好
- 风格的追求
- 美学的考量

#### 4. 高效的表示学习

残差网络能够学习非常抽象的棋盘特征，比手工设计的特征更有效。

### 与人类的对比

| 方面 | 人类 | AlphaGo Zero |
|------|------|--------------|
| 学习速度 | 每天 ~10 局 | 每天 ~100,000 局 |
| 记忆保留 | 有遗忘 | 完美保留 |
| 精力限制 | 需要休息 | 24/7 运行 |
| 创新能力 | 受传统影响 | 无预设限制 |

---

## 训练过程中的有趣现象

### 阶段性停滞

训练曲线不是完全平滑的，有时会出现**停滞期**：

```
ELO: 2000 -----> 2000 -----> 2500 ---->
          (停滞)       (突破)
```

这可能是因为网络在学习某个新概念，需要时间「消化」。

### 策略的涌现和消失

某些策略会在训练过程中涌现，然后又消失：

```
阶段 1：发现某个攻击手段
阶段 2：对手学会防守
阶段 3：该手段使用频率降低
阶段 4：发现新的攻击手段
```

这是军备竞赛的缩影。

### 「重新发明轮子」

训练过程中，Zero 会「重新发明」人类已知的概念：

- **征子**：发现连续叫吃可以吃掉棋子
- **倒脱靴**：发现可以先送子再反杀
- **打劫**：发现回避规则的利用方式

这些发现的顺序与人类学棋的顺序类似。

---

## 动画对应

本文涉及的核心概念与动画编号：

| 编号 | 概念 | 物理/数学对应 |
|------|------|--------------|
| 🎬 E12 | 棋力成长曲线 | S 型增长（逻辑斯蒂） |
| 🎬 E7 | 从零开始 | 自组织现象 |
| 🎬 E5 | 自我对弈 | 不动点收敛 |
| 🎬 F8 | 涌现能力 | 相变 |

---

## 延伸阅读

- **上一篇**：[双头网络与残差网络](../dual-head-resnet) — 支撑这一切的神经网络架构
- **下一篇**：[分布式系统与 TPU](../distributed-systems) — 让这一切成为可能的硬件
- **相关文章**：[自我对弈](../self-play) — 为什么自我对弈如此有效

---

## 参考资料

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
3. DeepMind. (2017). "AlphaGo Zero: Learning from scratch." *YouTube*.
4. Wang, F., et al. (2019). "A Survey on the Evolution of AlphaGo." *arXiv:1907.11180*.
