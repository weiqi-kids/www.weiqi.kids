---
sidebar_position: 9
title: Value Network è¯¦è§£
description: æ·±å…¥ç†è§£ AlphaGo çš„ä»·å€¼ç½‘ç»œæ¶æ„ã€è®­ç»ƒæŒ‘æˆ˜ä¸åœ¨ MCTS ä¸­çš„å…³é”®ä½œç”¨
---

# Value Network è¯¦è§£

å¦‚æœè¯´ Policy Network å‘Šè¯‰ AlphaGoã€Œä¸‹ä¸€æ­¥åº”è¯¥ä¸‹å“ªé‡Œã€ï¼Œé‚£ä¹ˆ Value Network å›ç­”çš„æ˜¯ä¸€ä¸ªæ›´æ ¹æœ¬çš„é—®é¢˜ï¼š

> **ã€Œè¿™ç›˜æ£‹ï¼Œæˆ‘ä¼šèµ¢å—ï¼Ÿã€**

---

## ä»€ä¹ˆæ˜¯ Value Networkï¼Ÿ

### æ ¸å¿ƒåŠŸèƒ½

Value Network æ˜¯ä¸€ä¸ªæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼Œå®ƒçš„ä»»åŠ¡æ˜¯ï¼š

> **ç»™å®šå½“å‰æ£‹ç›˜çŠ¶æ€ï¼Œé¢„æµ‹æœ€ç»ˆçš„èƒœç‡**

ç”¨æ•°å­¦è¡¨ç¤ºï¼š

```
v = f_Î¸(s)
```

å…¶ä¸­ï¼š
- `s`ï¼šå½“å‰æ£‹ç›˜çŠ¶æ€
- `f_Î¸`ï¼šValue Networkï¼ˆÎ¸ æ˜¯ç½‘ç»œå‚æ•°ï¼‰
- `v`ï¼šä¸€ä¸ªä»‹äº -1 åˆ° +1 ä¹‹é—´çš„æ•°å€¼

### è¾“å‡ºçš„å«ä¹‰

| è¾“å‡ºå€¼ | å«ä¹‰ |
|--------|------|
| +1 | å½“å‰ç©å®¶å¿…èƒœ |
| +0.5 | å½“å‰ç©å®¶çº¦ 75% èƒœç‡ |
| 0 | åŒæ–¹èƒœç‡ç›¸ç­‰ |
| -0.5 | å½“å‰ç©å®¶çº¦ 25% èƒœç‡ |
| -1 | å½“å‰ç©å®¶å¿…è´¥ |

### ä¸ºä»€ä¹ˆéœ€è¦å•ä¸€æ•°å€¼ï¼Ÿ

#### æ¯”è¾ƒä¸åŒé€‰æ‹©

åœ¨ä¸‹æ£‹æ—¶ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦åœ¨å¤šä¸ªé€‰é¡¹ä¸­åšé€‰æ‹©ã€‚Value Network è®©è¿™ä¸ªæ¯”è¾ƒå˜å¾—ç®€å•ï¼š

```
é€‰é¡¹ A çš„å±€é¢ä»·å€¼ï¼š0.3
é€‰é¡¹ B çš„å±€é¢ä»·å€¼ï¼š0.5
é€‰é¡¹ C çš„å±€é¢ä»·å€¼ï¼š0.2

â†’ é€‰æ‹© Bï¼ˆæœ€é«˜çš„ä»·å€¼ï¼‰
```

å¦‚æœæ²¡æœ‰å•ä¸€æ•°å€¼ï¼Œæˆ‘ä»¬å¦‚ä½•æ¯”è¾ƒã€Œåƒæ‰å¯¹æ–¹ä¸€å—æ£‹ã€å’Œã€Œå›´ä½ä¸€å¤§å—ç©ºã€å“ªä¸ªæ›´å¥½ï¼Ÿ

#### å–ä»£å¤§é‡æ¨¡æ‹Ÿ

åœ¨ä¼ ç»Ÿçš„è’™åœ°å¡ç½—æ ‘æœç´¢ä¸­ï¼Œè¯„ä¼°ä¸€ä¸ªå±€é¢éœ€è¦è¿›è¡Œ **éšæœºæ¨¡æ‹Ÿï¼ˆrolloutï¼‰**ï¼š

1. ä»å½“å‰å±€é¢å¼€å§‹
2. åŒæ–¹éšæœºä¸‹æ£‹ç›´åˆ°æ¸¸æˆç»“æŸ
3. è®°å½•èƒœè´Ÿ
4. é‡å¤æ•°åƒæ¬¡ï¼Œè®¡ç®—èƒœç‡

è¿™éå¸¸æ…¢ã€‚Value Network å¯ä»¥**ä¸€æ¬¡å‰å‘ä¼ æ’­**å°±ç»™å‡ºè¯„ä¼°ï¼Œé€Ÿåº¦å¿«å‡ ä¸ªæ•°é‡çº§ã€‚

| æ–¹æ³• | è¯„ä¼°æ—¶é—´ | ç²¾åº¦ |
|------|---------|------|
| 1000 æ¬¡éšæœºæ¨¡æ‹Ÿ | ~2000 æ¯«ç§’ | è¾ƒä½ |
| 15000 æ¬¡éšæœºæ¨¡æ‹Ÿ | ~30000 æ¯«ç§’ | ä¸­ç­‰ |
| Value Network | ~3 æ¯«ç§’ | é«˜ï¼ˆç­‰ä»·äº 15000 æ¬¡æ¨¡æ‹Ÿï¼‰ |

---

## ç½‘ç»œæ¶æ„

### ä¸ Policy Network çš„ç›¸ä¼¼æ€§

Value Network çš„æ¶æ„ä¸ Policy Network éå¸¸ç›¸ä¼¼ï¼Œéƒ½æ˜¯æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼š

```
è¾“å…¥å±‚ â†’ å·ç§¯å±‚ Ã—12 â†’ å…¨è¿æ¥å±‚ â†’ è¾“å‡º
   â†“         â†“           â†“         â†“
19Ã—19Ã—48   19Ã—19Ã—192    256ç»´     å•ä¸€æ•°å€¼
```

### è¾“å…¥å±‚

ä¸ Policy Network ç›¸åŒï¼Œè¾“å…¥æ˜¯ **19Ã—19Ã—49** çš„ç‰¹å¾å¼ é‡ï¼š

- **19Ã—19**ï¼šæ£‹ç›˜å¤§å°
- **49**ï¼š48 ä¸ªç‰¹å¾å¹³é¢ + 1 ä¸ªè¡¨ç¤ºå½“å‰è½®åˆ°è°çš„å¹³é¢

å¤šå‡ºçš„ 1 ä¸ªå¹³é¢å¾ˆé‡è¦ï¼šValue Network éœ€è¦çŸ¥é“æ˜¯è°çš„å›åˆï¼Œå› ä¸ºåŒä¸€å±€é¢å¯¹é»‘æ£‹å’Œç™½æ£‹çš„ä»·å€¼æ˜¯ç›¸åçš„ã€‚

### å·ç§¯å±‚

ä¸ Policy Network ç›¸åŒï¼š
- **12 å±‚å·ç§¯å±‚**
- **192 ä¸ªæ»¤æ³¢å™¨**
- **3Ã—3 å·ç§¯æ ¸**ï¼ˆç¬¬ä¸€å±‚ 5Ã—5ï¼‰
- **ReLU æ¿€æ´»å‡½æ•°**

### è¾“å‡ºå±‚çš„å·®å¼‚

è¿™æ˜¯ Value Network ä¸ Policy Network çš„å…³é”®å·®å¼‚ï¼š

#### Policy Network è¾“å‡º
```
19Ã—19Ã—192 â†’ 1Ã—1 å·ç§¯ â†’ 19Ã—19Ã—1 â†’ å±•å¹³ â†’ 361ç»´ â†’ Softmax â†’ æ¦‚ç‡åˆ†å¸ƒ
```

#### Value Network è¾“å‡º
```
19Ã—19Ã—192 â†’ 1Ã—1 å·ç§¯ â†’ 19Ã—19Ã—1 â†’ å±•å¹³ â†’ 361ç»´ â†’ å…¨è¿æ¥256 â†’ ReLU â†’ å…¨è¿æ¥1 â†’ Tanh â†’ å•ä¸€æ•°å€¼
```

### Tanh æ¿€æ´»å‡½æ•°

Value Network çš„æœ€åä¸€å±‚ä½¿ç”¨ **Tanh**ï¼ˆåŒæ›²æ­£åˆ‡ï¼‰å‡½æ•°ï¼š

```
Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```

Tanh çš„è¾“å‡ºèŒƒå›´æ˜¯ **(-1, +1)**ï¼Œæ­£å¥½å¯¹åº”èƒœè´Ÿã€‚

#### ä¸ºä»€ä¹ˆç”¨ Tanh è€Œé Sigmoidï¼Ÿ

Sigmoid çš„è¾“å‡ºèŒƒå›´æ˜¯ (0, 1)ï¼Œä¹Ÿå¯ä»¥è¡¨ç¤ºèƒœç‡ã€‚ä½† Tanh æœ‰å‡ ä¸ªä¼˜ç‚¹ï¼š

1. **å¯¹ç§°æ€§**ï¼šä»¥ 0 ä¸ºä¸­å¿ƒï¼Œè¾“å‡ºå¯æ­£å¯è´Ÿ
2. **æ¢¯åº¦æ›´å¥½**ï¼šåœ¨ 0 é™„è¿‘æ¢¯åº¦æ¥è¿‘ 1
3. **è¯­æ„æ¸…æ™°**ï¼šæ­£å€¼èµ¢ã€è´Ÿå€¼è¾“ã€é›¶æ˜¯å¹³å±€

### å®Œæ•´æ¶æ„å›¾

```
è¾“å…¥: 19Ã—19Ã—49
        â†“
    Conv 5Ã—5, 192 filters
        â†“
    ReLU
        â†“
    Conv 3Ã—3, 192 filters (Ã—11)
        â†“
    ReLU
        â†“
    Conv 1Ã—1, 1 filter
        â†“
    å±•å¹³ (361 ç»´)
        â†“
    å…¨è¿æ¥ (256 ç»´)
        â†“
    ReLU
        â†“
    å…¨è¿æ¥ (1 ç»´)
        â†“
    Tanh
        â†“
è¾“å‡º: [-1, +1]
```

### å‚æ•°æ•°é‡

| å±‚ | è®¡ç®— | å‚æ•°æ•°é‡ |
|---|------|---------|
| å·ç§¯å±‚ | åŒ Policy Network | ~3.9M |
| å…¨è¿æ¥å±‚ 1 | 361Ã—256 + 256 | 92,672 |
| å…¨è¿æ¥å±‚ 2 | 256Ã—1 + 1 | 257 |
| **æ€»è®¡** | | **~4.0M** |

çº¦ 400 ä¸‡ä¸ªå‚æ•°ï¼Œæ¯” Policy Network ç•¥å¤šã€‚

---

## è®­ç»ƒçš„æŒ‘æˆ˜

### è¿‡æ‹Ÿåˆé—®é¢˜

Value Network çš„è®­ç»ƒæ¯” Policy Network å›°éš¾å¾—å¤šã€‚ä¸»è¦é—®é¢˜æ˜¯**è¿‡æ‹Ÿåˆ**ã€‚

#### ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿ

è¿‡æ‹Ÿåˆæ˜¯æŒ‡æ¨¡å‹ã€Œè®°ä½ã€äº†è®­ç»ƒèµ„æ–™ï¼Œè€Œéå­¦ä¼šæ³›åŒ–ã€‚è¡¨ç°ä¸ºï¼š
- è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½
- æµ‹è¯•é›†ä¸Šè¡¨ç°å¾ˆå·®

#### ä¸ºä»€ä¹ˆ Value Network å®¹æ˜“è¿‡æ‹Ÿåˆï¼Ÿ

è€ƒè™‘ä¸€ç›˜æ£‹çš„èµ„æ–™ï¼š

```
å±€é¢ 1 â†’ å±€é¢ 2 â†’ å±€é¢ 3 â†’ ... â†’ å±€é¢ 200 â†’ ç»“æœï¼šé»‘èƒœ
```

å¦‚æœç›´æ¥ç”¨è¿™äº›èµ„æ–™è®­ç»ƒï¼š
- è¿™ 200 ä¸ªå±€é¢æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§
- å®ƒä»¬æ¥è‡ªåŒä¸€ç›˜æ£‹ï¼Œæœ‰ç›¸åŒçš„ç»“æœ
- æ¨¡å‹å¯èƒ½å­¦ä¼šã€Œè®¤å‡ºã€è¿™ç›˜æ£‹ï¼Œè€Œéç†è§£å±€é¢

DeepMind å‘ç°ï¼šå¦‚æœç”¨ç›¸åŒçš„äººç±»æ£‹è°±è®­ç»ƒ Policy å’Œ Value Networkï¼ŒValue Network ä¼šä¸¥é‡è¿‡æ‹Ÿåˆã€‚

### è§£å†³æ–¹æ¡ˆï¼šè‡ªæˆ‘å¯¹å¼ˆèµ„æ–™

DeepMind çš„è§£å†³æ–¹æ¡ˆæ˜¯ç”¨**è‡ªæˆ‘å¯¹å¼ˆ**ç”Ÿæˆæ–°çš„è®­ç»ƒèµ„æ–™ï¼š

```
1. ç”¨è®­ç»ƒå¥½çš„ RL Policy Network è‡ªæˆ‘å¯¹å¼ˆ
2. ä»æ¯ç›˜æ£‹ä¸­åªå–ä¸€ä¸ªå±€é¢ï¼ˆé¿å…ç›¸å…³æ€§ï¼‰
3. è¿™ä¸ªå±€é¢çš„æ ‡ç­¾æ˜¯è¯¥ç›˜æ£‹çš„æœ€ç»ˆç»“æœ
4. ç”Ÿæˆ 3000 ä¸‡ä¸ªè¿™æ ·çš„æ ·æœ¬
```

#### ä¸ºä»€ä¹ˆè¿™èƒ½è§£å†³è¿‡æ‹Ÿåˆï¼Ÿ

1. **èµ„æ–™é‡å¤§**ï¼š3000 ä¸‡ä¸ªç‹¬ç«‹çš„å±€é¢
2. **æ— ç›¸å…³æ€§**ï¼šæ¯ç›˜æ£‹åªå–ä¸€ä¸ªå±€é¢
3. **åˆ†å¸ƒä¸åŒ**ï¼šè‡ªæˆ‘å¯¹å¼ˆçš„å±€é¢åˆ†å¸ƒä¸åŒäºäººç±»æ£‹è°±

### è®­ç»ƒèµ„æ–™çš„äº§ç”Ÿ

```python
# ä¼ªä»£ç 
training_data = []

for game_id in range(30_000_000):
    # è‡ªæˆ‘å¯¹å¼ˆä¸€ç›˜
    states, result = self_play(rl_policy_network)

    # éšæœºé€‰å–ä¸€ä¸ªå±€é¢
    random_index = random.randint(0, len(states) - 1)
    state = states[random_index]

    # è®°å½•å±€é¢å’Œç»“æœ
    training_data.append((state, result))
```

---

## è®­ç»ƒç›®æ ‡ä¸æ–¹æ³•

### å‡æ–¹è¯¯å·®æŸå¤±

Value Network ä½¿ç”¨**å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰**ä½œä¸ºæŸå¤±å‡½æ•°ï¼š

```
L(Î¸) = (1/n) Ã— Î£ (v_Î¸(s) - z)Â²
```

å…¶ä¸­ï¼š
- `v_Î¸(s)`ï¼šæ¨¡å‹é¢„æµ‹çš„ä»·å€¼
- `z`ï¼šå®é™…ç»“æœï¼ˆ+1 æˆ– -1ï¼‰

#### ä¸ºä»€ä¹ˆç”¨ MSE è€Œéäº¤å‰ç†µï¼Ÿ

- **äº¤å‰ç†µ**é€‚åˆåˆ†ç±»é—®é¢˜ï¼ˆç¦»æ•£çš„æ ‡ç­¾ï¼‰
- **MSE** é€‚åˆå›å½’é—®é¢˜ï¼ˆè¿ç»­çš„æ•°å€¼ï¼‰

è™½ç„¶ç»“æœåªæœ‰ +1 æˆ– -1ï¼Œä½†æ¨¡å‹é¢„æµ‹çš„æ˜¯è¿ç»­å€¼ï¼ˆ-1 åˆ° +1 ä¹‹é—´çš„ä»»ä½•æ•°ï¼‰ã€‚MSE è®©æ¨¡å‹å­¦ä¼šé¢„æµ‹æ¥è¿‘ +1 æˆ– -1 çš„å€¼ã€‚

### è®­ç»ƒè¿‡ç¨‹

```python
# ä¼ªä»£ç 
for epoch in range(num_epochs):
    for batch in dataloader:
        states, outcomes = batch

        # å‰å‘ä¼ æ’­
        values = network(states)  # (batch, 1)

        # è®¡ç®—æŸå¤±ï¼ˆMSEï¼‰
        loss = mse_loss(values, outcomes)

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
```

è®­ç»ƒç»†èŠ‚ï¼š
- **ä¼˜åŒ–å™¨**ï¼šSGD with momentum
- **å­¦ä¹ ç‡**ï¼š0.003
- **æ‰¹æ¬¡å¤§å°**ï¼š32
- **è®­ç»ƒæ—¶é—´**ï¼šçº¦ 1 å‘¨ï¼ˆ50 GPUsï¼‰

---

## å‡†ç¡®åº¦åˆ†æ

### ä¸éšæœºæ¨¡æ‹Ÿçš„æ¯”è¾ƒ

DeepMind åœ¨è®ºæ–‡ä¸­è¿›è¡Œäº†è¯¦ç»†çš„æ¯”è¾ƒï¼š

| è¯„ä¼°æ–¹æ³• | é¢„æµ‹è¯¯å·® |
|---------|---------|
| 1000 æ¬¡éšæœºæ¨¡æ‹Ÿ | è¾ƒé«˜ |
| 15000 æ¬¡éšæœºæ¨¡æ‹Ÿ | ä¸­ç­‰ |
| Value Network | ä¸ 15000 æ¬¡æ¨¡æ‹Ÿç›¸å½“ |

è¿™æ„å‘³ç€ä¸€æ¬¡ Value Network è¯„ä¼° â‰ˆ 15000 æ¬¡éšæœºæ¨¡æ‹Ÿï¼Œä½†é€Ÿåº¦å¿«çº¦ 1000 å€ã€‚

### å„é˜¶æ®µçš„å‡†ç¡®åº¦

Value Network çš„å‡†ç¡®åº¦å–å†³äºæ¸¸æˆè¿›ç¨‹ï¼š

| é˜¶æ®µ | å‰©ä½™æ‰‹æ•° | é¢„æµ‹éš¾åº¦ | å‡†ç¡®åº¦ |
|------|---------|---------|--------|
| å¼€å±€ | ~300 | å¾ˆéš¾ | è¾ƒä½ |
| ä¸­ç›˜ | ~150 | å›°éš¾ | ä¸­ç­‰ |
| æ”¶å®˜ | ~50 | è¾ƒæ˜“ | è¾ƒé«˜ |
| ç»ˆå±€ | ~10 | ç®€å• | å¾ˆé«˜ |

è¿™æ˜¯ç›´è§‰ä¸Šåˆç†çš„ï¼šè¶Šæ¥è¿‘æ¸¸æˆç»“æŸï¼Œç»“æœè¶Šç¡®å®šã€‚

### è¾“å‡ºåˆ†å¸ƒ

ä¸€ä¸ªè®­ç»ƒè‰¯å¥½çš„ Value Network çš„è¾“å‡ºåˆ†å¸ƒï¼š

```
        é¢‘ç‡
          |
          |    *
          |   * *
          |  *   *
          | *     *
          |*       *
          +----+----+---- è¾“å‡ºå€¼
         -1    0   +1

å¤§å¤šæ•°è¾“å‡ºé›†ä¸­åœ¨ -1 å’Œ +1 é™„è¿‘
ï¼ˆå› ä¸ºå¤§å¤šæ•°å±€é¢æœ‰æ˜ç¡®çš„èƒœè´Ÿå€¾å‘ï¼‰
```

### ä¸ç¡®å®šçš„å±€é¢

å½“ Value Network è¾“å‡ºæ¥è¿‘ 0 æ—¶ï¼Œè¡¨ç¤ºå±€é¢éå¸¸å¤æ‚ï¼Œèƒœè´Ÿéš¾æ–™ã€‚è¿™äº›å±€é¢é€šå¸¸æ˜¯ï¼š
- å¤§å‹æˆ˜æ–—ä¸­
- åŒæ–¹åŠ¿å‡åŠ›æ•Œ
- å­˜åœ¨å¤šä¸ªå¯èƒ½çš„å˜åŒ–

åœ¨ MCTS ä¸­ï¼Œè¿™äº›èŠ‚ç‚¹ä¼šè·å¾—æ›´å¤šçš„æœç´¢èµ„æºï¼ˆå› ä¸ºä¸ç¡®å®šæ€§é«˜ï¼‰ã€‚

---

## åœ¨ MCTS ä¸­çš„ä½œç”¨

### å¶èŠ‚ç‚¹è¯„ä¼°

Value Network åœ¨ MCTS çš„ **Evaluation** é˜¶æ®µå‘æŒ¥å…³é”®ä½œç”¨ï¼š

```
MCTS æœç´¢æ ‘ï¼š

        æ ¹èŠ‚ç‚¹ (å½“å‰å±€é¢)
           /    \
         A        B
        /  \    /  \
       A1  A2  B1  B2 â† å¶èŠ‚ç‚¹
        â†“   â†“   â†“   â†“
       è¯„ä¼°  è¯„ä¼°  è¯„ä¼°  è¯„ä¼°
```

å½“ MCTS åˆ°è¾¾ä¸€ä¸ªå¶èŠ‚ç‚¹æ—¶ï¼Œéœ€è¦è¯„ä¼°è¿™ä¸ªå±€é¢çš„ä»·å€¼ã€‚æœ‰ä¸¤ç§æ–¹æ³•ï¼š

1. **éšæœºæ¨¡æ‹Ÿï¼ˆRolloutï¼‰**ï¼šä»å¶èŠ‚ç‚¹éšæœºä¸‹åˆ°æ¸¸æˆç»“æŸ
2. **Value Network è¯„ä¼°**ï¼šç›´æ¥ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹

AlphaGo ç»“åˆäº†ä¸¤è€…ï¼š

```
V(leaf) = (1-Î») Ã— V_network(leaf) + Î» Ã— V_rollout(leaf)
```

å…¶ä¸­ Î» = 0.5ï¼Œå³å„å ä¸€åŠæƒé‡ã€‚

#### ä¸ºä»€ä¹ˆè¦ç»“åˆï¼Ÿ

- **Value Network** æ›´å‡†ç¡®ï¼Œä½†å¯èƒ½æœ‰ç³»ç»Ÿæ€§åå·®
- **éšæœºæ¨¡æ‹Ÿ** è¾ƒä¸å‡†ç¡®ï¼Œä½†æä¾›ç‹¬ç«‹çš„ä¼°è®¡
- ç»“åˆä¸¤è€…å¯ä»¥äº’è¡¥

### AlphaGo Zero çš„ç®€åŒ–

åæ¥çš„ AlphaGo Zero å®Œå…¨å¼ƒç”¨äº†éšæœºæ¨¡æ‹Ÿï¼š

```
V(leaf) = V_network(leaf)
```

è¿™å¤§å¤§ç®€åŒ–äº†ç³»ç»Ÿï¼ŒåŒæ—¶æ£‹åŠ›æ›´å¼ºã€‚è¿™è¯æ˜äº† Value Network è¶³å¤Ÿå¯é ï¼Œä¸éœ€è¦éšæœºæ¨¡æ‹Ÿçš„ã€Œä¿é™©ã€ã€‚

### å›æº¯æ›´æ–°

è¯„ä¼°å®Œå¶èŠ‚ç‚¹åï¼Œè¿™ä¸ªå€¼ä¼šæ²¿è·¯å¾„å›æº¯æ›´æ–°ï¼š

```
v3 = V(leaf) = 0.6
      â†‘
A2 çš„ Q å€¼æ›´æ–°
      â†‘
A çš„ Q å€¼æ›´æ–°
      â†‘
æ ¹èŠ‚ç‚¹çš„ç»Ÿè®¡æ›´æ–°
```

æ¯ä¸ªèŠ‚ç‚¹ç»´æŠ¤çš„ Q å€¼æ˜¯æ‰€æœ‰ç»è¿‡å®ƒçš„å¶èŠ‚ç‚¹è¯„ä¼°çš„å¹³å‡ï¼š

```
Q(s, a) = (1/N(s,a)) Ã— Î£ V(leaf)
```

---

## è§†è§‰åŒ–åˆ†æ

### ä»·å€¼æ›²é¢

æƒ³åƒä¸€ä¸ªç®€åŒ–çš„ 3Ã—3 æ£‹ç›˜ã€‚Value Network å­¦åˆ°çš„æ˜¯ä¸€ä¸ªã€Œä»·å€¼æ›²é¢ã€ï¼š

**ä»·å€¼çŸ©é˜µç¤ºä¾‹ï¼ˆé»‘å­ä½ç½® vs ç™½å­ä½ç½®ï¼‰**

| é»‘\ç™½ | 1 | 2 | 3 |
|:---:|:---:|:---:|:---:|
| **1** | +0.3 | -0.1 | +0.2 |
| **2** | -0.2 | +0.5 | -0.3 |
| **3** | +0.1 | -0.2 | +0.4 |

è¿™ä¸ªæ›²é¢å‘Šè¯‰æˆ‘ä»¬æ¯ä¸ªä½ç½®ç»„åˆçš„ä»·å€¼ã€‚æ­£å€¼æœ‰åˆ©äºé»‘æ£‹ï¼Œè´Ÿå€¼æœ‰åˆ©äºç™½æ£‹ã€‚

### è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¼”å˜

éšç€è®­ç»ƒè¿›è¡Œï¼ŒValue Network çš„é¢„æµ‹é€æ¸å˜å¾—æ›´å‡†ç¡®ï¼š

**è®­ç»ƒè¿›ç¨‹ä¸­çš„é¢„æµ‹è¯¯å·®**

| è®­ç»ƒæ­¥æ•° | é¢„æµ‹è¯¯å·® |
|:---:|:---:|
| 0 | 1.0 |
| 100K | 0.6 |
| 500K | 0.15 |
| 1M | 0.1 |

è¯¯å·®ä¼šå¿«é€Ÿä¸‹é™ï¼Œç„¶åè¶‹äºç¨³å®šã€‚

### å›°éš¾å±€é¢çš„è¯†åˆ«

Value Network å¯ä»¥å¸®åŠ©è¯†åˆ«å›°éš¾å±€é¢ï¼š

| è¾“å‡º | å«ä¹‰ | åº”å¯¹ç­–ç•¥ |
|------|------|---------|
| æ¥è¿‘ +1 | å¤§ä¼˜ | ç¨³å¥ä¸‹æ³• |
| æ¥è¿‘ -1 | å¤§åŠ£ | å¯»æ‰¾ç¿»ç›˜æœºä¼š |
| æ¥è¿‘ 0 | å¤æ‚å±€é¢ | éœ€è¦æ·±å…¥è®¡ç®— |

AlphaGo ä¼šåœ¨æ¥è¿‘ 0 çš„å±€é¢æŠ•å…¥æ›´å¤šæ€è€ƒæ—¶é—´ã€‚

---

## å®ä½œè¦ç‚¹

### PyTorch å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ValueNetwork(nn.Module):
    def __init__(self, input_channels=49, num_filters=192, num_layers=12):
        super().__init__()

        # ç¬¬ä¸€å·ç§¯å±‚ï¼ˆ5Ã—5ï¼‰
        self.conv1 = nn.Conv2d(input_channels, num_filters,
                               kernel_size=5, padding=2)

        # ä¸­é—´å·ç§¯å±‚ï¼ˆ3Ã—3ï¼‰Ã—11
        self.conv_layers = nn.ModuleList([
            nn.Conv2d(num_filters, num_filters,
                     kernel_size=3, padding=1)
            for _ in range(num_layers - 1)
        ])

        # è¾“å‡ºå·ç§¯å±‚
        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)

        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(361, 256)
        self.fc2 = nn.Linear(256, 1)

    def forward(self, x):
        # x: (batch, 49, 19, 19)

        # å·ç§¯å±‚
        x = F.relu(self.conv1(x))
        for conv in self.conv_layers:
            x = F.relu(conv(x))
        x = self.conv_out(x)

        # å±•å¹³
        x = x.view(x.size(0), -1)  # (batch, 361)

        # å…¨è¿æ¥å±‚
        x = F.relu(self.fc1(x))
        x = torch.tanh(self.fc2(x))

        return x.squeeze(-1)  # (batch,)
```

### è®­ç»ƒå¾ªç¯

```python
def train_value_network(model, optimizer, states, outcomes):
    """
    states: (batch, 49, 19, 19) - æ£‹ç›˜ç‰¹å¾
    outcomes: (batch,) - æ¸¸æˆç»“æœ (+1 æˆ– -1)
    """
    # å‰å‘ä¼ æ’­
    values = model(states)  # (batch,)

    # MSE æŸå¤±
    loss = F.mse_loss(values, outcomes)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # è®¡ç®—å‡†ç¡®ç‡ï¼ˆé¢„æµ‹æ­£ç¡®çš„èƒœè´Ÿï¼‰
    predictions = (values > 0).float() * 2 - 1  # è½¬æ¢ä¸º +1/-1
    accuracy = (predictions == outcomes).float().mean()

    return loss.item(), accuracy.item()
```

### é¿å…è¿‡æ‹Ÿåˆçš„æŠ€å·§

```python
# 1. èµ„æ–™å¢å¼ºï¼ˆ8é‡å¯¹ç§°æ€§ï¼‰
def augment(state, outcome):
    augmented = []
    for rotation in [0, 90, 180, 270]:
        s = rotate(state, rotation)
        augmented.append((s, outcome))
        augmented.append((flip(s), outcome))
    return augmented

# 2. Dropout
class ValueNetworkWithDropout(ValueNetwork):
    def __init__(self, *args, dropout_rate=0.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        # ... å·ç§¯å±‚ ...
        x = self.dropout(x)  # åœ¨å…¨è¿æ¥å±‚å‰ dropout
        # ... å…¨è¿æ¥å±‚ ...

# 3. æ—©åœï¼ˆEarly Stoppingï¼‰
best_val_loss = float('inf')
patience = 10
counter = 0

for epoch in range(max_epochs):
    train_loss = train_one_epoch()
    val_loss = evaluate()

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        save_model()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping!")
            break
```

---

## ä¸ Policy Network çš„åä½œ

### äº’è¡¥å…³ç³»

Policy Network å’Œ Value Network åœ¨ AlphaGo ä¸­äº’è¡¥ï¼š

| ç½‘ç»œ | å›ç­”çš„é—®é¢˜ | è¾“å‡º | MCTS è§’è‰² |
|------|-----------|------|----------|
| Policy | ä¸‹ä¸€æ­¥ä¸‹å“ªé‡Œï¼Ÿ | æ¦‚ç‡åˆ†å¸ƒ | å¼•å¯¼æœç´¢æ–¹å‘ |
| Value | è¿™ç›˜æ£‹ä¼šèµ¢å—ï¼Ÿ | å•ä¸€æ•°å€¼ | è¯„ä¼°å¶èŠ‚ç‚¹ |

### ç»Ÿä¸€çš„åŒå¤´ç½‘ç»œ

åœ¨ AlphaGo Zero ä¸­ï¼Œè¿™ä¸¤ä¸ªç½‘ç»œè¢«åˆå¹¶æˆä¸€ä¸ª**åŒå¤´ç½‘ç»œ**ï¼š

```
       å…±äº«çš„ç‰¹å¾æå–å±‚
              |
       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
       â†“              â†“
  Policy Head    Value Head
       â†“              â†“
  361ä¸ªæ¦‚ç‡       å•ä¸€æ•°å€¼
```

è¿™ç§è®¾è®¡çš„ä¼˜ç‚¹ï¼š
- **å‚æ•°å…±äº«**ï¼šå‡å°‘è®¡ç®—é‡
- **ç‰¹å¾å…±äº«**ï¼šPolicy å’Œ Value ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾
- **è®­ç»ƒæ›´ç¨³å®š**ï¼šä¸¤ä¸ªç›®æ ‡äº’ç›¸æ­£åˆ™åŒ–

è¯¦è§ [åŒå¤´ç½‘ç»œä¸æ®‹å·®ç½‘ç»œ](../dual-head-resnet)ã€‚

---

## åŠ¨ç”»å¯¹åº”

æœ¬æ–‡æ¶‰åŠçš„æ ¸å¿ƒæ¦‚å¿µä¸åŠ¨ç”»ç¼–å·ï¼š

| ç¼–å· | æ¦‚å¿µ | ç‰©ç†/æ•°å­¦å¯¹åº” |
|------|------|--------------|
| ğŸ¬ E2 | Value Network | åŠ¿èƒ½é¢ |
| ğŸ¬ D4 | ä»·å€¼å‡½æ•° | æœŸæœ›æŠ¥é…¬ |
| ğŸ¬ C6 | å¶èŠ‚ç‚¹è¯„ä¼° | å‡½æ•°é€¼è¿‘ |
| ğŸ¬ H3 | æ—¶åºå·®åˆ† | å¼•å¯¼å­¦ä¹  |

---

## å»¶ä¼¸é˜…è¯»

- **ä¸Šä¸€ç¯‡**ï¼š[Policy Network è¯¦è§£](../policy-network) â€” ç­–ç•¥ç½‘ç»œå¦‚ä½•é€‰æ‹©ç€æ³•
- **ä¸‹ä¸€ç¯‡**ï¼š[è¾“å…¥ç‰¹å¾è®¾è®¡](../input-features) â€” 48 ä¸ªç‰¹å¾å¹³é¢è¯¦è§£
- **è¿›é˜¶ä¸»é¢˜**ï¼š[MCTS ä¸ç¥ç»ç½‘ç»œçš„ç»“åˆ](../mcts-neural-combo) â€” å®Œæ•´çš„æœç´¢æµç¨‹

---

## å…³é”®è¦ç‚¹

1. **Value Network é¢„æµ‹èƒœç‡**ï¼šè¾“å‡º -1 åˆ° +1 ä¹‹é—´çš„å•ä¸€æ•°å€¼
2. **Tanh è¾“å‡º**ï¼šç¡®ä¿è¾“å‡ºåœ¨æ­£ç¡®çš„èŒƒå›´å†…
3. **MSE æŸå¤±**ï¼šå°†é¢„æµ‹å€¼é€¼è¿‘å®é™…ç»“æœ
4. **è¿‡æ‹ŸåˆæŒ‘æˆ˜**ï¼šéœ€è¦ç”¨è‡ªæˆ‘å¯¹å¼ˆèµ„æ–™æ¥é¿å…
5. **å–ä»£éšæœºæ¨¡æ‹Ÿ**ï¼šä¸€æ¬¡è¯„ä¼° â‰ˆ 15000 æ¬¡æ¨¡æ‹Ÿ

Value Network æ˜¯ AlphaGo çš„ã€Œåˆ¤æ–­åŠ›ã€â€”â€”å®ƒè®© AI èƒ½å¤Ÿè¯„ä¼°ä»»ä½•å±€é¢çš„å¥½åï¼Œè€Œä¸éœ€è¦ç©·å°½æ‰€æœ‰å¯èƒ½ã€‚

---

## å‚è€ƒèµ„æ–™

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 551, 354-359.
3. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.
4. Tesauro, G. (1995). "Temporal difference learning and TD-Gammon." *Communications of the ACM*, 38(3), 58-68.
