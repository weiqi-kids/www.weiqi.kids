---
sidebar_position: 8
title: شرح مفصل لشبكة Policy
description: فهم عميق لمعمارية شبكة السياسة في AlphaGo، طرق التدريب والتطبيق العملي، من 13 طبقة التفافية إلى مخرجات Softmax
---

import { PolicyHeatmap } from '@site/src/components/D3Charts';

# شرح مفصل لشبكة Policy

في أي وضع من أوضاع الغو، يبلغ متوسط الحركات المشروعة 250 حركة. إذا سمحنا للكمبيوتر بالاختيار عشوائياً، لن يتمكن أبداً من لعب حركات جيدة.

كان اختراق AlphaGo في أنه تعلم "النظر إلى الرقعة ومعرفة المواقع التي تستحق النظر فيها" بلمحة واحدة.

هذه القدرة تأتي من **شبكة Policy (شبكة السياسة)**.

---

## ما هي شبكة Policy؟

### الوظيفة الأساسية

شبكة Policy هي شبكة عصبية التفافية عميقة، مهمتها:

> **بالنظر إلى حالة الرقعة الحالية، إخراج احتمالية اللعب في كل موقع**

بالتعبير الرياضي:

```
p = f_θ(s)
```

حيث:
- `s`: حالة الرقعة الحالية (رقعة 19×19 + ميزات أخرى)
- `f_θ`: شبكة Policy (θ هي معاملات الشبكة)
- `p`: توزيع احتمالي لـ 361 موقعاً (بما فيها التمرير)

### الفهم البديهي

تخيل أنك لاعب محترف. عندما ترى وضعاً ما، يقوم دماغك تلقائياً "بإضاءة" عدة مواقع مهمة — هذه هي النقاط التي تعتقد بديهياً أنها تستحق النظر.

شبكة Policy تحاكي هذه العملية بالضبط.

<PolicyHeatmap initialPosition="corner" size={400} />

تُظهر الخريطة الحرارية أعلاه مخرجات شبكة Policy. المواقع الأكثر سطوعاً هي التي يعتقد النموذج أنها تستحق اللعب فيها أكثر.

### لماذا نحتاج شبكة Policy؟

مساحة البحث في الغو كبيرة جداً. إذا بحثنا في جميع الحركات الممكنة بدون تصفية:

| الاستراتيجية | الحركات المدروسة لكل خطوة | عقد البحث لـ 10 خطوات |
|------|--------------|------------------|
| دراسة الكل | 361 | 361^10 ≈ 10^25 |
| تصفية شبكة Policy | ~20 | 20^10 ≈ 10^13 |

شبكة Policy تقلص مساحة البحث بـ **10^12 مرة** (تريليون مرة).

---

## معمارية الشبكة

### الهيكل العام

تعتمد شبكة Policy في AlphaGo على معمارية الشبكة العصبية الالتفافية العميقة (CNN):

```
طبقة المدخلات → طبقات التفافية ×12 → طبقة التفاف المخرجات → Softmax
      ↓              ↓                    ↓              ↓
   19×19×48      19×19×192            19×19×1      362 احتمال
```

### طبقة المدخلات

المدخل هو موتر ميزات بحجم **19×19×48**:
- **19×19**: حجم الرقعة
- **48**: 48 مستوى ميزات (راجع [تصميم ميزات المدخلات](../input-features))

هذه الـ 48 مستوى تتضمن:
- مواقع الأسود، مواقع الأبيض
- تاريخ آخر 8 حركات
- ميزات الحريات، الأتاري، السلم
- المشروعية (أي المواقع يمكن اللعب فيها)

### الطبقات الالتفافية

تحتوي الشبكة على **12 طبقة التفافية**، إعدادات كل طبقة:

| المعامل | القيمة | الشرح |
|------|------|------|
| عدد المرشحات | 192 | كل طبقة تُخرج 192 خريطة ميزات |
| حجم النواة | 3×3 (الأولى 5×5) | ترى منطقة 3×3 كل مرة |
| طريقة الحشو | same | تحافظ على حجم 19×19 |
| دالة التفعيل | ReLU | max(0, x) |

#### لماذا 192 مرشحاً؟

هذه قيمة تجريبية. قليل جداً يحد من سعة النموذج، كثير جداً يزيد الحسابات وخطر الإفراط في التعلم. حدد فريق DeepMind من خلال التجارب أن 192 هي نقطة توازن جيدة.

#### لماذا نواة التفاف 3×3؟

3×3 هو الحجم الأكثر شيوعاً في الشبكات العصبية الالتفافية، الأسباب:
1. **كافية لالتقاط الأنماط المحلية**: العين، الوصل، القطع في الغو كلها ضمن نطاق 3×3
2. **كفاءة حسابية عالية**: مقارنة بالنوى الكبيرة، 3×3 لها معاملات أقل
3. **قابلة للتراكم**: عدة طبقات 3×3 يمكنها تحقيق مجال استقبال كبير

#### لماذا الطبقة الأولى تستخدم 5×5؟

الطبقة الأولى تستخدم نواة 5×5 الأكبر لالتقاط أنماط بنطاق أوسع قليلاً من طبقة المدخلات (مثل الطيران الصغير، القفز). هذا خيار تصميمي، AlphaGo Zero اللاحق وحّد الاستخدام إلى 3×3.

### دالة التفعيل ReLU

بعد كل طبقة التفافية توجد دالة تفعيل ReLU (Rectified Linear Unit):

```
ReLU(x) = max(0, x)
```

لماذا نستخدم ReLU؟

1. **حسابات بسيطة**: مجرد أخذ القيمة الأكبر، أسرع بكثير من sigmoid
2. **تخفيف تلاشي التدرج**: التدرج في المنطقة الموجبة دائماً 1
3. **تفعيل متناثر**: القيم السالبة تصبح صفراً، ينتج تمثيل متناثر

### طبقة المخرجات

الطبقة الأخيرة هي طبقة التفاف خاصة:

```
19×19×192 → التفاف(1×1، مرشح واحد) → 19×19×1 → تسطيح → متجه 362 بُعد → Softmax
```

#### التفاف 1×1

طبقة المخرجات تستخدم التفاف 1×1، لضغط 192 قناة إلى قناة واحدة. هذا يعادل تركيباً خطياً لميزات 192 بُعد لكل موقع.

#### مخرجات Softmax

المتجه 362 بُعد (361 موقع على الرقعة + 1 تمرير) يمر عبر دالة Softmax:

```
Softmax(z_i) = exp(z_i) / Σ_j exp(z_j)
```

Softmax تضمن أن المخرجات توزيع احتمالي صحيح:
- جميع القيم بين 0 و 1
- مجموع جميع القيم يساوي 1

### عدد المعاملات

لنحسب إجمالي معاملات الشبكة:

| الطبقة | الحساب | عدد المعاملات |
|---|------|---------|
| طبقة الالتفاف الأولى | 5×5×48×192 + 192 | 230,592 |
| الطبقات الالتفافية الوسطى ×11 | (3×3×192×192 + 192) × 11 | 3,633,792 |
| طبقة التفاف المخرجات | 1×1×192×1 + 1 | 193 |
| **المجموع** | | **~3.9M** |

حوالي **3.9 مليون معامل**، وهي شبكة صغيرة بمعايير اليوم.

---

## هدف التدريب والطرق

### بيانات التدريب

يتم تدريب شبكة Policy باستخدام **التعلم الخاضع للإشراف**، التعلم من سجلات المباريات البشرية.

مصادر البيانات:
- **KGS Go Server**: مباريات لاعبين هواة ومحترفين
- **حوالي 30 مليون وضع**: مأخوذة من 160,000 مباراة
- **التسميات**: الحركة التالية للإنسان لكل وضع

### دالة خسارة الإنتروبيا المتقاطعة

هدف التدريب هو تعظيم احتمال التنبؤ بحركة الإنسان. باستخدام دالة خسارة الإنتروبيا المتقاطعة:

```
L(θ) = -Σ log p_θ(a | s)
```

حيث:
- `s`: حالة الرقعة
- `a`: الموقع الذي لعب فيه الإنسان فعلياً
- `p_θ(a | s)`: احتمال تنبؤ النموذج لذلك الموقع

#### الفهم البديهي

لخسارة الإنتروبيا المتقاطعة معنى بسيط:

> **كلما ارتفع احتمال تنبؤ النموذج بالموقع الصحيح، انخفضت الخسارة**

إذا لعب الإنسان في K10، واحتمال النموذج لـ K10 هو:
- 0.9 → الخسارة = -log(0.9) ≈ 0.1 (منخفضة جداً، جيد)
- 0.1 → الخسارة = -log(0.1) ≈ 2.3 (مرتفعة، سيء)
- 0.01 → الخسارة = -log(0.01) ≈ 4.6 (مرتفعة جداً، سيء جداً)

### عملية التدريب

```python
# كود زائف
for epoch in range(num_epochs):
    for batch in dataloader:
        states, actions = batch

        # الانتشار الأمامي
        policy = network(states)  # متجه احتمالات 361 بُعد

        # حساب الخسارة (الإنتروبيا المتقاطعة)
        loss = cross_entropy(policy, actions)

        # الانتشار العكسي
        loss.backward()
        optimizer.step()
```

تفاصيل التدريب:
- **المُحسّن**: SGD with momentum
- **معدل التعلم**: ابتدائي 0.003، يتناقص تدريجياً
- **حجم الدفعة**: 16
- **وقت التدريب**: حوالي 3 أسابيع (50 GPU)

### زيادة البيانات

رقعة الغو لها 8 تناظرات (4 دورانات × 2 انعكاس). كل عينة تدريب يمكن تحويلها إلى 8 عينات متكافئة:

```
الأصلي → دوران 90° → دوران 180° → دوران 270°
   ↓        ↓            ↓             ↓
قلب أفقي → ...
```

هذا يزيد بيانات التدريب الفعالة 8 أضعاف، ويضمن أن الأنماط التي يتعلمها النموذج لا تعتمد على الاتجاه.

---

## نتائج التدريب

### دقة 57%

بعد التدريب، حققت شبكة Policy **دقة top-1 بنسبة 57%**.

هذا يعني: لأي وضع معطى، النموذج لديه فرصة 57% للتنبؤ بالحركة التي لعبها الخبير البشري فعلياً.

#### هل هذه الدقة عالية؟

بالنظر إلى أن كل وضع لديه في المتوسط 250 حركة مشروعة، دقة التخمين العشوائي هي 0.4% فقط.

| الطريقة | دقة Top-1 |
|------|-------------|
| التخمين العشوائي | 0.4% |
| أقوى برنامج غو سابقاً | ~44% |
| شبكة Policy في AlphaGo | **57%** |

تحسن 13 نقطة مئوية يبدو قليلاً، لكنه ذو أهمية كبيرة.

### تحسين قوة اللعب

ما مستوى اللعب الذي يمكن الوصول إليه باستخدام شبكة Policy فقط (بدون بحث)؟

| الإعداد | تصنيف Elo | المستوى التقريبي |
|------|---------|---------|
| أقوى برنامج سابقاً (Pachi) | 2,500 | هاوٍ 4-5 دان |
| شبكة Policy وحدها | 2,800 | هاوٍ 6-7 دان |
| + MCTS 1600 محاكاة | 3,200+ | مستوى محترف |

شبكة Policy وحدها هي بالفعل هاوٍ عالي الدان، ومع إضافة MCTS تقفز إلى المستوى المحترف.

### لماذا 57% فقط؟

سجلات المباريات البشرية لها الخصائص التالية التي تحد الدقة:

#### 1. حركات جيدة متعددة

كثير من الأوضاع لها عدة حركات جيدة. مثلاً "مقاربة الزاوية" و"الدفاع عن الزاوية" قد يكونان كلاهما صحيحين. إذا اختار النموذج حركة جيدة أخرى، تُحتسب "خطأ".

#### 2. اختلاف الأساليب

اللاعبون المختلفون لهم أساليب مختلفة. اللاعب العدواني واللاعب الحذر قد يلعبان بشكل مختلف في نفس الوضع. النموذج يتعلم الأسلوب "المتوسط".

#### 3. البشر يخطئون أيضاً

بيانات KGS تتضمن مباريات لاعبين هواة، خياراتهم ليست بالضرورة الأفضل. من الطبيعي أن يتعلم النموذج بعض "الأخطاء".

---

## الدور في MCTS

تلعب شبكة Policy دورين رئيسيين في MCTS الخاص بـ AlphaGo:

### 1. توجيه اتجاه البحث

في مرحلة **Selection** من MCTS، يُستخدم ناتج شبكة Policy لحساب UCB (Upper Confidence Bound):

```
UCB(s, a) = Q(s, a) + c_puct × P(s, a) × √(N(s)) / (1 + N(s, a))
```

حيث `P(s, a)` هو الاحتمال الذي تعطيه شبكة Policy.

هذا يعني:
- **الحركات ذات الاحتمال العالي تُستكشف أولاً**
- **الحركات ذات الاحتمال المنخفض لديها فرصة للاستكشاف أيضاً** (بسبب حد الاستكشاف)

### 2. الاحتمال المسبق لتوسيع العقد

عندما يوسع MCTS عقدة جديدة، توفر شبكة Policy **الاحتمال المسبق** لجميع العقد الفرعية.

```
توسيع العقدة s:
  for each action a:
    child = Node()
    child.prior = policy_network(s)[a]  # الاحتمال المسبق
    child.value = 0
    child.visits = 0
```

هذه الاحتمالات المسبقة تجعل MCTS "يعرف" أي العقد الفرعية تستحق الاستكشاف أكثر، حتى لو لم تُزَر بعد.

---

## النسخة الخفيفة مقابل النسخة الكاملة

في الواقع، AlphaGo لديه شبكتا Policy:

### النسخة الكاملة (SL Policy Network)

- **المعمارية**: 13 طبقة CNN، 192 مرشح
- **الدقة**: 57%
- **وقت الاستدلال**: حوالي 3 ميلي ثانية/وضع
- **الاستخدام**: Selection و Expansion في MCTS

### النسخة الخفيفة (Rollout Policy Network)

- **المعمارية**: نموذج خطي + ميزات يدوية
- **الدقة**: 24%
- **وقت الاستدلال**: حوالي 2 ميكرو ثانية/وضع (أسرع 1500 مرة)
- **الاستخدام**: المحاكاة السريعة (rollout)

### لماذا نحتاج النسخة الخفيفة؟

في مرحلة **Simulation** من MCTS، نحتاج للعب من العقدة الحالية حتى نهاية اللعبة، قد نحتاج للعب 100+ حركة. إذا استخدمنا النسخة الكاملة من شبكة Policy لكل حركة، سيكون بطيئاً جداً.

النسخة الخفيفة رغم أن دقتها 24% فقط، لكنها أسرع 1500 مرة. في rollout، السرعة أهم من الدقة.

### ميزات النسخة الخفيفة

النسخة الخفيفة تستخدم ميزات مصممة يدوياً، تتضمن:

| نوع الميزة | أمثلة |
|---------|------|
| أنماط محلية | تكوين الأحجار في منطقة 3×3 |
| ميزات شاملة | هل في الزاوية/الحافة، النقاط الكبيرة |
| ميزات تكتيكية | أتاري، سلم، استجابة |

هذه الميزات تُدخل في نموذج خطي (بدون طبقات مخفية)، سرعة الحساب عالية جداً.

### تحسين AlphaGo Zero

لاحقاً، AlphaGo Zero تخلى تماماً عن النسخة الخفيفة و rollout. يستخدم مباشرة شبكة Value لتقييم العقد الورقية، دون حاجة للمحاكاة السريعة. هذا تبسيط كبير.

---

## الضبط بالتعلم المعزز (RL Policy Network)

### قيود التعلم الخاضع للإشراف

شبكة Policy المدربة بالتعلم الخاضع للإشراف لها مشكلة جوهرية:

> **تتعلم "تقليد البشر"، وليس "الفوز"**

هذا يعني أنها ستتعلم العادات السيئة للبشر، وستؤدي بشكل سيء في أوضاع لم يواجهها البشر من قبل.

### التعزيز باللعب الذاتي

حل DeepMind هو استخدام **تدرج السياسة** (Policy Gradient) للتعلم المعزز:

```
1. جعل شبكة Policy تلعب ضد نفسها
2. تسجيل جميع الحركات في كل مباراة
3. تعديل المعاملات حسب النتيجة:
   - فوز → زيادة احتمال هذه الحركات
   - خسارة → تقليل احتمال هذه الحركات
```

### خوارزمية REINFORCE

تُستخدم خوارزمية REINFORCE تحديداً:

```
∇J(θ) = E[Σ_t ∇log π_θ(a_t | s_t) × z]
```

حيث:
- `z`: نتيجة هذه المباراة (+1 فوز، -1 خسارة)
- `π_θ(a_t | s_t)`: احتمال اختيار الإجراء `a_t` في الحالة `s_t`

### النتائج

بعد حوالي يوم واحد من تدريب اللعب الذاتي (1.28 مليون مباراة)، شبكة RL Policy:

| المقياس | SL Policy | RL Policy |
|------|-----------|-----------|
| ضد SL Policy | 50% | **80%** |
| تحسين Elo | - | +100 |

قد تنخفض الدقة قليلاً (لأنها لم تعد تقلد البشر تماماً)، لكن نسبة الفوز الفعلية تتحسن بشكل كبير.

### من "التقليد" إلى "الابتكار"

التعلم المعزز جعل شبكة Policy تتعلم بعض الحركات التي لم يفكر فيها البشر من قبل. هذه الحركات لم تظهر قط في بيانات التدريب، لكنها فعالة.

هذا هو السبب في أن AlphaGo يمكنه لعب "الحركة الإلهية" — لأنه غير مقيد بالخبرة البشرية.

---

## تحليل مرئي

### توزيع الاحتمالات في أوضاع مختلفة

لنرى مخرجات شبكة Policy في أوضاع مختلفة:

#### الافتتاح (مرحلة التخطيط)

<PolicyHeatmap initialPosition="opening" size={400} />

في الافتتاح، الاحتمالات تتركز بشكل رئيسي في:
- الزوايا (احتلال الزوايا)
- الحواف (مقاربة الزاوية، الدفاع عن الزاوية)
- مواقع "النقاط الكبيرة"

هذا يتوافق مع المبادئ الأساسية للغو: الزوايا ذهبية، الحواف فضية، الوسط عشب.

#### وضع القتال

<PolicyHeatmap initialPosition="fighting" size={400} />

أثناء القتال، الاحتمالات تتركز في:
- نقاط القطع الحرجة
- أتاري، استجابة
- صنع العين، تدمير العين

هذا يُظهر أن النموذج تعلم التكتيكات المحلية.

#### مرحلة النهاية

<PolicyHeatmap initialPosition="endgame" size={400} />

في النهاية، الاحتمالات تتوزع على نقاط النهاية المختلفة، تحتاج حساب دقيق للنقاط.

### ماذا تتعلم الطبقات المخفية؟

من خلال تصور مخرجات الطبقات الالتفافية، يمكننا رؤية "الميزات" التي تعلمها النموذج:

- **الطبقات الدنيا**: أشكال أساسية (عين، نقطة قطع)
- **الطبقات الوسطى**: أنماط تكتيكية (أتاري، سلم)
- **الطبقات العليا**: مفاهيم شاملة (النفوذ، السماكة)

هذا مشابه جداً للبنية الهرمية لإدراك البشر للغو.

---

## نقاط التنفيذ

### تنفيذ PyTorch

إليك تنفيذ مبسط لشبكة Policy:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class PolicyNetwork(nn.Module):
    def __init__(self, input_channels=48, num_filters=192, num_layers=12):
        super().__init__()

        # طبقة الالتفاف الأولى (5×5)
        self.conv1 = nn.Conv2d(input_channels, num_filters,
                               kernel_size=5, padding=2)

        # الطبقات الالتفافية الوسطى (3×3)×11
        self.conv_layers = nn.ModuleList([
            nn.Conv2d(num_filters, num_filters,
                     kernel_size=3, padding=1)
            for _ in range(num_layers - 1)
        ])

        # طبقة التفاف المخرجات (1×1)
        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)

    def forward(self, x):
        # x: (batch, 48, 19, 19)

        # الطبقة الأولى
        x = F.relu(self.conv1(x))

        # الطبقات الوسطى
        for conv in self.conv_layers:
            x = F.relu(conv(x))

        # طبقة المخرجات
        x = self.conv_out(x)  # (batch, 1, 19, 19)

        # تسطيح + Softmax
        x = x.view(x.size(0), -1)  # (batch, 361)
        x = F.softmax(x, dim=1)

        return x
```

### حلقة التدريب

```python
def train_step(model, optimizer, states, actions):
    """
    states: (batch, 48, 19, 19) - ميزات الرقعة
    actions: (batch,) - الموقع الذي لعب فيه الإنسان (0-360)
    """
    # الانتشار الأمامي
    policy = model(states)  # (batch, 361)

    # خسارة الإنتروبيا المتقاطعة
    loss = F.cross_entropy(
        torch.log(policy + 1e-8),  # لمنع log(0)
        actions
    )

    # الانتشار العكسي
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # حساب الدقة
    predictions = policy.argmax(dim=1)
    accuracy = (predictions == actions).float().mean()

    return loss.item(), accuracy.item()
```

### ملاحظات عند الاستدلال

عند اللعب الفعلي، يجب الانتباه إلى:

1. **تصفية الحركات غير المشروعة**: ضع احتمال المواقع غير المشروعة إلى 0، ثم أعد التطبيع
2. **تعديل درجة الحرارة**: يمكن استخدام معامل حرارة للتحكم في "حدة" توزيع الاحتمالات
3. **استدلال دفعي**: في MCTS يمكن معالجة عدة أوضاع دفعياً

```python
def get_move_probabilities(model, state, legal_moves, temperature=1.0):
    """الحصول على توزيع احتمالات الحركات المشروعة"""
    policy = model(state)  # (361,)

    # الاحتفاظ بالحركات المشروعة فقط
    mask = torch.zeros(361)
    mask[legal_moves] = 1
    policy = policy * mask

    # تعديل درجة الحرارة
    if temperature != 1.0:
        policy = policy ** (1 / temperature)

    # إعادة التطبيع
    policy = policy / policy.sum()

    return policy
```

---

## الرسوم المتحركة المقابلة

المفاهيم الأساسية في هذه المقالة وأرقام الرسوم المتحركة المقابلة:

| الرقم | المفهوم | المقابل الفيزيائي/الرياضي |
|------|------|--------------|
| E1 | شبكة Policy | مجال الاحتمالات |
| D9 | استخراج ميزات CNN | استجابة المرشحات |
| D3 | التعلم الخاضع للإشراف | تقدير الإمكانية القصوى |
| H4 | تدرج السياسة | التحسين العشوائي |

---

## قراءات إضافية

- **المقالة التالية**: [شرح مفصل لشبكة Value](../value-network) — كيف يقيّم AlphaGo الأوضاع
- **موضوع ذو صلة**: [تصميم ميزات المدخلات](../input-features) — شرح تفصيلي لـ 48 مستوى ميزات
- **تعمق في المبادئ**: [دمج CNN مع الغو](../cnn-and-go) — لماذا الشبكات العصبية الالتفافية مناسبة للرقعة

---

## النقاط الرئيسية

1. **شبكة Policy هي مولد توزيع احتمالي**: مدخلها الرقعة، مخرجها احتمالات 361 موقع
2. **13 طبقة CNN + Softmax**: التفاف عميق لاستخراج الميزات، Softmax لإخراج الاحتمالات
3. **دقة 57%**: تتفوق بكثير على برامج الغو السابقة
4. **نسختان**: النسخة الكاملة لقرارات MCTS، النسخة الخفيفة للمحاكاة السريعة
5. **الضبط بالتعلم المعزز**: التطور من "تقليد البشر" إلى "السعي للفوز"

شبكة Policy هي "الحدس" في AlphaGo — تتيح للذكاء الاصطناعي التعرف بسرعة على الحركات التي تستحق الدراسة، تماماً مثل البشر.

---

## المراجع

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Maddison, C. J., et al. (2014). "Move Evaluation in Go Using Deep Convolutional Neural Networks." *arXiv:1412.6564*.
3. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.
4. LeCun, Y., Bengio, Y., & Hinton, G. (2015). "Deep learning." *Nature*, 521, 436-444.
