---
sidebar_position: 1
title: تحليل شامل لـ AlphaGo
description: من الخلفية التاريخية إلى التفاصيل التقنية، 20 مقالة لفهم AlphaGo بشكل كامل
---

import { PolicyHeatmap, EloChart } from '@site/src/components/D3Charts';

# تحليل شامل لـ AlphaGo

في مارس 2016، هزم AlphaGo بطل العالم لي سيدول بنتيجة 4:1، مما أذهل العالم بأسره. لم تكن هذه مجرد فوز في لعبة غو، بل كانت أيضاً علامة فارقة في تقدم الذكاء الاصطناعي.

هذه السلسلة المكونة من **20 مقالة معمقة** ستأخذك من الخلفية التاريخية، عبر المبادئ التقنية، إلى تفاصيل التنفيذ، لفهم كل شيء عن AlphaGo بشكل كامل.

---

## دليل السلسلة

### الوحدة 1: التاريخ والإنجازات

| المقالة | الوصف |
|---------|-------|
| [ولادة AlphaGo](./birth-of-alphago) | تأسيس DeepMind، استحواذ Google، تكوين الفريق |
| [مراجعة المباريات الرئيسية](./key-matches) | فان هوي، لي سيدول، كي جي، 60 فوز متتالي لـ Master |
| [تحليل معمق "للحركة الإلهية"](./move-37) | منطق اللعبة وتفسير الذكاء الاصطناعي للحركة 37 |

### الوحدة 2: تحدي غو

| المقالة | الوصف |
|---------|-------|
| [لماذا غو صعبة؟](./why-go-is-hard) | مساحة الحالة 10^170، عامل التفرع ~250 |
| [حدود الطرق التقليدية](./traditional-limits) | Minimax، Alpha-Beta، MCTS النقي |
| [تمثيل حالة اللوحة](./board-representation) | Zobrist Hashing، Union-Find، ترميز الميزات |

### الوحدة 3: جوهر الشبكات العصبية

| المقالة | الوصف |
|---------|-------|
| [شبكة السياسة بالتفصيل](./policy-network) | البنية، مخرج Softmax، هدف التدريب |
| [شبكة القيمة بالتفصيل](./value-network) | البنية، مخرج Tanh، تجنب الإفراط في التخصيص |
| [تصميم ميزات الإدخال](./input-features) | التطور من 48 إلى 17 مستوى ميزات |
| [CNN والغو](./cnn-and-go) | لماذا CNN مناسبة للوحة |
| [مرحلة التعلم الموجه](./supervised-learning) | مجموعة بيانات KGS، دقة تنبؤ 57% |

### الوحدة 4: التعلم المعزز والبحث

| المقالة | الوصف |
|---------|-------|
| [مقدمة في التعلم المعزز](./reinforcement-intro) | MDP، تدرج السياسة، دالة القيمة |
| [اللعب الذاتي](./self-play) | لماذا يعمل، منحنى نمو ELO |
| [الجمع بين MCTS والشبكات العصبية](./mcts-neural-combo) | Selection→Expansion→Evaluation→Backup |
| [صيغة PUCT بالتفصيل](./puct-formula) | الاشتقاق الرياضي، الاستكشاف مقابل الاستغلال |

### الوحدة 5: تطور AlphaGo Zero

| المقالة | الوصف |
|---------|-------|
| [نظرة عامة على AlphaGo Zero](./alphago-zero) | لماذا لا يحتاج إلى ألعاب بشرية |
| [الشبكة ذات الرأسين و ResNet](./dual-head-resnet) | التمثيل المشترك، تدفق التدرج، ResNet 40 طبقة |
| [عملية التدريب من الصفر](./training-from-scratch) | تغييرات اليوم 0-3، تجاوز البشر في 3 أيام |

### الوحدة 6: التفاصيل التقنية والامتدادات

| المقالة | الوصف |
|---------|-------|
| [الأنظمة الموزعة و TPU](./distributed-systems) | بنية التدريب، بنية الاستدلال، MCTS المتوازي |
| [إرث AlphaGo](./legacy-and-impact) | التأثير على عالم غو، AlphaZero، MuZero، AlphaFold |

---

## معاينة سريعة

### مثال على مخرج شبكة السياسة

تنتج شبكة السياسة احتمالية الحركة لكل موقع:

<PolicyHeatmap initialPosition="corner" size={400} />

### منحنى التدريب

تجاوز AlphaGo Zero البشر في 3 أيام بدءاً من الصفر:

<EloChart mode="zero" width={600} height={350} />

---

## اقتراحات القراءة

### اختر نقطة البداية بناءً على خلفيتك

| خلفيتك | نقطة البداية المقترحة |
|--------|----------------------|
| **مبتدئ تماماً** | ابدأ من [ولادة AlphaGo](./birth-of-alphago)، اقرأ بالترتيب |
| **تعرف غو** | ابدأ من [لماذا غو صعبة؟](./why-go-is-hard) |
| **لديك أساس في التعلم الآلي** | ابدأ من [شبكة السياسة بالتفصيل](./policy-network) |
| **تريد فهم الجوهر بسرعة** | اقرأ [الجمع بين MCTS والشبكات العصبية](./mcts-neural-combo) |
| **تريد فهم إنجاز Zero** | ابدأ من [نظرة عامة على AlphaGo Zero](./alphago-zero) |

### الوقت المقدر للقراءة

- **القراءة الكاملة**: حوالي 8-10 ساعات
- **التصفح السريع**: حوالي 2-3 ساعات
- **كل مقالة**: حوالي 15-25 دقيقة

---

## مطابقة الرسوم المتحركة

تشير هذه السلسلة من المقالات إلى السلاسل التالية من [109 مفهوم متحرك](/docs/tech/how-it-works/concepts/):

| السلسلة | الموضوع | المقالات المرتبطة |
|---------|--------|------------------|
| **سلسلة C** | طرق مونت كارلو | #5، #14، #15 |
| **سلسلة D** | الشبكات العصبية | #7، #8، #10، #11 |
| **سلسلة E** | بنية AlphaGo | #13، #16، #17، #18 |
| **سلسلة H** | التعلم المعزز | #12، #13 |

---

## المراجع

### الأوراق البحثية

1. Silver, D., et al. (2016). ["Mastering the game of Go with deep neural networks and tree search."](https://www.nature.com/articles/nature16961) *Nature*.
2. Silver, D., et al. (2017). ["Mastering the game of Go without human knowledge."](https://www.nature.com/articles/nature24270) *Nature*.
3. Silver, D., et al. (2018). ["A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play."](https://www.science.org/doi/10.1126/science.aar6404) *Science*.

### قراءات إضافية

- [الابتكارات الرئيسية لـ KataGo](/docs/tech/how-it-works/katago-innovations) — كيف تحقق قوة لعب أعلى بموارد أقل
- [جدول المرجع السريع للمفاهيم](/docs/tech/how-it-works/concepts/) — القائمة الكاملة لـ 109 مفهوم متحرك
- [شغّل أول ذكاء اصطناعي للغو في 30 دقيقة](/docs/tech/hands-on/) — تجربة عملية
