---
sidebar_position: 15
title: ุฏูุฌ MCTS ูุน ุงูุดุจูุงุช ุงูุนุตุจูุฉ
description: ููู ุนููู ูููููุฉ ุฏูุฌ AlphaGo ุจูู ุจุญุซ ุดุฌุฑุฉ ูููุช ูุงุฑูู ูุงูุดุจูุงุช ุงูุนุตุจูุฉ ุงูุนูููุฉ
---

import { MCTSTree } from '@site/src/components/D3Charts';

# ุฏูุฌ MCTS ูุน ุงูุดุจูุงุช ุงูุนุตุจูุฉ

ูู ุงูููุงูุงุช ุงูุณุงุจูุฉุ ูุฏููุง ุงูุดุจูุงุช ุงูุนุตุจูุฉ (Policy Network ู Value Network) ูููุงููู ุงูุชุนูู ุงููุนุฒุฒ ุจุดูู ูููุตู. ุงูุขูุ ุฏุนููุง ูุณุชูุดู ุงูุงุจุชูุงุฑ ุงูุฃุณุงุณู ูู AlphaGo โ **ููููุฉ ุงูุฏูุฌ ุงููุซุงูู ุจูู ุจุญุซ ุดุฌุฑุฉ ูููุช ูุงุฑูู (MCTS) ูุงูุดุจูุงุช ุงูุนุตุจูุฉ**.

ูุฐุง ุงูุฏูุฌ ูู ููุชุงุญ ูุฌุงุญ AlphaGo: ุงูุดุจูุงุช ุงูุนุตุจูุฉ ุชููุฑ "ุงูุญุฏุณ"ุ ู MCTS ูููุฑ "ุงูุงุณุชุฏูุงู"ุ ูููุงููุง ูููู ุงูุขุฎุฑ.

---

## ูุฑุงุฌุนุฉ MCTS ุงูุชูููุฏู

### ูุง ูู MCTSุ

**ุจุญุซ ุดุฌุฑุฉ ูููุช ูุงุฑูู (Monte Carlo Tree Search, MCTS)** ูู ุฎูุงุฑุฒููุฉ ุจุญุซ ุชุนุชูุฏ ุนูู ุฃุฎุฐ ุงูุนููุงุช ุงูุนุดูุงุฆูุฉุ ููู ููุงุณุจุฉ ุจุดูู ุฎุงุต ููุฐูุงุก ุงูุงุตุทูุงุนู ูู ุงูุฃูุนุงุจ.

ุงูููุฑุฉ ุงูุฃุณุงุณูุฉ ูู MCTS ูู: **ุจุฏูุงู ูู ุงุณุชูุดุงู ุฌููุน ุงูุญุฑูุงุช ุงูููููุฉุ ูู ุงูุฃูุถู ูุญุงูุงุฉ ุนุฏุฏ ูุจูุฑ ูู ุงููุจุงุฑูุงุช ุจุดูู ุนุดูุงุฆูุ ูุงุณุชุฎุฏุงู ุงูุฅุญุตุงุฆูุงุช ูุชูุฏูุฑ ุฌูุฏุฉ ูู ุญุฑูุฉ**.

### ุงููุฑุงุญู ุงูุฃุฑุจุน

ูุชุถูู MCTS ุงูุชูููุฏู ุฃุฑุจุน ูุฑุงุญู ุชุชูุฑุฑ ุจุงุณุชูุฑุงุฑ:

```mermaid
flowchart LR
    subgraph MCTS["ุฏูุฑุฉ MCTS"]
        S["Selection<br/>ุงูุงุฎุชูุงุฑ"] --> E["Expansion<br/>ุงูุชูุณูุน"]
        E --> Sim["Simulation<br/>ุงููุญุงูุงุฉ"]
        Sim --> B["Backprop<br/>ุงูุงูุชุดุงุฑ"]
        B -.->|ุชูุฑุงุฑ N ูุฑุฉ| S
    end
```

ุฏุนููุง ูููู ูู ูุฑุญูุฉ ุจุงูุชูุตูู:

### 1. Selection (ุงูุงุฎุชูุงุฑ)

ุจุฏุกุงู ูู ุงูุนูุฏุฉ ุงูุฌุฐุฑูุฉุ ููุฒู ุนุจุฑ ุงูุดุฌุฑุฉุ ูุฎุชุงุฑ ุงูุนูุฏุฉ ุงููุฑุนูุฉ "ุงูุฃูุซุฑ ูุนุฏุงู"ุ ุญุชู ูุตู ุฅูู ุนูุฏุฉ ูุฑููุฉ.

ูุนูุงุฑ ุงูุงุฎุชูุงุฑ ูู ูุนุงุฏูุฉ **UCB1 (Upper Confidence Bound)**:

$$\text{UCB1}(s, a) = \bar{X}_{s,a} + c \sqrt{\frac{\ln N_s}{N_{s,a}}}$$

ุญูุซ:
- $\bar{X}_{s,a}$: ูุชูุณุท ุงูุนุงุฆุฏ ูู ุงูุนูุฏุฉ $(s, a)$ (**ุนูุตุฑ ุงูุงุณุชุบูุงู**)
- $\sqrt{\frac{\ln N_s}{N_{s,a}}}$: ููุงูุฃุฉ ุงูุงุณุชูุดุงู (**ุนูุตุฑ ุงูุงุณุชูุดุงู**)
- $N_s$: ุนุฏุฏ ุฒูุงุฑุงุช ุงูุนูุฏุฉ ุงูุฃุจ
- $N_{s,a}$: ุนุฏุฏ ุฒูุงุฑุงุช ุงูุนูุฏุฉ ุงูุงุจู
- $c$: ุซุงุจุช ููุงุฒูุฉ ุงูุงุณุชูุดุงู ูุงูุงุณุชุบูุงู

ุญููุฉ ูุฐู ุงููุนุงุฏูุฉ ุชููู ูู:
- ุงูุนูุฏ ุฐุงุช ุงูุฒูุงุฑุงุช ุงูููููุฉ ุชุญุตู ุนูู ููุงูุฃุฉ ุงุณุชูุดุงู ุฃุนูู
- ูุน ุฒูุงุฏุฉ ุนุฏุฏ ุงูุฒูุงุฑุงุชุ ูููู ุงูุงุฎุชูุงุฑ ุฃูุซุฑ ูุญู ุงูุนูุฏ ุฐุงุช ุงููููุฉ ุงููุนููุฉ ุงูุฃุนูู

### 2. Expansion (ุงูุชูุณูุน)

ุจุนุฏ ุงููุตูู ุฅูู ุงูุนูุฏุฉ ุงููุฑููุฉุ ูุฎุชุงุฑ ุฅุฌุฑุงุกู ูู ููุณุชูุดู ุจุนุฏุ ูููุดุฆ ุนูุฏุฉ ูุฑุนูุฉ ุฌุฏูุฏุฉ.

```mermaid
flowchart TB
    subgraph Before["ูุจู ุงูุชูุณูุน"]
        R1((ุงูุฌุฐุฑ))
        R1 --- A1(( ))
        R1 --- A2(( ))
        R1 --- A3(( ))
        A1 --- B1(( ))
        A1 --- B2["ุนูุฏุฉ ูุฑููุฉ"]
    end

    subgraph After["ุจุนุฏ ุงูุชูุณูุน"]
        R2((ุงูุฌุฐุฑ))
        R2 --- C1(( ))
        R2 --- C2(( ))
        R2 --- C3(( ))
        C1 --- D1(( ))
        C1 --- D2(( ))
        D2 --- New["ุนูุฏุฉ ุฌุฏูุฏุฉ"]
    end

    Before -->|ุงูุชูุณูุน| After
```

### 3. Simulation (ุงููุญุงูุงุฉ/Rollout)

ุจุฏุกุงู ูู ุงูุนูุฏุฉ ุงูุฌุฏูุฏุฉุ ูุณุชุฎุฏู ุงุณุชุฑุงุชูุฌูุฉ ูุนููุฉ (ุนุงุฏุฉู ุนุดูุงุฆูุฉ ุฃู ุฅุฑุดุงุฏูุฉ ุจุณูุทุฉ) ูุฅููุงู ุงููุนุจุฉ ุจุณุฑุนุฉ ูุงูุญุตูู ุนูู ุงููุชูุฌุฉ.

ูุฐุง ูู ูุตุฏุฑ ุชุณููุฉ "ูููุช ูุงุฑูู" โ **ุงุณุชุฎุฏุงู ุงููุญุงูุงุฉ ุงูุนุดูุงุฆูุฉ ูุชูุฏูุฑ ุงููุชุงุฆุฌ**.

ุงุณุชุฑุงุชูุฌูุฉ rollout ูู MCTS ุงูุชูููุฏู ูุฏ ุชููู:
- **ุนุดูุงุฆูุฉ ุจุญุชุฉ**: ุงุฎุชูุงุฑ ุนุดูุงุฆู ูุชุณุงูู ูู ุงูุญุฑูุงุช ุงููุงููููุฉ
- **ุฅุฑุดุงุฏูุฉ ุฎูููุฉ**: ุงุณุชุฎุฏุงู ููุงุนุฏ ุจุณูุทุฉ ูุชุตููุฉ ุงูุญุฑูุงุช ุงูุณูุฆุฉ ุงููุงุถุญุฉ

### 4. Backpropagation (ุงูุงูุชุดุงุฑ ุงูุนูุณู)

ููุฑุฌุน ูุชูุฌุฉ ุงููุญุงูุงุฉ (ููุฒ/ุฎุณุงุฑุฉ) ุนุจุฑ ุงููุณุงุฑุ ููุญุฏุซ ุงููุนูููุงุช ุงูุฅุญุตุงุฆูุฉ ููู ุนูุฏุฉ:

```
ูุญุชูู ุงูุชุญุฏูุซ:
- ุนุฏุฏ ุงูุฒูุงุฑุงุช: N(s, a) โ N(s, a) + 1
- ุงููููุฉ ุงูุชุฑุงูููุฉ: W(s, a) โ W(s, a) + z
- ูุชูุณุท ุงููููุฉ: Q(s, a) = W(s, a) / N(s, a)
```

ุญูุซ $z$ ูู ูุชูุฌุฉ ุงููุญุงูุงุฉ (+1 ุฃู -1).

### ูููุฏ MCTS ุงูุชูููุฏู

ุฃุฏุงุก MCTS ุงูุชูููุฏู ูู ูุนุจุฉ ุงูุบู ูุญุฏูุฏุ ุงููุดุงูู ุงูุฑุฆูุณูุฉ ูู:

1. **ุฌูุฏุฉ Rollout ุถุนููุฉ**: ุงููุญุงูุงุฉ ุงูุนุดูุงุฆูุฉ ุบุงูุจุงู ูุง ุชูุชุฌ ุฃูุนุงุจุงู ุบูุฑ ููุทููุฉ
2. **ูุญุชุงุฌ ูุญุงูุงุฉ ูุซูุฑุฉ**: ูู ุฎุทูุฉ ูุฏ ุชุญุชุงุฌ ุนุดุฑุงุช ุงูุขูุงู ูู ุงููุญุงูุงุงุช
3. **ุชูููู ุบูุฑ ุฏููู**: ุงูุงุนุชูุงุฏ ููุท ุนูู ุฅุญุตุงุฆูุงุช ุงูููุฒ/ุงูุฎุณุงุฑุฉุ ููุงุกุฉ ุงุณุชุฎุฏุงู ุงููุนูููุงุช ููุฎูุถุฉ
4. **ูุง ูุณุชููุฏ ูู ุงูุฃููุงุท**: ูุจุญุซ ูู ุฌุฏูุฏ ูู ูู ูุฑุฉุ ูุง ูุฑุงูู ุงูุฎุจุฑุฉ

ูุฐู ุงููุดุงูู ุชู ุญููุง ุจุฃูุงูุฉ ูู AlphaGo ุจูุงุณุทุฉ ุงูุดุจูุงุช ุงูุนุตุจูุฉ.

---

## ููู ุชุญุณู ุงูุดุจูุงุช ุงูุนุตุจูุฉ MCTS

### ุงูุจููุฉ ุงููููุฉ

ูุฏูุฌ AlphaGo ุดุจูุชูู ุนุตุจูุชูู ูู MCTS:

```mermaid
flowchart TB
    subgraph AlphaGo["ุจููุฉ ุจุญุซ AlphaGo"]
        subgraph MCTS["MCTS"]
            Selection["Selection"]
            Expansion["Expansion"]
            Evaluation["Evaluation"]
            Backup["Backup"]

            Selection --> PUCT["PUCT<br/>ูุนุงุฏูุฉ"]
            Expansion --> PolicyNet["Policy<br/>Net"]
            Evaluation --> ValueNet["Value Net<br/>+ Rollout"]
            Backup --> Stats["ุชุญุฏูุซ<br/>ุงูุฅุญุตุงุก"]
        end

        subgraph NN["Neural Networks"]
            Policy["Policy Network<br/>ฯ(a|s)<br/>ุงูุฅุฎุฑุงุฌ: ุงุญุชูุงูุงุช ุงูุฅุฌุฑุงุกุงุช"]
            Value["Value Network<br/>V(s)<br/>ุงูุฅุฎุฑุงุฌ: ุชูุฏูุฑ ูุณุจุฉ ุงูููุฒ"]
        end

        NN --> MCTS
    end
```

### ุฏูุฑ Policy Network

**Policy Network ูุนูู ูู ูุฑุญูุฉ ุงูุชูุณูุน (Expansion)**.

ูู MCTS ุงูุชูููุฏูุ ุนูุฏ ุงูุชูุณูุนุ ุฌููุน ุงูุฅุฌุฑุงุกุงุช ุบูุฑ ุงููุณุชูุดูุฉ ุชูุนุชุจุฑ ูุชุณุงููุฉ ุงูุฃูููุฉ. ููู Policy Network ูููุฑ **ุงูุงุญุชูุงููุฉ ุงููุณุจูุฉ (prior probability)**:

$$P(s, a) = \pi_\theta(a|s)$$

ูุฐุง ูุฌุนู MCTS ูุณุชูุดู ุฃููุงู ุงูุญุฑูุงุช ุงูุชู "ุชุจุฏู ุฃูุถู"ุ ููุง ูุญุณู ููุงุกุฉ ุงูุจุญุซ ุจุดูู ูุจูุฑ.

ุนูู ุณุจูู ุงููุซุงูุ ูู ูุถุน ูุนูู:
- "ุชููุบู" (ูุณุท ุงูููุญุฉ) ูุฏ ูููู ููุง 0.01% ููุท ูู ุงูุงุญุชูุงููุฉ
- "ุฌูุณููู ุงูุฒุงููุฉ" ูุฏ ูููู ููุง 15% ูู ุงูุงุญุชูุงููุฉ
- "ุงูููุทุฉ ุงููุจูุฑุฉ" ูุฏ ูููู ููุง 10% ูู ุงูุงุญุชูุงููุฉ

MCTS ุณูุณุชูุดู ุงูุญุฑูุงุช ุฐุงุช ุงูุงุญุชูุงููุฉ ุงูุนุงููุฉ ุฃููุงูุ ุจุฏูุงู ูู ุฅุถุงุนุฉ ุงูููุช ุนูู ุงูุฎูุงุฑุงุช ุงููุงุถุญุฉ ุงูุณูุฆุฉ.

### ุฏูุฑ Value Network

**Value Network ูุนูู ูู ูุฑุญูุฉ ุงูุชูููู (Evaluation)**.

MCTS ุงูุชูููุฏู ูุญุชุงุฌ ุฅููุงู ุงููุนุจุฉ ุจุงููุงูู ููุญุตูู ุนูู ุงูุชูููู. ููู Value Network ููููู ุชูููู ูุณุจุฉ ุงูููุฒ ูุฃู ูุถุน ูุจุงุดุฑุฉ:

$$v(s) = V_\phi(s)$$

ูุฐุง ูุซู ุทูุจ ุฑุฃู ุฎุจูุฑ ูู ุชูููู ุงููุถุนุ ุจุฏูุงู ูู ุชุฑู ูุจุชุฏุฆูู ููููุงู ุงููุนุจุฉ ุซู ุฑุคูุฉ ุงููุชูุฌุฉ.

ุงููุณุฎุฉ ุงูุฃุตููุฉ ูู AlphaGo ุชูุฒุฌ ุจูู Value Network ู Rollout:

$$V(s_L) = (1 - \lambda) \cdot v_\theta(s_L) + \lambda \cdot z_L$$

ุญูุซ:
- $v_\theta(s_L)$: ุชูููู Value Network
- $z_L$: ูุชูุฌุฉ Rollout
- $\lambda$: ูุนุงูู ุงููุฒุฌ (AlphaGo ุงุณุชุฎุฏู $\lambda = 0.5$)

### ุชุตูุฑ ุดุฌุฑุฉ ุงูุจุญุซ

ุฏุนููุง ูุชุตูุฑ ุดุฌุฑุฉ ุจุญุซ MCTS:

<MCTSTree width={700} height={450} showPUCT={true} interactive={true} />

ูู ูุฐุง ุงูุชุตูุฑุ ููููู ุฑุคูุฉ:
- ุญุฌู ุงูุนูุฏุฉ ูุนูุณ ุนุฏุฏ ุงูุฒูุงุฑุงุช
- ุงููุณุงุฑ ุงูุฃุฒุฑู ูู ุงููุณุงุฑ ุงูุฃูุถู ุงูุฐู ุงุฎุชุงุฑู MCTS
- ูู ุนูุฏุฉ ุชุนุฑุถ ุนุฏุฏ ุงูุฒูุงุฑุงุช N ููุชูุณุท ุงููููุฉ Q

---

## ุดุฑุญ ุชูุตููู ูุนูููุฉ ุงูุจุญุซ

### ุงูุชุฏูู ุงููุงูู

ุฏุนููุง ูุชุชุจุน ูุญุงูุงุฉ MCTS ูุงุญุฏุฉ ูุงููุฉ:

```
ุงูุฎูุงุฑุฒููุฉ: ูุญุงูุงุฉ MCTS ูุงุญุฏุฉ ูู AlphaGo

ุงููุฏุฎูุงุช: ุงูุนูุฏุฉ ุงูุฌุฐุฑูุฉ s_rootุ Policy Network ฯุ Value Network V

1. Selection (ุงูุงุฎุชูุงุฑ)
   s = s_root
   ุงููุณุงุฑ = []

   while s ููุณุช ุนูุฏุฉ ูุฑููุฉ:
       # ุงุณุชุฎุฏุงู ูุนุงุฏูุฉ PUCT ูุงุฎุชูุงุฑ ุงูุฅุฌุฑุงุก
       a* = argmax_a [Q(s,a) + U(s,a)]

       ุญูุซ U(s,a) = c_puct ยท P(s,a) ยท โN(s) / (1 + N(s,a))

       ุงููุณุงุฑ.append((s, a*))
       s = ุงูุญุงูุฉ ุจุนุฏ ุชูููุฐ ุงูุฅุฌุฑุงุก a*

2. Expansion (ุงูุชูุณูุน)
   ุฅุฐุง ูู ุชูู s ุญุงูุฉ ููุงุฆูุฉ:
       # ุงุณุชุฎุฏุงู Policy Network ูุญุณุงุจ ุงูุงุญุชูุงููุฉ ุงููุณุจูุฉ
       P(s, ยท) = ฯ(ยท|s)

       # ุฅูุดุงุก ุนูุฏ ูุฑุนูุฉ ูุฌููุน ุงูุฅุฌุฑุงุกุงุช ุงููุงููููุฉ
       for a in ุงูุฅุฌุฑุงุกุงุช_ุงููุงููููุฉ:
           ุฅูุดุงุก ุนูุฏุฉ ูุฑุนูุฉ (s, a)
           ุชุนููู P(s,a), N(s,a)=0, W(s,a)=0

3. Evaluation (ุงูุชูููู)
   # ูุฒุฌ Value Network ู Rollout
   v = V(s)                          # ุชูููู Value Network
   z = rollout(s)                    # ูุชูุฌุฉ Rollout
   value = (1-ฮป)ยทv + ฮปยทz             # ุงููุฒุฌ

   # AlphaGo Zero ุจุณูุท ุฅูู ุงุณุชุฎุฏุงู Value Network ููุท
   # value = V(s)

4. Backpropagation (ุงูุงูุชุดุงุฑ ุงูุนูุณู)
   for (s', a') in ุนูุณ(ุงููุณุงุฑ):
       N(s', a') += 1
       W(s', a') += value
       Q(s', a') = W(s', a') / N(s', a')
       value = -value                 # ุชุจุฏูู ูุฌูุฉ ุงููุธุฑ
```

### ุดุฑุญ ุชูุตููู ููุฑุญูุฉ ุงูุงุฎุชูุงุฑ

ูุฑุญูุฉ ุงูุงุฎุชูุงุฑ ุชุณุชุฎุฏู **ูุนุงุฏูุฉ PUCT** (ุณุชููุงูุด ุจุงูุชูุตูู ูู ุงูููุงู ุงูุชุงูู):

$$a^* = \arg\max_a \left[ Q(s,a) + c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,a)} \right]$$

ูุฐู ุงููุนุงุฏูุฉ ุชูุงุฒู ุจูู:
- **Q(s,a)**: ูุชูุณุท ุงููููุฉ ุงููุนุฑูู (ุงูุงุณุชุบูุงู)
- **U(s,a)**: ููุงูุฃุฉ ุงูุงุณุชูุดุงูุ ุชุฌูุน ุจูู ุงูุงุญุชูุงููุฉ ุงููุณุจูุฉ ูุนุฏุฏ ุงูุฒูุงุฑุงุช (ุงูุงุณุชูุดุงู)

### ุดุฑุญ ุชูุตููู ููุฑุญูุฉ ุงูุชูุณูุน

ุนูุฏ ุงููุตูู ุฅูู ุนูุฏุฉ ูุฑููุฉุ ูุณุชุฎุฏู Policy Network ูุชููุฆุฉ ุงูุนูุฏ ุงูุฌุฏูุฏุฉ:

```python
def expand(state, policy_network):
    # ุงูุญุตูู ุนูู ุงุญุชูุงููุฉ ุฌููุน ุงูุฅุฌุฑุงุกุงุช ุงููุงููููุฉ
    action_probs = policy_network(state)

    # ุชุตููุฉ ุงูุฅุฌุฑุงุกุงุช ุบูุฑ ุงููุงููููุฉ ูุฅุนุงุฏุฉ ุงูุชุทุจูุน
    legal_actions = get_legal_actions(state)
    legal_probs = action_probs[legal_actions]
    legal_probs = legal_probs / legal_probs.sum()

    # ุฅูุดุงุก ุงูุนูุฏ ุงููุฑุนูุฉ
    for action, prob in zip(legal_actions, legal_probs):
        child = create_node(
            state=apply_action(state, action),
            prior=prob,
            visit_count=0,
            value_sum=0
        )
        add_child(current_node, action, child)
```

### ุดุฑุญ ุชูุตููู ููุฑุญูุฉ ุงูุชูููู

ุงููุณุฎุฉ ุงูุฃุตููุฉ ูู AlphaGo ุชุณุชุฎุฏู ููุนูู ูู ุงูุชูููู ุจุดูู ูุฎุชูุท:

**ุชูููู Value Network**:
- ุฅุฏุฎุงู ุงููุถุน ูุจุงุดุฑุฉุ ุฅุฎุฑุงุฌ ูุณุจุฉ ุงูููุฒ
- ุญุณุงุจ ุณุฑูุน (ุงุณุชูุชุงุฌ ุดุจูุฉ ุนุตุจูุฉ ูุงุญุฏุฉ)
- ูููุฑ ุชููููุงู ูู ููุธูุฑ ุดุงูู

**ุชูููู Rollout**:
- ุงุณุชุฎุฏุงู ุงุณุชุฑุงุชูุฌูุฉ ุณุฑูุนุฉ (Fast Rollout Policy) ูุฅููุงู ุงููุนุจุฉ
- ุฃุจุทุฃ ูู ุงูุญุณุงุจ ููู ูููุฑ ูุชูุฌุฉ ูุนุจุฉ ูุงููุฉ
- ููููู ุงูุชุดุงู ุจุนุถ ุงูุชูุชููุงุช ุงูุชู ูุฏ ุชููุชูุง ุงูุดุจูุฉ ุงูุนุตุจูุฉ

```python
def evaluate(state, value_network, rollout_policy, lambda_mix=0.5):
    # ุชูููู Value Network
    v = value_network(state)

    # ุชูููู Rollout
    current = state
    while not is_terminal(current):
        action = rollout_policy(current)
        current = apply_action(current, action)
    z = get_result(current)

    # ุงููุฒุฌ
    return (1 - lambda_mix) * v + lambda_mix * z
```

AlphaGo Zero ุฃุฒุงู Rollout ูุงุณุชุฎุฏู Value Network ููุท. ูุฐุง ุจุณูุท ุงููุธุงู ูุญุณูู ุงูููุงุกุฉ.

### ุดุฑุญ ุชูุตููู ููุงูุชุดุงุฑ ุงูุนูุณู

ุฅุฑุฌุงุน ูุชูุฌุฉ ุงูุชูููู ุนุจุฑ ุงููุณุงุฑ ูุชุญุฏูุซ ุงูุฅุญุตุงุฆูุงุช:

```python
def backpropagate(path, value):
    for state, action in reversed(path):
        # ุชุญุฏูุซ ุนุฏุฏ ุงูุฒูุงุฑุงุช
        state.visit_count[action] += 1
        # ุชุญุฏูุซ ูุฌููุน ุงููููุฉ
        state.value_sum[action] += value
        # ุชุญุฏูุซ ูุชูุณุท ุงููููุฉ
        state.Q[action] = state.value_sum[action] / state.visit_count[action]
        # ุชุจุฏูู ูุฌูุฉ ุงููุธุฑ (ููุณุจ ุงูุฎุตู ูู ุฎุณุงุฑุชู)
        value = -value
```

ูุงุญุธ ุฎุทูุฉ `value = -value`: ุงูุบู ูุนุจุฉ ูุฌููุนูุง ุตูุฑุ ููุฒ ุทุฑู ูู ุฎุณุงุฑุฉ ุงูุทุฑู ุงูุขุฎุฑ.

---

## ุชูุฒูุน ููุงุฑุฏ ุงูุญุณุงุจ

### ุนุฏุฏ ุนูููุงุช ุงูุจุญุซ

AlphaGo ูููุฐ ุนุฏุฏุงู ูุจูุฑุงู ูู ูุญุงูุงุงุช MCTS ููู ุฎุทูุฉ:

| ุงูุฅุตุฏุงุฑ | ุนุฏุฏ ุงููุญุงูุงุงุช ููู ุฎุทูุฉ | ููุช ุงูุชูููุฑ |
|------|-------------|---------|
| AlphaGo Fan | ~100,000 | ุฏูุงุฆู |
| AlphaGo Lee | ~100,000 | ุฏูุงุฆู |
| AlphaGo Zero (ุงูุชุฏุฑูุจ) | 1,600 | ุซูุงูู |
| AlphaGo Zero (ุงููุจุงุฑุงุฉ) | ~1,600 | ุซูุงูู |

AlphaGo Zero ูุญูู ููุฉ ุฃูุจุฑ ุจูุญุงูุงุงุช ุฃููุ ููุฐุง ูุชูุฌุฉ ุชุญุณูู ุฌูุฏุฉ ุงูุดุจูุฉ ุงูุนุตุจูุฉ.

### ุงุณุชุฑุงุชูุฌูุฉ ุชูุฒูุน ุงูููุช

ุงูุฃูุถุงุน ุงููุฎุชููุฉ ูุฏ ุชุญุชุงุฌ ููุช ุชูููุฑ ูุฎุชูู:

```python
def allocate_time(game_state, remaining_time):
    # ุงูุชูุฒูุน ุงูุฃุณุงุณู
    num_moves_remaining = estimate_remaining_moves(game_state)
    base_time = remaining_time / num_moves_remaining

    # ุนูุงูู ุงูุชุนุฏูู
    complexity = estimate_complexity(game_state)
    importance = estimate_importance(game_state)

    # ุงูุฃูุถุงุน ุงููุนูุฏุฉ ุฃู ุงููููุฉ ุชุญุตู ุนูู ููุช ุฃูุซุฑ
    allocated_time = base_time * complexity * importance

    # ุงูุชุฃูุฏ ูู ุนุฏู ุชุฌุงูุฒ ุงูููุช
    return min(allocated_time, remaining_time * 0.3)
```

ูู ุงููุจุงุฑูุงุช ุงููุนููุฉุ AlphaGo ูุณุชุซูุฑ ููุช ุชูููุฑ ุฃูุซุฑ ูู ุงููุญุธุงุช ุงูุญุงุณูุฉ (ูุซู ุงููุญุธุงุช ุงููุฑูุจุฉ ูู ุญุฏ ุงูููุฒ/ุงูุฎุณุงุฑุฉ).

### ุงูุจุญุซ ุงููุชูุงุฒู

MCTS ููุงุณุจ ุจุทุจูุนุชู ููุชูุงุฒู:

**ุชูููุฉ ุงูุฎุณุงุฑุฉ ุงูุงูุชุฑุงุถูุฉ (Virtual Loss)**:

```
ุนูุฏูุง ูุณุชูุดู thread ูุณุงุฑุงู P:
1. ููุชุฑุถ ูุคูุชุงู ุฃู ูุฐุง ุงููุณุงุฑ ุฎุณุฑ (virtual loss)
2. ุงูู threads ุงูุฃุฎุฑู ุณุชููู ูุงุณุชูุดุงู ูุณุงุฑุงุช ุฃุฎุฑู
3. ุนูุฏูุง ุชุตู ุงููุชูุฌุฉุ ูุญุฏุซ ุงูุฅุญุตุงุฆูุงุช ุงูุญููููุฉ ููุฒูู ุงูุฎุณุงุฑุฉ ุงูุงูุชุฑุงุถูุฉ
```

ูุฐุง ูุถูู ุฃู threads ูุชุนุฏุฏุฉ ูู ุชุณุชูุดู ููุณ ุงููุณุงุฑ ุจุดูู ูุชูุฑุฑ.

```python
def parallel_mcts_simulation(root, num_threads=8):
    virtual_losses = {}

    def simulate(thread_id):
        # ูุฑุญูุฉ ุงูุงุฎุชูุงุฑ (ูุน ุงูุฎุณุงุฑุฉ ุงูุงูุชุฑุงุถูุฉ)
        path = []
        node = root
        while not node.is_leaf():
            action = select_with_virtual_loss(node, virtual_losses)
            add_virtual_loss(node, action, virtual_losses)
            path.append((node, action))
            node = node.children[action]

        # ุงูุชูุณูุน ูุงูุชูููู
        value = expand_and_evaluate(node)

        # ุงูุงูุชุดุงุฑ ุงูุนูุณู ูุฅุฒุงูุฉ ุงูุฎุณุงุฑุฉ ุงูุงูุชุฑุงุถูุฉ
        backpropagate(path, value)
        remove_virtual_losses(path, virtual_losses)

    # ุชูููุฐ ูุญุงูุงุงุช ูุชุนุฏุฏุฉ ุจุงูุชูุงุฒู
    threads = [Thread(target=simulate, args=(i,)) for i in range(num_threads)]
    for t in threads:
        t.start()
    for t in threads:
        t.join()
```

### ุงููุนุงูุฌุฉ ุงูุฏูุนูุฉ ุนูู GPU

ุงุณุชูุชุงุฌ ุงูุดุจูุฉ ุงูุนุตุจูุฉ ูููู ุฃูุซุฑ ููุงุกุฉ ุนูู GPU ุจุงููุนุงูุฌุฉ ุงูุฏูุนูุฉ. AlphaGo ูุณุชุฎุฏู **ุงูุชูููู ุงูุฏูุนู**:

```
ุจุฏูู ุงูุฏูุนุงุช:
  ูุญุงูุงุฉ 1 โ ุชูููู 1 โ ูุญุงูุงุฉ 2 โ ุชูููู 2 โ ...
  ุงุณุชุฎุฏุงู GPU ููุฎูุถ

ูุน ุงูุฏูุนุงุช:
  ุฌูุน 32 ูุถุนุงู ููุชูููู
  โ ุฅุฑุณุงููุง ุฏูุนุฉ ูุงุญุฏุฉ ุฅูู GPU ููุชูููู
  โ ุฅุฑุฌุงุน 32 ูุชูุฌุฉ
  ุงุณุชุฎุฏุงู GPU ุนุงูู
```

ูุฐุง ูุชุทูุจ ุฌุฏููุฉ ุฃูุซุฑ ุชุนููุฏุงูุ ูููู ูุญุณู ุงูุฅูุชุงุฌูุฉ ุจุดูู ูุจูุฑ.

---

## ุฏุฑุฌุฉ ุงูุญุฑุงุฑุฉ ูุงูุงุฎุชูุงุฑ ุงูููุงุฆู

### ุฏุฑุฌุฉ ุงูุญุฑุงุฑุฉ ุฃุซูุงุก ุงูุชุฏุฑูุจ

ุฃุซูุงุก ุชุฏุฑูุจ ุงููุนุจ ุงูุฐุงุชูุ AlphaGo ูุณุชุฎุฏู **ุฏุฑุฌุฉ ุงูุญุฑุงุฑุฉ** ููุชุญูู ูู ุงูุงุณุชูุดุงู:

$$\pi(a) = \frac{N(s,a)^{1/\tau}}{\sum_{a'} N(s,a')^{1/\tau}}$$

ุญูุซ $\tau$ ูู ูุนุงูู ุฏุฑุฌุฉ ุงูุญุฑุงุฑุฉ.

- $\tau = 1$: ุงูุงุญุชูุงููุฉ ูุชูุงุณุจุฉ ูุน ุนุฏุฏ ุงูุฒูุงุฑุงุช (ุงูุญูุงุธ ุนูู ุงูุชููุน)
- $\tau \to 0$: ุงุฎุชูุงุฑ ุงูุฅุฌุฑุงุก ุฐู ุฃูุซุฑ ุนุฏุฏ ุฒูุงุฑุงุช (ุงุฎุชูุงุฑ ุญุชูู)

ุงุณุชุฑุงุชูุฌูุฉ AlphaGo Zero:
- **ุฃูู 30 ุฎุทูุฉ**: $\tau = 1$ุ ููุญูุงุธ ุนูู ุชููุน ุงูุงูุชุชุงุญ
- **ุจุนุฏ ุฐูู**: $\tau \to 0$ุ ุงุฎุชูุงุฑ ุฃูุถู ุญุฑูุฉ

### ุงูุงุฎุชูุงุฑ ุฃุซูุงุก ุงููุจุงุฑุงุฉ

ูู ุงููุจุงุฑูุงุช ุงููุนููุฉุ ุงูุงุฎุชูุงุฑ ุนุงุฏุฉู ูุง ูููู ุญุชููุงู:

```python
def select_move(root, temperature=0):
    if temperature == 0:
        # ุงุฎุชูุงุฑ ุงูุฅุฌุฑุงุก ุฐู ุฃูุซุฑ ุนุฏุฏ ุฒูุงุฑุงุช
        return argmax(root.visit_counts)
    else:
        # ุฃุฎุฐ ุนููุฉ ูู ุชูุฒูุน ุงูุงุญุชูุงููุฉ ุงููุนุฏู ุจุฏุฑุฌุฉ ุงูุญุฑุงุฑุฉ
        probs = root.visit_counts ** (1 / temperature)
        probs = probs / probs.sum()
        return np.random.choice(actions, p=probs)
```

### ุงููุธุฑ ูู ูุณุจุฉ ุงูููุฒ

ุฃุญูุงูุงู ูุฃุฎุฐ ูุชูุณุท ุงููููุฉ ุจุนูู ุงูุงุนุชุจุงุฑ ุจุฏูุงู ูู ุนุฏุฏ ุงูุฒูุงุฑุงุช ููุท:

```python
def select_move_with_value(root, temperature=0):
    # ูุฒุฌ ุนุฏุฏ ุงูุฒูุงุฑุงุช ูุงููููุฉ
    scores = root.visit_counts * (1 + root.Q_values)
    scores = scores / scores.sum()

    if temperature == 0:
        return argmax(scores)
    else:
        probs = scores ** (1 / temperature)
        probs = probs / probs.sum()
        return np.random.choice(actions, p=probs)
```

---

## ุงูููุงุฑูุฉ ูุน ุงูุดุจูุฉ ุงูุนุตุจูุฉ ุงูุตุฑูุฉ

### ููุงุฐุง ูุญุชุงุฌ ุงูุจุญุซุ

ุณุคุงู ุทุจูุนู ูู: **ุจูุง ุฃู ุงูุดุจูุฉ ุงูุนุตุจูุฉ ูููููุง ุงูุชูุจุค ุจุงูุญุฑูุงุช ุงูุฌูุฏุฉุ ููุงุฐุง ูุญุชุงุฌ ุงูุจุญุซุ**

ุงูุฌูุงุจ ูู: **ุงูุจุญุซ ููููู ุชุตุญูุญ ุฃุฎุทุงุก ุงูุดุจูุฉ ุงูุนุตุจูุฉ ูุงูุชุดุงู ุญุฑูุงุช ุฃูุถู**.

| ุงูุทุฑููุฉ | ุงููุฒุงูุง | ุงูุนููุจ |
|------|------|------|
| ุดุจูุฉ ุนุตุจูุฉ ุตุฑูุฉ | ุณุฑูุนุฉุ ุญุฏุณูุฉ | ูุฏ ูููู ููุง ููุงุท ุนููุงุก |
| MCTS ุตุฑู | ููููู ุงูุชุญููู ุงูุนููู | ุจุทูุกุ ูุญุชุงุฌ ุชูููู |
| ุดุจูุฉ ุนุตุจูุฉ + MCTS | ูุฌูุน ูุฒุงูุง ุงูุงุซููู | ูุซูู ุงูุญุณุงุจ |

### ุงูุฃุฏูุฉ ุงูุชุฌุฑูุจูุฉ

ุชุฌุงุฑุจ DeepMind ุฃุธูุฑุช:

```
Policy Network ุตุฑู: ุญูุงูู 3000 Elo
Policy + ูููู ูู MCTS: ุญูุงูู 3500 Elo
Policy + Value + MCTS: ุญูุงูู 4500 Elo
```

ุงูุจุญุซ ูููุฑ ุชุญุณููุงู ููุญูุธุงู ูู ููุฉ ุงููุนุจ.

### ุฏูุฑ ุงูุจุญุซ

ุงูุจุญุซ ูู ูููุฉ ุฎุงุตุฉ ูู ุงูููุงูู ุงูุชุงููุฉ:

1. **ุงูุญุณุงุจ ุงูุชูุชููู**: ูุฑุงุกุฉ ุงููุฌูู ูุงููุชู ุงููุนูุฏ
2. **ุชุตุญูุญ ุงูุชุญูุฒ**: ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงููููุฌูุฉ ููุดุจูุฉ ุงูุนุตุจูุฉ
3. **ุงูุชุนุงูู ูุน ุงูุฃูุถุงุน ุงููุงุฏุฑุฉ**: ุฃูุถุงุน ูุฏ ูุง ุชููู ุงูุดุจูุฉ ุงูุนุตุจูุฉ ุฑุฃุชูุง ุฃุซูุงุก ุงูุชุฏุฑูุจ
4. **ุงูุชุญูู ูู ุงูุญุฏุณ**: ุงูุชุฃูุฏ ูู ุฃู ุงูุญุฑูุฉ "ุงูุฌูุฏุฉ ุงูุธุงูุฑ" ูู ูุนูุงู ุฌูุฏุฉ

---

## ุงููุฑููุงุช ุจูู ุฅุตุฏุงุฑุงุช AlphaGo

### AlphaGo Fan/Lee

```
ุงูุจููุฉ:
- SL Policy Network (ุชุนูู ุฅุดุฑุงูู)
- RL Policy Network (ุชุนูู ูุนุฒุฒ)
- Value Network
- Fast Rollout Policy

ุฃุซูุงุก ุงูุจุญุซ:
- ุงุณุชุฎุฏุงู ุงูุงุญุชูุงููุฉ ุงููุณุจูุฉ ูู SL Policy Network
- ูุฒุฌ ุชูููู Value Network ู Rollout
```

### AlphaGo Master

```
ุงูุจููุฉ:
- ุดุจูุฉ ุนุตุจูุฉ ุฃูุจุฑ
- ุจูุงูุงุช ุชุฏุฑูุจ ุฃูุซุฑ
- ููุฒุงุช ูุญุณูุฉ

ุฃุซูุงุก ุงูุจุญุซ:
- ูุดุงุจู ูู AlphaGo Lee
- ุดุจูุฉ ุฃููู = ุญุงุฌุฉ ุฃูู ููุจุญุซ
```

### AlphaGo Zero

```
ุงูุจููุฉ:
- ResNet ูุฒุฏูุฌ ุงูุฑุฃุณ ูุงุญุฏ
- ุชุฏุฑูุจ ูู ุงูุตูุฑ
- ุจุฏูู Rollout

ุฃุซูุงุก ุงูุจุญุซ:
- ุฑุฃุณ ุงูุณูุงุณุฉ ูููุฑ ุงูุงุญุชูุงููุฉ ุงููุณุจูุฉ
- ุฑุฃุณ ุงููููุฉ ููููู ูุจุงุดุฑุฉ
- ุฃุจุณุท ูุฃููู
```

### ููุฎุต ุงูุชุทูุฑ

```
AlphaGo Fan (2015)
    โ
    โ + ุดุจูุฉ ุฃูุจุฑุ ุชุฏุฑูุจ ุฃูุซุฑ
    โผ
AlphaGo Lee (2016)
    โ
    โ + ูุนุจ ุฐุงุชู ุฃูุซุฑ
    โผ
AlphaGo Master (2017)
    โ
    โ + ุฅุฒุงูุฉ ุจูุงูุงุช ุงูุจุดุฑุ ุดุจูุฉ ููุญุฏุฉุ ุฅุฒุงูุฉ Rollout
    โผ
AlphaGo Zero (2017)
    โ
    โ + ุงูุชุนููู ุนูู ุฃูุนุงุจ ุฃุฎุฑู
    โผ
AlphaZero (2018)
```

---

## ุงุนุชุจุงุฑุงุช ุงูุชูููุฐ

### ุฅุฏุงุฑุฉ ุงูุฐุงูุฑุฉ

ุดุฌุฑุฉ MCTS ูููู ุฃู ุชุตุจุญ ูุจูุฑุฉ ุฌุฏุงู:

```
ุงูุชุฑุงุถ:
- ูุชูุณุท 200 ุฅุฌุฑุงุก ูุงูููู ููู ุฎุทูุฉ
- ุนูู ุงูุจุญุซ 10
- ุงูุชูุณูุน ุงููุงูู: 200^10 โ 10^23 ุนูุฏุฉ (ูุณุชุญูู)

ุงูููุงุฑุณุฉ ุงููุนููุฉ:
- ุชูุณูุน ุงูุนูุฏ ุงูุชู ุชูุช ุฒูุงุฑุชูุง ููุท
- ุชูุธูู ุงูุนูุฏ ูุงุฏุฑุฉ ุงูุฒูุงุฑุฉ ุจุดูู ุฏูุฑู
- ุฅุนุงุฏุฉ ุงุณุชุฎุฏุงู ุดุฌุฑุฉ ุงูุจุญุซ ูู ุงูุฎุทูุฉ ุงูุณุงุจูุฉ
```

### ุฅุนุงุฏุฉ ุงุณุชุฎุฏุงู ุงูุดุฌุฑุฉ

ุนูุฏูุง ููุนุจ ุงูุฎุตูุ ูููู ุฅุนุงุฏุฉ ุงุณุชุฎุฏุงู ุฌุฒุก ูู ุดุฌุฑุฉ ุงูุจุญุซ:

```python
def reuse_tree(root, opponent_move):
    if opponent_move in root.children:
        new_root = root.children[opponent_move]
        # ุชูุธูู ุงููุฑูุน ุงูุฃุฎุฑู ุบูุฑ ุงููุทููุจุฉ
        for action in root.children:
            if action != opponent_move:
                delete_subtree(root.children[action])
        return new_root
    else:
        # ุงูุฎุตู ูุนุจ ุญุฑูุฉ ุบูุฑ ูุชููุนุฉุ ูุฌุจ ุงูุจุฏุก ูู ุฌุฏูุฏ
        return create_new_root()
```

### ุงูุชุฎุฒูู ุงููุคูุช ููุดุจูุฉ ุงูุนุตุจูุฉ

ููุณ ุงููุถุน ูุฏ ููููููู ุนุฏุฉ ูุฑุงุชุ ุงุณุชุฎุฏุงู ุงูุชุฎุฒูู ุงููุคูุช ูุชุฌูุจ ุงูุญุณุงุจ ุงูููุฑุฑ:

```python
class NeuralNetworkCache:
    def __init__(self, max_size=100000):
        self.cache = LRUCache(max_size)

    def evaluate(self, state, network):
        state_hash = hash(state)
        if state_hash in self.cache:
            return self.cache[state_hash]
        else:
            result = network(state)
            self.cache[state_hash] = result
            return result
```

### ุงุณุชุบูุงู ุงูุชูุงุธุฑ

ููุญุฉ ุงูุบู ููุง 8 ุชูุงุธุฑุงุชุ ูููู ุงุณุชุฎุฏุงููุง ูุชุนุฒูุฒ ุงูุจุญุซ:

```python
def evaluate_with_symmetry(state, network):
    # ุชูููุฏ ุฌููุน ุงูุชุญูููุงุช ุงููุชูุงุธุฑุฉ
    symmetries = generate_symmetries(state)  # 8 ูุณุฎ

    # ุชูููู ุฌููุน ุงููุณุฎ
    values = [network(s) for s in symmetries]

    # ุงููุชูุณุท (ุฃูุซุฑ ุงุณุชูุฑุงุฑุงู)
    return np.mean(values)
```

---

## ุนูู ุงูุจุญุซ ูุนุฑุถู

### ุงูุชุนุฏูู ุงูุฏููุงูููู

MCTS ููุงุฒู ุชููุงุฆูุงู ุจูู ุงูุนูู ูุงูุนุฑุถ:

- **ุงูุนุฑุถ**: ููุชุญูู ุจู ุจูุงุณุทุฉ ุงูุงุญุชูุงููุฉ ุงููุณุจูุฉ ูู Policy Network
- **ุงูุนูู**: ููุญุฏุฏ ุจูุงุณุทุฉ ุฏูุฉ Value Network

ุนูุฏูุง ุชููู ุงูุดุจูุฉ ุงูุนุตุจูุฉ ุฌูุฏุฉ:
- ุงูุญุฑูุงุช ุฐุงุช ุงูุซูุฉ ุงูุนุงููุฉ ูุชู ุงุณุชูุดุงููุง ุจุนูู
- ุงูุญุฑูุงุช ุฐุงุช ุงูุซูุฉ ุงูููุฎูุถุฉ ุชูุณุชุจุนุฏ ุจุณุฑุนุฉ
- ุงูุจุญุซ ูุฑูุฒ ุจุดูู ุทุจูุนู ุนูู ุงููุฑูุน ุงููููุฉ

### ุงูููุงุฑูุฉ ูุน ุงูุจุญุซ ุงูุชูููุฏู

| ุงูุทุฑููุฉ | ุงูุชุญูู ูู ุงูุนูู | ุงูุชุญูู ูู ุงูุนุฑุถ |
|------|---------|---------|
| Minimax | ุนูู ุซุงุจุช | ุชูููู Alpha-Beta |
| MCTS ุงูุชูููุฏู | ููุญุฏุฏ ุจุงููุญุงูุงุฉ | UCB1 |
| AlphaGo MCTS | ูููุฌู ุจู Policy + Value | PUCT + Policy |

ุจุญุซ AlphaGo ุฃูุซุฑ "ุฐูุงุกู" โ ูุนุฑู ุฃูู ูุณุชุญู ุงูุชุนููุ ูุฃูู ูููู ุชุฌุงูุฒู ุจุณุฑุนุฉ.

---

## ุชุทุงุจู ุงูุฑุณูู ุงููุชุญุฑูุฉ

ุงูููุงููู ุงูุฃุณุงุณูุฉ ูู ูุฐู ุงูููุงูุฉ ูุฑูู ุงูุฑุณูู ุงููุชุญุฑูุฉ ุงูููุงุจู:

| ุงูุฑูู | ุงูููููู | ุงูุชุทุงุจู ุงูููุฒูุงุฆู/ุงูุฑูุงุถู |
|------|------|--------------|
| ๐ฌ C5 | ูุฑุงุญู MCTS ุงูุฃุฑุจุน | ุจุญุซ ุงูุดุฌุฑุฉ |

---

## ุงูููุฎุต

ุฏูุฌ MCTS ูุน ุงูุดุจูุงุช ุงูุนุตุจูุฉ ูู ุงูุงุจุชูุงุฑ ุงูุฃุณุงุณู ูู AlphaGo. ุชุนูููุง:

1. **MCTS ุงูุชูููุฏู**: ุงูุงุฎุชูุงุฑุ ุงูุชูุณูุนุ ุงููุญุงูุงุฉุ ุงูุงูุชุดุงุฑ ุงูุนูุณู
2. **ุชุญุณููุงุช ุงูุดุจูุฉ ุงูุนุตุจูุฉ**: Policy Network ููุฌู ุงูุชูุณูุนุ Value Network ูุญู ูุญู Rollout
3. **ุนูููุฉ ุงูุจุญุซ**: ุงุฎุชูุงุฑ PUCTุ ุงูุชูููู ุงูุฏูุนูุ ุงูุงูุชุดุงุฑ ุงูุนูุณู
4. **ุชูุฒูุน ุงูููุงุฑุฏ**: ุนุฏุฏ ุงููุญุงูุงุงุชุ ุฅุฏุงุฑุฉ ุงูููุชุ ุงูุจุญุซ ุงููุชูุงุฒู
5. **ุงุฎุชูุงุฑ ุฏุฑุฌุฉ ุงูุญุฑุงุฑุฉ**: ุงุณุชุฑุงุชูุฌูุงุช ูุฎุชููุฉ ููุชุฏุฑูุจ ูุงููุจุงุฑุงุฉ
6. **ุชูุงุตูู ุงูุชูููุฐ**: ุฅุฏุงุฑุฉ ุงูุฐุงูุฑุฉุ ุฅุนุงุฏุฉ ุงุณุชุฎุฏุงู ุงูุดุฌุฑุฉุ ุงูุชุฎุฒูู ุงููุคูุช

ูู ุงูููุงู ุงูุชุงููุ ุณูุณุชูุดู ุจุงูุชูุตูู ุงูุฃุณุงุณูุงุช ุงูุฑูุงุถูุฉ ููุนุงุฏูุฉ PUCT.

---

## ูุฑุงุกุฉ ููุณุนุฉ

- **ุงูููุงู ุงูุชุงูู**: [ุดุฑุญ ุชูุตููู ููุนุงุฏูุฉ PUCT](../puct-formula) โ ุงููุจุงุฏุฆ ุงูุฑูุงุถูุฉ ูุงุฎุชูุงุฑ MCTS
- **ุงูููุงู ุงูุณุงุจู**: [ุงููุนุจ ุงูุฐุงุชู](../self-play) โ ุขููุฉ ูุชุฃุซูุฑ ุงููุนุจ ุงูุฐุงุชู
- **ุฐู ุตูุฉ**: [ุดุฑุญ ุชูุตููู ูู Policy Network](../policy-network) โ ุจููุฉ ุดุจูุฉ ุงูุณูุงุณุฉ

---

## ุงููุฑุงุฌุน

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
3. Coulom, R. (2006). "Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search." *Computers and Games*.
4. Kocsis, L., & Szepesvรกri, C. (2006). "Bandit based Monte-Carlo Planning." *ECML*.
5. Browne, C., et al. (2012). "A Survey of Monte Carlo Tree Search Methods." *IEEE TCIAIG*.
6. Rosin, C. D. (2011). "Multi-armed bandits with episode context." *Annals of Mathematics and Artificial Intelligence*.
