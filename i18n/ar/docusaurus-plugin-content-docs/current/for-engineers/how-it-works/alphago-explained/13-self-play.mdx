---
sidebar_position: 14
title: اللعب الذاتي
description: فهم عميق لكيفية اختراق AlphaGo لحدود قوة اللعب البشرية من خلال اللعب الذاتي
---

import { EloChart } from '@site/src/components/D3Charts';

# اللعب الذاتي

في المقال السابق، قدمنا المفاهيم الأساسية للتعلم المعزز. الآن، دعونا نستكشف أحد مفاتيح نجاح AlphaGo — **اللعب الذاتي (Self-Play)**.

هذا مفهوم يبدو متناقضاً: **كيف يمكن للذكاء الاصطناعي أن يصبح أقوى باللعب مع نفسه؟**

الإجابة عميقة وأنيقة في آن واحد، تتضمن نظرية الألعاب، ديناميكيات التطور، وجوهر التعلم.

---

## لماذا اللعب الذاتي فعال؟

### الشرح البديهي

تخيل أنك مبتدئ في الجو، تتدرب وحدك على جزيرة مهجورة:

1. تلعب مباراة، تمثل الأسود والأبيض في نفس الوقت
2. بعد انتهاء المباراة، تحلل أي حركات كانت جيدة وأيها كانت سيئة
3. في المباراة التالية، تحاول تجنب الأخطاء السابقة
4. تكرر هذه العملية ملايين المرات

بديهياً، هذا يبدو به مشكلة:
- إذا كان مستواك ضعيفاً، كلا الطرفين يلعبان حركات سيئة، ماذا يمكن أن تتعلم؟
- هل ستقع في "توازن خاطئ" — كلا الطرفين يخطئان لكن الأخطاء تلغي بعضها؟

لكن في الواقع، اللعب الذاتي يمكن أن ينتج تحسناً مستمراً. الأسباب كما يلي:

### الاكتشاف التدريجي لنقاط الضعف

الرؤية الرئيسية هي: **حتى لو كان كلا الطرفين نفس الذكاء الاصطناعي، نتيجة كل مباراة لا تزال تحتوي على معلومات**.

```
الموقف A: الذكاء الاصطناعي اختار الحركة X، وفاز في النهاية
الموقف A: الذكاء الاصطناعي اختار الحركة Y، وخسر في النهاية

← الاستنتاج: في الموقف A، X أفضل من Y
```

من خلال إحصاء عدد كبير من المباريات، يمكن للذكاء الاصطناعي تعلم أي خيارات أفضل في كل موقف. هذا هو جوهر **تدرج السياسة**: الخيارات الجيدة تُعزز، الخيارات السيئة تُقمع.

### التعلم التنافسي

للعب الذاتي خاصية مميزة: **خصم التدريب يتكيف تلقائياً مع مستواك**.

```
دورة التدريب 1: الذكاء الاصطناعي اكتشف تكتيك فعال T
دورة التدريب 2: الذكاء الاصطناعي كخصم تعلم كيف يدافع ضد T
دورة التدريب 3: الذكاء الاصطناعي الأصلي مجبر على البحث عن تكتيك أفضل T'
```

هذا يشكل **سباق تسلح (Arms Race)**، كلا الطرفين يكتشفان ويتغلبان على نقاط ضعف بعضهما باستمرار.

### المقارنة مع سجلات اللعب البشرية

| طريقة التدريب | المزايا | العيوب |
|---------|------|------|
| **سجلات اللعب البشرية** | تعلم جوهر الحكمة البشرية | محدود بالمستوى البشري |
| **اللعب الذاتي** | إمكانية تحسين بلا حدود | قد يقع في المحلي الأمثل |
| **الجمع بينهما** | بداية سريعة + تحسين مستمر | الاستراتيجية المثلى |

النسخة الأصلية من AlphaGo استخدمت التعلم الخاضع للإشراف من سجلات اللعب البشرية أولاً، ثم التعلم المعزز باللعب الذاتي. AlphaGo Zero أثبت أنه باستخدام اللعب الذاتي فقط يمكن الوصول إلى مستوى فوق بشري.

---

## منظور نظرية الألعاب

### توازن ناش

في نظرية الألعاب، **توازن ناش (Nash Equilibrium)** هو حالة مستقرة: في هذه الحالة، لا يوجد لاعب لديه دافع لتغيير استراتيجيته من جانب واحد.

للعبة الجو كـ **لعبة صفرية المجموع، معلومات كاملة**، لتوازن ناش معنى خاص:

$$\pi^* = \arg\max_\pi \min_{\pi'} V(\pi, \pi')$$

حيث $V(\pi, \pi')$ هي القيمة المتوقعة عندما تواجه الاستراتيجية $\pi$ الاستراتيجية $\pi'$.

هذا هو مبدأ **Minimax** الشهير: الاستراتيجية المثلى هي تلك التي تعمل بشكل أفضل في أسوأ الحالات.

### اللعب الذاتي وتوازن ناش

نظرياً، إذا تقارب اللعب الذاتي، يجب أن يتقارب إلى توازن ناش. للعبة حتمية مثل الجو، توازن ناش هو **اللعب المثالي**.

لكن فضاء حالة الجو ضخم جداً ($10^{170}$)، من المستحيل إيجاد توازن ناش الحقيقي. اللعب الذاتي في الواقع **يقرّب** هذا التوازن.

### اللعب الافتراضي (Fictitious Play)

اللعب الذاتي مرتبط بمفهوم **اللعب الافتراضي** في نظرية الألعاب:

1. كل لاعب يراقب استراتيجيات الخصم التاريخية
2. يحسب متوسط توزيع استراتيجية الخصم
3. يختار أفضل استجابة لهذا التوزيع المتوسط

في ظل شروط معينة، يمكن إثبات أن اللعب الافتراضي سيتقارب إلى توازن ناش.

يمكن اعتبار اللعب الذاتي في AlphaGo كتنفيذ شبكة عصبية لهذا المفهوم.

---

## آلية اللعب الذاتي

### التدفق الأساسي

تدفق اللعب الذاتي في AlphaGo:

```
الخوارزمية: Self-Play Training

التهيئة: شبكة Policy Network π_θ (يمكن أن تبدأ من التعلم الخاضع للإشراف أو تهيئة عشوائية)

كرر الخطوات التالية حتى التقارب:

1. إنتاج بيانات اللعب
   لـ i = 1 إلى N (بالتوازي):
     a. باستخدام السياسة الحالية π_θ قم بجولة لعب ذاتي
     b. اجمع المسار: τ_i = (s_0, a_0, r_1, s_1, a_1, ...)
     c. سجل النتيجة النهائية z_i ∈ {-1, +1}

2. تحديث السياسة
   a. احسب تدرج السياسة:
      ∇J = (1/N) Σ_i Σ_t ∇_θ log π_θ(a_t|s_t) · z_i
   b. حدّث المعلمات: θ ← θ + α · ∇J

3. تحديث شبكة القيمة
   a. درّب شبكة Value Network بأزواج (s, z)
   b. صغّر: L = E[(V_φ(s) - z)²]

4. اختياري: تقييم وحفظ نقاط التحقق
   a. اجعل السياسة الجديدة تواجه النسخ القديمة
   b. إذا معدل الفوز > 55%، حدّث مجمع الخصوم
```

### إنتاج بيانات التدريب

كل جولة لعب ذاتي تنتج **مساراً (trajectory)**:

$$\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, z)$$

حيث:
- $s_t$: حالة اللوحة في الخطوة الزمنية $t$
- $a_t$: الإجراء المختار في الخطوة الزمنية $t$
- $z$: النتيجة النهائية (+1 فوز، -1 خسارة)

مباراة من 200 حركة تنتج 200 عينة تدريب. إجراء مئات الآلاف من المباريات الذاتية يومياً، كمية بيانات التدريب مذهلة.

### تحديث السياسة

استخدام تدرج السياسة لتحديث شبكة Policy Network:

$$\theta \leftarrow \theta + \alpha \cdot \nabla_\theta \mathbb{E}\left[\sum_t \log \pi_\theta(a_t|s_t) \cdot z\right]$$

تأثير هذا التحديث:
- إذا فاز في النهاية ($z = +1$)، زِد احتمال جميع الحركات
- إذا خسر في النهاية ($z = -1$)، قلل احتمال جميع الحركات

يبدو هذا خشناً — مباراة فائزة قد تحتوي أيضاً على حركات سيئة، ومباراة خاسرة قد تحتوي على حركات جيدة. لكن من خلال إحصاء عدد كبير من المباريات، هذه "الضوضاء" ستُتوسط، والحركات الجيدة الحقيقية ستُحدد.

### تدريب شبكة القيمة

تُدرّب شبكة Value Network باستخدام **الانحدار (regression)**:

$$\phi \leftarrow \phi - \beta \cdot \nabla_\phi \mathbb{E}\left[(V_\phi(s) - z)^2\right]$$

هذا يجعل شبكة Value Network تتعلم التنبؤ: من الموقف الحالي، ما هو احتمال الفوز في النهاية؟

دور شبكة Value Network:
1. توفير تقييم عقدة الورقة في MCTS
2. كخط أساس (baseline) لتدرج السياسة
3. للاستخدام المباشر في تقييم الموقف

---

## أهمية العشوائية

### تجنب الدورات الحتمية

إذا كان اللعب الذاتي حتمياً بالكامل، قد يقع في دورة:

```
السياسة A تلعب دائماً نفس الافتتاحية الثابتة
السياسة A ضد السياسة A تنتج دائماً نفس المباراة
مباراة واحدة فقط تُتعلم بشكل متكرر
الذكاء الاصطناعي لا يستطيع استكشاف إمكانيات أخرى
```

لهذا السبب **العشوائية** حاسمة في اللعب الذاتي.

### مصادر العشوائية

طرق AlphaGo لإدخال العشوائية في اللعب الذاتي:

**1. شبكة السياسة نفسها عشوائية**

شبكة Policy Network تخرج توزيع احتمالي، وليس اختياراً حتمياً:

$$a \sim \pi_\theta(a|s)$$

نفس الموقف، قد يُختار لعب مختلف في كل مرة.

**2. معلمة الحرارة**

في التدريب نستخدم حرارة (temperature) أعلى لزيادة التنوع:

$$\pi_\tau(a|s) = \frac{\pi_\theta(a|s)^{1/\tau}}{\sum_{a'} \pi_\theta(a'|s)^{1/\tau}}$$

- $\tau > 1$: أكثر عشوائية، أكثر استكشافاً
- $\tau < 1$: أكثر حتمية، أكثر استغلالاً
- $\tau = 1$: التوزيع الأصلي

**3. ضوضاء ديريكليت (Dirichlet Noise)**

AlphaGo Zero في اللعب الذاتي، يضيف ضوضاء ديريكليت على الاحتمالات المسبقة في العقدة الجذر:

$$P(s, a) = (1 - \varepsilon) \cdot \pi_\theta(a|s) + \varepsilon \cdot \eta_a$$

حيث $\eta \sim \text{Dir}(\alpha)$، $\varepsilon = 0.25$، $\alpha = 0.03$ (لـ 361 إجراء في الجو).

هذا يضمن أنه حتى الحركات ذات الاحتمال المنخفض جداً، لديها فرصة للاستكشاف.

### طريقة مجمع اللعب (Population)

طريقة أخرى لزيادة التنوع هي الحفاظ على **مجمع لعب**:

```
مجمع اللعب = [π_1, π_2, π_3, ..., π_k] (نسخ مختلفة من السياسة)

كل مباراة:
1. اختر عشوائياً خصماً من المجمع
2. العب ضد هذا الخصم
3. استخدم النتيجة لتحديث السياسة الحالية
4. أضف دورياً السياسات المحسنة للمجمع
```

فوائد هذه الطريقة:
- **التنوع**: خصوم بأساليب مختلفة
- **الاستقرار**: تجنب الإفراط في التكيف مع خصم معين
- **المتانة**: تعلم التعامل مع استراتيجيات متنوعة

كلا النسختين الأصلية والصفر من AlphaGo استخدمتا تقنيات مشابهة.

---

## منحنى نمو قوة اللعب

### نظام تصنيف Elo

لتتبع تغيرات قوة لعب الذكاء الاصطناعي، استخدم AlphaGo **نظام تصنيف Elo**.

المبدأ الأساسي لنظام Elo:

$$P(\text{فوز A}) = \frac{1}{1 + 10^{(R_B - R_A)/400}}$$

حيث $R_A$ و $R_B$ هما نقاط Elo للطرفين.

- فرق 200: الأقوى يُتوقع أن يفوز 75%
- فرق 400: الأقوى يُتوقع أن يفوز 90%
- فرق 800: الأقوى يُتوقع أن يفوز 99%

### نمو قوة لعب AlphaGo

دعونا نتخيل بصرياً نمو قوة لعب نسخ AlphaGo المختلفة:

<EloChart mode="zero" width={700} height={400} showMilestones={true} />

### تحليل سرعة النمو

من المنحنى يمكن ملاحظة عدة ظواهر مثيرة:

**1. نمو سريع في البداية**

في الساعات الأولى من التدريب، تعلم الذكاء الاصطناعي القواعد الأساسية والتكتيكات البسيطة. هذه مرحلة **الثمار الدانية** — هناك الكثير من الأخطاء الواضحة التي يمكن تصحيحها.

**2. نمو مستقر في الوسط**

مع إزالة الأخطاء الأساسية، بدأ الذكاء الاصطناعي يتعلم تكتيكات وجوسيكي أكثر دقة. سرعة النمو تبطأ، لكنها لا تزال مستقرة.

**3. تباطؤ النمو في النهاية**

عندما يصبح الذكاء الاصطناعي قوياً جداً، يصبح التحسين الإضافي صعباً. قد يحتاج لاكتشاف استراتيجيات جديدة تماماً، وليس فقط تصحيح الأخطاء.

### لحظة تجاوز البشر

المعالم الرئيسية في منحنى تدريب AlphaGo:

| المعلم | المكافئ | وقت الوصول |
|--------|--------|---------|
| تجاوز الهاوي القوي | Elo ~2700 | حوالي 3 ساعات |
| تجاوز Fan Hui | Elo ~3500 | حوالي 36 ساعة |
| تجاوز Lee Sedol | Elo ~4500 | حوالي 60 ساعة |
| تجاوز AlphaGo الأصلي | Elo ~5000 | حوالي 72 ساعة |

هذه الأرقام (من AlphaGo Zero) مذهلة: **الذكاء الاصطناعي تجاوز في 3 أيام من الصفر آلاف السنين من حكمة الجو البشرية**.

---

## تحليل التقارب

### هل اللعب الذاتي يتقارب؟

هذا سؤال نظري مهم. الإجابة القصيرة: **في ظل شروط معينة نعم، لكن الجو معقد جداً، لا يمكننا الإثبات بدقة**.

### الضمانات النظرية

للألعاب الأبسط (مثل إكس-أو)، يمكن إثبات:

1. **الوجود**: توازن ناش موجود (نظرية Minimax)
2. **التقارب**: بعض الخوارزميات (مثل اللعب الافتراضي) ستتقارب إلى توازن ناش

للجو، ليس لدينا ضمان تقارب صارم، لكن الأدلة التجريبية تظهر:
- قوة اللعب تستمر في التحسن
- لم يظهر تذبذب أو تدهور واضح
- قوة اللعب النهائية تجاوزت جميع البشر المعروفين

### أنماط الفشل المحتملة

مشاكل قد يواجهها اللعب الذاتي:

**1. دورة الاستراتيجيات (Strategy Cycling)**

```
الاستراتيجية A تهزم الاستراتيجية B
الاستراتيجية B تهزم الاستراتيجية C
الاستراتيجية C تهزم الاستراتيجية A
```

هذا يحدث فعلاً في بعض الألعاب (مثل الحجر-الورقة-المقص). لكن الجو معقد بما يكفي، هذه الدورات النقية لا يبدو أنها تحدث.

**2. الإفراط في التكيف مع الذات**

الذكاء الاصطناعي قد يتعلم استراتيجيات موجهة فقط لأسلوبه، ولا يستطيع التعامل مع أساليب خصوم أخرى. لهذا السبب يلعب AlphaGo مع نسخ مختلفة من نفسه، ويُختبر في النهاية مع لاعبين بشريين.

**3. المحلي الأمثل**

الذكاء الاصطناعي قد يقع في محلي أمثل — استراتيجية "جيدة لكن ليست الأفضل". العشوائية واللعب الكثيف يساعدان في تجنب هذه المشكلة.

### الملاحظات الفعلية

من عملية تدريب AlphaGo لوحظ:

1. **تحسن مستمر**: نقاط Elo تستمر في الارتفاع مع التدريب
2. **لا تدهور**: لم يحدث انخفاض مفاجئ في قوة اللعب
3. **تطور الأسلوب**: أسلوب لعب الذكاء الاصطناعي يتغير تدريجياً مع التدريب
4. **اكتشاف جوسيكي جديد**: الذكاء الاصطناعي اكتشف افتتاحيات وتكتيكات لم يستخدمها البشر

هذه الملاحظات تظهر أنه رغم عدم وجود ضمان نظري، اللعب الذاتي فعال عملياً.

---

## تفاصيل التنفيذ

### اللعب الذاتي المتوازي

لتسريع التدريب، استخدم AlphaGo لعباً ذاتياً متوازياً واسع النطاق:

```
البنية:

    ┌────────────────────────────────────────────┐
    │           Parameter Server                  │
    │    (يخزن أحدث θ، يستقبل تحديثات التدرج)    │
    └────────────────────────────────────────────┘
         ▲                              │
         │ تحديث التدرج                 │ أحدث المعلمات
         │                              ▼
    ┌─────────┐  ┌─────────┐  ┌─────────┐
    │ Worker 1 │  │ Worker 2 │  │ Worker N │
    │ لعب ذاتي │  │ لعب ذاتي │  │ لعب ذاتي │
    │ جمع المسارات│  │ جمع المسارات│  │ جمع المسارات│
    └─────────┘  └─────────┘  └─────────┘
```

**قرارات التصميم الرئيسية**:

- **متزامن مقابل غير متزامن**: AlphaGo يستخدم تحديثات غير متزامنة، العمال لا يحتاجون انتظار بعضهم
- **تكرار التحديث**: كل N مباراة مكتملة تحديث واحد للمعلمات
- **اختيار الخصم**: اختيار عشوائي من أحد النسخ الأخيرة كخصم

### استراتيجية نقاط التحقق

حفظ دوري لنقاط تحقق النموذج، للاستخدام في:

1. **مجمع اللعب**: الحفاظ على نسخ مختلفة من الخصوم
2. **التقييم**: تتبع تغيرات قوة اللعب
3. **استعادة الأعطال**: يمكن الاستعادة عند انقطاع التدريب

```python
# شبه كود
def training_loop():
    for iteration in range(num_iterations):
        # إنتاج بيانات اللعب
        trajectories = parallel_self_play(current_policy, num_games=1000)

        # تحديث السياسة
        update_policy(trajectories)

        # تقييم دوري وحفظ
        if iteration % 100 == 0:
            elo = evaluate_against_pool(current_policy)
            save_checkpoint(current_policy, elo)

            if elo > best_elo:
                add_to_pool(current_policy)
                best_elo = elo
```

### متطلبات موارد التدريب

نطاق تدريب AlphaGo مثير للإعجاب:

| النسخة | الأجهزة | وقت التدريب | عدد مباريات اللعب الذاتي |
|------|------|---------|-------------|
| AlphaGo Fan | 176 GPU | أشهر | ~30M |
| AlphaGo Lee | 48 TPU | أسابيع | ~30M |
| AlphaGo Zero | 4 TPU | 3 أيام | ~5M |
| AlphaGo Zero (نسخة 40 يوم) | 4 TPU | 40 يوم | ~30M |

لاحظ أن AlphaGo Zero بأجهزة أقل ووقت أقصر وصل إلى قوة لعب أعلى — هذا تحسن في كفاءة الخوارزمية.

### إعداد المعلمات الفائقة

بعض المعلمات الفائقة الرئيسية:

```python
# إعداد اللعب الذاتي
NUM_PARALLEL_GAMES = 5000      # عدد المباريات المتزامنة
GAMES_PER_ITERATION = 25000    # عدد المباريات لكل دورة
MCTS_SIMULATIONS = 1600        # عدد محاكاات MCTS لكل حركة

# إعداد التدريب
BATCH_SIZE = 2048              # حجم دفعة التدريب
LEARNING_RATE = 0.01           # معدل التعلم الأولي
L2_REGULARIZATION = 1e-4       # تناقص الأوزان

# إعداد الاستكشاف
TEMPERATURE = 1.0              # درجة الحرارة لأول 30 حركة
DIRICHLET_ALPHA = 0.03         # معلمة ضوضاء ديريكليت
EXPLORATION_FRACTION = 0.25    # نسبة الضوضاء
```

هذه المعلمات الفائقة ضُبطت من خلال تجارب كثيرة، ولها تأثير كبير على نتائج التدريب.

---

## تنويعات اللعب الذاتي

### AlphaGo الأصلي

تدفق تدريب النسخة الأصلية من AlphaGo:

```
1. التعلم الخاضع للإشراف (SL): التعلم من سجلات اللعب البشرية
   ← إنتاج شبكة SL Policy Network (π_SL)

2. التعلم المعزز (RL): اللعب الذاتي
   تهيئة π_RL = π_SL
   مجمع الخصوم = [π_SL]

   كرر:
     a. π_RL تلعب ضد استراتيجيات من المجمع
     b. تحديث π_RL بتدرج السياسة
     c. إذا تحسنت π_RL، أضفها للمجمع

   ← إنتاج شبكة RL Policy Network (π_RL)

3. تدريب شبكة القيمة:
   استخدام π_RL للعب الذاتي لإنتاج المواقف
   تدريب V(s) للتنبؤ بمعدل الفوز
```

### AlphaGo Zero

AlphaGo Zero بسّط هذا التدفق:

```
1. لعب ذاتي محض (بدون بيانات بشرية)
   تهيئة شبكة عشوائية f_θ

   كرر:
     a. استخدام MCTS + f_θ للعب الذاتي
     b. تدريب رأس السياسة ورأس القيمة معاً
     c. تحديث f_θ

   ← شبكة واحدة تخرج السياسة والقيمة معاً
```

التحسينات الرئيسية:
- **لا حاجة لبيانات بشرية**: البداية من الصفر
- **شبكة واحدة**: السياسة والقيمة تشتركان في الميزات
- **تدريب أكثر بساطة**: تعلم من طرف إلى طرف

### AlphaZero

AlphaZero عمّم أكثر:

```
نفس الخوارزمية، ألعاب مختلفة:
- الجو: وصل لمستوى يتجاوز AlphaGo Zero
- الشطرنج: تجاوز Stockfish
- الشوغي: تجاوز Elmo

الجزء الوحيد الخاص باللعبة: ترميز القواعد
```

هذا يثبت أن اللعب الذاتي هو **نموذج تعلم عام**، ليس مقتصراً على الجو.

---

## ماذا تعلم البشر من هذا؟

### الجوسيكي الجديد الذي اكتشفه الذكاء الاصطناعي

اللعب الذاتي أنتج العديد من الحركات التي لم يستخدمها البشر:

**1. ابتكارات افتتاحية**

بعض الافتتاحيات المفضلة لدى AlphaGo:
- غزو 3-3: الغزو المبكر للزاوية
- حركات عالية: كانت تُعتبر تقليدياً "غير مستقرة"
- تغيرات الانهيار الثلجي الكبير: كان البشر يعتبرونها معقدة يصعب حسابها

**2. تقييم موقف جديد**

تقييم الذكاء الاصطناعي لبعض المواقف يختلف كثيراً عن البشر:
- بعض الأشكال التي تبدو "ضعيفة" هي في الواقع صلبة
- قيمة بعض "السماكة" كانت مبالغاً فيها
- إعادة تقييم "سنتي" و"غوتي"

### التأثير على الجو البشري

بعد AlphaGo، حدثت تغييرات ملحوظة في الجو المحترف:

1. **تنويع الافتتاحيات**: اللاعبون المحترفون بدأوا باستخدام افتتاحيات جديدة اكتشفها الذكاء الاصطناعي
2. **تغيير طرق التدريب**: الذكاء الاصطناعي أصبح أداة التدريب الرئيسية للمحترفين
3. **إعادة التفكير في نظرية الجو**: الكثير من "النظريات" التقليدية تم التشكيك فيها وتعديلها
4. **جمالية جديدة**: بدأ تقدير أسلوب لعب الذكاء الاصطناعي

قال كيه جيه بعد خسارته أمام AlphaGo:

> "AlphaGo جعلني أعيد التعرف على الجو. كنت أظن أن البشر يفهمون الجو، الآن أعرف أننا لم نمس سوى السطح."

---

## تأملات فلسفية

### جوهر التعلم

اللعب الذاتي يطرح أسئلة عميقة حول التعلم:

**من أين تأتي المعرفة؟**

- التعلم البشري يعتمد على معلومات خارجية (المعلم، الكتب، الخبرة)
- الذكاء الاصطناعي باللعب الذاتي لديه فقط القواعد، لا معرفة خارجية
- لكنه لا يزال يستطيع "اكتشاف" المعرفة — من أين تأتي هذه المعرفة؟

الجواب قد يكون: **المعرفة مضمّنة في قواعد وبنية اللعبة**. قواعد الجو تحدد ما هو حركة جيدة وما هو حركة سيئة، اللعب الذاتي يكشف فقط عن هذه البنى المضمّنة.

### الإبداع والاكتشاف

عندما يلعب الذكاء الاصطناعي "الحركة 37 الإلهية"، هل هذا إبداع أم اكتشاف؟

وجهة نظر: تلك الحركة كانت دائماً "موجودة" في قواعد الجو، الذكاء الاصطناعي فقط "اكتشفها".
وجهة نظر أخرى: الذكاء الاصطناعي "أبدع" تلك الحركة، لأن لا أحد (بما في ذلك الذكاء الاصطناعي نفسه) كان يعرفها مسبقاً.

هذا السؤال ليس له إجابة قياسية، لكنه يتحدى فهمنا التقليدي للإبداع.

### موقع الذكاء البشري

إذا كان الذكاء الاصطناعي يستطيع من الصفر، من خلال اللعب الذاتي، تجاوز آلاف السنين من الحكمة البشرية، ماذا يعني هذا للبشر؟

وجهة نظر متفائلة:
- الذكاء الاصطناعي أداة صنعها البشر
- اكتشافات الذكاء الاصطناعي يمكن أن تعزز الفهم البشري
- البشر يمكنهم التعاون مع الذكاء الاصطناعي للوصول إلى مستويات أعلى

وجهة نظر حذرة:
- في بعض المجالات، الحساب المحض قد يتجاوز الحدس البشري
- يجب إعادة التفكير في قيمة "المهارات المتخصصة"
- طرق التعليم والتدريب قد تحتاج للتغيير

---

## مطابقة الرسوم المتحركة

المفاهيم الأساسية المذكورة في هذا المقال ومطابقتها لأرقام الرسوم المتحركة:

| الرقم | المفهوم | المقابلة الفيزيائية/الرياضية |
|------|------|--------------|
| E5 | دورة اللعب الذاتي | تكرار النقطة الثابتة |
| E6 | تطور السياسة | ديناميكيات التطور |

---

## الملخص

اللعب الذاتي هو أحد التقنيات الرئيسية لنجاح AlphaGo. تعلمنا:

1. **لماذا فعال**: التعلم التنافسي، الاكتشاف التدريجي لنقاط الضعف
2. **الآلية**: جمع المسارات، تدرج السياسة، تدريب شبكة القيمة
3. **العشوائية**: معلمة الحرارة، ضوضاء ديريكليت، مجمع اللعب
4. **نمو قوة اللعب**: نظام Elo، تحليل منحنى النمو
5. **التقارب**: الضمانات النظرية والملاحظات الفعلية
6. **تفاصيل التنفيذ**: التدريب المتوازي، استراتيجية نقاط التحقق، المعلمات الفائقة

في المقال التالي، سنستكشف كيف يجمع AlphaGo بين الشبكات العصبية وMCTS، للاستفادة من مزايا كليهما.

---

## قراءة موسعة

- **المقال التالي**: [دمج MCTS مع الشبكات العصبية](../mcts-neural-combo) — الدمج المثالي بين الحدس والاستدلال
- **المقال السابق**: [مقدمة في التعلم المعزز](../reinforcement-intro) — المفاهيم الأساسية للتعلم المعزز
- **ذو صلة**: [نظرة عامة على AlphaGo Zero](../alphago-zero) — الاختراق من الصفر

---

## المراجع

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
3. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
4. Heinrich, J., & Silver, D. (2016). "Deep Reinforcement Learning from Self-Play in Imperfect-Information Games." *arXiv preprint*.
5. Lanctot, M., et al. (2017). "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning." *NeurIPS*.
