---
sidebar_position: 11
title: الشبكات العصبية الالتفافية ولعبة الجو
description: استكشاف معمق لماذا الشبكات العصبية الالتفافية مناسبة بشكل خاص للعبة الجو، من حقول الاستقبال إلى تطبيع الدفعات
---

# الشبكات العصبية الالتفافية ولعبة الجو

عندما اختارت DeepMind استخدام **الشبكات العصبية الالتفافية (CNN)** لمعالجة لعبة الجو، كان ذلك قراراً تصميمياً عبقرياً.

صُممت CNN في الأصل للتعرف على الصور. فلماذا تناسب أيضاً لعبة الجو؟ سيستكشف هذا المقال مبادئ عمل CNN بعمق، وتناسبها المثالي مع لعبة الجو.

---

## لماذا CNN مناسبة للوحة اللعب؟

### لوحة اللعب هي "صورة"

من منظور معين، لوحة الجو 19×19 هي **صورة**:

| الصورة | لوحة الجو |
|------|---------|
| البكسل | نقاط التقاطع |
| قنوات RGB | مستويات الميزات (أسود، أبيض، فارغ...) |
| 224×224 | 19×19 |
| تمييز القطط والكلاب | الحكم على الحركة الجيدة والسيئة |

هذا التشبيه ليس صدفة. الأسباب التي تجعل CNN بارعة في الصور هي نفسها التي تجعلها بارعة في لوحات اللعب.

### ثلاث خصائص رئيسية

تمتلك CNN ثلاث خصائص تجعلها مناسبة بشكل خاص لبيانات نوع لوحة اللعب:

#### 1. الاتصال المحلي (Local Connectivity)

نواة الالتفاف في CNN تنظر فقط إلى منطقة محلية، وهذا يتطابق تماماً مع خصائص لعبة الجو:

```
التعرف على الصور:             الجو:
آذان القطة ميزة محلية      "العين" شكل حجري محلي
لا تحتاج لرؤية الصورة كاملة    لا تحتاج لرؤية اللوحة كاملة

    منطقة 3×3               منطقة 3×3
   ┌───────┐            ┌───────┐
   │ ○ ● ○ │            │ ○ ● ○ │
   │ ● ○ ● │  ←  عين   │ ● · ● │
   │ ○ ● ○ │            │ ○ ● ○ │
   └───────┘            └───────┘
```

العديد من مفاهيم الجو "محلية":
- **العين**: منطقة 2×2 أو 3×3
- **الأتاري**: منطقة 3×3
- **الوصل والقطع**: منطقة 2×2

#### 2. مشاركة الأوزان (Weight Sharing)

نفس نواة الالتفاف تمسح اللوحة بأكملها، مما يعني:

> **"العين" في الزاوية العلوية اليسرى من اللوحة و"العين" في الزاوية السفلية اليمنى، يتم التعرف عليهما بنفس الطريقة**

هذا منطقي - قواعد الجو لا تتغير حسب الموقع (باستثناء الحواف والزوايا، ولكن يمكن معالجتها بمستويات ميزات الحافة والزاوية).

مشاركة الأوزان تقلل أيضاً بشكل كبير من عدد المعلمات:

| الطريقة | عدد المعلمات |
|------|--------|
| شبكة متصلة بالكامل | 361 × 361 × عدد القنوات = عشرات الملايين |
| CNN | 3 × 3 × عدد القنوات × عدد المرشحات = ملايين |

#### 3. التكافؤ الانتقالي (Translation Equivariance)

إذا انتقل المدخل، فإن مخرجات CNN تنتقل أيضاً بشكل مقابل:

```
المدخل:                     المخرج (منطقة احتمال عالي):
  A B C D E                 A B C D E
1 . . . . .              1  . . . . .
2 . ● . . .   →          2  . * . . .
3 . . . . .              3  . . . . .

بعد انتقال المدخل:          المخرج ينتقل أيضاً:
  A B C D E                 A B C D E
1 . . . . .              1  . . . . .
2 . . . . .   →          2  . . . . .
3 . . ● . .              3  . . * . .
```

هذا مهم للجو: نفس الشكل الحجري المحلي، أينما ظهر على اللوحة، يجب أن يكون له تقييم مماثل.

---

## عملية الالتفاف

### المبدأ الأساسي

عملية الالتفاف هي جوهر CNN. إنها عملية "نافذة منزلقة":

```
المدخل (5×5):              نواة الالتفاف (3×3):      المخرج (5×5, padding=same):
┌───────────────┐         ┌─────────┐             ┌───────────────┐
│ 1 0 1 0 0     │         │ 1 0 1   │             │ 2 1 3 1 2     │
│ 0 1 1 1 0     │   *     │ 0 1 0   │      =      │ 1 4 3 3 1     │
│ 1 1 1 1 1     │         │ 1 0 1   │             │ 3 3 5 3 3     │
│ 0 0 1 1 0     │         └─────────┘             │ 1 3 3 4 1     │
│ 0 1 0 0 1     │                                 │ 2 1 3 1 2     │
└───────────────┘                                 └───────────────┘
```

عملية الحساب (مثال النقطة المركزية):

```
المخرج[2,2] = 1×1 + 1×0 + 1×1 +
              1×0 + 1×1 + 1×0 +
              1×1 + 1×0 + 1×1
            = 1 + 0 + 1 + 0 + 1 + 0 + 1 + 0 + 1
            = 5
```

### الالتفاف متعدد القنوات

عندما يحتوي المدخل على قنوات متعددة (مثل 48 مستوى ميزة)، تصبح نواة الالتفاف ثلاثية الأبعاد:

```
المدخل: 19×19×48          نواة الالتفاف: 3×3×48      المخرج: 19×19×1
    ┌─────┐                 ┌───┐                   ┌─────┐
   ╱     ╱│                ╱   ╱│                  │     │
  ╱     ╱ │   التفاف      ╱   ╱ │                  │     │
 ╱     ╱  │   →          ╱   ╱  │    →             │     │
┌─────┐   │             ┌───┐   │                  └─────┘
│     │  ╱              │   │  ╱
│     │ ╱               │   │╱
└─────┘╱                └───┘
48 طبقة                  48 طبقة                   1 طبقة
```

كل نواة التفاف تحسب عبر جميع قنوات المدخل، وتنتج قناة مخرج واحدة.

### مرشحات متعددة

يستخدم AlphaGo 192 مرشحاً، كل مرشح يتعلم ميزات مختلفة:

```
المدخل: 19×19×48

      ┌─────┐
     ╱     ╱│
    ╱     ╱ │     192 نواة التفاف 3×3×48
   ╱     ╱  │     ────────────────────►
  ┌─────┐   │
  │     │  ╱
  │     │ ╱
  └─────┘╱

المخرج: 19×19×192

      ┌─────┐
     ╱     ╱│
    ╱     ╱ │
   ╱     ╱  │
  ┌─────┐   │
  │     │  ╱
  │     │ ╱
  └─────┘╱
```

كل مرشح قد يتعلم أشكال حجرية مختلفة:
- المرشح 1: كشف العين
- المرشح 2: كشف نقطة القطع
- المرشح 3: كشف الوصل
- ...
- المرشح 192: نمط معقد ما

---

## حقل الاستقبال

### ما هو حقل الاستقبال؟

**حقل الاستقبال (Receptive Field)** يشير إلى أي مواقع في المدخل تؤثر على موقع واحد في المخرج.

#### التفاف طبقة واحدة

عند استخدام نواة التفاف 3×3، كل موقع في المخرج يتأثر فقط بمنطقة 3×3 في المدخل:

```
المدخل:                     المخرج:
  ┌─────────────┐           ┌───────────┐
  │ . . . . .   │           │ . . . .   │
  │ . ● ● ● .   │  →        │ . ● . .   │
  │ . ● ● ● .   │           │ . . . .   │
  │ . ● ● ● .   │           │ . . . .   │
  │ . . . . .   │           └───────────┘
  └─────────────┘
     حقل استقبال 3×3
```

#### التفاف متعدد الطبقات

بعد تكديس طبقات متعددة من الالتفاف، يتوسع حقل الاستقبال:

| عدد الطبقات | حقل الاستقبال | الحساب |
|------|--------|------|
| 1 | 3×3 | 3 |
| 2 | 5×5 | 3 + (3-1) = 5 |
| 3 | 7×7 | 5 + (3-1) = 7 |
| ... | ... | ... |
| 12 | 25×25 | 3 + 11×2 = 25 |

12 طبقة التفاف في AlphaGo تعطي **حقل استقبال 25×25**، وهو يتجاوز بالفعل لوحة 19×19!

هذا يعني:
- **كل موقع في المخرج يمكنه "رؤية" اللوحة بأكملها**
- لكن طريقة "الرؤية" مختلفة: التفاصيل القريبة واضحة، البعيدة ملخصة
- هذا يشبه طريقة تفكير لاعبي الجو البشريين

### حقل الاستقبال والجو

مفهوم حقل الاستقبال يفسر لماذا يستطيع AlphaGo التعامل مع المشاكل "العامة":

```
مشاكل محلية (حقل استقبال 3×3):     مشاكل عامة (حقل استقبال 25×25):
- هل هناك عين هنا؟                - هل لهذه المجموعة مساحة عين؟
- هل يمكن الأتاري؟                - هل اللادر مفيد؟
- هل يمكن الوصل؟                  - كيف الوضع العام؟
```

الطبقات الضحلة تعالج الميزات المحلية، الطبقات العميقة تعالج الميزات العامة.

---

## الميزات المحلية مقابل العامة

### البنية الهرمية لـ CNN

تشكل CNN بنية هرمية طبيعياً:

```
طبقة المدخل:      أحجار سوداء، أحجار بيضاء، نقاط فارغة
   ↓
الطبقات الضحلة (1-3):    عين، وصل، قطع، أتاري
   ↓
الطبقات الوسطى (4-8):    أشكال حجرية، حياة، موت
   ↓
الطبقات العميقة (9-12):   النفوذ، السماكة/الرقة، النقاط الكبيرة
   ↓
طبقة المخرج:      احتمال اللعب / معدل الفوز
```

هذا يشبه بشكل مدهش عملية تعلم الإنسان للجو:
1. تعلم القواعد أولاً (أين الأحجار)
2. ثم تعلم التكتيكات (كيف تأسر)
3. ثم تعلم الأشكال (ما هو الشكل الجيد)
4. أخيراً تعلم الرؤية الكبيرة (الحكم العام)

### تصور الطبقات المخفية

وجد الباحثون أن الطبقات المخفية في CNN تتعلم بالفعل ميزات ذات معنى:

#### مرشحات الطبقات الضحلة

```
المرشح A (كشف العين):     المرشح B (كشف الأتاري):
┌───────┐                ┌───────┐
│ + - + │                │ + + + │
│ - + - │                │ + - - │
│ + - + │                │ + + + │
└───────┘                └───────┘
```

#### مرشحات الطبقات العميقة

مرشحات الطبقات العميقة أكثر تجريداً، يصعب شرحها مباشرة، لكنها تلتقط أنماط أشكال حجرية معقدة.

---

## اختيار دالة التنشيط

### ReLU: بسيط وفعال

يستخدم AlphaGo **ReLU (Rectified Linear Unit)** بعد جميع طبقات الالتفاف:

```python
def relu(x):
    return max(0, x)
```

رسم توضيحي:

```
المخرج
  │
  │    /
  │   /
  │  /
  │ /
──┼────── المدخل
  │
```

### لماذا لا نستخدم دوال أخرى؟

| دالة التنشيط | الصيغة | المزايا | العيوب |
|----------|------|------|------|
| ReLU | max(0, x) | حساب سريع، تدرج جيد | موت القيم السالبة |
| Sigmoid | 1/(1+e^-x) | مخرج محدود | تلاشي التدرج |
| Tanh | (e^x-e^-x)/(e^x+e^-x) | مركز صفري | تلاشي التدرج |
| LeakyReLU | max(0.01x, x) | يحل مشكلة الموت | معلمة فائقة إضافية |

للشبكات العميقة، مزايا ReLU واضحة:
1. **حساب بسيط**: فقط مقارنة وأخذ الأقصى
2. **التدرج لا يتلاشى**: التدرج ثابت = 1 في المنطقة الموجبة
3. **تنشيط متناثر**: العديد من الخلايا العصبية تخرج 0، مما يحسن الكفاءة

### معنى ReLU في الجو

التناثر في ReLU له تفسير مثير في الجو:

```
مرشح معين يكشف "نقطة القطع":
- هناك نقطة قطع → مخرج موجب (تنشيط)
- لا توجد نقطة قطع → مخرج صفر (عدم تنشيط)

هذا مثل اللاعب الذي يهتم فقط بالمواقع "المهمة"
```

---

## تطبيع الدفعات

### ما هو تطبيع الدفعات؟

**تطبيع الدفعات (Batch Normalization)** هي تقنية تحافظ على توزيع مستقر لمخرجات كل طبقة:

```python
def batch_norm(x, gamma, beta):
    # حساب المتوسط والانحراف المعياري للدفعة
    mean = x.mean(axis=0)
    std = x.std(axis=0)

    # التطبيع
    x_norm = (x - mean) / (std + 1e-8)

    # القياس والإزاحة
    return gamma * x_norm + beta
```

### لماذا نحتاجه؟

#### انزياح التغاير الداخلي

عندما تتدرب الشبكة، يتغير توزيع مدخلات كل طبقة مع تغير أوزان الطبقات السابقة. هذا يسمى "انزياح التغاير الداخلي":

```
تحديث أوزان الطبقة الأولى → تغير توزيع مخرجات الطبقة الأولى
                    ↓
               تغير توزيع مدخلات الطبقة الثانية → الطبقة الثانية تحتاج للتكيف مرة أخرى
                                        ↓
                                   ... (ينتشر للأسفل)
```

تطبيع الدفعات يثبت التدريب بإجبار مدخلات كل طبقة على توزيع ثابت (متوسط 0، انحراف معياري 1).

### التطبيق في AlphaGo

يستخدم AlphaGo تطبيع الدفعات بعد كل طبقة التفاف، قبل دالة التنشيط:

```
Conv → BatchNorm → ReLU → Conv → BatchNorm → ReLU → ...
```

الفوائد:
1. **تدريب أسرع**: يمكن استخدام معدل تعلم أكبر
2. **أكثر استقراراً**: يقلل الحساسية للتهيئة
3. **تأثير تنظيمي**: له تأثير dropout خفيف

### المعالجة أثناء الاستدلال

أثناء التدريب، نستخدم إحصائيات الدفعة الحالية. أثناء الاستدلال، نستخدم إحصائيات مجموعة التدريب الكاملة (المتوسط المتحرك):

```python
# أثناء التدريب
mean = batch_mean
var = batch_var

# أثناء الاستدلال
mean = running_mean  # المتوسط المتراكم أثناء التدريب
var = running_var    # التباين المتراكم أثناء التدريب
```

---

## التكوين المحدد لـ AlphaGo

### البنية الكاملة

```
المدخل: 19×19×48

الطبقة 1:
  Conv2D(5×5, 192 filters, padding='same')
  BatchNorm
  ReLU
  المخرج: 19×19×192

الطبقات 2-12 (11 طبقة):
  Conv2D(3×3, 192 filters, padding='same')
  BatchNorm
  ReLU
  المخرج: 19×19×192

طبقة المخرج (Policy):
  Conv2D(1×1, 1 filter)
  Flatten
  Softmax
  المخرج: 361 احتمال بُعدي

طبقة المخرج (Value):
  Conv2D(1×1, 1 filter)
  Flatten
  Dense(256)
  ReLU
  Dense(1)
  Tanh
  المخرج: قيمة واحدة
```

### تكوين المعلمات

| المعلمة | القيمة | الشرح |
|------|------|------|
| قنوات المدخل | 48 | عدد مستويات الميزات |
| عدد المرشحات | 192 | عدد القنوات في كل طبقة |
| حجم نواة الالتفاف | 3×3 (الطبقة الأولى 5×5) | حقل الاستقبال |
| عدد الطبقات | 13 (شاملة طبقة المخرج) | العمق |
| دالة التنشيط | ReLU | اللاخطية |
| التطبيع | BatchNorm | تثبيت التدريب |

### التنفيذ بـ PyTorch

```python
import torch
import torch.nn as nn

class AlphaGoCNN(nn.Module):
    def __init__(self, input_channels=48, num_filters=192, num_layers=12):
        super().__init__()

        # الطبقة الأولى (التفاف 5×5)
        self.conv1 = nn.Sequential(
            nn.Conv2d(input_channels, num_filters, kernel_size=5, padding=2),
            nn.BatchNorm2d(num_filters),
            nn.ReLU(inplace=True)
        )

        # الطبقات الوسطى (التفاف 3×3)
        self.conv_layers = nn.Sequential(*[
            nn.Sequential(
                nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),
                nn.BatchNorm2d(num_filters),
                nn.ReLU(inplace=True)
            )
            for _ in range(num_layers - 1)
        ])

        # رأس مخرج Policy
        self.policy_head = nn.Sequential(
            nn.Conv2d(num_filters, 1, kernel_size=1),
            nn.Flatten(),
            nn.Softmax(dim=1)
        )

        # رأس مخرج Value
        self.value_head = nn.Sequential(
            nn.Conv2d(num_filters, 1, kernel_size=1),
            nn.Flatten(),
            nn.Linear(361, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 1),
            nn.Tanh()
        )

    def forward(self, x):
        # استخراج الميزات المشتركة
        x = self.conv1(x)
        x = self.conv_layers(x)

        # مخرجات منفصلة
        policy = self.policy_head(x)
        value = self.value_head(x)

        return policy, value
```

---

## المقارنة مع البنى الأخرى

### الشبكة المتصلة بالكامل

إذا استخدمنا شبكة متصلة بالكامل لمعالجة الجو:

| الخاصية | متصلة بالكامل | CNN |
|------|--------|-----|
| عدد المعلمات | ضخم جداً (مئات الملايين) | أصغر (ملايين) |
| ثبات الموقع | لا يوجد | يوجد |
| الميزات المحلية | صعبة التعلم | التقاط طبيعي |
| كفاءة التدريب | منخفضة | عالية |

الشبكة المتصلة بالكامل لا تستطيع الاستفادة من البنية المكانية للوحة، وكفاءتها منخفضة للغاية.

### الشبكات العصبية المتكررة (RNN)

RNN مناسبة للبيانات التسلسلية (مثل تاريخ اللعبة)، لكن:

| الخاصية | RNN | CNN |
|------|-----|-----|
| المعالجة المكانية | ضعيفة | قوية |
| المعالجة التسلسلية | قوية | ضعيفة (تحتاج مستويات تاريخ) |
| التوازي | صعب | سهل |
| الاعتماديات طويلة المدى | تحتاج LSTM | الطبقات العميقة كافية |

اختار AlphaGo CNN + مستويات التاريخ، بدلاً من CNN + RNN.

### الشبكات المتبقية (ResNet)

ترقى AlphaGo Zero إلى ResNet:

```
CNN عادية:                ResNet:
  x                        x
  ↓                        ↓
 Conv                     Conv
  ↓                        ↓
 ReLU                    ReLU
  ↓                        ↓
 Conv                     Conv
  ↓                        ↓
  y                      y + x  ← اتصال متبقي
```

الاتصال المتبقي يسهل تدفق التدرج، ويمكن تدريب شبكات أعمق (40 طبقة مقابل 12 طبقة).

انظر [الشبكة ثنائية الرأس والشبكة المتبقية](./17-dual-head-resnet) للتفاصيل.

---

## الفهم المرئي

### عملية الالتفاف

```
لوحة المدخل (مبسطة إلى 5×5):

   A B C D E
1  . . . . .
2  . ● . . .
3  . . ○ . .
4  . . . ● .
5  . . . . .

مرشح معين (3×3, كشف "شكل الصليب"):
┌───────┐
│ 0 1 0 │
│ 1 1 1 │
│ 0 1 0 │
└───────┘

مخرج الالتفاف:
   A B C D E
1  0 0 0 0 0
2  0 0 0 0 0
3  0 0 1 0 0   ← استجابة قوية في المركز (تطابق شكل الصليب)
4  0 0 0 0 0
5  0 0 0 0 0
```

### ميزات متعددة الطبقات

```
مخرج الطبقة 1 (4 من 192 قناة):

القناة 1 (العين):    القناة 2 (الخط الحدي):    القناة 3 (نقطة القطع):    القناة 4 (الوصل):
┌─────────┐        ┌─────────┐             ┌─────────┐             ┌─────────┐
│ 0 0 0 0 │        │ 0.8 0 0 │             │ 0 0 0 0 │             │ 0 0 0 0 │
│ 0 0.9 0 │        │ 0.8 0 0 │             │ 0 0 0.7 │             │ 0 0.5 0 │
│ 0 0 0 0 │        │ 0.8 0 0 │             │ 0 0 0 0 │             │ 0 0.8 0 │
│ 0 0 0 0 │        │ 0.8 0 0 │             │ 0 0 0 0 │             │ 0 0.5 0 │
└─────────┘        └─────────┘             └─────────┘             └─────────┘

هذه الميزات ستُدمج في طبقات أعمق لتكوين مفاهيم أكثر تعقيداً...
```

---

## مطابقة الرسوم المتحركة

المفاهيم الأساسية المذكورة في هذا المقال ومطابقتها لأرقام الرسوم المتحركة:

| الرقم | المفهوم | المقابلة الفيزيائية/الرياضية |
|------|------|--------------|
| D9 | عملية الالتفاف | استجابة المرشح |
| D10 | حقل الاستقبال | محلي→عام |
| D11 | تطبيع الدفعات | استقرار التوزيع |
| D1 | مدخل متعدد القنوات | عملية التنسور |

---

## قراءة موسعة

- **المقال السابق**: [تصميم ميزات المدخلات](./09-input-features) — شرح تفصيلي لـ 48 مستوى ميزة
- **المقال التالي**: [مرحلة التعلم الخاضع للإشراف](./11-supervised-learning) — كيف نتعلم من سجلات اللعب البشرية
- **موضوع متقدم**: [الشبكة ثنائية الرأس والشبكة المتبقية](./17-dual-head-resnet) — ترقية شبكة AlphaGo Zero

---

## النقاط الرئيسية

1. **CNN مناسبة طبيعياً للوحات اللعب**: الاتصال المحلي، مشاركة الأوزان، التكافؤ الانتقالي
2. **الالتفاف يستخرج الميزات المحلية**: التعرف على أنماط منطقة 3×3
3. **الشبكة العميقة تحصل على رؤية عامة**: 12 طبقة → حقل استقبال 25×25
4. **ReLU سريع وفعال**: تنشيط لاخطي بسيط
5. **BatchNorm يثبت التدريب**: تطبيع مخرجات كل طبقة

CNN تمكن AlphaGo من "رؤية" لوحة اللعب — بشكل طبيعي كما يرى الإنسان الصور.

---

## المراجع

1. LeCun, Y., Bengio, Y., & Hinton, G. (2015). "Deep learning." *Nature*, 521, 436-444.
2. He, K., et al. (2015). "Deep Residual Learning for Image Recognition." *CVPR*.
3. Ioffe, S., & Szegedy, C. (2015). "Batch Normalization: Accelerating Deep Network Training." *ICML*.
4. Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). "ImageNet Classification with Deep Convolutional Neural Networks." *NeurIPS*.
