---
sidebar_position: 17
title: Visión General de AlphaGo Zero
description: Desde cero, completamente autodidacta, cómo AlphaGo Zero superó todas las versiones anteriores sin usar partidas humanas
keywords: [AlphaGo Zero, auto-juego, aprendizaje por refuerzo, deep learning, Go AI, aprendizaje no supervisado]
---

# Visión General de AlphaGo Zero

En octubre de 2017, DeepMind publicó un resultado que conmocionó al mundo de la IA: **AlphaGo Zero**, entrenado desde un estado completamente aleatorio sin usar ninguna partida humana, superó al AlphaGo original que derrotó a Lee Sedol en solo tres días, ganando por un marcador de **100:0**.

Esto no es solo un avance numérico. Representa un paradigma completamente nuevo: **la IA no necesita conocimiento humano, puede descubrir todo desde cero**.

---

## ¿Por Qué No Se Necesitan Partidas Humanas?

### Limitaciones de las Partidas Humanas

El proceso de entrenamiento del AlphaGo original se dividió en dos etapas:

1. **Aprendizaje supervisado**: Entrenar la Policy Network con 30 millones de partidas humanas
2. **Aprendizaje por refuerzo**: Mejorar aún más a través del auto-juego

Este método tiene varios problemas fundamentales:

#### 1. Las Partidas Humanas Tienen un Límite Superior

La fuerza de los jugadores humanos tiene límites, y las partidas contienen la comprensión humana, incluyendo errores y sesgos humanos. Cuando la IA aprende de partidas humanas, aprende:

- Lo que los humanos creen que son buenos movimientos (pero no necesariamente óptimos)
- Patrones de pensamiento humano (pero pueden limitar la innovación)
- Errores humanos (que se aprenden como muestras correctas)

#### 2. El Cuello de Botella del Aprendizaje Supervisado

El objetivo del aprendizaje supervisado es "imitar a humanos" -- predecir qué movimiento jugará un jugador humano. Esto significa que el límite superior de la capacidad de la IA está limitado por la capacidad de los jugadores humanos.

Es como un aprendiz que solo puede imitar al maestro, nunca puede superar al maestro.

#### 3. Costo de Recolección de Datos

Las partidas humanas de alta calidad necesitan años para acumularse, y solo existen para juegos con larga historia como Go. Si quisieras aplicar IA a nuevos dominios (como predicción de estructuras de proteínas), simplemente no hay "partidas de expertos humanos" disponibles.

### El Avance de Zero

AlphaGo Zero omitió completamente la etapa de aprendizaje supervisado, comenzando directamente desde **inicialización aleatoria** con auto-juego. Esto resolvió todos los problemas mencionados:

| Problema | AlphaGo Original | AlphaGo Zero |
|----------|------------------|--------------|
| Límite del conocimiento humano | Limitado por calidad de partidas | Sin esta limitación |
| Objetivo de aprendizaje | Imitar humanos | Maximizar tasa de victoria |
| Requisitos de datos | 30 millones de partidas | 0 |
| Generalización | Solo Go | Generalizable a otros dominios |

Este es un cambio de paradigma fundamental: de "aprender conocimiento humano" a "descubrir conocimiento desde primeros principios".

---

## Comparación con AlphaGo Original: 100:0

### Victoria Aplastante

DeepMind hizo que AlphaGo Zero entrenado jugara contra varias versiones de AlphaGo:

| Oponente | Récord de AlphaGo Zero |
|----------|------------------------|
| AlphaGo Fan (versión que derrotó a Fan Hui) | 100:0 |
| AlphaGo Lee (versión que derrotó a Lee Sedol) | 100:0 |
| AlphaGo Master (versión 60 victorias consecutivas) | 89:11 |

**100:0** -- esto significa que en 100 partidas, el AlphaGo original no pudo ganar ni una sola.

### Menos Recursos, Mayor Fuerza

No solo ganó, AlphaGo Zero logró mayor fuerza con menos recursos:

| Métrica | AlphaGo Lee | AlphaGo Zero |
|---------|-------------|--------------|
| Tiempo de entrenamiento | Varios meses | 40 días (3 días para superar a AlphaGo Lee) |
| Partidas de entrenamiento | 30 millones humanas + auto-juego | 4.9 millones de auto-juego |
| TPUs (entrenamiento) | 50+ | 4 |
| TPUs (inferencia) | 48 | 4 |
| Características de entrada | 48 planos | 17 planos |
| Red neuronal | Redes SL + RL separadas | Red única de doble cabeza |

Esta es una mejora de eficiencia asombrosa: **más de 10 veces menos recursos, pero fuerza significativamente mayor**.

### ¿Por Qué Zero Es Más Fuerte?

Las razones por las que AlphaGo Zero es más fuerte se pueden entender desde varios ángulos:

#### 1. Aprendizaje Sin Sesgos

El AlphaGo original aprendió de partidas humanas, heredando sesgos humanos. Por ejemplo, los jugadores humanos pueden sobrevalorar ciertas joseki, o tener evaluaciones incorrectas de ciertas posiciones.

AlphaGo Zero no tiene esta carga. Comenzó desde una hoja en blanco, aprendiendo solo a través de resultados de victoria/derrota qué es un buen movimiento. Esto le permitió descubrir movimientos que los humanos nunca habían pensado.

#### 2. Objetivo de Aprendizaje Consistente

El entrenamiento del AlphaGo original tenía dos objetivos diferentes:
- Aprendizaje supervisado: Maximizar precisión de predicción de movimientos humanos
- Aprendizaje por refuerzo: Maximizar tasa de victoria

Estos dos objetivos pueden entrar en conflicto. AlphaGo Zero tiene solo un objetivo: **maximización de tasa de victoria**. Esto hace que el proceso de aprendizaje sea más consistente y efectivo.

#### 3. Arquitectura Más Simple

El AlphaGo original usaba Policy Network y Value Network separadas. AlphaGo Zero usa una red única de doble cabeza (ver siguiente artículo), permitiendo compartir representaciones de características, mejorando la eficiencia del aprendizaje.

---

## Características de Entrada Simplificadas: De 48 a 17

### Los 48 Planos de Características del AlphaGo Original

La entrada de la red neuronal del AlphaGo original incluía 48 planos de 19x19, codificando muchas características diseñadas por humanos:

| Categoría | Número de características | Contenido |
|-----------|--------------------------|-----------|
| Posición de piedras | 3 | Negras, blancas, vacías |
| Libertades | 8 | Grupos con 1-8 libertades |
| Capturas | 8 | Puede capturar 1-8 piedras |
| Ko | 1 | Posición de ko |
| Distancia al borde | 4 | Primera a cuarta línea |
| Legalidad de jugada | 1 | Qué posiciones pueden jugarse |
| Estado histórico | 8 | Posiciones de últimos 8 movimientos |
| Turno | 1 | Negro o blanco |
| Otros | 14 | Escalera, ojos, etc. |

Estas 48 características fueron cuidadosamente diseñadas por expertos en Go, conteniendo mucho conocimiento del dominio.

### Los 17 Planos de Características de AlphaGo Zero

AlphaGo Zero simplificó dramáticamente la entrada, usando solo 17 planos de características:

| Número de plano | Contenido | Cantidad |
|-----------------|-----------|----------|
| 1-8 | Posición de negras (últimos 8 movimientos) | 8 |
| 9-16 | Posición de blancas (últimos 8 movimientos) | 8 |
| 17 | Turno actual (todo 1 o todo 0) | 1 |

Estos 17 planos solo incluyen:
- **Estado actual del tablero**: Cada posición tiene piedra negra, blanca o vacía
- **Información histórica**: Estados del tablero de los últimos 8 movimientos
- **Información de turno**: Quién juega

Sin libertades, sin juicio de escalera, sin distancia al borde -- todo este "conocimiento de Go" lo aprende la red neuronal por sí misma.

### ¿Por Qué la Simplificación Es Buena?

#### 1. Dejar que la Red Descubra Características

Características manuales complejas pueden perder información importante, o codificar suposiciones erróneas. Dejar que la red neuronal aprenda de datos crudos puede descubrir mejores representaciones de características.

De hecho, AlphaGo Zero aprendió todas las características diseñadas por humanos (libertades, escalera, etc.), y también aprendió algunos patrones que los humanos no habían identificado conscientemente.

#### 2. Mejor Generalización

Muchas de las 48 características eran específicas de Go (como escalera, distancia al borde). Los 17 planos simplificados son universales -- cualquier juego de tablero puede codificarse de manera similar.

Esto sentó las bases para el posterior **AlphaZero** (IA de juegos general).

#### 3. Reducir Errores Humanos

Las características diseñadas manualmente pueden contener definiciones erróneas o incompletas. La entrada simplificada elimina la posibilidad de tales problemas.

---

## Arquitectura de Red Única

### Diseño de Doble Red Original

El AlphaGo original usaba dos redes neuronales independientes:

```
Policy Network:  Entrada → CNN → Probabilidades de jugada 19x19
Value Network:   Entrada → CNN → Evaluación de tasa de victoria (-1 a 1)
```

Estas dos redes:
- Tenían arquitecturas diferentes (número de capas, canales ligeramente diferentes)
- Se entrenaban independientemente (primero Policy, luego Value)
- No compartían ningún parámetro

### Red de Doble Cabeza de Zero

AlphaGo Zero usa una red única, pero con dos cabezas de salida (heads):

```
Entrada → ResNet Backbone Compartido → Policy Head → Probabilidades de jugada 19x19
                                    → Value Head  → Evaluación de tasa de victoria
```

Las dos Heads comparten el mismo backbone ResNet (ver [siguiente artículo: Red de Doble Cabeza y Redes Residuales](../dual-head-resnet)), lo que trae varios beneficios:

#### 1. Eficiencia de Parámetros

Compartir el backbone significa que la mayoría de parámetros son usados por ambas tareas. Esto reduce la cantidad total de parámetros, disminuyendo el riesgo de sobreajuste.

#### 2. Compartición de Características

"Dónde debería jugar" (Policy) y "Quién ganará" (Value) necesitan entender patrones de tablero similares. El backbone compartido permite que estas características sean aprendidas y utilizadas simultáneamente por ambas tareas.

#### 3. Estabilidad de Entrenamiento

El entrenamiento conjunto hace que las señales de gradiente vengan de dos fuentes, proporcionando señales de supervisión más ricas, haciendo el entrenamiento más estable.

### El Poder de las Redes Residuales

El backbone de AlphaGo Zero usa una **Red Residual de 40 capas (ResNet)**, mucho más profunda que la CNN de 13 capas del AlphaGo original.

Las conexiones residuales (skip connections) permiten entrenar efectivamente redes profundas, evitando el problema de desvanecimiento de gradientes. Esta fue la tecnología revolucionaria de la competencia ImageNet 2015, aplicada exitosamente por AlphaGo Zero al dominio del Go.

---

## Mejora en la Eficiencia del Entrenamiento

### Crecimiento Exponencial del Auto-juego

El proceso de entrenamiento de AlphaGo Zero mostró una eficiencia asombrosa:

| Tiempo de entrenamiento | Puntuación ELO | Equivalente a |
|------------------------|----------------|---------------|
| 0 horas | 0 | Jugadas aleatorias |
| 3 horas | ~1000 | Descubriendo reglas básicas |
| 12 horas | ~3000 | Descubriendo joseki |
| 36 horas | ~4500 | Superando versión Fan Hui |
| 60 horas | ~5200 | Superando versión Lee Sedol |
| 72 horas | ~5400 | Superando AlphaGo original |
| 40 días | ~5600 | Versión más fuerte |

**Tres días para superar humanos, tres días para superar IA que tomó meses entrenar** -- esta es una mejora de eficiencia exponencial.

### ¿Por Qué Tan Rápido?

#### 1. Guía de Búsqueda Más Fuerte

El MCTS de AlphaGo Zero está completamente guiado por la red neuronal, sin usar más la política de rollout rápido. Esto hace la búsqueda más eficiente y precisa.

#### 2. Auto-juego Más Rápido

Ya que solo se necesita una red (en lugar de dos), el costo computacional de cada partida de auto-juego se reduce. Esto significa que se pueden generar más datos de entrenamiento en el mismo tiempo.

#### 3. Aprendizaje Más Efectivo

El entrenamiento conjunto de la red de doble cabeza hace que la información de cada partida sea utilizada más efectivamente. Los gradientes de Policy y Value se refuerzan mutuamente, acelerando la convergencia.

### Comparación con Aprendizaje Humano

¿Cuánto tiempo necesitan los jugadores humanos para alcanzar diferentes niveles?

| Nivel | Tiempo requerido por humanos | AlphaGo Zero |
|-------|------------------------------|--------------|
| Principiante | Semanas | Minutos |
| 1 dan amateur | Años | Horas |
| Nivel profesional | 10-20 años | 1-2 días |
| Campeón mundial | 20+ años de dedicación a tiempo completo | 3 días |
| Superar humanos | Imposible | 3 días |

Esta comparación no pretende menospreciar a los jugadores humanos -- ellos usan neuronas biológicas, mientras AlphaGo Zero usa TPUs especialmente diseñados y miles de vatios de electricidad. Pero sí demuestra cuán eficiente puede ser el método de aprendizaje correcto.

---

## Generalidad: Ajedrez, Shogi

### El Nacimiento de AlphaZero

En diciembre de 2017, DeepMind publicó **AlphaZero** -- la versión general de AlphaGo Zero. El mismo algoritmo, solo cambiando las reglas del juego, alcanzó nivel mundial en tres juegos de tablero:

| Juego | Tiempo de entrenamiento | Oponente | Récord |
|-------|------------------------|----------|--------|
| Go | 8 horas | AlphaGo Zero | 60:40 |
| Ajedrez | 4 horas | Stockfish 8 | 28 victorias 72 empates 0 derrotas |
| Shogi | 2 horas | Elmo | 90:8:2 |

Nota los oponentes aquí:
- **Stockfish** era el motor de ajedrez más fuerte entonces, usando décadas de conocimiento humano y optimización
- **Elmo** era la IA de Shogi más fuerte entonces

AlphaZero con unas pocas horas de entrenamiento superó estos sistemas especializados desarrollados durante años.

### El Significado de la Generalidad

AlphaGo Zero / AlphaZero probó algo importante:

> **El mismo algoritmo de aprendizaje puede alcanzar nivel sobrehumano en diferentes dominios.**

Estos no son tres IAs diferentes, sino un marco de aprendizaje general:

1. **Auto-juego** genera experiencia
2. **Búsqueda de Árbol Monte Carlo** explora posibilidades
3. **Redes neuronales** aprenden funciones de política y valor
4. **Aprendizaje por refuerzo** optimiza la función objetivo

Este marco no depende de conocimiento específico del dominio, dando un paso importante hacia la generalización de la IA.

### Impacto en la IA Tradicional

Antes de AlphaZero, las IAs más fuertes de ajedrez y shogi eran estilo "sistema experto":

- **Mucho conocimiento humano**: Libros de apertura, tablas de finales, funciones de evaluación
- **Décadas de optimización**: Esfuerzo de incontables jugadores e ingenieros
- **Extremadamente especializadas**: Stockfish no puede jugar Go, Elmo no puede jugar ajedrez

AlphaZero superó todo esto con un algoritmo general en unas pocas horas. Esto hizo que muchos investigadores de IA reconsideraran:

> ¿Deberíamos invertir más esfuerzo en "algoritmos de aprendizaje general" o en "codificación de conocimiento experto"?

La respuesta parece cada vez más clara: dejar que la máquina aprenda por sí misma es más efectivo que enseñarle conocimiento.

---

## El Estilo de Juego de AlphaGo Zero

### Superando la Estética Humana

La comunidad del Go tiene una evaluación universal del estilo de juego de AlphaGo Zero: **más elegante**.

Los movimientos de AlphaGo Lee a veces parecían "extraños" -- como el movimiento 37, los humanos necesitaron análisis posterior para entender su brillantez. Pero los movimientos de AlphaGo Zero a menudo se evaluaban después como "obviamente buenos a primera vista".

Esto puede ser porque:

1. **Mayor fuerza de juego**: Zero puede ver más profundo, jugando más compuesto
2. **Sin sesgos humanos**: No restringido por joseki tradicional
3. **Objetivo consistente**: Solo persigue tasa de victoria, no imita humanos

### Redescubriendo la Teoría del Go Humana

Interesantemente, AlphaGo Zero "redescubrió" durante el entrenamiento el conocimiento del Go acumulado por humanos durante miles de años:

- **Joseki**: Zero descubrió muchas joseki comunes, porque estas son realmente las soluciones óptimas para ambos lados
- **Principios de apertura**: Importancia de esquinas, lados, centro en ese orden
- **Conocimiento de forma**: Diferencia entre mala forma y buena forma

Esto validó la racionalidad de la teoría del Go humana -- este conocimiento no es coincidencia, sino un reflejo de la naturaleza del Go.

### Innovaciones Que Superan a Humanos

Pero Zero también descubrió movimientos que los humanos nunca habían pensado:

- **Aperturas no convencionales**: Variaciones sobre aperturas tradicionales
- **Sacrificios agresivos**: Más dispuesto que humanos a abandonar ventaja local por ventaja global
- **Formas contra-intuitivas**: "Mala forma" superficial que en realidad es óptima

Estas innovaciones están cambiando la comprensión humana del Go. Muchos jugadores profesionales dicen que estudiar las partidas de AlphaGo Zero les dio una comprensión completamente nueva del Go.

---

## Resumen de Detalles Técnicos

### Comparación Completa con AlphaGo Original

| Aspecto | AlphaGo (original) | AlphaGo Zero |
|---------|-------------------|--------------|
| **Datos de entrenamiento** | Partidas humanas + auto-juego | Auto-juego puro |
| **Método de aprendizaje** | Supervisado + refuerzo | Refuerzo puro |
| **Características de entrada** | 48 planos | 17 planos |
| **Arquitectura de red** | Policy/Value separadas | ResNet de doble cabeza |
| **Profundidad de red** | 13 capas | 40 capas (o más) |
| **Evaluación MCTS** | Red neuronal + Rollout | Red neuronal pura |
| **Búsquedas por movimiento** | ~100,000 | ~1,600 |
| **TPUs de entrenamiento** | 50+ | 4 |
| **TPUs de inferencia** | 48 | 4 (escalable) |

### Algoritmo Central

El bucle de entrenamiento de AlphaGo Zero es muy conciso:

```
1. Auto-juego
   - Usar red actual para MCTS
   - Seleccionar movimientos según probabilidades de búsqueda MCTS
   - Registrar cada movimiento (posición, probabilidades MCTS, resultado)

2. Entrenar red
   - Muestrear del pool de experiencia
   - Policy Head: Minimizar entropía cruzada con probabilidades MCTS
   - Value Head: Minimizar error cuadrático medio con resultado real
   - Optimizar conjuntamente ambos objetivos

3. Actualizar red
   - Reemplazar red vieja con nueva (verificar que nueva red sea más fuerte jugando)
   - Volver al paso 1
```

Este bucle se ejecuta continuamente, la red se vuelve más fuerte constantemente. Sin datos humanos, sin conocimiento humano, solo reglas del juego y objetivo de victoria.

---

## Implicaciones para la Investigación en IA

### Aprendizaje desde Primeros Principios

AlphaGo Zero demostró un método de aprendizaje de "primeros principios":

> No digas a la IA cómo hacerlo, solo dile cuál es el objetivo, deja que descubra el método por sí misma.

Esto contrasta fuertemente con el enfoque tradicional de sistemas expertos. Los sistemas expertos intentan codificar conocimiento humano en la IA, mientras AlphaGo Zero deja que la IA descubra el conocimiento por sí misma.

El resultado es: el conocimiento que descubre la IA puede ser más completo y preciso que el conocimiento humano.

### El Poder del Auto-juego

AlphaGo Zero probó que el auto-juego puede generar datos de entrenamiento infinitos, y la calidad de estos datos mejora a medida que la red mejora.

Este es un "ciclo positivo":
- Red más fuerte → Mejores datos de auto-juego
- Mejores datos → Red más fuerte

Este ciclo puede continuar ejecutándose hasta alcanzar el límite teórico del juego (si existe).

### La Importancia de la Simplificación

El éxito de AlphaGo Zero probó la importancia de la "simplificación":

- Simplificar entrada (48 → 17)
- Simplificar arquitectura (doble red → red única)
- Simplificar entrenamiento (supervisado + refuerzo → refuerzo puro)

Cada simplificación hizo el sistema más poderoso. Esto nos dice: complejidad no es igual a bueno, la solución más simple a menudo es la mejor.

---

## Correspondencia con Animaciones

Conceptos centrales cubiertos en este artículo y sus números de animación:

| Número | Concepto | Correspondencia Física/Matemática |
|--------|----------|----------------------------------|
| E7 | Entrenamiento desde cero | Fenómeno de auto-organización |
| E5 | Auto-juego | Convergencia de punto fijo |
| E12 | Curva de crecimiento de fuerza | Crecimiento en S |
| D12 | Red residual | Autopista de gradientes |

---

## Lecturas Adicionales

- **Siguiente artículo**: [Red de Doble Cabeza y Redes Residuales](../dual-head-resnet) — Arquitectura de red neuronal de AlphaGo Zero en detalle
- **Artículo relacionado**: [Auto-juego](../self-play) — Por qué el auto-juego puede producir nivel sobrehumano
- **Profundización técnica**: [Proceso de Entrenamiento desde Cero](../training-from-scratch) — Evolución detallada del Día 0-3

---

## Referencias

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
3. DeepMind. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
4. Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." *Nature*, 588, 604-609.
