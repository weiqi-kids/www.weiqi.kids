---
sidebar_position: 20
title: Sistemas distribuidos y TPU
description: An√°lisis profundo de la arquitectura de entrenamiento distribuido de AlphaGo, aceleraci√≥n con TPU y MCTS paralelo a gran escala
keywords: [sistemas distribuidos, TPU, computaci√≥n paralela, MCTS, virtual loss, deep learning, aceleraci√≥n de hardware]
---

# Sistemas distribuidos y TPU

El √©xito de AlphaGo no es solo una victoria algor√≠tmica, sino tambi√©n una victoria de ingenier√≠a. Para entrenar una IA de Go que supere a los humanos en un tiempo razonable, se necesita un sistema distribuido cuidadosamente dise√±ado y el soporte de hardware especializado.

Este art√≠culo analizar√° en profundidad la arquitectura del sistema detr√°s de AlphaGo, incluyendo el proceso de entrenamiento, la arquitectura de inferencia, MCTS paralelo y el papel crucial de las TPU.

---

## Visi√≥n general de la arquitectura de entrenamiento

### Arquitectura de entrenamiento del AlphaGo original

El entrenamiento del AlphaGo original (la versi√≥n que derrot√≥ a Lee Sedol) se dividi√≥ en m√∫ltiples etapas, cada una usando diferentes configuraciones de recursos:

```mermaid
flowchart TB
    subgraph E1["Etapa 1: Aprendizaje supervisado"]
        H1["Partidas humanas<br/>(30M)"] --> G1["Cluster GPU<br/>(50 GPUs)"] --> P1["Policy Net<br/>(version SL)"]
    end

    subgraph E2["Etapa 2: Aprendizaje por refuerzo"]
        S2["Auto-juego<br/>(millones partidas)"] --> G2["Cluster GPU<br/>(50 GPUs)"] --> P2["Policy Net<br/>(version RL)"]
    end

    subgraph E3["Etapa 3: Entrenamiento de Value Net"]
        D3["Datos de auto-juego<br/>(30M pos.)"] --> G3["Cluster GPU<br/>(50 GPUs)"] --> V3["Value Net"]
    end

    E1 --> E2
    E2 --> E3
```

### Arquitectura de entrenamiento de AlphaGo Zero

AlphaGo Zero simplific√≥ enormemente el proceso de entrenamiento, usando un √∫nico ciclo de entrenamiento de extremo a extremo:

```mermaid
flowchart TB
    subgraph Ciclo["Ciclo de entrenamiento AlphaGo Zero"]
        SP["Self-play Workers<br/>(TPU x N)<br/>‚Üê Red mas reciente"]
        RB["Replay Buffer<br/>(RAM/SSD)<br/>(ultimas 500K partidas)"]
        TW["Training Workers<br/>(TPU x M)"]
        NC["Network Checkpoint<br/>‚Üí Actualiza red usada en Self-play"]

        SP --> RB
        RB --> TW
        TW --> NC
        NC -.->|"actualizar"| SP
    end
```

Las ventajas de esta arquitectura:

1. **Aprendizaje continuo**: Self-play y Training ocurren simult√°neamente, sin necesidad de esperar
2. **Eficiencia de recursos**: Todos los recursos hacen trabajo √∫til
3. **Iteraci√≥n r√°pida**: La red se usa inmediatamente para generar nuevos datos despu√©s de actualizarse

---

## Estaciones de auto-juego (Self-play Workers)

### Asignaci√≥n de tareas

Los Self-play Workers son responsables de realizar auto-juego con la red m√°s fuerte actual, produciendo datos de entrenamiento.

| Configuraci√≥n | AlphaGo Zero |
|---------------|--------------|
| N√∫mero de Workers | Decenas |
| Por Worker | 1-4 TPU |
| MCTS por partida | 1600 simulaciones |
| Producci√≥n diaria | ~100,000 partidas |

### Flujo de trabajo

El flujo de trabajo de cada Self-play Worker:

```python
while True:
    # 1. Descargar los pesos de red m√°s recientes
    network = download_latest_checkpoint()

    # 2. Realizar m√∫ltiples auto-juegos
    for game in range(batch_size):
        positions = []
        board = EmptyBoard()

        while not board.is_terminal():
            # Ejecutar MCTS
            mcts = MCTS(network, board)
            policy = mcts.search(num_simulations=1600)

            # Elegir movimiento
            action = sample(policy)

            # Registrar
            positions.append((board.state, policy))

            # Jugar
            board = board.play(action)

        # 3. Obtener resultado del juego
        result = board.get_result()

        # 4. Subir datos
        upload_to_replay_buffer(positions, result)
```

### Balanceo de carga

M√∫ltiples Workers necesitan balanceo de carga:

- **Sincronizaci√≥n de red**: Todos los Workers usan la misma versi√≥n de la red
- **Balance de datos**: Asegurar que los datos de diferentes Workers sean usados
- **Manejo de errores**: El fallo de un Worker no afecta el entrenamiento general

---

## Estaciones de entrenamiento (Training Workers)

### Asignaci√≥n de tareas

Los Training Workers son responsables de muestrear datos del Replay Buffer y entrenar la red neuronal.

| Configuraci√≥n | AlphaGo Zero |
|---------------|--------------|
| N√∫mero de Workers | 1-4 |
| Por Worker | 4 TPU |
| Batch Size | 2048 (512 por TPU) |
| Pasos de entrenamiento | Decenas de miles por d√≠a |

### Entrenamiento distribuido

El entrenamiento a gran escala usa **paralelismo de datos (Data Parallelism)**:

```mermaid
flowchart TB
    PS["Parameter Server"]

    TPU0["TPU 0<br/>Batch 0"]
    TPU1["TPU 1<br/>Batch 1"]
    TPU2["TPU 2<br/>Batch 2"]

    GA["Gradient Aggregation"]

    PS --> TPU0
    PS --> TPU1
    PS --> TPU2

    TPU0 --> GA
    TPU1 --> GA
    TPU2 --> GA

    GA --> PS
```

Cada TPU procesa diferentes mini-batches, calcula gradientes locales, luego los agrega para actualizar par√°metros globales.

### Actualizaci√≥n s√≠ncrona vs. as√≠ncrona

| Tipo de actualizaci√≥n | Ventajas | Desventajas |
|-----------------------|----------|-------------|
| S√≠ncrona | Estable, reproducible | Workers deben esperar al m√°s lento |
| As√≠ncrona | Alto throughput | Los gradientes pueden estar obsoletos |

AlphaGo Zero usa **actualizaci√≥n s√≠ncrona** para asegurar la estabilidad del entrenamiento.

---

## El papel de las TPU

### ¬øQu√© es una TPU?

**TPU (Tensor Processing Unit)** es un acelerador dise√±ado por Google espec√≠ficamente para deep learning:

| Caracter√≠stica | TPU | GPU | CPU |
|----------------|-----|-----|-----|
| Objetivo de dise√±o | Operaciones matriciales | Paralelismo general | Computaci√≥n general |
| Precisi√≥n | Optimizado FP16/BF16 | FP32/FP16 | FP64/FP32 |
| Consumo | Relativamente bajo | M√°s alto | El m√°s alto |
| Latencia | Baja | Media | Alta |

### Arquitectura de las TPU

El n√∫cleo de las TPU es la **MXU (Matrix Multiply Unit)**:

**Arquitectura TPU v2/v3:**

| Componente | Especificacion |
|:----------:|:--------------:|
| MXU (Matrix Multiply Unit) | 128 x 128 = 16K MACs/ciclo |
| Vector Unit | Operaciones vectoriales |
| HBM (High Bandwidth Memory) | 16-32 GB |

La MXU puede ejecutar 16K operaciones de multiplicaci√≥n-acumulaci√≥n por ciclo, crucial para la multiplicaci√≥n de matrices de redes neuronales.

### ¬øPor qu√© AlphaGo necesita TPU?

El cuello de botella computacional de la IA de Go est√° en la **inferencia de red neuronal**:

| Operaci√≥n | Proporci√≥n |
|-----------|------------|
| Forward pass de red neuronal | ~95% |
| Operaciones del √°rbol MCTS | ~4% |
| Otros | ~1% |

Cada paso de MCTS requiere 1600 inferencias de red neuronal. El alto throughput de las TPU hace esto posible.

### Evoluci√≥n del uso de TPU

| Versi√≥n | TPU de entrenamiento | TPU de inferencia |
|---------|----------------------|-------------------|
| AlphaGo Lee | 50 GPU | 48 TPU (v1) |
| AlphaGo Master | 4 TPU (v2) | 4 TPU (v2) |
| AlphaGo Zero | 4 TPU (v2) | 4 TPU (v2) (escalable) |

El n√∫mero de TPU usadas por AlphaGo Zero se redujo significativamente, gracias a arquitecturas m√°s eficientes y versiones m√°s nuevas de TPU.

---

## MCTS paralelo y Virtual Loss

### El desaf√≠o de la paralelizaci√≥n

La implementaci√≥n est√°ndar de MCTS es **serial**:

```
for i in range(num_simulations):
    1. Selection: Seleccionar hacia abajo desde la ra√≠z
    2. Expansion: Expandir nodo hoja
    3. Evaluation: Evaluaci√≥n con red neuronal
    4. Backup: Retropropagar actualizaciones
```

Pero la evaluaci√≥n de red neuronal es una **operaci√≥n por lotes** amigable para GPU/TPU. ¬øC√≥mo hacer que m√∫ltiples simulaciones ocurran simult√°neamente?

### Paralelizaci√≥n de hojas (Leaf Parallelization)

El m√©todo de paralelizaci√≥n m√°s simple: ejecutar m√∫ltiples simulaciones completas simult√°neamente, luego fusionar resultados.

```mermaid
flowchart TB
    Root["Root"]

    Sim1["Sim 1<br/>(indep.)"]
    Sim2["Sim 2<br/>(indep.)"]
    Sim3["Sim 3<br/>(indep.)"]
    Sim4["Sim 4<br/>(indep.)"]

    Merge["Merge Trees"]

    Root --> Sim1
    Root --> Sim2
    Root --> Sim3
    Root --> Sim4

    Sim1 --> Merge
    Sim2 --> Merge
    Sim3 --> Merge
    Sim4 --> Merge
```

Problema: Cada simulaci√≥n comienza desde la ra√≠z, explorando repetidamente los mismos caminos.

### Virtual Loss

DeepMind adopt√≥ la t√©cnica de **Virtual Loss** para implementar paralelismo de √°rbol (Tree Parallelization).

#### Concepto b√°sico

Cuando un hilo est√° explorando un nodo, reduce temporalmente el valor de ese nodo, haciendo que otros hilos elijan otros caminos.

```
UCB normal: Q(s,a) + c * P(s,a) * sqrt(N(s)) / (1 + N(s,a))

Con virtual loss:
(Q(s,a) * N(s,a) - v * n_virtual) / (N(s,a) + n_virtual) + c * P(s,a) * sqrt(N(s)) / (1 + N(s,a) + n_virtual)
```

Donde:
- `n_virtual` es el n√∫mero de hilos actualmente explorando ese nodo
- `v` es el valor del virtual loss (usualmente 1 o valor correspondiente a tasa de victoria)

#### Flujo de operaci√≥n

```
Tiempo T1:
  Thread 1 elige camino A ‚Üí B ‚Üí C
  Nodo C recibe virtual loss -1

Tiempo T2:
  Thread 2 elige camino A ‚Üí B ‚Üí D (porque C fue "penalizado")
  Nodo D recibe virtual loss -1

Tiempo T3:
  Thread 1 completa evaluaci√≥n, actualiza valor real de C, remueve virtual loss
  Thread 3 ahora puede elegir C (si el valor real es suficientemente bueno)
```

#### Efecto del virtual loss

| Aspecto | Efecto |
|---------|--------|
| Diversidad de exploraci√≥n | Fuerza exploraci√≥n de diferentes caminos |
| Eficiencia de lotes | Puede evaluar m√∫ltiples hojas simult√°neamente |
| Convergencia | El virtual loss es finalmente cubierto por valores reales, no afecta convergencia |

### Evaluaci√≥n de red neuronal por lotes

A trav√©s del virtual loss, se pueden recoger m√∫ltiples nodos hoja pendientes de evaluaci√≥n para **inferencia por lotes**:

```mermaid
flowchart LR
    subgraph Parallel["MCTS Paralelo"]
        T1["Thread 1 ‚Üí nodo hoja L1"]
        T2["Thread 2 ‚Üí nodo hoja L2"]
        T3["Thread 3 ‚Üí nodo hoja L3"]
        T4["Thread 4 ‚Üí nodo hoja L4"]
    end

    Batch["Batch"]
    TPU["TPU"]
    Results["Obtiene (P1,V1), (P2,V2), ...<br/>simultaneamente"]

    T1 --> Batch
    T2 --> Batch
    T3 --> Batch
    T4 --> Batch

    Batch --> TPU
    TPU --> Results
```

La eficiencia de inferencia por lotes de TPU es mucho mayor que inferencia uno por uno, haciendo posible el MCTS paralelo.

---

## Arquitectura de inferencia

### Configuraci√≥n durante competencias

Arquitectura de inferencia de AlphaGo en competencias oficiales:

| Versi√≥n | Configuraci√≥n de hardware |
|---------|---------------------------|
| AlphaGo Fan | 176 GPU |
| AlphaGo Lee | 48 TPU + m√∫ltiples servidores |
| AlphaGo Master | 4 TPU |
| AlphaGo Zero | 4 TPU (escalable) |

### Flujo de inferencia distribuida

Flujo de inferencia durante competencias (ejemplo de AlphaGo Lee):

```mermaid
flowchart TB
    subgraph Arch["Arquitectura de inferencia distribuida"]
        Master["Nodo maestro<br/>‚Üê Recibe movimiento del oponente<br/>‚Üí Envia movimiento de AlphaGo"]

        MCTS["Controlador MCTS<br/>Gestiona arbol de busqueda,<br/>asigna tareas, recopila resultados"]

        subgraph Cluster["Cluster TPU (48 TPUs)"]
            TPU1["TPU 1"]
            TPU2["TPU 2"]
            TPU3["TPU 3"]
            TPU4["TPU 4"]
            TPU5["TPU 5"]
            TPUN["... TPU 48"]
        end

        Master --> MCTS
        MCTS --> Cluster
    end
```

### Gesti√≥n del tiempo de pensamiento

Estrategia de gesti√≥n de tiempo de AlphaGo:

| Posici√≥n | Tiempo de pensamiento | Simulaciones MCTS |
|----------|----------------------|-------------------|
| Apertura (con joseki) | M√°s corto | ~10,000 |
| Medio juego (complejo) | M√°s largo | ~100,000 |
| Posici√≥n simple | M√°s corto | ~5,000 |
| Byoyomi | Fijo | ~1,600 |

M√°s simulaciones MCTS generalmente significan mejor calidad de movimiento.

---

## Comunicaci√≥n y sincronizaci√≥n

### Formato de datos

Formato de transmisi√≥n de datos de entrenamiento:

```protobuf
message TrainingExample {
    // Estado del tablero (17 √ó 19 √ó 19)
    repeated float board_planes = 1;

    // Resultado de b√∫squeda MCTS (362)
    repeated float mcts_policy = 2;

    // Resultado del juego (1 = actual gana, -1 = actual pierde)
    float game_result = 3;
}
```

### Requisitos de ancho de banda

| Flujo de datos | Tama√±o | Frecuencia |
|----------------|--------|------------|
| Muestra de entrenamiento | ~10 KB/muestra | Miles de muestras/segundo |
| Pesos de red | ~200 MB | Varias veces/hora |
| Mensajes de control | < 1 KB | Continuo |

Requisito total de ancho de banda: ~100 Mbps (red interna suficiente)

### Manejo de fallos

Manejo de fallos en sistemas distribuidos:

| Tipo de fallo | Manejo |
|---------------|--------|
| Worker cae | Reiniciar, continuar usando √∫ltimo checkpoint |
| Desconexi√≥n de red | Almacenar en b√∫fer, retransmitir tras reconexi√≥n |
| Fallo de TPU | Cambio autom√°tico a TPU de respaldo |
| Corrupci√≥n de datos | Descartar tras verificaci√≥n, regenerar |

---

## An√°lisis de costos

### Estimaci√≥n de costo de hardware

Estimaci√≥n de costo de entrenamiento de AlphaGo Zero basada en precios de TPU de Google Cloud:

| Recurso | Cantidad | Precio/hora | Precio total/d√≠a |
|---------|----------|-------------|------------------|
| TPU v2 Pod | 4 | ~$32 | ~$3,000 |
| VM alta memoria | Varios | ~$5 | ~$500 |
| Almacenamiento | 10 TB | ~$0.02/GB | ~$200 |
| Red | - | Incluido | - |

**Aproximadamente $3,700/d√≠a**, entrenamiento completo (40 d√≠as) aproximadamente **$150,000**.

Nota: Esta es una estimaci√≥n de 2017, DeepMind como subsidiaria de Google puede tener descuentos internos.

### Comparaci√≥n con entrenamiento humano

| Aspecto | AlphaGo Zero | Jugador profesional humano |
|---------|--------------|----------------------------|
| Alcanzar nivel profesional | 2 d√≠as | 10-15 a√±os |
| Costo de entrenamiento | ~$7,500 | Millones (matr√≠cula, gastos, costo de oportunidad) |
| Costo continuo | Electricidad | Gastos de vida |
| Replicabilidad | Perfecta | No replicable |

Por supuesto, esta comparaci√≥n no es completamente justa: los humanos aprenden m√°s que solo Go durante el proceso.

### Costo de inferencia

Costo de inferencia en competencias oficiales:

| Configuraci√≥n | Costo por partida |
|---------------|-------------------|
| 48 TPU (AlphaGo Lee) | ~$500 |
| 4 TPU (AlphaGo Zero) | ~$50 |
| GPU √∫nica (KataGo) | ~$1 |

El costo de inferencia ha disminuido dr√°sticamente con el progreso tecnol√≥gico.

---

## Evoluci√≥n tecnol√≥gica

### De AlphaGo a AlphaZero

| Aspecto | AlphaGo Lee | AlphaGo Zero | AlphaZero |
|---------|-------------|--------------|-----------|
| TPU entrenamiento | 50+ GPU ‚Üí TPU | 4 TPU | 4 TPU |
| TPU inferencia | 48 TPU | 4 TPU | 4 TPU |
| MCTS/movimiento | ~100,000 | ~1,600 | ~800 |
| Tiempo de entrenamiento | Meses | 40 d√≠as | Horas-d√≠as |

Mejora de eficiencia de aproximadamente 100 veces.

### Impacto en la comunidad de c√≥digo abierto

La arquitectura de AlphaGo inspir√≥ m√∫ltiples proyectos de c√≥digo abierto:

| Proyecto | Caracter√≠sticas |
|----------|-----------------|
| Leela Zero | Entrenamiento distribuido comunitario, replica AlphaGo Zero |
| KataGo | Entrenamiento eficiente en una sola GPU, supera AlphaGo Zero |
| ELF OpenGo | C√≥digo abierto de Facebook, usa PyTorch |
| Minigo | C√≥digo abierto de Google, usa TensorFlow |

Estos proyectos permiten a investigadores ordinarios entrenar IA de Go potentes.

---

## Correspondencia con animaciones

Conceptos centrales de este art√≠culo y n√∫meros de animaci√≥n:

| N√∫mero | Concepto | Correspondencia f√≠sica/matem√°tica |
|--------|----------|-----------------------------------|
| üé¨ C9 | MCTS paralelo | Problema de muchos cuerpos |
| üé¨ E9 | Entrenamiento distribuido | Computaci√≥n distribuida |
| üé¨ C5 | Virtual loss | Potencial de repulsi√≥n |
| üé¨ D15 | Inferencia por lotes | C√°lculo vectorizado |

---

## Lecturas adicionales

- **Art√≠culo anterior**: [El proceso de entrenamiento desde cero](../training-from-scratch) ‚Äî An√°lisis detallado de la curva de entrenamiento
- **Art√≠culo siguiente**: [El legado de AlphaGo](../legacy-and-impact) ‚Äî El profundo impacto de AlphaGo en el campo de IA
- **Art√≠culo relacionado**: [Combinaci√≥n de MCTS y redes neuronales](../mcts-neural-combo) ‚Äî Conocimientos b√°sicos de MCTS

---

## Referencias

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Jouppi, N., et al. (2017). "In-Datacenter Performance Analysis of a Tensor Processing Unit." *ISCA 2017*.
3. Dean, J., et al. (2012). "Large Scale Distributed Deep Networks." *NeurIPS 2012*.
4. Chaslot, G., et al. (2008). "Parallel Monte-Carlo Tree Search." *CIG 2008*.
5. Segal, R. (2010). "On the Scalability of Parallel UCT." *CIG 2010*.
