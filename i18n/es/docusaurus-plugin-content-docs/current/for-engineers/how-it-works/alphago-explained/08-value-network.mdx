---
sidebar_position: 9
title: Value Network en detalle
description: Comprensión profunda de la arquitectura de la red de valores de AlphaGo, desafíos de entrenamiento y su rol clave en MCTS
---

# Value Network en detalle

Si la Policy Network le dice a AlphaGo "dónde debería jugar el siguiente movimiento", la Value Network responde una pregunta más fundamental:

> **"¿Voy a ganar esta partida?"**

---

## ¿Qué es la Value Network?

### Función principal

La Value Network es una red neuronal convolucional profunda cuya tarea es:

> **Dado el estado actual del tablero, predecir la probabilidad de victoria final**

Expresado matemáticamente:

```
v = f_θ(s)
```

Donde:
- `s`: estado actual del tablero
- `f_θ`: Value Network (θ son los parámetros de la red)
- `v`: un valor entre -1 y +1

### Significado de la salida

| Valor de salida | Significado |
|-----------------|-------------|
| +1 | El jugador actual gana con certeza |
| +0.5 | El jugador actual tiene ~75% de probabilidad de ganar |
| 0 | Probabilidades iguales para ambos |
| -0.5 | El jugador actual tiene ~25% de probabilidad de ganar |
| -1 | El jugador actual pierde con certeza |

### ¿Por qué se necesita un solo valor?

#### Comparar diferentes opciones

Al jugar, frecuentemente necesitamos elegir entre múltiples opciones. La Value Network simplifica esta comparación:

```
Valor de la posición de opción A: 0.3
Valor de la posición de opción B: 0.5
Valor de la posición de opción C: 0.2

→ Elegir B (el valor más alto)
```

Sin un solo valor, ¿cómo comparamos "capturar un grupo del oponente" con "rodear un territorio grande"?

#### Reemplazar muchas simulaciones

En el Monte Carlo Tree Search tradicional, evaluar una posición requiere realizar **simulaciones aleatorias (rollout)**:

1. Comenzar desde la posición actual
2. Ambos jugadores juegan aleatoriamente hasta que termine el juego
3. Registrar victoria/derrota
4. Repetir miles de veces, calcular tasa de victoria

Esto es muy lento. La Value Network puede dar una evaluación con **una sola propagación hacia adelante**, órdenes de magnitud más rápido.

| Método | Tiempo de evaluación | Precisión |
|--------|---------------------|-----------|
| 1000 simulaciones aleatorias | ~2000 ms | Baja |
| 15000 simulaciones aleatorias | ~30000 ms | Media |
| Value Network | ~3 ms | Alta (equivalente a 15000 simulaciones) |

---

## Arquitectura de la red

### Similitud con la Policy Network

La arquitectura de la Value Network es muy similar a la Policy Network, ambas son redes neuronales convolucionales profundas:

```
Capa de entrada → Capas convolucionales ×12 → Capa totalmente conectada → Salida
       ↓                   ↓                          ↓                    ↓
   19×19×48            19×19×192                   256-dim            Un solo valor
```

### Capa de entrada

Similar a la Policy Network, la entrada es un tensor de características de **19×19×49**:

- **19×19**: tamaño del tablero
- **49**: 48 planos de características + 1 plano indicando de quién es el turno

El plano adicional es importante: la Value Network necesita saber de quién es el turno, porque la misma posición tiene valores opuestos para negro y blanco.

### Capas convolucionales

Igual que la Policy Network:
- **12 capas convolucionales**
- **192 filtros**
- **Kernel 3×3** (primera capa 5×5)
- **Función de activación ReLU**

### Diferencia en la capa de salida

Esta es la diferencia clave entre Value Network y Policy Network:

#### Salida de Policy Network
```
19×19×192 → Conv 1×1 → 19×19×1 → Aplanar → 361-dim → Softmax → Distribución de probabilidad
```

#### Salida de Value Network
```
19×19×192 → Conv 1×1 → 19×19×1 → Aplanar → 361-dim → FC 256 → ReLU → FC 1 → Tanh → Un solo valor
```

### Función de activación Tanh

La última capa de la Value Network usa la función **Tanh** (tangente hiperbólica):

```
Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```

El rango de salida de Tanh es **(-1, +1)**, correspondiendo exactamente a victoria/derrota.

#### ¿Por qué Tanh en lugar de Sigmoid?

El rango de salida de Sigmoid es (0, 1), que también puede representar tasa de victoria. Pero Tanh tiene varias ventajas:

1. **Simetría**: centrado en 0, salida puede ser positiva o negativa
2. **Mejor gradiente**: gradiente cercano a 1 alrededor de 0
3. **Semántica clara**: positivo es ganar, negativo es perder, cero es empate

### Diagrama de arquitectura completa

```
Entrada: 19×19×49
        ↓
    Conv 5×5, 192 filtros
        ↓
    ReLU
        ↓
    Conv 3×3, 192 filtros (×11)
        ↓
    ReLU
        ↓
    Conv 1×1, 1 filtro
        ↓
    Aplanar (361 dim)
        ↓
    Totalmente conectada (256 dim)
        ↓
    ReLU
        ↓
    Totalmente conectada (1 dim)
        ↓
    Tanh
        ↓
Salida: [-1, +1]
```

### Cantidad de parámetros

| Capa | Cálculo | Cantidad de parámetros |
|------|---------|------------------------|
| Capas conv | Igual que Policy Network | ~3.9M |
| Capa FC 1 | 361×256 + 256 | 92,672 |
| Capa FC 2 | 256×1 + 1 | 257 |
| **Total** | | **~4.0M** |

Aproximadamente 4 millones de parámetros, ligeramente más que la Policy Network.

---

## Desafíos del entrenamiento

### Problema de sobreajuste

El entrenamiento de la Value Network es mucho más difícil que el de la Policy Network. El problema principal es el **sobreajuste**.

#### ¿Qué es el sobreajuste?

El sobreajuste es cuando el modelo "memoriza" los datos de entrenamiento en lugar de aprender a generalizar. Se manifiesta como:
- Muy buen rendimiento en el conjunto de entrenamiento
- Mal rendimiento en el conjunto de prueba

#### ¿Por qué la Value Network es propensa al sobreajuste?

Considera los datos de una partida:

```
Posición 1 → Posición 2 → Posición 3 → ... → Posición 200 → Resultado: Negro gana
```

Si se entrena directamente con estos datos:
- Estas 200 posiciones están fuertemente correlacionadas
- Vienen de la misma partida, con el mismo resultado
- El modelo puede aprender a "reconocer" esta partida, en lugar de entender posiciones

DeepMind descubrió: si se entrenan Policy y Value Network con las mismas partidas humanas, la Value Network sufre severo sobreajuste.

### Solución: datos de auto-juego

La solución de DeepMind fue usar **auto-juego** para generar nuevos datos de entrenamiento:

```
1. Usar la RL Policy Network entrenada para auto-juego
2. Tomar solo una posición de cada partida (evitar correlación)
3. La etiqueta de esta posición es el resultado final de esa partida
4. Generar 30 millones de tales muestras
```

#### ¿Por qué esto resuelve el sobreajuste?

1. **Gran cantidad de datos**: 30 millones de posiciones independientes
2. **Sin correlación**: solo una posición por partida
3. **Distribución diferente**: la distribución de posiciones de auto-juego difiere de las partidas humanas

### Generación de datos de entrenamiento

```python
# Pseudocódigo
training_data = []

for game_id in range(30_000_000):
    # Auto-juego de una partida
    states, result = self_play(rl_policy_network)

    # Seleccionar aleatoriamente una posición
    random_index = random.randint(0, len(states) - 1)
    state = states[random_index]

    # Registrar posición y resultado
    training_data.append((state, result))
```

---

## Objetivo y métodos de entrenamiento

### Pérdida de error cuadrático medio

La Value Network usa el **error cuadrático medio (MSE)** como función de pérdida:

```
L(θ) = (1/n) × Σ (v_θ(s) - z)²
```

Donde:
- `v_θ(s)`: valor predicho por el modelo
- `z`: resultado real (+1 o -1)

#### ¿Por qué MSE en lugar de entropía cruzada?

- **Entropía cruzada** es adecuada para problemas de clasificación (etiquetas discretas)
- **MSE** es adecuado para problemas de regresión (valores continuos)

Aunque los resultados son solo +1 o -1, el modelo predice valores continuos (cualquier número entre -1 y +1). MSE hace que el modelo aprenda a predecir valores cercanos a +1 o -1.

### Proceso de entrenamiento

```python
# Pseudocódigo
for epoch in range(num_epochs):
    for batch in dataloader:
        states, outcomes = batch

        # Propagación hacia adelante
        values = network(states)  # (batch, 1)

        # Calcular pérdida (MSE)
        loss = mse_loss(values, outcomes)

        # Retropropagación
        loss.backward()
        optimizer.step()
```

Detalles de entrenamiento:
- **Optimizador**: SGD con momentum
- **Tasa de aprendizaje**: 0.003
- **Tamaño de lote**: 32
- **Tiempo de entrenamiento**: aproximadamente 1 semana (50 GPUs)

---

## Análisis de precisión

### Comparación con simulaciones aleatorias

DeepMind realizó una comparación detallada en el artículo:

| Método de evaluación | Error de predicción |
|---------------------|---------------------|
| 1000 simulaciones aleatorias | Alto |
| 15000 simulaciones aleatorias | Medio |
| Value Network | Comparable a 15000 simulaciones |

Esto significa que una evaluación de Value Network ≈ 15000 simulaciones aleatorias, pero aproximadamente 1000 veces más rápido.

### Precisión por etapa

La precisión de la Value Network depende del progreso del juego:

| Etapa | Movimientos restantes | Dificultad de predicción | Precisión |
|-------|----------------------|--------------------------|-----------|
| Apertura | ~300 | Muy difícil | Baja |
| Medio juego | ~150 | Difícil | Media |
| Final | ~50 | Más fácil | Alta |
| Cierre | ~10 | Simple | Muy alta |

Esto es intuitivamente razonable: cuanto más cerca del final del juego, más determinado está el resultado.

### Distribución de salida

La distribución de salida de una Value Network bien entrenada:

```
        Frecuencia
          |
          |    *
          |   * *
          |  *   *
          | *     *
          |*       *
          +----+----+---- Valor de salida
         -1    0   +1

La mayoría de las salidas se concentran cerca de -1 y +1
(porque la mayoría de las posiciones tienen una tendencia clara de victoria/derrota)
```

### Posiciones inciertas

Cuando la salida de la Value Network está cerca de 0, indica una posición muy compleja donde el resultado es difícil de predecir. Estas posiciones suelen ser:
- En medio de batallas grandes
- Ambos lados igualmente equilibrados
- Existen múltiples variaciones posibles

En MCTS, estos nodos reciben más recursos de búsqueda (debido a la alta incertidumbre).

---

## Rol en MCTS

### Evaluación de nodos hoja

La Value Network juega un rol clave en la fase de **Evaluación** de MCTS:

```
Árbol de búsqueda MCTS:

        Nodo raíz (posición actual)
           /    \
         A        B
        /  \    /  \
       A1  A2  B1  B2 ← Nodos hoja
        ↓   ↓   ↓   ↓
      Eval Eval Eval Eval
```

Cuando MCTS llega a un nodo hoja, necesita evaluar el valor de esta posición. Hay dos métodos:

1. **Simulación aleatoria (Rollout)**: jugar aleatoriamente desde el nodo hoja hasta el final
2. **Evaluación por Value Network**: usar directamente la red neuronal para predecir

AlphaGo combina ambos:

```
V(hoja) = (1-λ) × V_network(hoja) + λ × V_rollout(hoja)
```

Donde λ = 0.5, es decir, peso igual para ambos.

#### ¿Por qué combinar?

- **Value Network** es más precisa, pero puede tener sesgo sistemático
- **Simulación aleatoria** es menos precisa, pero proporciona una estimación independiente
- Combinar ambas puede ser complementario

### Simplificación de AlphaGo Zero

El posterior AlphaGo Zero abandonó completamente las simulaciones aleatorias:

```
V(hoja) = V_network(hoja)
```

Esto simplificó enormemente el sistema, mientras que la fuerza de juego mejoró. Esto demuestra que la Value Network es lo suficientemente confiable, sin necesidad del "seguro" de simulaciones aleatorias.

### Actualización de retropropagación

Después de evaluar el nodo hoja, este valor se propaga hacia atrás a lo largo del camino:

```
v3 = V(hoja) = 0.6
      ↑
Actualizar valor Q de A2
      ↑
Actualizar valor Q de A
      ↑
Actualizar estadísticas del nodo raíz
```

El valor Q mantenido por cada nodo es el promedio de todas las evaluaciones de nodos hoja que pasaron por él:

```
Q(s, a) = (1/N(s,a)) × Σ V(hoja)
```

---

## Análisis visual

### Superficie de valor

Imagina un tablero simplificado de 3×3. La Value Network aprende una "superficie de valor":

```
        Posición de blanco
       1   2   3
    ┌───┬───┬───┐
  1 │+0.3│-0.1│+0.2│
N   ├───┼───┼───┤
e  2 │-0.2│+0.5│-0.3│
g   ├───┼───┼───┤
r  3 │+0.1│-0.2│+0.4│
o   └───┴───┴───┘
```

Esta superficie nos dice el valor de cada combinación de posiciones. Valores positivos favorecen a negro, valores negativos favorecen a blanco.

### Evolución durante el entrenamiento

A medida que progresa el entrenamiento, las predicciones de la Value Network se vuelven más precisas:

```
       Error de predicción
          |
     1.0  |*
          | *
     0.5  |  *
          |   *
     0.1  |    * * * * *
          +─────────────── Pasos de entrenamiento
          0   100K  500K  1M
```

El error disminuye rápidamente, luego se estabiliza.

### Identificación de posiciones difíciles

La Value Network puede ayudar a identificar posiciones difíciles:

| Salida | Significado | Estrategia |
|--------|-------------|------------|
| Cerca de +1 | Gran ventaja | Juego sólido |
| Cerca de -1 | Gran desventaja | Buscar oportunidades de remontada |
| Cerca de 0 | Posición compleja | Necesita cálculo profundo |

AlphaGo invierte más tiempo de pensamiento en posiciones cercanas a 0.

---

## Puntos clave de implementación

### Implementación en PyTorch

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ValueNetwork(nn.Module):
    def __init__(self, input_channels=49, num_filters=192, num_layers=12):
        super().__init__()

        # Primera capa convolucional (5×5)
        self.conv1 = nn.Conv2d(input_channels, num_filters,
                               kernel_size=5, padding=2)

        # Capas convolucionales intermedias (3×3) ×11
        self.conv_layers = nn.ModuleList([
            nn.Conv2d(num_filters, num_filters,
                     kernel_size=3, padding=1)
            for _ in range(num_layers - 1)
        ])

        # Capa convolucional de salida
        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)

        # Capas totalmente conectadas
        self.fc1 = nn.Linear(361, 256)
        self.fc2 = nn.Linear(256, 1)

    def forward(self, x):
        # x: (batch, 49, 19, 19)

        # Capas convolucionales
        x = F.relu(self.conv1(x))
        for conv in self.conv_layers:
            x = F.relu(conv(x))
        x = self.conv_out(x)

        # Aplanar
        x = x.view(x.size(0), -1)  # (batch, 361)

        # Capas totalmente conectadas
        x = F.relu(self.fc1(x))
        x = torch.tanh(self.fc2(x))

        return x.squeeze(-1)  # (batch,)
```

### Ciclo de entrenamiento

```python
def train_value_network(model, optimizer, states, outcomes):
    """
    states: (batch, 49, 19, 19) - características del tablero
    outcomes: (batch,) - resultado del juego (+1 o -1)
    """
    # Propagación hacia adelante
    values = model(states)  # (batch,)

    # Pérdida MSE
    loss = F.mse_loss(values, outcomes)

    # Retropropagación
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Calcular precisión (predicción correcta de victoria/derrota)
    predictions = (values > 0).float() * 2 - 1  # Convertir a +1/-1
    accuracy = (predictions == outcomes).float().mean()

    return loss.item(), accuracy.item()
```

### Técnicas para evitar sobreajuste

```python
# 1. Aumento de datos (8 simetrías)
def augment(state, outcome):
    augmented = []
    for rotation in [0, 90, 180, 270]:
        s = rotate(state, rotation)
        augmented.append((s, outcome))
        augmented.append((flip(s), outcome))
    return augmented

# 2. Dropout
class ValueNetworkWithDropout(ValueNetwork):
    def __init__(self, *args, dropout_rate=0.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        # ... capas conv ...
        x = self.dropout(x)  # dropout antes de capas FC
        # ... capas FC ...

# 3. Detención temprana (Early Stopping)
best_val_loss = float('inf')
patience = 10
counter = 0

for epoch in range(max_epochs):
    train_loss = train_one_epoch()
    val_loss = evaluate()

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        save_model()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping!")
            break
```

---

## Colaboración con Policy Network

### Relación complementaria

Policy Network y Value Network son complementarias en AlphaGo:

| Red | Pregunta que responde | Salida | Rol en MCTS |
|-----|----------------------|--------|-------------|
| Policy | ¿Dónde jugar? | Distribución de probabilidad | Guiar dirección de búsqueda |
| Value | ¿Ganaré? | Un solo valor | Evaluar nodos hoja |

### Red unificada de dos cabezas

En AlphaGo Zero, estas dos redes se fusionaron en una **red de dos cabezas**:

```
       Capas compartidas de extracción de características
              |
       ┌──────┴──────┐
       ↓              ↓
  Cabeza Policy  Cabeza Value
       ↓              ↓
  361 probabilidades  Un solo valor
```

Ventajas de este diseño:
- **Compartir parámetros**: reduce cálculo
- **Compartir características**: Policy y Value usan las mismas características
- **Entrenamiento más estable**: los dos objetivos se regularizan mutuamente

Ver más en [Red de dos cabezas y red residual](../dual-head-resnet).

---

## Correspondencia de animaciones

Los conceptos principales de este artículo y los números de animación correspondientes:

| Número | Concepto | Correspondencia física/matemática |
|--------|----------|-----------------------------------|
| E2 | Value Network | Superficie de energía potencial |
| D4 | Función de valor | Retorno esperado |
| C6 | Evaluación de nodo hoja | Aproximación de función |
| H3 | Diferencia temporal | Aprendizaje bootstrap |

---

## Lecturas adicionales

- **Artículo anterior**: [Policy Network en detalle](../policy-network) — Cómo la red de políticas selecciona movimientos
- **Siguiente artículo**: [Diseño de características de entrada](../input-features) — Detalles de los 48 planos de características
- **Tema avanzado**: [Combinación de MCTS con redes neuronales](../mcts-neural-combo) — El proceso completo de búsqueda

---

## Puntos clave

1. **Value Network predice tasa de victoria**: salida un solo valor entre -1 y +1
2. **Salida Tanh**: asegura que la salida esté en el rango correcto
3. **Pérdida MSE**: aproxima el valor predicho al resultado real
4. **Desafío de sobreajuste**: necesita datos de auto-juego para evitar
5. **Reemplaza simulaciones aleatorias**: una evaluación ≈ 15000 simulaciones

La Value Network es el "juicio" de AlphaGo — permite que la IA evalúe si cualquier posición es buena o mala, sin necesidad de agotar todas las posibilidades.

---

## Referencias

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 551, 354-359.
3. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.
4. Tesauro, G. (1995). "Temporal difference learning and TD-Gammon." *Communications of the ACM*, 38(3), 58-68.
