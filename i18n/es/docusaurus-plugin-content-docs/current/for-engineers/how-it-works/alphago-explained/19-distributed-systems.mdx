---
sidebar_position: 20
title: Sistemas distribuidos y TPU
description: AnÃ¡lisis profundo de la arquitectura de entrenamiento distribuido de AlphaGo, aceleraciÃ³n con TPU y MCTS paralelo a gran escala
keywords: [sistemas distribuidos, TPU, computaciÃ³n paralela, MCTS, virtual loss, deep learning, aceleraciÃ³n de hardware]
---

# Sistemas distribuidos y TPU

El Ã©xito de AlphaGo no es solo una victoria algorÃ­tmica, sino tambiÃ©n una victoria de ingenierÃ­a. Para entrenar una IA de Go que supere a los humanos en un tiempo razonable, se necesita un sistema distribuido cuidadosamente diseÃ±ado y el soporte de hardware especializado.

Este artÃ­culo analizarÃ¡ en profundidad la arquitectura del sistema detrÃ¡s de AlphaGo, incluyendo el proceso de entrenamiento, la arquitectura de inferencia, MCTS paralelo y el papel crucial de las TPU.

---

## VisiÃ³n general de la arquitectura de entrenamiento

### Arquitectura de entrenamiento del AlphaGo original

El entrenamiento del AlphaGo original (la versiÃ³n que derrotÃ³ a Lee Sedol) se dividiÃ³ en mÃºltiples etapas, cada una usando diferentes configuraciones de recursos:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Etapa 1: Aprendizaje supervisado          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Partidas   â”‚ â†’  â”‚  ClÃºster    â”‚ â†’  â”‚ Policy Net  â”‚     â”‚
â”‚  â”‚  humanas    â”‚    â”‚   GPU       â”‚    â”‚  (versiÃ³n   â”‚     â”‚
â”‚  â”‚  (30M)      â”‚    â”‚  (50 GPUs)  â”‚    â”‚    SL)      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Etapa 2: Aprendizaje por refuerzo         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Auto-juego â”‚ â†’  â”‚  ClÃºster    â”‚ â†’  â”‚ Policy Net  â”‚     â”‚
â”‚  â”‚  (millones  â”‚    â”‚   GPU       â”‚    â”‚  (versiÃ³n   â”‚     â”‚
â”‚  â”‚   partidas) â”‚    â”‚  (50 GPUs)  â”‚    â”‚    RL)      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Etapa 3: Entrenamiento de Value Net       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Datos de   â”‚ â†’  â”‚  ClÃºster    â”‚ â†’  â”‚ Value Net   â”‚     â”‚
â”‚  â”‚  auto-juego â”‚    â”‚   GPU       â”‚    â”‚             â”‚     â”‚
â”‚  â”‚  (30M pos.) â”‚    â”‚  (50 GPUs)  â”‚    â”‚             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Arquitectura de entrenamiento de AlphaGo Zero

AlphaGo Zero simplificÃ³ enormemente el proceso de entrenamiento, usando un Ãºnico ciclo de entrenamiento de extremo a extremo:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Ciclo de entrenamiento AlphaGo Zero        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚   Self-play       â”‚  â† Red mÃ¡s reciente                  â”‚
â”‚  â”‚   Workers         â”‚                                      â”‚
â”‚  â”‚   (TPU Ã— N)       â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚            â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚   Replay Buffer   â”‚  (Ãºltimas 500K partidas)             â”‚
â”‚  â”‚   (RAM/SSD)       â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚            â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚   Training        â”‚                                      â”‚
â”‚  â”‚   Workers         â”‚                                      â”‚
â”‚  â”‚   (TPU Ã— M)       â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚            â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚   Network         â”‚  â†’ Actualiza red usada en Self-play  â”‚
â”‚  â”‚   Checkpoint      â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Las ventajas de esta arquitectura:

1. **Aprendizaje continuo**: Self-play y Training ocurren simultÃ¡neamente, sin necesidad de esperar
2. **Eficiencia de recursos**: Todos los recursos hacen trabajo Ãºtil
3. **IteraciÃ³n rÃ¡pida**: La red se usa inmediatamente para generar nuevos datos despuÃ©s de actualizarse

---

## Estaciones de auto-juego (Self-play Workers)

### AsignaciÃ³n de tareas

Los Self-play Workers son responsables de realizar auto-juego con la red mÃ¡s fuerte actual, produciendo datos de entrenamiento.

| ConfiguraciÃ³n | AlphaGo Zero |
|---------------|--------------|
| NÃºmero de Workers | Decenas |
| Por Worker | 1-4 TPU |
| MCTS por partida | 1600 simulaciones |
| ProducciÃ³n diaria | ~100,000 partidas |

### Flujo de trabajo

El flujo de trabajo de cada Self-play Worker:

```python
while True:
    # 1. Descargar los pesos de red mÃ¡s recientes
    network = download_latest_checkpoint()

    # 2. Realizar mÃºltiples auto-juegos
    for game in range(batch_size):
        positions = []
        board = EmptyBoard()

        while not board.is_terminal():
            # Ejecutar MCTS
            mcts = MCTS(network, board)
            policy = mcts.search(num_simulations=1600)

            # Elegir movimiento
            action = sample(policy)

            # Registrar
            positions.append((board.state, policy))

            # Jugar
            board = board.play(action)

        # 3. Obtener resultado del juego
        result = board.get_result()

        # 4. Subir datos
        upload_to_replay_buffer(positions, result)
```

### Balanceo de carga

MÃºltiples Workers necesitan balanceo de carga:

- **SincronizaciÃ³n de red**: Todos los Workers usan la misma versiÃ³n de la red
- **Balance de datos**: Asegurar que los datos de diferentes Workers sean usados
- **Manejo de errores**: El fallo de un Worker no afecta el entrenamiento general

---

## Estaciones de entrenamiento (Training Workers)

### AsignaciÃ³n de tareas

Los Training Workers son responsables de muestrear datos del Replay Buffer y entrenar la red neuronal.

| ConfiguraciÃ³n | AlphaGo Zero |
|---------------|--------------|
| NÃºmero de Workers | 1-4 |
| Por Worker | 4 TPU |
| Batch Size | 2048 (512 por TPU) |
| Pasos de entrenamiento | Decenas de miles por dÃ­a |

### Entrenamiento distribuido

El entrenamiento a gran escala usa **paralelismo de datos (Data Parallelism)**:

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Parameter  â”‚
                    â”‚   Server    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚  TPU 0  â”‚      â”‚  TPU 1  â”‚      â”‚  TPU 2  â”‚
    â”‚ Batch 0 â”‚      â”‚ Batch 1 â”‚      â”‚ Batch 2 â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚                 â”‚                 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚  Gradient   â”‚
                    â”‚  Aggregationâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Cada TPU procesa diferentes mini-batches, calcula gradientes locales, luego los agrega para actualizar parÃ¡metros globales.

### ActualizaciÃ³n sÃ­ncrona vs. asÃ­ncrona

| Tipo de actualizaciÃ³n | Ventajas | Desventajas |
|-----------------------|----------|-------------|
| SÃ­ncrona | Estable, reproducible | Workers deben esperar al mÃ¡s lento |
| AsÃ­ncrona | Alto throughput | Los gradientes pueden estar obsoletos |

AlphaGo Zero usa **actualizaciÃ³n sÃ­ncrona** para asegurar la estabilidad del entrenamiento.

---

## El papel de las TPU

### Â¿QuÃ© es una TPU?

**TPU (Tensor Processing Unit)** es un acelerador diseÃ±ado por Google especÃ­ficamente para deep learning:

| CaracterÃ­stica | TPU | GPU | CPU |
|----------------|-----|-----|-----|
| Objetivo de diseÃ±o | Operaciones matriciales | Paralelismo general | ComputaciÃ³n general |
| PrecisiÃ³n | Optimizado FP16/BF16 | FP32/FP16 | FP64/FP32 |
| Consumo | Relativamente bajo | MÃ¡s alto | El mÃ¡s alto |
| Latencia | Baja | Media | Alta |

### Arquitectura de las TPU

El nÃºcleo de las TPU es la **MXU (Matrix Multiply Unit)**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TPU v2/v3                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         MXU (128Ã—128)           â”‚    â”‚
â”‚  â”‚    Matrix Multiply Unit         â”‚    â”‚
â”‚  â”‚    (128Ã—128 = 16K MACs/ciclo)   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Vector   â”‚  â”‚     HBM          â”‚     â”‚
â”‚  â”‚ Unit     â”‚  â”‚   (16-32 GB)     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

La MXU puede ejecutar 16K operaciones de multiplicaciÃ³n-acumulaciÃ³n por ciclo, crucial para la multiplicaciÃ³n de matrices de redes neuronales.

### Â¿Por quÃ© AlphaGo necesita TPU?

El cuello de botella computacional de la IA de Go estÃ¡ en la **inferencia de red neuronal**:

| OperaciÃ³n | ProporciÃ³n |
|-----------|------------|
| Forward pass de red neuronal | ~95% |
| Operaciones del Ã¡rbol MCTS | ~4% |
| Otros | ~1% |

Cada paso de MCTS requiere 1600 inferencias de red neuronal. El alto throughput de las TPU hace esto posible.

### EvoluciÃ³n del uso de TPU

| VersiÃ³n | TPU de entrenamiento | TPU de inferencia |
|---------|----------------------|-------------------|
| AlphaGo Lee | 50 GPU | 48 TPU (v1) |
| AlphaGo Master | 4 TPU (v2) | 4 TPU (v2) |
| AlphaGo Zero | 4 TPU (v2) | 4 TPU (v2) (escalable) |

El nÃºmero de TPU usadas por AlphaGo Zero se redujo significativamente, gracias a arquitecturas mÃ¡s eficientes y versiones mÃ¡s nuevas de TPU.

---

## MCTS paralelo y Virtual Loss

### El desafÃ­o de la paralelizaciÃ³n

La implementaciÃ³n estÃ¡ndar de MCTS es **serial**:

```
for i in range(num_simulations):
    1. Selection: Seleccionar hacia abajo desde la raÃ­z
    2. Expansion: Expandir nodo hoja
    3. Evaluation: EvaluaciÃ³n con red neuronal
    4. Backup: Retropropagar actualizaciones
```

Pero la evaluaciÃ³n de red neuronal es una **operaciÃ³n por lotes** amigable para GPU/TPU. Â¿CÃ³mo hacer que mÃºltiples simulaciones ocurran simultÃ¡neamente?

### ParalelizaciÃ³n de hojas (Leaf Parallelization)

El mÃ©todo de paralelizaciÃ³n mÃ¡s simple: ejecutar mÃºltiples simulaciones completas simultÃ¡neamente, luego fusionar resultados.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Root     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
  â”‚ Sim 1   â”‚ â”‚ Sim 2  â”‚ â”‚ Sim 3  â”‚ â”‚ Sim 4  â”‚
  â”‚ (indep.)â”‚ â”‚(indep.)â”‚ â”‚(indep.)â”‚ â”‚(indep.)â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Merge Trees  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Problema: Cada simulaciÃ³n comienza desde la raÃ­z, explorando repetidamente los mismos caminos.

### Virtual Loss

DeepMind adoptÃ³ la tÃ©cnica de **Virtual Loss** para implementar paralelismo de Ã¡rbol (Tree Parallelization).

#### Concepto bÃ¡sico

Cuando un hilo estÃ¡ explorando un nodo, reduce temporalmente el valor de ese nodo, haciendo que otros hilos elijan otros caminos.

```
UCB normal: Q(s,a) + c * P(s,a) * sqrt(N(s)) / (1 + N(s,a))

Con virtual loss:
(Q(s,a) * N(s,a) - v * n_virtual) / (N(s,a) + n_virtual) + c * P(s,a) * sqrt(N(s)) / (1 + N(s,a) + n_virtual)
```

Donde:
- `n_virtual` es el nÃºmero de hilos actualmente explorando ese nodo
- `v` es el valor del virtual loss (usualmente 1 o valor correspondiente a tasa de victoria)

#### Flujo de operaciÃ³n

```
Tiempo T1:
  Thread 1 elige camino A â†’ B â†’ C
  Nodo C recibe virtual loss -1

Tiempo T2:
  Thread 2 elige camino A â†’ B â†’ D (porque C fue "penalizado")
  Nodo D recibe virtual loss -1

Tiempo T3:
  Thread 1 completa evaluaciÃ³n, actualiza valor real de C, remueve virtual loss
  Thread 3 ahora puede elegir C (si el valor real es suficientemente bueno)
```

#### Efecto del virtual loss

| Aspecto | Efecto |
|---------|--------|
| Diversidad de exploraciÃ³n | Fuerza exploraciÃ³n de diferentes caminos |
| Eficiencia de lotes | Puede evaluar mÃºltiples hojas simultÃ¡neamente |
| Convergencia | El virtual loss es finalmente cubierto por valores reales, no afecta convergencia |

### EvaluaciÃ³n de red neuronal por lotes

A travÃ©s del virtual loss, se pueden recoger mÃºltiples nodos hoja pendientes de evaluaciÃ³n para **inferencia por lotes**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            MCTS Paralelo                â”‚
â”‚                                         â”‚
â”‚  Thread 1 â†’ nodo hoja L1 â”€â”€â”            â”‚
â”‚  Thread 2 â†’ nodo hoja L2 â”€â”€â”¼â”€â”€â†’ Batch â”€â†’ TPU
â”‚  Thread 3 â†’ nodo hoja L3 â”€â”€â”¤            â”‚
â”‚  Thread 4 â†’ nodo hoja L4 â”€â”€â”˜            â”‚
â”‚                                         â”‚
â”‚  â† Obtiene (P1,V1), (P2,V2), ... simul. â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

La eficiencia de inferencia por lotes de TPU es mucho mayor que inferencia uno por uno, haciendo posible el MCTS paralelo.

---

## Arquitectura de inferencia

### ConfiguraciÃ³n durante competencias

Arquitectura de inferencia de AlphaGo en competencias oficiales:

| VersiÃ³n | ConfiguraciÃ³n de hardware |
|---------|---------------------------|
| AlphaGo Fan | 176 GPU |
| AlphaGo Lee | 48 TPU + mÃºltiples servidores |
| AlphaGo Master | 4 TPU |
| AlphaGo Zero | 4 TPU (escalable) |

### Flujo de inferencia distribuida

Flujo de inferencia durante competencias (ejemplo de AlphaGo Lee):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Arquitectura de inferencia distribuida    â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   Nodo       â”‚ â† Recibe movimiento del oponente,         â”‚
â”‚  â”‚   maestro    â”‚   envÃ­a movimiento de AlphaGo             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚                                                    â”‚
â”‚         â†“                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚              Controlador MCTS                     â”‚       â”‚
â”‚  â”‚  Gestiona Ã¡rbol de bÃºsqueda, asigna tareas,      â”‚       â”‚
â”‚  â”‚  recopila resultados                             â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                                                    â”‚
â”‚         â†“                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚              ClÃºster TPU (48 TPUs)                â”‚       â”‚
â”‚  â”‚                                                   â”‚       â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”     â”‚       â”‚
â”‚  â”‚   â”‚TPU â”‚ â”‚TPU â”‚ â”‚TPU â”‚ â”‚TPU â”‚ â”‚TPU â”‚ â”‚... â”‚     â”‚       â”‚
â”‚  â”‚   â”‚ 1  â”‚ â”‚ 2  â”‚ â”‚ 3  â”‚ â”‚ 4  â”‚ â”‚ 5  â”‚ â”‚ 48 â”‚     â”‚       â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜     â”‚       â”‚
â”‚  â”‚                                                   â”‚       â”‚
â”‚  â”‚   Procesa solicitudes de inferencia por lotes     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### GestiÃ³n del tiempo de pensamiento

Estrategia de gestiÃ³n de tiempo de AlphaGo:

| PosiciÃ³n | Tiempo de pensamiento | Simulaciones MCTS |
|----------|----------------------|-------------------|
| Apertura (con joseki) | MÃ¡s corto | ~10,000 |
| Medio juego (complejo) | MÃ¡s largo | ~100,000 |
| PosiciÃ³n simple | MÃ¡s corto | ~5,000 |
| Byoyomi | Fijo | ~1,600 |

MÃ¡s simulaciones MCTS generalmente significan mejor calidad de movimiento.

---

## ComunicaciÃ³n y sincronizaciÃ³n

### Formato de datos

Formato de transmisiÃ³n de datos de entrenamiento:

```protobuf
message TrainingExample {
    // Estado del tablero (17 Ã— 19 Ã— 19)
    repeated float board_planes = 1;

    // Resultado de bÃºsqueda MCTS (362)
    repeated float mcts_policy = 2;

    // Resultado del juego (1 = actual gana, -1 = actual pierde)
    float game_result = 3;
}
```

### Requisitos de ancho de banda

| Flujo de datos | TamaÃ±o | Frecuencia |
|----------------|--------|------------|
| Muestra de entrenamiento | ~10 KB/muestra | Miles de muestras/segundo |
| Pesos de red | ~200 MB | Varias veces/hora |
| Mensajes de control | < 1 KB | Continuo |

Requisito total de ancho de banda: ~100 Mbps (red interna suficiente)

### Manejo de fallos

Manejo de fallos en sistemas distribuidos:

| Tipo de fallo | Manejo |
|---------------|--------|
| Worker cae | Reiniciar, continuar usando Ãºltimo checkpoint |
| DesconexiÃ³n de red | Almacenar en bÃºfer, retransmitir tras reconexiÃ³n |
| Fallo de TPU | Cambio automÃ¡tico a TPU de respaldo |
| CorrupciÃ³n de datos | Descartar tras verificaciÃ³n, regenerar |

---

## AnÃ¡lisis de costos

### EstimaciÃ³n de costo de hardware

EstimaciÃ³n de costo de entrenamiento de AlphaGo Zero basada en precios de TPU de Google Cloud:

| Recurso | Cantidad | Precio/hora | Precio total/dÃ­a |
|---------|----------|-------------|------------------|
| TPU v2 Pod | 4 | ~$32 | ~$3,000 |
| VM alta memoria | Varios | ~$5 | ~$500 |
| Almacenamiento | 10 TB | ~$0.02/GB | ~$200 |
| Red | - | Incluido | - |

**Aproximadamente $3,700/dÃ­a**, entrenamiento completo (40 dÃ­as) aproximadamente **$150,000**.

Nota: Esta es una estimaciÃ³n de 2017, DeepMind como subsidiaria de Google puede tener descuentos internos.

### ComparaciÃ³n con entrenamiento humano

| Aspecto | AlphaGo Zero | Jugador profesional humano |
|---------|--------------|----------------------------|
| Alcanzar nivel profesional | 2 dÃ­as | 10-15 aÃ±os |
| Costo de entrenamiento | ~$7,500 | Millones (matrÃ­cula, gastos, costo de oportunidad) |
| Costo continuo | Electricidad | Gastos de vida |
| Replicabilidad | Perfecta | No replicable |

Por supuesto, esta comparaciÃ³n no es completamente justa: los humanos aprenden mÃ¡s que solo Go durante el proceso.

### Costo de inferencia

Costo de inferencia en competencias oficiales:

| ConfiguraciÃ³n | Costo por partida |
|---------------|-------------------|
| 48 TPU (AlphaGo Lee) | ~$500 |
| 4 TPU (AlphaGo Zero) | ~$50 |
| GPU Ãºnica (KataGo) | ~$1 |

El costo de inferencia ha disminuido drÃ¡sticamente con el progreso tecnolÃ³gico.

---

## EvoluciÃ³n tecnolÃ³gica

### De AlphaGo a AlphaZero

| Aspecto | AlphaGo Lee | AlphaGo Zero | AlphaZero |
|---------|-------------|--------------|-----------|
| TPU entrenamiento | 50+ GPU â†’ TPU | 4 TPU | 4 TPU |
| TPU inferencia | 48 TPU | 4 TPU | 4 TPU |
| MCTS/movimiento | ~100,000 | ~1,600 | ~800 |
| Tiempo de entrenamiento | Meses | 40 dÃ­as | Horas-dÃ­as |

Mejora de eficiencia de aproximadamente 100 veces.

### Impacto en la comunidad de cÃ³digo abierto

La arquitectura de AlphaGo inspirÃ³ mÃºltiples proyectos de cÃ³digo abierto:

| Proyecto | CaracterÃ­sticas |
|----------|-----------------|
| Leela Zero | Entrenamiento distribuido comunitario, replica AlphaGo Zero |
| KataGo | Entrenamiento eficiente en una sola GPU, supera AlphaGo Zero |
| ELF OpenGo | CÃ³digo abierto de Facebook, usa PyTorch |
| Minigo | CÃ³digo abierto de Google, usa TensorFlow |

Estos proyectos permiten a investigadores ordinarios entrenar IA de Go potentes.

---

## Correspondencia con animaciones

Conceptos centrales de este artÃ­culo y nÃºmeros de animaciÃ³n:

| NÃºmero | Concepto | Correspondencia fÃ­sica/matemÃ¡tica |
|--------|----------|-----------------------------------|
| ğŸ¬ C9 | MCTS paralelo | Problema de muchos cuerpos |
| ğŸ¬ E9 | Entrenamiento distribuido | ComputaciÃ³n distribuida |
| ğŸ¬ C5 | Virtual loss | Potencial de repulsiÃ³n |
| ğŸ¬ D15 | Inferencia por lotes | CÃ¡lculo vectorizado |

---

## Lecturas adicionales

- **ArtÃ­culo anterior**: [El proceso de entrenamiento desde cero](../training-from-scratch) â€” AnÃ¡lisis detallado de la curva de entrenamiento
- **ArtÃ­culo siguiente**: [El legado de AlphaGo](../legacy-and-impact) â€” El profundo impacto de AlphaGo en el campo de IA
- **ArtÃ­culo relacionado**: [CombinaciÃ³n de MCTS y redes neuronales](../mcts-neural-combo) â€” Conocimientos bÃ¡sicos de MCTS

---

## Referencias

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Jouppi, N., et al. (2017). "In-Datacenter Performance Analysis of a Tensor Processing Unit." *ISCA 2017*.
3. Dean, J., et al. (2012). "Large Scale Distributed Deep Networks." *NeurIPS 2012*.
4. Chaslot, G., et al. (2008). "Parallel Monte-Carlo Tree Search." *CIG 2008*.
5. Segal, R. (2010). "On the Scalability of Parallel UCT." *CIG 2010*.
