---
sidebar_position: 19
title: El proceso de entrenamiento desde cero
description: Observa c√≥mo AlphaGo Zero pas√≥ de jugadas aleatorias a superar a los humanos en tres d√≠as, redescubriendo y superando miles de a√±os de teor√≠a del Go
keywords: [AlphaGo Zero, proceso de entrenamiento, auto-juego, crecimiento de fuerza, Go AI, deep learning]
---

import { EloChart } from '@site/src/components/D3Charts';

# El proceso de entrenamiento desde cero

Lo m√°s asombroso de AlphaGo Zero no es solo su fuerza final, sino su **proceso de crecimiento**: partiendo de un estado completamente aleatorio, en solo tres d√≠as atraves√≥ la acumulaci√≥n de conocimiento del Go que a los humanos les tom√≥ miles de a√±os completar, y luego super√≥ toda comprensi√≥n humana.

Este art√≠culo te guiar√° paso a paso a trav√©s de esta asombrosa transformaci√≥n.

---

## Curva de entrenamiento

Primero, veamos la curva de crecimiento de fuerza de AlphaGo Zero:

<EloChart mode="zero" width={700} height={400} />

Esta curva muestra los cambios en la fuerza de AlphaGo Zero durante 72 horas. Observa algunos hitos clave:

| Tiempo | Puntuaci√≥n ELO | Equivalente a |
|--------|----------------|---------------|
| 0 horas | 0 | Jugadas aleatorias |
| 3 horas | ~1000 | Descubre reglas b√°sicas |
| 12 horas | ~3000 | Descubre joseki y formas |
| 36 horas | ~4500 | Supera AlphaGo versi√≥n Fan Hui |
| 60 horas | ~5200 | Supera AlphaGo versi√≥n Lee Sedol |
| 72 horas | ~5400 | Supera todas las versiones anteriores |

**Tres d√≠as, de cero a superar la cima humana.**

---

## D√≠a 0: El comienzo ca√≥tico

### Estado inicial completamente aleatorio

Al inicio del entrenamiento, los pesos de la red neuronal se inicializan aleatoriamente. Esto significa:

- **Policy Head**: Produce una distribuci√≥n casi uniforme, con probabilidad de colocar en cada posici√≥n de aproximadamente 1/361
- **Value Head**: Produce valores cercanos a 0, incapaz de distinguir entre posiciones buenas y malas

En este punto, AlphaGo Zero juega de manera completamente aleatoria, peor que alguien que nunca ha visto un tablero de Go.

### El primer auto-juego

Imagina c√≥mo ser√≠a el primer auto-juego:

```
Negro 1: Coloca aleatoriamente en alg√∫n lugar (podr√≠a ser el tengen, una esquina, o la primera l√≠nea)
Blanco 2: Coloca aleatoriamente en otro lugar
Negro 3: Aleatorio...
...
Movimiento 200: El tablero est√° lleno de piedras aisladas, sin ninguna conexi√≥n
Final: El resultado se determina por factores aleatorios
```

La "calidad" de esta partida es extremadamente baja, pero contiene informaci√≥n valiosa: **qui√©n gan√≥ al final**.

### La primera se√±al de entrenamiento

Aunque ambos lados juegan aleatoriamente, el resultado de ganar o perder es definitivo. La red neuronal comienza a aprender:

> "En esta posici√≥n, negro finalmente gan√≥. Aunque no s√© por qu√©, esta posici√≥n podr√≠a ser mejor para negro."

Esta es una se√±al muy d√©bil, pero es real. Despu√©s de miles de estas "partidas basura", la red comienza a descubrir algunos patrones estad√≠sticos.

---

## Hora 1-3: Descubriendo las reglas del juego

### Emergencia de la conciencia de las reglas

Despu√©s de decenas de miles de auto-juegos, AlphaGo Zero comienza a "descubrir" las reglas b√°sicas del Go (aunque estas reglas ya est√°n incorporadas en el motor del juego):

#### 1. La importancia de la conexi√≥n

```
Observaci√≥n: Cuando las piedras est√°n conectadas, son m√°s dif√≠ciles de capturar
Aprendizaje: Comienza a priorizar colocar junto a piedras existentes
```

Esto no fue ense√±ado, sino aprendido de los resultados de ganar/perder. Las piedras dispersas son f√°ciles de derrotar una por una, mientras que las piedras conectadas sobreviven m√°s f√°cilmente.

#### 2. El concepto de libertades

```
Observaci√≥n: Cuando todos los puntos adyacentes de una piedra est√°n ocupados, la piedra desaparece
Aprendizaje: Comienza a evitar posiciones con pocas libertades, comienza a atacar piedras del oponente con pocas libertades
```

La red aprendi√≥ a rastrear las libertades, aunque no hay una caracter√≠stica expl√≠cita de "libertades" en la entrada, se puede inferir de los estados hist√≥ricos del tablero.

#### 3. Embri√≥n del ojo

```
Observaci√≥n: Ciertas formas son particularmente dif√≠ciles de capturar
Aprendizaje: Comienza a formar formas con espacio en esquinas y bordes
```

Este es el inicio del concepto de piedras vivas. La red descubri√≥ que los grupos con espacio interno sobreviven m√°s f√°cilmente.

### Evaluaci√≥n de fuerza

En este punto, AlphaGo Zero es aproximadamente:
- **ELO**: ~1000
- **Equivalente a**: Principiante que acaba de aprender las reglas
- **Caracter√≠sticas**: Sabe que debe conectar piedras, sabe capturar piedras del oponente

---

## Hora 3-12: Descubriendo joseki y formas

### El despertar de las esquinas

Con m√°s entrenamiento, la red descubri√≥ la importancia de las esquinas:

```
Observaci√≥n: Las piedras en las esquinas solo necesitan 2 ojos para vivir
           En los bordes, hacer 2 ojos es m√°s dif√≠cil
           En el centro, hacer 2 ojos es lo m√°s dif√≠cil
Aprendizaje: Prioriza ocupar las esquinas en la apertura
```

Este es el proceso de descubrimiento del principio humano "Las esquinas son oro, los bordes son plata, el centro es hierba". La red no fue informada de este principio, sino que lo descubri√≥ por s√≠ misma a partir de cientos de miles de partidas.

### La emergencia del joseki

M√°s sorprendente a√∫n, la red comenz√≥ a "inventar" joseki, secuencias est√°ndar de juego en las esquinas:

#### Fen√≥meno observado

```
Etapa temprana del entrenamiento: El juego en las esquinas es muy variado
Etapa media del entrenamiento: Ciertas jugadas aparecen repetidamente
Etapa tard√≠a del entrenamiento: Se forman joseki estables en las esquinas
```

Estos joseki son **muy similares** a los joseki que los humanos acumularon durante cientos de a√±os, validando que estos joseki son efectivamente aproximaciones a la soluci√≥n √≥ptima para ambos lados.

### Joseki emergente t√≠pico

Tomemos el joseki de komoku como ejemplo:

```
  A B C D E F G H J
9 . . . . . . . . .
8 . . . . . . . . .
7 . . . . . . . . .
6 . . . ‚óè . . . . .   ‚óè = Negro
5 . . . . . . . . .   ‚óã = Blanco
4 . . . ‚óã . ‚óè . . .
3 . . . . . . . . .
2 . . . . . . . . .
1 . . . . . . . . .
```

Negro ocupa el komoku, blanco hace kakari, negro pinza; esta secuencia emergi√≥ naturalmente durante el entrenamiento.

### Conocimiento de formas

Adem√°s del joseki, la red tambi√©n aprendi√≥ la diferencia entre buenas y malas formas:

| Forma | Evaluaci√≥n humana | Aprendizaje de Zero |
|-------|-------------------|---------------------|
| Tri√°ngulo vac√≠o | Forma torpe | Gradualmente evitada |
| Boca de tigre | Buena forma | Gradualmente preferida |
| Doble ala voladora | Forma de ataque cl√°sica | Descubierta naturalmente |
| Cabeza del dios | Ataque poderoso | Descubierto naturalmente |

### Evaluaci√≥n de fuerza

En este punto, AlphaGo Zero:
- **ELO**: ~3000
- **Equivalente a**: Dan alto amateur
- **Caracter√≠sticas**: Tiene conocimiento b√°sico de joseki, entiende formas b√°sicas

---

## Hora 12-36: Maduraci√≥n de la teor√≠a del Go

### Formaci√≥n de la visi√≥n global

Entrando en el segundo d√≠a, la red comienza a mostrar **visi√≥n global**:

#### Influencia y territorio

```
Observaci√≥n: Rodear espacio produce puntos
           Pero la influencia tambi√©n tiene valor, puede atacar al oponente
Aprendizaje: Buscar equilibrio entre tomar territorio y tomar influencia
```

Este es uno de los conceptos m√°s profundos del Go. La red aprendi√≥ a evaluar el valor de lo "virtual" y lo "real".

#### Juicio de grosor y delgadez

```
Observaci√≥n: Las piedras "gruesas" pueden apoyar luchas distantes
           Las piedras "delgadas" necesitan refuerzo, de lo contrario ser√°n atacadas
Aprendizaje: Construir activamente grosor, atacar las debilidades del oponente
```

### T√°cticas de medio juego

Las habilidades de lucha en el medio juego de la red mejoraron significativamente:

| T√©cnica | Descripci√≥n |
|---------|-------------|
| Atacar grupos d√©biles | Identificar grupos aislados del oponente, lanzar ofensiva |
| Usar grosor | Usar influencia gruesa para apoyar ataques, obtener beneficios |
| Intercambio | Renunciar a p√©rdidas locales, ganar ventaja global |
| Invasi√≥n | Reducir el marco del oponente |

### Habilidades de yose

El c√°lculo preciso en la fase de yose tambi√©n mejora:

```
Observaci√≥n: El valor de cada movimiento en yose puede calcularse con precisi√≥n
Aprendizaje: Hacer yose en orden de valor decreciente
```

La red aprendi√≥ conceptos de yose como "sente para ambos", "sente unilateral", "gote".

### Evaluaci√≥n de fuerza

En este punto, AlphaGo Zero:
- **ELO**: ~4500
- **Equivalente a**: Nivel de jugador profesional
- **Caracter√≠sticas**: Tiene comprensi√≥n completa del Go, puede jugar partidas de alta calidad

---

## Hora 36-72: Superando a los humanos

### Superando el nivel profesional

Alrededor de las 36 horas, la fuerza de AlphaGo Zero alcanz√≥ el nivel de jugadores profesionales. Pero el entrenamiento no se detuvo; continu√≥ el auto-juego, continu√≥ mejorando.

Lo que sucedi√≥ despu√©s es a√∫n m√°s interesante: **comenz√≥ a descubrir movimientos que los humanos nunca hab√≠an pensado**.

### Aperturas revolucionarias

Las aperturas tradicionales del Go tienen muchas "ideas establecidas":

| Visi√≥n tradicional | Descubrimiento de AlphaGo Zero |
|--------------------|--------------------------------|
| Ocupar esquinas primero en la apertura | En algunos casos es mejor ocupar los bordes primero |
| Komoku es lo m√°s s√≥lido | Ocupar san-san directamente es viable |
| Hay que memorizar bien el joseki | Puedes desviarte activamente del joseki |
| San-san temprano es codicioso | San-san es correcto en ciertas posiciones |

Estos "descubrimientos" fueron ampliamente estudiados por jugadores profesionales humanos despu√©s de AlphaGo, y muchos ya han sido incorporados a la teor√≠a moderna del Go.

### Formas contra-intuitivas

AlphaGo Zero a veces juega formas que los humanos consideran "feas":

```
Humano: "Esta es una forma torpe, no puede ser un buen movimiento"
Zero: (juega ese movimiento)
Despu√©s del an√°lisis: "Resulta que esto es m√°s eficiente"
```

Esto revela las limitaciones de la teor√≠a humana del Go: algunas "malas formas" son en realidad la soluci√≥n √≥ptima en posiciones espec√≠ficas.

### Sacrificio agresivo

Zero est√° m√°s dispuesto que los humanos a sacrificar piedras por otros beneficios:

```
P√©rdida local de 3 puntos
Ganar la iniciativa global
Tasa de victoria final aumenta
```

Los jugadores humanos a menudo se preocupan demasiado por ganancias y p√©rdidas locales, mientras que Zero siempre observa la tasa de victoria final.

### Evaluaci√≥n de fuerza

AlphaGo Zero despu√©s de 72 horas:
- **ELO**: ~5400
- **Equivalente a**: Supera a todos los jugadores humanos
- **Caracter√≠sticas**: Descubre movimientos desconocidos para humanos, crea nueva teor√≠a del Go

---

## Redescubriendo la teor√≠a humana del Go

### Miles de a√±os vs. tres d√≠as

El Go humano se desarroll√≥ durante miles de a√±os:
- Originado en China alrededor del a√±o 2000 a.C.
- Transmitido a Jap√≥n en la dinast√≠a Tang, desarroll√≥ teor√≠a refinada
- El sistema profesional apareci√≥ en el siglo XX, la teor√≠a se profundiz√≥ m√°s
- En 2016, los humanos pensaban que ya entend√≠an bastante bien el Go

AlphaGo Zero recorri√≥ este camino en tres d√≠as. M√°s sorprendente a√∫n, la teor√≠a que descubri√≥ es **altamente consistente** con la de los humanos.

### Validaci√≥n y superaci√≥n

| Conocimiento humano | Actitud de Zero |
|---------------------|-----------------|
| Las esquinas son oro, los bordes son plata | Confirmado (las esquinas son realmente importantes) |
| Joseki b√°sico | Mayormente confirmado, algunos mejorados |
| Buenas y malas formas | Mayormente confirmado, existen excepciones |
| Sacrificio e intercambio | M√°s agresivo que los humanos |
| Juicio de grosor y delgadez | Generalmente consistente, detalles diferentes |

Esto indica que la teor√≠a del Go acumulada por los humanos durante miles de a√±os **es correcta en la direcci√≥n general**. Pero tambi√©n hay algunas √°reas donde la comprensi√≥n humana necesita correcci√≥n.

### Implicaciones para el aprendizaje humano

El proceso de entrenamiento de AlphaGo Zero trae inspiraci√≥n al aprendizaje humano:

1. **Comenzar desde lo b√°sico**: Zero primero aprendi√≥ las reglas, luego las formas, finalmente desarroll√≥ visi√≥n global
2. **Mucha pr√°ctica**: 4.9 millones de auto-juegos equivalen a decenas de miles de a√±os de partidas humanas
3. **Enfocarse en ganar/perder**: No perseguir "Go bonito", solo perseguir ganar
4. **No estar limitado por la tradici√≥n**: Atreverse a probar movimientos "imposibles"

---

## Detalles t√©cnicos del proceso de entrenamiento

### Mecanismo de auto-juego

El flujo de cada auto-juego:

```
Inicializaci√≥n: Tablero vac√≠o
‚Üì
Cada movimiento:
  1. Usar red neuronal para evaluar la posici√≥n actual
  2. Ejecutar b√∫squeda MCTS (1600 simulaciones)
  3. Elegir movimiento seg√∫n resultado de b√∫squeda
  4. Registrar (posici√≥n, probabilidad MCTS, -)
‚Üì
Fin del juego:
  1. Determinar victoria/derrota z ‚àà {-1, +1}
  2. Completar todos los registros con resultado (posici√≥n, probabilidad MCTS, z)
  3. A√±adir datos al pool de entrenamiento
```

### Ritmo del entrenamiento

El entrenamiento de AlphaGo Zero es **continuo**:

```
Self-play Workers:       Producen datos de auto-juego constantemente
Training Workers:        Muestrean del pool de datos constantemente para entrenar
Network Updates:         Actualizan peri√≥dicamente la red usada para auto-juego
```

Estos tres procesos ocurren simult√°neamente, formando un ciclo de mejora continua.

### Gesti√≥n del pool de datos

Gesti√≥n del pool de datos de entrenamiento:

| Par√°metro | Valor |
|-----------|-------|
| Tama√±o del pool | √öltimas 500,000 partidas |
| Muestras por partida | ~200 movimientos |
| Total de muestras | ~100 millones |
| M√©todo de muestreo | Aleatorio uniforme |

Los datos antiguos son reemplazados por datos nuevos, asegurando que los datos de entrenamiento reflejen el nivel actual de la red.

### Estrategia de actualizaci√≥n de red

No se actualiza la red de auto-juego despu√©s de cada paso de entrenamiento. En cambio:

1. Despu√©s de entrenar un tiempo, se genera una red candidata
2. La red candidata juega contra la red actual (400 partidas)
3. Si la tasa de victoria de la candidata > 55%, se actualiza
4. De lo contrario, contin√∫a entrenando

Esto asegura que el auto-juego siempre use una red **suficientemente fuerte**.

---

## An√°lisis de la velocidad de aprendizaje

### ¬øPor qu√© tan r√°pido?

Razones de la asombrosa velocidad de aprendizaje de AlphaGo Zero:

#### 1. Recursos computacionales

- 4 TPUs, decenas de miles de inferencias por segundo
- Cientos de miles de auto-juegos generados por d√≠a
- Equivalente a miles de a√±os de partidas humanas

#### 2. Oponente perfecto

Auto-juego significa:
- El oponente siempre tiene nivel similar
- Ni demasiado d√©bil (no aprende nada) ni demasiado fuerte (no puede ganar)
- Estas son condiciones de aprendizaje ideales

#### 3. Objetivo directo

Solo un objetivo: ganar. Sin:
- Preferencias del maestro
- B√∫squeda de estilo
- Consideraciones est√©ticas

#### 4. Aprendizaje de representaci√≥n eficiente

Las redes residuales pueden aprender caracter√≠sticas de tablero muy abstractas, m√°s efectivas que las caracter√≠sticas dise√±adas manualmente.

### Comparaci√≥n con humanos

| Aspecto | Humano | AlphaGo Zero |
|---------|--------|--------------|
| Velocidad de aprendizaje | ~10 partidas/d√≠a | ~100,000 partidas/d√≠a |
| Retenci√≥n de memoria | Hay olvido | Retenci√≥n perfecta |
| L√≠mites de energ√≠a | Necesita descanso | Funciona 24/7 |
| Capacidad de innovaci√≥n | Influenciado por tradici√≥n | Sin l√≠mites preestablecidos |

---

## Fen√≥menos interesantes durante el entrenamiento

### Estancamiento por etapas

La curva de entrenamiento no es completamente suave, a veces aparecen **per√≠odos de estancamiento**:

```
ELO: 2000 -----> 2000 -----> 2500 ---->
          (estancamiento) (avance)
```

Esto puede ser porque la red est√° aprendiendo alg√∫n concepto nuevo y necesita tiempo para "digerirlo".

### Emergencia y desaparici√≥n de estrategias

Ciertas estrategias emergen durante el entrenamiento y luego desaparecen:

```
Etapa 1: Descubre cierto medio de ataque
Etapa 2: El oponente aprende a defenderse
Etapa 3: La frecuencia de uso de ese medio disminuye
Etapa 4: Descubre nuevo medio de ataque
```

Esto es una miniatura de la carrera armament√≠stica.

### "Reinventando la rueda"

Durante el entrenamiento, Zero "reinventa" conceptos ya conocidos por humanos:

- **Escalera**: Descubre que atari continuo puede capturar piedras
- **Snap-back**: Descubre que puede ofrecer piedras primero y luego contra-capturar
- **Ko**: Descubre c√≥mo explotar la regla de ko

El orden de estos descubrimientos es similar al orden en que los humanos aprenden Go.

---

## Correspondencia con animaciones

Conceptos centrales de este art√≠culo y n√∫meros de animaci√≥n:

| N√∫mero | Concepto | Correspondencia f√≠sica/matem√°tica |
|--------|----------|-----------------------------------|
| üé¨ E12 | Curva de crecimiento de fuerza | Crecimiento en S (log√≠stico) |
| üé¨ E7 | Desde cero | Fen√≥meno de auto-organizaci√≥n |
| üé¨ E5 | Auto-juego | Convergencia de punto fijo |
| üé¨ F8 | Capacidades emergentes | Transici√≥n de fase |

---

## Lecturas adicionales

- **Art√≠culo anterior**: [Red de doble cabeza y ResNet](../dual-head-resnet) ‚Äî La arquitectura de red neuronal que sustenta todo esto
- **Art√≠culo siguiente**: [Sistemas distribuidos y TPU](../distributed-systems) ‚Äî El hardware que hace todo esto posible
- **Art√≠culo relacionado**: [Auto-juego](../self-play) ‚Äî Por qu√© el auto-juego es tan efectivo

---

## Referencias

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
3. DeepMind. (2017). "AlphaGo Zero: Learning from scratch." *YouTube*.
4. Wang, F., et al. (2019). "A Survey on the Evolution of AlphaGo." *arXiv:1907.11180*.
