---
sidebar_position: 17
title: AlphaGo Zero 概述
description: 由零開始、完全自學，AlphaGo Zero 點樣喺無人類棋譜嘅情況下超越所有前代版本
keywords: [AlphaGo Zero, 自我對弈, 強化學習, 深度學習, 圍棋 AI, 無監督學習]
---

# AlphaGo Zero 概述

2017 年 10 月，DeepMind 發表咗一個震驚 AI 界嘅成果：**AlphaGo Zero** 喺無使用任何人類棋譜嘅情況下，由完全隨機嘅狀態開始訓練，淨係三日就超越咗擊敗李世乭嘅原版 AlphaGo，並以 **100:0** 嘅比數完勝。

呢個唔淨止係數字上嘅進步。呢個代表一個全新嘅範式：**AI 唔需要人類知識，可以由零發現一切**。

---

## 點解唔需要人類棋譜？

### 人類棋譜嘅限制

原版 AlphaGo 嘅訓練過程分為兩個階段：

1. **監督學習**：用 3000 萬局人類棋譜訓練 Policy Network
2. **強化學習**：透過自我對弈進一步提升

呢個方法有幾個根本性嘅問題：

#### 1. 人類棋譜有上限

人類棋手嘅棋力有極限，棋譜入面包含嘅係人類嘅理解，都包含人類嘅錯誤同偏見。當 AI 由人類棋譜學習嗰陣，佢學到嘅係：

- 人類認為好嘅著法（但唔一定係最優嘅）
- 人類嘅思維模式（但可能限制創新）
- 人類嘅錯誤（會被當作正確嘅樣本學習）

#### 2. 監督學習嘅瓶頸

監督學習嘅目標係「模仿人類」——預測人類棋手會落邊一步。呢個意味住 AI 嘅能力上限被人類棋手嘅能力所限制。

就好似一個學徒只能模仿師傅，永遠無法超越師傅一樣。

#### 3. 資料收集成本

高質素嘅人類棋譜需要多年累積，而且只存在於圍棋呢類有悠久歷史嘅遊戲入面。如果要將 AI 應用到新領域（好似蛋白質結構預測），根本無「人類專家棋譜」可用。

### Zero 嘅突破

AlphaGo Zero 完全跳過監督學習階段，直接由**隨機初始化**開始自我對弈。呢個解決咗上述所有問題：

| 問題 | 原版 AlphaGo | AlphaGo Zero |
|------|-------------|--------------|
| 人類知識上限 | 受限於棋譜質素 | 無此限制 |
| 學習目標 | 模仿人類 | 最大化勝率 |
| 資料需求 | 3000 萬局棋譜 | 0 |
| 可推廣性 | 只限圍棋 | 可推廣至其他領域 |

呢個係一個根本性嘅範式轉變：由「學習人類知識」轉向「由第一性原理發現知識」。

---

## 與原版 AlphaGo 嘅對比：100:0

### 碾壓性嘅勝利

DeepMind 令訓練完成嘅 AlphaGo Zero 與各個版本嘅 AlphaGo 對弈：

| 對手 | AlphaGo Zero 戰績 |
|------|-------------------|
| AlphaGo Fan（擊敗樊麾版本） | 100:0 |
| AlphaGo Lee（擊敗李世乭版本） | 100:0 |
| AlphaGo Master（60 連勝版本） | 89:11 |

**100:0**——呢個意味住喺 100 盤比賽入面，原版 AlphaGo 連一盤都贏唔到。

### 更少嘅資源，更強嘅棋力

唔淨止係贏，AlphaGo Zero 仲用更少嘅資源達成更強嘅棋力：

| 指標 | AlphaGo Lee | AlphaGo Zero |
|------|-------------|--------------|
| 訓練時間 | 數個月 | 40 日（3 日超越 AlphaGo Lee） |
| 訓練局數 | 3000 萬人類棋譜 + 自我對弈 | 490 萬局自我對弈 |
| TPU 數量（訓練） | 50+ | 4 |
| TPU 數量（推理） | 48 | 4 |
| 輸入特徵 | 48 個平面 | 17 個平面 |
| 神經網絡 | SL + RL 雙網絡 | 單一雙頭網絡 |

呢個係一個驚人嘅效率提升：**資源減少 10 倍以上，棋力卻大幅提升**。

### 點解 Zero 更強？

AlphaGo Zero 更強嘅原因可以由幾個角度理解：

#### 1. 無偏見嘅學習

原版 AlphaGo 由人類棋譜學習，繼承咗人類嘅偏見。例如，人類棋手可能過度重視某啲定式，或者對某啲局面有錯誤嘅評估。

AlphaGo Zero 無呢啲包袱。佢由白紙開始，只透過勝負結果嚟學習乜嘢係好棋。呢個令佢能夠發現人類從未諗過嘅著法。

#### 2. 一致嘅學習目標

原版 AlphaGo 嘅訓練有兩個唔同嘅目標：
- 監督學習：最大化對人類落子嘅預測準確率
- 強化學習：最大化勝率

呢兩個目標可能互相衝突。AlphaGo Zero 只有一個目標：**勝率最大化**。呢個令學習過程更加一致同有效。

#### 3. 更簡潔嘅架構

原版 AlphaGo 使用分離嘅 Policy Network 同 Value Network。AlphaGo Zero 使用單一嘅雙頭網絡（詳見下一篇），令特徵表示能夠被共享，提高咗學習效率。

---

## 簡化嘅輸入特徵：由 48 到 17

### 原版 AlphaGo 嘅 48 個特徵平面

原版 AlphaGo 嘅神經網絡輸入包含 48 個 19x19 嘅特徵平面，編碼咗大量人類設計嘅特徵：

| 類別 | 特徵數 | 內容 |
|------|--------|------|
| 棋子位置 | 3 | 黑子、白子、空點 |
| 氣數 | 8 | 1-8 氣嘅棋串 |
| 提子 | 8 | 能提 1-8 粒子 |
| 打劫 | 1 | 劫爭位置 |
| 邊線距離 | 4 | 一線到四線 |
| 落子合法性 | 1 | 邊啲位置可以落 |
| 歷史狀態 | 8 | 過去 8 手嘅位置 |
| 輪次 | 1 | 黑方或白方 |
| 其他 | 14 | 征子、眼位等 |

呢 48 個特徵係圍棋專家精心設計嘅，包含咗大量領域知識。

### AlphaGo Zero 嘅 17 個特徵平面

AlphaGo Zero 大幅簡化咗輸入，只使用 17 個特徵平面：

| 平面編號 | 內容 | 數量 |
|----------|------|------|
| 1-8 | 黑子位置（最近 8 步） | 8 |
| 9-16 | 白子位置（最近 8 步） | 8 |
| 17 | 當前輪次（全 1 或全 0） | 1 |

呢 17 個特徵只包含：
- **當前棋盤狀態**：每個位置有黑子、白子或空
- **歷史資訊**：過去 8 步嘅棋盤狀態
- **輪次資訊**：輪到邊個落

無氣數、無征子判斷、無邊線距離——所有呢啲「圍棋知識」都畀神經網絡自己學習。

### 點解簡化係好嘅？

#### 1. 令網絡自己發現特徵

複雜嘅手工特徵可能遺漏重要資訊，或者編碼錯誤嘅假設。令神經網絡由原始資料學習，佢可能發現更好嘅特徵表示。

事實證明，AlphaGo Zero 學識咗人類設計嘅所有特徵（氣數、征子等），仲學到咗一啲人類無明確意識到嘅模式。

#### 2. 更好嘅可推廣性

48 個特徵入面嘅好多係圍棋專用嘅（好似征子、邊線距離）。17 個簡化特徵則係通用嘅——任何棋盤遊戲都可以用類似嘅方式編碼。

呢個為後嚟嘅 **AlphaZero**（通用遊戲 AI）奠定咗基礎。

#### 3. 減少人為錯誤

手工設計嘅特徵可能包含錯誤或者唔完整嘅定義。簡化輸入消除咗呢類問題嘅可能性。

---

## 單一網絡架構

### 原版嘅雙網絡設計

原版 AlphaGo 使用兩個獨立嘅神經網絡：

```
Policy Network:  輸入 → CNN → 19x19 落子機率
Value Network:   輸入 → CNN → 勝率評估（-1 到 1）
```

呢兩個網絡：
- 有唔同嘅架構（層數、通道數略有唔同）
- 獨立訓練（先訓練 Policy，再訓練 Value）
- 唔共享任何參數

### Zero 嘅雙頭網絡

AlphaGo Zero 使用單一網絡，但有兩個輸出頭（heads）：

```
輸入 → ResNet 共享主幹 → Policy Head → 19x19 落子機率
                       → Value Head  → 勝率評估
```

兩個 Head 共享同一個 ResNet 主幹（詳見[下一篇：雙頭網絡與殘差網絡](../dual-head-resnet)），呢個帶嚟幾個好處：

#### 1. 參數效率

共享主幹意味住大部分參數被兩個任務共用。呢個減少咗總參數量，降低咗過擬合風險。

#### 2. 特徵共享

「應該落邊度」（Policy）同「邊個會贏」（Value）需要理解類似嘅棋盤模式。共享主幹令呢啲特徵能被兩個任務同時學習同利用。

#### 3. 訓練穩定性

聯合訓練令梯度訊號嚟自兩個來源，提供咗更豐富嘅監督訊號，令訓練更加穩定。

### 殘差網絡嘅威力

AlphaGo Zero 嘅主幹使用 **40 層殘差網絡（ResNet）**，比原版 AlphaGo 嘅 13 層 CNN 深得多。

殘差連接（skip connections）令深層網絡得以有效訓練，避免咗梯度消失問題。呢個係 2015 年 ImageNet 競賽嘅突破性技術，被 AlphaGo Zero 成功應用到圍棋領域。

---

## 訓練效率嘅提升

### 自我對弈嘅指數增長

AlphaGo Zero 嘅訓練過程展示咗令人驚嘆嘅效率：

| 訓練時間 | ELO 評分 | 相當於 |
|----------|----------|--------|
| 0 小時 | 0 | 隨機亂落 |
| 3 小時 | ~1000 | 發現基本規則 |
| 12 小時 | ~3000 | 發現定式 |
| 36 小時 | ~4500 | 超越樊麾版 |
| 60 小時 | ~5200 | 超越李世乭版 |
| 72 小時 | ~5400 | 超越原版 AlphaGo |
| 40 日 | ~5600 | 最強版本 |

**三日超越人類、三日超越之前花費數個月訓練嘅 AI**——呢個係指數級嘅效率提升。

### 點解咁快？

#### 1. 更強嘅搜索引導

AlphaGo Zero 嘅 MCTS 完全由神經網絡引導，唔再使用快速走子策略（rollout）。呢個令搜索更加高效同準確。

#### 2. 更快嘅自我對弈

由於只需要一個網絡（而唔係兩個），每局自我對弈嘅計算成本降低。呢個意味住喺相同時間內可以產生更多訓練資料。

#### 3. 更有效嘅學習

雙頭網絡嘅聯合訓練令每一局棋嘅資訊被更有效噉利用。Policy 同 Value 嘅梯度相互強化，加速咗收斂。

### 與人類學習嘅對比

人類棋手需要幾耐時間達到唔同水平？

| 水平 | 人類所需時間 | AlphaGo Zero |
|------|-------------|--------------|
| 入門 | 數週 | 幾分鐘 |
| 業餘初段 | 數年 | 數小時 |
| 職業水平 | 10-20 年 | 1-2 日 |
| 世界冠軍 | 20+ 年全職投入 | 3 日 |
| 超越人類 | 唔可能 | 3 日 |

呢個對比唔係要貶低人類棋手——佢哋用嘅係生物神經元，而 AlphaGo Zero 用嘅係專門設計嘅 TPU 同幾千瓦嘅電力。但佢確實展示咗正確嘅學習方法可以幾咁高效。

---

## 通用性：國際象棋、將棋

### AlphaZero 嘅誕生

2017 年 12 月，DeepMind 發表咗 **AlphaZero**——AlphaGo Zero 嘅通用版本。同一套演算法，只需修改遊戲規則，就能喺三種棋類遊戲入面達到世界頂級水平：

| 遊戲 | 訓練時間 | 對手 | 戰績 |
|------|----------|------|------|
| 圍棋 | 8 小時 | AlphaGo Zero | 60:40 |
| 國際象棋 | 4 小時 | Stockfish 8 | 28 勝 72 和 0 負 |
| 將棋 | 2 小時 | Elmo | 90:8:2 |

留意呢度嘅對手：
- **Stockfish** 係當時最強嘅國際象棋引擎，使用幾十年人類知識同優化
- **Elmo** 係當時最強嘅將棋 AI

AlphaZero 用幾小時訓練，就超越咗呢啲耗費多年開發嘅專用系統。

### 通用性嘅意義

AlphaGo Zero / AlphaZero 證明咗一件重要嘅事：

> **同一套學習演算法，可以喺唔同領域達到超人水平。**

呢個唔係三個唔同嘅 AI，而係一個通用嘅學習框架：

1. **自我對弈**產生經驗
2. **蒙地卡羅樹搜索**探索可能性
3. **神經網絡**學習策略同價值函數
4. **強化學習**優化目標函數

呢個框架唔依賴領域特定嘅知識，呢個為 AI 嘅通用化邁出咗重要一步。

### 對傳統 AI 嘅衝擊

喺 AlphaZero 之前，國際象棋同將棋嘅最強 AI 都係「專家系統」風格嘅：

- **大量人類知識**：開局庫、殘局庫、評估函數
- **數十年優化**：無數棋手同工程師嘅心血
- **極度專業化**：Stockfish 唔識落圍棋，Elmo 唔識落國際象棋

AlphaZero 用一個通用演算法喺幾小時內超越咗呢一切。呢個令好多 AI 研究者重新思考：

> 我哋應該投入更多精力喺「通用學習演算法」，定係「專家知識編碼」？

答案似乎越嚟越清楚：令機器自己學習，比教佢知識更有效。

---

## AlphaGo Zero 嘅落棋風格

### 超越人類嘅審美

圍棋界對 AlphaGo Zero 嘅著法有一個普遍評價：**更加優美**。

AlphaGo Lee 嘅著法有時顯得「怪異」——好似第 37 手嗰類落子，人類需要事後分析先能理解佢嘅妙處。但 AlphaGo Zero 嘅著法常常喺事後被評價為「一眼就知道係好棋」。

呢個可能係因為：

1. **更強嘅棋力**：Zero 能睇得更深，落子更加從容
2. **無人類偏見**：唔受傳統定式嘅束縛
3. **一致嘅目標**：只追求勝率，唔模仿人類

### 重新發現人類棋理

有趣嘅係，AlphaGo Zero 喺訓練過程入面「重新發現」咗人類數千年累積嘅圍棋知識：

- **定式**：Zero 自己發現咗好多常見定式，因為呢啲確實係雙方最優解
- **佈局原則**：角、邊、中央嘅重要性順序
- **棋形知識**：愚形同好形嘅區別

呢個驗證咗人類棋理嘅合理性——呢啲知識唔係偶然嘅，而係圍棋本質嘅反映。

### 超越人類嘅創新

但 Zero 都發現咗人類從未諗過嘅著法：

- **非常規開局**：喺傳統開局基礎上嘅變化
- **激進嘅棄子**：比人類更願意放棄局部換取全局優勢
- **反直覺嘅形狀**：表面上嘅「壞形」其實係最優解

呢啲創新正喺改變人類對圍棋嘅理解。好多職業棋手表示，研究 AlphaGo Zero 嘅棋譜令佢哋對圍棋有咗全新嘅認識。

---

## 技術細節總結

### 與原版 AlphaGo 嘅完整對比

| 方面 | AlphaGo（原版） | AlphaGo Zero |
|------|----------------|--------------|
| **訓練資料** | 人類棋譜 + 自我對弈 | 純自我對弈 |
| **學習方法** | 監督學習 + 強化學習 | 純強化學習 |
| **輸入特徵** | 48 個平面 | 17 個平面 |
| **網絡架構** | 分離嘅 Policy/Value | 雙頭 ResNet |
| **網絡深度** | 13 層 | 40 層（或更多） |
| **MCTS 評估** | 神經網絡 + Rollout | 純神經網絡 |
| **搜索次數** | 每步 ~100,000 | 每步 ~1,600 |
| **訓練 TPU** | 50+ | 4 |
| **推理 TPU** | 48 | 4（可擴展） |

### 核心算法

AlphaGo Zero 嘅訓練循環非常簡潔：

```
1. 自我對弈
   - 用當前網絡進行 MCTS
   - 按 MCTS 搜索機率揀選落子
   - 記錄每一步嘅 (局面, MCTS機率, 勝負結果)

2. 訓練網絡
   - 由經驗池入面取樣
   - Policy Head：最小化與 MCTS 機率嘅交叉熵
   - Value Head：最小化與實際勝負嘅均方誤差
   - 聯合優化兩個目標

3. 更新網絡
   - 用新網絡替換舊網絡（通過對弈驗證新網絡更強）
   - 返回步驟 1
```

呢個循環持續運行，網絡不斷變強。無人類數據、無人類知識，只有遊戲規則同勝負目標。

---

## 對 AI 研究嘅啟示

### 第一性原理學習

AlphaGo Zero 展示咗一種「第一性原理」嘅學習方法：

> 唔好話 AI 點樣做，只話佢目標係乜嘢，令佢自己發現方法。

呢個同傳統嘅專家系統方法形成鮮明對比。專家系統嘗試將人類知識編碼落 AI，而 AlphaGo Zero 令 AI 自己發現知識。

結果係：AI 發現嘅知識可能比人類知識更完整、更準確。

### 自我對弈嘅威力

AlphaGo Zero 證明咗自我對弈可以產生無限嘅訓練資料，而且呢啲資料嘅質素會隨住網絡嘅提升而提升。

呢個係一個「正向循環」：
- 更強嘅網絡 → 更好嘅自我對弈資料
- 更好嘅資料 → 更強嘅網絡

呢個循環可以持續運行，直到達到遊戲嘅理論上限（如果存在嘅話）。

### 簡化嘅重要性

AlphaGo Zero 嘅成功證明咗「簡化」嘅重要性：

- 簡化輸入（48 → 17）
- 簡化架構（雙網絡 → 單網絡）
- 簡化訓練（監督 + 強化 → 純強化）

每一次簡化都令系統更加強大。呢個話畀我哋知：複雜唔等於好，最簡單嘅解決方案往往係最好嘅。

---

## 動畫對應

本文涉及嘅核心概念與動畫編號：

| 編號 | 概念 | 物理/數學對應 |
|------|------|--------------|
| 🎬 E7 | 由零開始訓練 | 自組織現象 |
| 🎬 E5 | 自我對弈 | 不動點收斂 |
| 🎬 E12 | 棋力成長曲線 | S 型增長 |
| 🎬 D12 | 殘差網絡 | 梯度高速公路 |

---

## 延伸閱讀

- **下一篇**：[雙頭網絡與殘差網絡](../dual-head-resnet) — 詳解 AlphaGo Zero 嘅神經網絡架構
- **相關文章**：[自我對弈](../self-play) — 點解自我對弈能產生超人水平
- **技術深入**：[由零訓練嘅過程](../training-from-scratch) — Day 0-3 嘅詳細演進

---

## 參考資料

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
3. DeepMind. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
4. Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." *Nature*, 588, 604-609.
