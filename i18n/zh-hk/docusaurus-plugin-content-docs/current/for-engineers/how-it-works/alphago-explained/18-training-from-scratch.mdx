---
sidebar_position: 19
title: 從零訓練嘅過程
description: 見證 AlphaGo Zero 點樣喺三日內從隨機亂落到超越人類，重新發現並超越千年棋理
keywords: [AlphaGo Zero, 訓練過程, 自我對弈, 棋力成長, 圍棋 AI, 深度學習]
---

import { EloChart } from '@site/src/components/D3Charts';

# 從零訓練嘅過程

AlphaGo Zero 最令人驚嘆嘅唔單止係最終嘅棋力，而係佢嘅**成長過程**——從完全隨機嘅狀態開始，喺短短三日內經歷咗人類數千年先完成嘅圍棋知識積累，然後超越人類所有嘅理解。

本文將帶你一步步見證呢個驚人嘅蛻變過程。

---

## 訓練曲線

首先，我哋睇吓 AlphaGo Zero 嘅棋力成長曲線：

<EloChart mode="zero" width={700} height={400} />

呢條曲線展示咗 AlphaGo Zero 喺 72 小時內嘅棋力變化。注意幾個關鍵里程碑：

| 時間 | ELO 評分 | 相當於 |
|------|----------|--------|
| 0 小時 | 0 | 隨機亂落 |
| 3 小時 | ~1000 | 發現基本規則 |
| 12 小時 | ~3000 | 發現定式同棋形 |
| 36 小時 | ~4500 | 超越樊麾版 AlphaGo |
| 60 小時 | ~5200 | 超越李世乭版 AlphaGo |
| 72 小時 | ~5400 | 超越所有之前嘅版本 |

**三日，從零到超越人類頂峰。**

---

## Day 0：混沌嘅開始

### 完全隨機嘅初始狀態

訓練開始嗰陣，神經網絡嘅權重係隨機初始化嘅。呢個意味住：

- **Policy Head**：輸出接近均勻分佈，每個位置嘅落子機率約為 1/361
- **Value Head**：輸出接近 0，冇辦法區分好局面同壞局面

呢個時候嘅 AlphaGo Zero 落棋完全係亂落——比一個從未見過棋盤嘅人仲要差。

### 第一局自我對弈

想像吓第一局自我對弈係點樣嘅：

```
黑 1：隨機落喺某處（可能係天元，可能係角上，可能係一線）
白 2：隨機落喺另一處
黑 3：隨機...
...
第 200 手：棋盤上到處都係孤立嘅棋子，冇任何連接
最終：勝負由隨機因素決定
```

呢局棋嘅「質素」極低，但佢包含咗寶貴嘅資訊：**最終邊個贏咗**。

### 第一個訓練訊號

雖然雙方都喺度亂落，但勝負結果係確定嘅。神經網絡開始學習：

> 「喺呢個局面之下，最終黑方贏咗。雖然我唔知道點解，但呢個局面對黑方可能比較好。」

呢個係一個非常弱嘅訊號，但佢係真實嘅。經過數千局呢類「垃圾棋」之後，網絡開始發現一啲統計規律。

---

## Hour 1-3：發現遊戲規則

### 湧現嘅規則意識

經過數萬局自我對弈之後，AlphaGo Zero 開始「發現」圍棋嘅基本規則（雖然呢啲規則早就內建喺遊戲引擎入面）：

#### 1. 連接嘅重要性

```
觀察：當棋子相連嗰陣，比較唔容易被食
學習：開始優先喺已有棋子旁邊落子
```

呢個唔係被教導嘅，而係從勝負結果中學到嘅。散落嘅棋子容易被各個擊破，連成一片嘅棋子更容易存活。

#### 2. 氣嘅概念

```
觀察：當棋子嘅鄰接空點全部被佔據嗰陣，棋子會消失
學習：開始避免氣好少嘅位置，開始攻擊對手氣少嘅棋子
```

網絡學識咗追蹤氣數——雖然輸入入面冇明確嘅「氣數」特徵，但從歷史棋盤狀態可以推斷出嚟。

#### 3. 眼嘅雛形

```
觀察：某啲形狀特別難被食
學習：開始喺角落同邊上形成有空間嘅形狀
```

呢個係活棋概念嘅萌芽。網絡發現，有內部空間嘅棋子群更容易存活。

### 棋力評估

呢個時候嘅 AlphaGo Zero 大約係：
- **ELO**：~1000
- **相當於**：啱啱學識規則嘅初學者
- **特徵**：知道要連接棋子，知道要食對方嘅棋

---

## Hour 3-12：發現定式與棋形

### 角部嘅覺醒

經過更多訓練，網絡發現咗角部嘅重要性：

```
觀察：角部嘅棋子只需要 2 隻眼就可以活
     邊上需要 2 隻眼較難
     中央需要 2 隻眼最難
學習：開局時優先佔角
```

呢個就係人類棋理中「金角銀邊草肚皮」嘅發現過程。網絡冇被告知呢個原則，而係從數十萬局對弈中自己發現嘅。

### 定式嘅湧現

更加令人驚奇嘅係，網絡開始「發明」定式——雙方喺角部嘅標準下法：

#### 觀察到嘅現象

```
訓練初期：角部下法五花八門
訓練中期：某啲下法反覆出現
訓練後期：形成穩定嘅角部定式
```

呢啲定式同人類數百年累積嘅定式**高度相似**，驗證咗呢啲定式的確係雙方最優解嘅近似。

### 典型嘅湧現定式

以小目定式為例：

```
  A B C D E F G H J
9 . . . . . . . . .
8 . . . . . . . . .
7 . . . . . . . . .
6 . . . ● . . . . .   ● = 黑
5 . . . . . . . . .   ○ = 白
4 . . . ○ . ● . . .
3 . . . . . . . . .
2 . . . . . . . . .
1 . . . . . . . . .
```

黑方佔據小目，白方掛角，黑方夾擊——呢個序列喺訓練過程中自然湧現。

### 棋形知識

除咗定式，網絡都學識咗好形同壞形嘅區別：

| 形狀 | 人類評價 | Zero 嘅學習 |
|------|----------|-------------|
| 空三角 | 愚形 | 逐漸避免 |
| 虎口 | 好形 | 逐漸偏好 |
| 雙飛燕 | 經典攻擊形 | 自然發現 |
| 鎮神頭 | 強力攻擊 | 自然發現 |

### 棋力評估

呢個時候嘅 AlphaGo Zero：
- **ELO**：~3000
- **相當於**：業餘高段
- **特徵**：有基本嘅定式知識，識得基本棋形

---

## Hour 12-36：棋理嘅成熟

### 全局觀嘅形成

進入第二日，網絡開始展現出**全局觀**：

#### 勢力與實地

```
觀察：圍住空間可以攞到目數
     但勢力都有價值——可以攻擊對方
學習：喺取地同取勢之間搵平衡
```

呢個係圍棋入面最深奧嘅概念之一。網絡學識咗評估「虛」同「實」嘅價值。

#### 厚薄判斷

```
觀察：「厚」嘅棋可以支援遠處嘅戰鬥
     「薄」嘅棋需要補強，否則會被攻擊
學習：主動建立厚勢，攻擊對方嘅薄弱
```

### 中盤戰術

網絡嘅中盤戰鬥能力大幅提升：

| 技術 | 描述 |
|------|------|
| 攻擊弱棋 | 識別對方嘅孤棋，發動攻勢 |
| 利用厚味 | 用厚勢支援攻擊，獲得利益 |
| 轉換 | 放棄局部損失，換取全局優勢 |
| 打入 | 侵消對方嘅模樣 |

### 官子技巧

收官階段嘅精確計算都喺度提升：

```
觀察：官子階段每一手嘅價值可以精確計算
學習：按照價值大小依序收官
```

網絡學識咗「雙方先手」「單方先手」「後手」等官子概念。

### 棋力評估

呢個時候嘅 AlphaGo Zero：
- **ELO**：~4500
- **相當於**：職業棋手水平
- **特徵**：有完整嘅圍棋理解，能夠落出高質素嘅對局

---

## Hour 36-72：超越人類

### 突破職業水平

喺 36 小時左右，AlphaGo Zero 嘅棋力達到咗職業棋手水平。但訓練並冇停止——佢繼續自我對弈，繼續提升。

跟住發生嘅事更加有趣：**佢開始發現人類從未諗過嘅下法**。

### 顛覆性嘅開局

傳統圍棋開局有好多「定見」：

| 傳統觀點 | AlphaGo Zero 嘅發現 |
|----------|---------------------|
| 開局先佔角 | 某啲情況下先佔邊更好 |
| 小目最穩健 | 三三直接佔角可行 |
| 定式要記熟 | 可以主動偏離定式 |
| 點三三太早貪 | 某啲局面下點三三正確 |

呢啲「發現」喺 AlphaGo 之後被人類職業棋手廣泛研究，好多已經被納入現代棋理。

### 反直覺嘅棋形

AlphaGo Zero 有時會落出人類認為「唔好睇」嘅形狀：

```
人類：「呢個係愚形，唔可能係好棋」
Zero：（落咗嗰步棋）
分析後：「原來咁樣更加有效率」
```

呢個揭示咗人類棋理嘅局限：有啲「壞形」其實係特定局面下嘅最優解。

### 激進嘅棄子

Zero 比人類更願意棄子換取其他利益：

```
局部蝕 3 目
全局獲得主動權
最終勝率提升
```

人類棋手往往過度在意局部得失，而 Zero 始終盯住最終勝率。

### 棋力評估

72 小時後嘅 AlphaGo Zero：
- **ELO**：~5400
- **相當於**：超越所有人類棋手
- **特徵**：發現人類未知嘅下法，創造新嘅棋理

---

## 重新發現人類棋理

### 數千年 vs. 三日

人類圍棋發展咗數千年：
- 公元前 2000 年左右起源於中國
- 唐朝傳入日本，發展出精密嘅棋理
- 20 世紀出現職業體系，棋理進一步深化
- 2016 年，人類認為已經相當理解圍棋

AlphaGo Zero 用三日走完咗呢段路程。更加驚人嘅係，佢發現嘅棋理同人類嘅**高度一致**。

### 驗證與超越

| 人類知識 | Zero 嘅態度 |
|----------|-------------|
| 金角銀邊草肚皮 | 確認（角落的確重要） |
| 基本定式 | 大部分確認，少數改進 |
| 好形壞形 | 大部分確認，特例存在 |
| 棄子轉換 | 比人類更激進 |
| 厚薄判斷 | 大致一致，細節唔同 |

呢個說明人類數千年累積嘅棋理**大方向係正確嘅**。但都有一啲領域，人類嘅理解需要修正。

### 對人類學習嘅啟示

AlphaGo Zero 嘅訓練過程畀人類學習帶來啟示：

1. **從基礎開始**：Zero 先學識規則，再學識棋形，最後發展全局觀
2. **大量練習**：490 萬局自我對弈相當於數十萬年嘅人類對局量
3. **專注勝負**：唔追求「靚嘅棋」，只追求贏
4. **唔受傳統束縛**：夠膽嘗試「唔可能」嘅下法

---

## 訓練過程嘅技術細節

### 自我對弈嘅機制

每一局自我對弈嘅流程：

```
初始化：空棋盤
↓
每一步：
  1. 用神經網絡評估當前局面
  2. 執行 MCTS 搜索（1600 次模擬）
  3. 根據搜索結果揀落子
  4. 記錄 (局面, MCTS機率, -)
↓
遊戲結束：
  1. 判定勝負 z ∈ {-1, +1}
  2. 為所有記錄補上勝負 (局面, MCTS機率, z)
  3. 將資料加入訓練池
```

### 訓練嘅節奏

AlphaGo Zero 嘅訓練係**持續進行**嘅：

```
Self-play Workers:       不斷產生自我對弈資料
Training Workers:        不斷從資料池取樣訓練
Network Updates:         定期更新自我對弈用嘅網絡
```

呢三個過程同時進行，形成一個持續改進嘅循環。

### 資料池管理

訓練資料池嘅管理：

| 參數 | 值 |
|------|-----|
| 池大小 | 最近 50 萬局 |
| 每局樣本 | ~200 步 |
| 總樣本數 | ~1 億 |
| 取樣方式 | 均勻隨機 |

舊嘅資料會被新資料替換，確保訓練資料反映當前網絡嘅水平。

### 網絡更新策略

唔係每訓練一步就更新自我對弈嘅網絡。而係：

1. 訓練一段時間之後，產生候選網絡
2. 用候選網絡對戰當前網絡（400 局）
3. 如果候選網絡勝率 > 55%，更新
4. 否則繼續訓練

呢個確保咗自我對弈始終使用**足夠強**嘅網絡。

---

## 學習速度嘅分析

### 點解咁快？

AlphaGo Zero 學習速度驚人嘅原因：

#### 1. 計算資源

- 4 個 TPU，每秒數萬次推理
- 每日產生數十萬局自我對弈
- 相當於人類數千年嘅對局量

#### 2. 完美嘅對手

自我對弈意味住：
- 對手水平始終同自己相當
- 唔會太弱（學唔到嘢）都唔會太強（冇辦法獲勝）
- 呢個係理想嘅學習條件

#### 3. 直接嘅目標

只有一個目標：贏。冇：
- 老師嘅偏好
- 風格嘅追求
- 美學嘅考量

#### 4. 高效嘅表示學習

殘差網絡能夠學習非常抽象嘅棋盤特徵，比手工設計嘅特徵更有效。

### 同人類嘅對比

| 方面 | 人類 | AlphaGo Zero |
|------|------|--------------|
| 學習速度 | 每日 ~10 局 | 每日 ~100,000 局 |
| 記憶保留 | 有遺忘 | 完美保留 |
| 精力限制 | 需要休息 | 24/7 運行 |
| 創新能力 | 受傳統影響 | 冇預設限制 |

---

## 訓練過程中嘅有趣現象

### 階段性停滯

訓練曲線唔係完全平滑嘅，有時會出現**停滯期**：

```
ELO: 2000 -----> 2000 -----> 2500 ---->
          (停滯)       (突破)
```

呢個可能係因為網絡喺度學習某個新概念，需要時間「消化」。

### 策略嘅湧現同消失

某啲策略會喺訓練過程中湧現，然後又消失：

```
階段 1：發現某個攻擊手段
階段 2：對手學識防守
階段 3：該手段使用頻率降低
階段 4：發現新嘅攻擊手段
```

呢個係軍備競賽嘅縮影。

### 「重新發明輪子」

訓練過程中，Zero 會「重新發明」人類已知嘅概念：

- **征子**：發現連續叫食可以食到棋子
- **倒脫靴**：發現可以先送子再反殺
- **打劫**：發現迴避規則嘅利用方式

呢啲發現嘅順序同人類學棋嘅順序類似。

---

## 動畫對應

本文涉及嘅核心概念與動畫編號：

| 編號 | 概念 | 物理/數學對應 |
|------|------|--------------|
| E12 | 棋力成長曲線 | S 型增長（邏輯斯蒂） |
| E7 | 從零開始 | 自組織現象 |
| E5 | 自我對弈 | 不動點收斂 |
| F8 | 湧現能力 | 相變 |

---

## 延伸閱讀

- **上一篇**：[雙頭網絡與殘差網絡](../dual-head-resnet) — 支撐呢一切嘅神經網絡架構
- **下一篇**：[分散式系統與 TPU](../distributed-systems) — 令呢一切成為可能嘅硬件
- **相關文章**：[自我對弈](../self-play) — 點解自我對弈咁有效

---

## 參考資料

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
3. DeepMind. (2017). "AlphaGo Zero: Learning from scratch." *YouTube*.
4. Wang, F., et al. (2019). "A Survey on the Evolution of AlphaGo." *arXiv:1907.11180*.
