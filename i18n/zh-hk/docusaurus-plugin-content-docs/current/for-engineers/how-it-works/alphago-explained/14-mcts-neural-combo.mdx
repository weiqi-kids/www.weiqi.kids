---
sidebar_position: 15
title: MCTS èˆ‡ç¥ç¶“ç¶²çµ¡å˜…çµåˆ
description: æ·±å…¥ç†è§£ AlphaGo é»æ¨£å°‡è’™åœ°å¡ç¾…æ¨¹æœç´¢èˆ‡æ·±åº¦ç¥ç¶“ç¶²çµ¡å®Œç¾çµåˆ
---

import { MCTSTree } from '@site/src/components/D3Charts';

# MCTS èˆ‡ç¥ç¶“ç¶²çµ¡å˜…çµåˆ

å–ºå‰é¢å˜…æ–‡ç« å…¥é¢ï¼Œæˆ‘å“‹åˆ†åˆ¥ä»‹ç´¹å’—ç¥ç¶“ç¶²çµ¡ï¼ˆPolicy Network åŒ Value Networkï¼‰ä»¥åŠå¼·åŒ–å­¸ç¿’å˜…æ¦‚å¿µã€‚è€Œå®¶ï¼Œç­‰æˆ‘å“‹æ¢è¨ AlphaGo å˜…æ ¸å¿ƒå‰µæ–°â€”â€”**é»æ¨£å°‡è’™åœ°å¡ç¾…æ¨¹æœç´¢ï¼ˆMCTSï¼‰èˆ‡ç¥ç¶“ç¶²çµ¡å®Œç¾çµåˆ**ã€‚

å‘¢å€‹çµåˆä¿‚ AlphaGo æˆåŠŸå˜…é—œéµï¼šç¥ç¶“ç¶²çµ¡æä¾›ã€Œç›´è¦ºã€ï¼ŒMCTS æä¾›ã€Œæ¨ç†ã€ï¼Œå…©è€…ç›¸è¼”ç›¸æˆã€‚

---

## å‚³çµ± MCTS å›é¡§

### ä¹œå˜¢ä¿‚ MCTSï¼Ÿ

**è’™åœ°å¡ç¾…æ¨¹æœç´¢ï¼ˆMonte Carlo Tree Search, MCTSï¼‰** ä¿‚ä¸€ç¨®åŸºæ–¼éš¨æ©Ÿæ¡æ¨£å˜…æœç´¢æ¼”ç®—æ³•ï¼Œç‰¹åˆ¥é©åˆç”¨å–ºéŠæˆ² AIã€‚

MCTS å˜…æ ¸å¿ƒè«—æ³•ä¿‚ï¼š**èˆ‡å…¶çª®èˆ‰æ‰€æœ‰å¯èƒ½å˜…è‘—æ³•ï¼Œä¸å¦‚éš¨æ©Ÿæ¨¡æ“¬å¤§é‡å°å±€ï¼Œç”¨çµ±è¨ˆåšŸä¼°è¨ˆæ¯å€‹è‘—æ³•å˜…å¥½å£**ã€‚

### å››å€‹éšæ®µ

å‚³çµ±å˜… MCTS åŒ…å«å››å€‹éšæ®µï¼Œä¸æ–·é‡è¤‡ï¼š

```mermaid
flowchart LR
    subgraph MCTS["MCTS å¾ªç’°ï¼ˆé‡è¤‡ N æ¬¡ï¼‰"]
        A["Selection<br/>æ€é¸"] --> B["Expansion<br/>å±•é–‹"]
        B --> C["Simulation<br/>æ¨¡æ“¬"]
        C --> D["Backprop<br/>åå‘å‚³æ’­"]
        D -.-> A
    end
```

ç­‰æˆ‘å“‹è©³ç´°äº†è§£æ¯å€‹éšæ®µï¼š

### 1. Selectionï¼ˆæ€é¸ï¼‰

ç”±æ ¹ç¯€é»é–‹å§‹ï¼Œæ²¿ä½æ£µæ¨¹å‘ä¸‹ï¼Œæ€ã€Œæœ€æœ‰å¸Œæœ›ã€å˜…å­ç¯€é»ï¼Œç›´åˆ°å»åˆ°è‘‰ç¯€é»ã€‚

æ€é¸å˜…æ¨™æº–ä¿‚ **UCB1ï¼ˆUpper Confidence Boundï¼‰** å…¬å¼ï¼š

$$\text{UCB1}(s, a) = \bar{X}_{s,a} + c \sqrt{\frac{\ln N_s}{N_{s,a}}}$$

å…¶ä¸­ï¼š
- $\bar{X}_{s,a}$ï¼šç”±ç¯€é» $(s, a)$ å‡ºç™¼å˜…å¹³å‡å›å ±ï¼ˆ**åˆ©ç”¨é …**ï¼‰
- $\sqrt{\frac{\ln N_s}{N_{s,a}}}$ï¼šæ¢ç´¢åŠ æˆï¼ˆ**æ¢ç´¢é …**ï¼‰
- $N_s$ï¼šçˆ¶ç¯€é»å˜…è¨ªå•æ¬¡æ•¸
- $N_{s,a}$ï¼šå­ç¯€é»å˜…è¨ªå•æ¬¡æ•¸
- $c$ï¼šå¹³è¡¡æ¢ç´¢åŒåˆ©ç”¨å˜…å¸¸æ•¸

å‘¢æ¢å…¬å¼å˜…æ™ºæ…§åœ¨æ–¼ï¼š
- è¨ªå•æ¬¡æ•¸å°‘å˜…ç¯€é»æœƒå¾—åˆ°æ›´é«˜å˜…æ¢ç´¢åŠ æˆ
- éš¨ä½è¨ªå•æ¬¡æ•¸å¢åŠ ï¼Œæ€é¸æœƒè¶ŠåšŸè¶Šåå‘å¯¦éš›åƒ¹å€¼é«˜å˜…ç¯€é»

### 2. Expansionï¼ˆå±•é–‹ï¼‰

å»åˆ°è‘‰ç¯€é»ä¹‹å¾Œï¼Œæ€ä¸€å€‹æœªè¢«æ¢ç´¢å˜…å‹•ä½œï¼Œå‰µå»ºæ–°å˜…å­ç¯€é»ã€‚

```
å±•é–‹å‰ï¼š                    å±•é–‹å¾Œï¼š
     â—‹ (æ ¹)                      â—‹ (æ ¹)
    /â”‚\                         /â”‚\
   â—‹ â—‹ â—‹                       â—‹ â—‹ â—‹
  /â”‚              â†’           /â”‚
 â—‹ â—‹                         â—‹ â—‹
   â†‘                            \
   è‘‰ç¯€é»                         â— (æ–°ç¯€é»)
```

### 3. Simulationï¼ˆæ¨¡æ“¬/Rolloutï¼‰

ç”±æ–°ç¯€é»é–‹å§‹ï¼Œç”¨æŸç¨®ç­–ç•¥ï¼ˆé€šå¸¸ä¿‚éš¨æ©Ÿæˆ–è€…ç°¡å–®å•Ÿç™¼å¼ï¼‰å¿«é€Ÿå®Œæˆå°å±€ï¼Œæ”åˆ°çµæœã€‚

å‘¢å€‹å°±ä¿‚ã€Œè’™åœ°å¡ç¾…ã€åç¨±å˜…ä¾†æºâ€”â€”**ç”¨éš¨æ©Ÿæ¨¡æ“¬åšŸä¼°è¨ˆçµæœ**ã€‚

å‚³çµ± MCTS å˜… rollout ç­–ç•¥å¯èƒ½ä¿‚ï¼š
- **ç´”éš¨æ©Ÿ**ï¼šå‡å‹»éš¨æ©Ÿæ€é¸åˆæ³•è‘—æ³•
- **è¼•é‡ç´šå•Ÿç™¼å¼**ï¼šç”¨ç°¡å–®è¦å‰‡éæ¿¾æ˜é¡¯å˜…å£æ£‹

### 4. Backpropagationï¼ˆåå‘å‚³æ’­ï¼‰

å°‡æ¨¡æ“¬å˜…çµæœï¼ˆå‹/è² ï¼‰æ²¿ä½è·¯å¾‘å›å‚³ï¼Œæ›´æ–°æ¯å€‹ç¯€é»å˜…çµ±è¨ˆè³‡è¨Šï¼š

```
æ›´æ–°å…§å®¹ï¼š
- è¨ªå•æ¬¡æ•¸ï¼šN(s, a) â† N(s, a) + 1
- ç´¯ç©åƒ¹å€¼ï¼šW(s, a) â† W(s, a) + z
- å¹³å‡åƒ¹å€¼ï¼šQ(s, a) = W(s, a) / N(s, a)
```

å…¶ä¸­ $z$ ä¿‚æ¨¡æ“¬çµæœï¼ˆ+1 æˆ– -1ï¼‰ã€‚

### å‚³çµ± MCTS å˜…é™åˆ¶

å‚³çµ± MCTS å–ºåœæ£‹ä¸Šå˜…è¡¨ç¾æœ‰é™ï¼Œä¸»è¦å•é¡Œä¿‚ï¼š

1. **Rollout è³ªç´ å·®**ï¼šéš¨æ©Ÿæ¨¡æ“¬ç¶“å¸¸ç”¢ç”Ÿå””åˆç†å˜…å°å±€
2. **éœ€è¦å¤§é‡æ¨¡æ“¬**ï¼šæ¯æ­¥æ£‹å¯èƒ½éœ€è¦æ•¸è¬æ¬¡æ¨¡æ“¬
3. **è©•ä¼°å””æº–ç¢º**ï¼šæ·¨ä¿‚é å‹è² çµ±è¨ˆï¼Œè³‡è¨Šåˆ©ç”¨æ•ˆç‡ä½
4. **ç„¡æ³•åˆ©ç”¨æ¨¡å¼**ï¼šæ¯æ¬¡éƒ½é‡æ–°æœç´¢ï¼Œå””ç´¯ç©ç¶“é©—

å‘¢å•²å•é¡Œå–º AlphaGo å…¥é¢è¢«ç¥ç¶“ç¶²çµ¡å„ªé›…å™‰è§£æ±ºå’—ã€‚

---

## ç¥ç¶“ç¶²çµ¡é»æ¨£æ”¹é€² MCTS

### æ•´é«”æ¶æ§‹

AlphaGo å°‡å…©å€‹ç¥ç¶“ç¶²çµ¡æ•´åˆè½ MCTSï¼š

```mermaid
flowchart TB
    subgraph Architecture["AlphaGo æœç´¢æ¶æ§‹"]
        subgraph MCTS["MCTS"]
            S["Selection"] --> P["PUCT å…¬å¼"]
            E["Expansion"] --> PN["Policy Net"]
            EV["Evaluation"] --> VN["Value Net<br/>+ Rollout"]
            B["Backup"] --> U["æ›´æ–°çµ±è¨ˆ"]
        end
        subgraph NN["Neural Networks"]
            Policy["Policy Network<br/>Ï€(a|s)<br/>è¼¸å‡ºï¼šå‹•ä½œæ©Ÿç‡"]
            Value["Value Network<br/>V(s)<br/>è¼¸å‡ºï¼šå‹ç‡ä¼°è¨ˆ"]
        end
        PN -.-> Policy
        VN -.-> Value
        P -.-> Policy
    end
```

### Policy Network å˜…è§’è‰²

**Policy Network å–º Expansion éšæ®µç™¼æ®ä½œç”¨**ã€‚

å‚³çµ± MCTS å–ºå±•é–‹å—°é™£ï¼Œæ‰€æœ‰æœªæ¢ç´¢å˜…å‹•ä½œè¢«è¦–ç‚ºåŒç­‰é‡è¦ã€‚ä½† Policy Network æä¾›å’—**å…ˆé©—æ©Ÿç‡ï¼ˆprior probabilityï¼‰**ï¼š

$$P(s, a) = \pi_\theta(a|s)$$

å‘¢å€‹ä»¤ MCTS å„ªå…ˆæ¢ç´¢å—°å•²ã€Œç‡è½æ›´å¥½ã€å˜…è‘—æ³•ï¼Œå¤§å¹…æé«˜æœç´¢æ•ˆç‡ã€‚

ä¾‹å¦‚ï¼Œå–ºä¸€å€‹å±€é¢å…¥é¢ï¼š
- ã€Œå¤©å…ƒã€å¯èƒ½åªæœ‰ 0.01% å˜…æ©Ÿç‡
- ã€Œè§’éƒ¨å®šå¼ã€å¯èƒ½æœ‰ 15% å˜…æ©Ÿç‡
- ã€Œå¤§å ´ã€å¯èƒ½æœ‰ 10% å˜…æ©Ÿç‡

MCTS æœƒå„ªå…ˆæ¢ç´¢é«˜æ©Ÿç‡å˜…è‘—æ³•ï¼Œè€Œå””ä¿‚å˜¥æ™‚é–“å–ºæ˜é¡¯å””å¥½å˜…é¸æ“‡ä¸Šé¢ã€‚

### Value Network å˜…è§’è‰²

**Value Network å–º Evaluation éšæ®µç™¼æ®ä½œç”¨**ã€‚

å‚³çµ± MCTS éœ€è¦å®Œæˆæˆå±€æ¨¡æ“¬å…ˆå¾—åˆ°è©•ä¼°ã€‚ä½† Value Network å¯ä»¥ç›´æ¥è©•ä¼°ä»»ä½•å±€é¢å˜…å‹ç‡ï¼š

$$v(s) = V_\phi(s)$$

å‘¢å€‹å°±å¥½ä¼¼è«‹ä¸€ä½å¤§å¸«è©•ä¼°å±€é¢ï¼Œè€Œå””ä¿‚ç•€å…©å€‹åˆå­¸è€…è½å®Œæˆå±€å…ˆç‡çµæœã€‚

AlphaGo åŸç‰ˆæ··åˆä½¿ç”¨ Value Network åŒ Rolloutï¼š

$$V(s_L) = (1 - \lambda) \cdot v_\theta(s_L) + \lambda \cdot z_L$$

å…¶ä¸­ï¼š
- $v_\theta(s_L)$ï¼šValue Network å˜…è©•ä¼°
- $z_L$ï¼šRollout å˜…çµæœ
- $\lambda$ï¼šæ··åˆä¿‚æ•¸ï¼ˆAlphaGo ä½¿ç”¨ $\lambda = 0.5$ï¼‰

### æœç´¢æ¨¹è¦–è¦ºåŒ–

ç­‰æˆ‘å“‹è¦–è¦ºåŒ–ä¸€æ£µ MCTS æœç´¢æ¨¹ï¼š

<MCTSTree width={700} height={450} showPUCT={true} interactive={true} />

å–ºå‘¢å€‹è¦–è¦ºåŒ–å…¥é¢ï¼Œä½ å¯ä»¥ç‡åˆ°ï¼š
- ç¯€é»å¤§å°åæ˜ è¨ªå•æ¬¡æ•¸
- è—è‰²è·¯å¾‘ä¿‚ MCTS æ€é¸å˜…æœ€ä½³è·¯å¾‘
- æ¯å€‹ç¯€é»é¡¯ç¤ºè¨ªå•æ¬¡æ•¸ N åŒå¹³å‡åƒ¹å€¼ Q

---

## æœç´¢éç¨‹è©³è§£

### å®Œæ•´æµç¨‹

ç­‰æˆ‘å“‹è·Ÿè¹¤ä¸€æ¬¡å®Œæ•´å˜… MCTS æ¨¡æ“¬ï¼š

```
æ¼”ç®—æ³•ï¼šAlphaGo MCTS å–®æ¬¡æ¨¡æ“¬

è¼¸å…¥ï¼šæ ¹ç¯€é» s_rootï¼ŒPolicy Network Ï€ï¼ŒValue Network V

1. Selectionï¼ˆæ€é¸ï¼‰
   s = s_root
   è·¯å¾‘ = []

   while s å””ä¿‚è‘‰ç¯€é»:
       # ä½¿ç”¨ PUCT å…¬å¼æ€é¸å‹•ä½œ
       a* = argmax_a [Q(s,a) + U(s,a)]

       å…¶ä¸­ U(s,a) = c_puct Â· P(s,a) Â· âˆšN(s) / (1 + N(s,a))

       è·¯å¾‘.append((s, a*))
       s = åŸ·è¡Œå‹•ä½œ a* ä¹‹å¾Œå˜…ç‹€æ…‹

2. Expansionï¼ˆå±•é–‹ï¼‰
   å¦‚æœ s å””ä¿‚çµ‚å±€ç‹€æ…‹:
       # ç”¨ Policy Network è¨ˆç®—å…ˆé©—æ©Ÿç‡
       P(s, Â·) = Ï€(Â·|s)

       # ç‚ºæ‰€æœ‰åˆæ³•å‹•ä½œå‰µå»ºå­ç¯€é»
       for a in åˆæ³•å‹•ä½œ:
           å‰µå»ºå­ç¯€é» (s, a)
           è¨­ç½® P(s,a), N(s,a)=0, W(s,a)=0

3. Evaluationï¼ˆè©•ä¼°ï¼‰
   # æ··åˆ Value Network åŒ Rollout
   v = V(s)                          # Value Network è©•ä¼°
   z = rollout(s)                    # Rollout çµæœ
   value = (1-Î»)Â·v + Î»Â·z             # æ··åˆ

   # AlphaGo Zero ç°¡åŒ–ç‚ºåªç”¨ Value Network
   # value = V(s)

4. Backpropagationï¼ˆåå‘å‚³æ’­ï¼‰
   for (s', a') in åå‘(è·¯å¾‘):
       N(s', a') += 1
       W(s', a') += value
       Q(s', a') = W(s', a') / N(s', a')
       value = -value                 # åˆ‡æ›è¦–è§’
```

### æ€é¸éšæ®µè©³è§£

æ€é¸éšæ®µä½¿ç”¨ **PUCT å…¬å¼**ï¼ˆå°‡æœƒå–ºä¸‹ä¸€ç¯‡è©³ç´°è¨è«–ï¼‰ï¼š

$$a^* = \arg\max_a \left[ Q(s,a) + c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,a)} \right]$$

å‘¢æ¢å…¬å¼å¹³è¡¡å’—ï¼š
- **Q(s,a)**ï¼šå·²çŸ¥å˜…å¹³å‡åƒ¹å€¼ï¼ˆåˆ©ç”¨ï¼‰
- **U(s,a)**ï¼šæ¢ç´¢åŠ æˆï¼Œçµåˆå…ˆé©—æ©Ÿç‡åŒè¨ªå•æ¬¡æ•¸ï¼ˆæ¢ç´¢ï¼‰

### å±•é–‹éšæ®µè©³è§£

ç•¶å»åˆ°è‘‰ç¯€é»å—°é™£ï¼Œä½¿ç”¨ Policy Network åˆå§‹åŒ–æ–°ç¯€é»ï¼š

```python
def expand(state, policy_network):
    # æ”æ‰€æœ‰åˆæ³•å‹•ä½œå˜…æ©Ÿç‡
    action_probs = policy_network(state)

    # éæ¿¾éæ³•å‹•ä½œä¸¦é‡æ–°æ­¸ä¸€åŒ–
    legal_actions = get_legal_actions(state)
    legal_probs = action_probs[legal_actions]
    legal_probs = legal_probs / legal_probs.sum()

    # å‰µå»ºå­ç¯€é»
    for action, prob in zip(legal_actions, legal_probs):
        child = create_node(
            state=apply_action(state, action),
            prior=prob,
            visit_count=0,
            value_sum=0
        )
        add_child(current_node, action, child)
```

### è©•ä¼°éšæ®µè©³è§£

AlphaGo åŸç‰ˆæ··åˆä½¿ç”¨å…©ç¨®è©•ä¼°ï¼š

**Value Network è©•ä¼°**ï¼š
- ç›´æ¥è¼¸å…¥å±€é¢ï¼Œè¼¸å‡ºå‹ç‡
- è¨ˆç®—å¿«é€Ÿï¼ˆä¸€æ¬¡ç¥ç¶“ç¶²çµ¡æ¨ç†ï¼‰
- æä¾›å…¨å±€è¦–è§’å˜…è©•ä¼°

**Rollout è©•ä¼°**ï¼š
- ç”¨å¿«é€Ÿç­–ç•¥ï¼ˆFast Rollout Policyï¼‰å®Œæˆå°å±€
- è¨ˆç®—è¼ƒæ…¢ä½†æä¾›å®Œæ•´å˜…å°å±€çµæœ
- å¯ä»¥ç™¼ç¾ä¸€å•²ç¥ç¶“ç¶²çµ¡å¯èƒ½å¿½ç•¥å˜…æˆ°è¡“

```python
def evaluate(state, value_network, rollout_policy, lambda_mix=0.5):
    # Value Network è©•ä¼°
    v = value_network(state)

    # Rollout è©•ä¼°
    current = state
    while not is_terminal(current):
        action = rollout_policy(current)
        current = apply_action(current, action)
    z = get_result(current)

    # æ··åˆ
    return (1 - lambda_mix) * v + lambda_mix * z
```

AlphaGo Zero ç§»é™¤å’— Rolloutï¼Œåªä½¿ç”¨ Value Networkã€‚å‘¢å€‹ç°¡åŒ–å’—ç³»çµ±ä¸¦æé«˜å’—æ•ˆç‡ã€‚

### åå‘å‚³æ’­è©³è§£

å°‡è©•ä¼°çµæœæ²¿è·¯å¾‘å›å‚³ï¼Œæ›´æ–°çµ±è¨ˆï¼š

```python
def backpropagate(path, value):
    for state, action in reversed(path):
        # æ›´æ–°è¨ªå•æ¬¡æ•¸
        state.visit_count[action] += 1
        # æ›´æ–°åƒ¹å€¼ç¸½å’Œ
        state.value_sum[action] += value
        # æ›´æ–°å¹³å‡åƒ¹å€¼
        state.Q[action] = state.value_sum[action] / state.visit_count[action]
        # åˆ‡æ›è¦–è§’ï¼ˆå°æ‰‹å˜…å¥½è™•ä¿‚æˆ‘å˜…å£è™•ï¼‰
        value = -value
```

ç•™æ„ `value = -value` å‘¢ä¸€æ­¥ï¼šåœæ£‹ä¿‚é›¶å’ŒéŠæˆ²ï¼Œä¸€æ–¹å˜…å‹åˆ©å°±ä¿‚å¦ä¸€æ–¹å˜…å¤±æ•—ã€‚

---

## è¨ˆç®—è³‡æºåˆ†é…

### æœç´¢æ¬¡æ•¸

AlphaGo å–ºæ¯æ­¥æ£‹ä¸Šé¢åŸ·è¡Œå¤§é‡å˜… MCTS æ¨¡æ“¬ï¼š

| ç‰ˆæœ¬ | æ¯æ­¥æ¨¡æ“¬æ¬¡æ•¸ | æ€è€ƒæ™‚é–“ |
|------|-------------|---------|
| AlphaGo Fan | ~100,000 | åˆ†é˜ç´š |
| AlphaGo Lee | ~100,000 | åˆ†é˜ç´š |
| AlphaGo Zero (è¨“ç·´) | 1,600 | ç§’ç´š |
| AlphaGo Zero (æ¯”è³½) | ~1,600 | ç§’ç´š |

AlphaGo Zero ç”¨æ›´å°‘å˜…æ¨¡æ“¬é”åˆ°æ›´å¼·å˜…æ£‹åŠ›ï¼Œå‘¢å€‹ä¿‚ç¥ç¶“ç¶²çµ¡è³ªç´ æå‡å˜…çµæœã€‚

### æ™‚é–“åˆ†é…ç­–ç•¥

å””åŒå±€é¢å¯èƒ½éœ€è¦å””åŒå˜…æ€è€ƒæ™‚é–“ï¼š

```python
def allocate_time(game_state, remaining_time):
    # åŸºæœ¬åˆ†é…
    num_moves_remaining = estimate_remaining_moves(game_state)
    base_time = remaining_time / num_moves_remaining

    # èª¿æ•´å› ç´ 
    complexity = estimate_complexity(game_state)
    importance = estimate_importance(game_state)

    # è¤‡é›œæˆ–é‡è¦å˜…å±€é¢ç•€æ›´å¤šæ™‚é–“
    allocated_time = base_time * complexity * importance

    # ç¢ºä¿å””è¶…æ™‚
    return min(allocated_time, remaining_time * 0.3)
```

å–ºå¯¦éš›æ¯”è³½å…¥é¢ï¼ŒAlphaGo æœƒå–ºé—œéµå±€é¢ï¼ˆå¥½ä¼¼æ¥è¿‘å‹è² åˆ†ç•Œå˜…æ™‚åˆ»ï¼‰æŠ•å…¥æ›´å¤šæ€è€ƒæ™‚é–“ã€‚

### ä¸¦è¡Œæœç´¢

MCTS å¤©ç”Ÿé©åˆä¸¦è¡ŒåŒ–ï¼š

**è™›æ“¬æå¤±ï¼ˆVirtual Lossï¼‰** æŠ€è¡“ï¼š

```
ç•¶ä¸€å€‹ç·šç¨‹æ­£å–ºæ¢ç´¢è·¯å¾‘ P å—°é™£ï¼š
1. æš«æ™‚å‡è£å‘¢æ¢è·¯å¾‘å·²ç¶“è¼¸å’—ï¼ˆvirtual lossï¼‰
2. å…¶ä»–ç·šç¨‹æœƒå‚¾å‘æ¢ç´¢å…¶ä»–è·¯å¾‘
3. ç•¶çµæœè¿”åšŸå—°é™£ï¼Œæ›´æ–°çœŸå¯¦çµ±è¨ˆä¸¦ç§»é™¤è™›æ“¬æå¤±
```

å‘¢å€‹ç¢ºä¿å’—å¤šå€‹ç·šç¨‹å””æœƒé‡è¤‡æ¢ç´¢ç›¸åŒå˜…è·¯å¾‘ã€‚

```python
def parallel_mcts_simulation(root, num_threads=8):
    virtual_losses = {}

    def simulate(thread_id):
        # æ€é¸éšæ®µï¼ˆå¸¶è™›æ“¬æå¤±ï¼‰
        path = []
        node = root
        while not node.is_leaf():
            action = select_with_virtual_loss(node, virtual_losses)
            add_virtual_loss(node, action, virtual_losses)
            path.append((node, action))
            node = node.children[action]

        # å±•é–‹åŒè©•ä¼°
        value = expand_and_evaluate(node)

        # åå‘å‚³æ’­ä¸¦ç§»é™¤è™›æ“¬æå¤±
        backpropagate(path, value)
        remove_virtual_losses(path, virtual_losses)

    # ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹æ¨¡æ“¬
    threads = [Thread(target=simulate, args=(i,)) for i in range(num_threads)]
    for t in threads:
        t.start()
    for t in threads:
        t.join()
```

### GPU æ‰¹æ¬¡è™•ç†

ç¥ç¶“ç¶²çµ¡æ¨ç†å–º GPU ä¸Šé¢æœ€æœ‰æ•ˆç‡å˜…æ–¹å¼ä¿‚æ‰¹æ¬¡è™•ç†ã€‚AlphaGo ä½¿ç”¨**æ‰¹æ¬¡è©•ä¼°**ï¼š

```
å””ä½¿ç”¨æ‰¹æ¬¡ï¼š
  æ¨¡æ“¬ 1 â†’ è©•ä¼° 1 â†’ æ¨¡æ“¬ 2 â†’ è©•ä¼° 2 â†’ ...
  GPU åˆ©ç”¨ç‡ä½

ä½¿ç”¨æ‰¹æ¬¡ï¼š
  æ”¶é›† 32 å€‹å¾…è©•ä¼°å˜…å±€é¢
  â†’ ä¸€æ¬¡éé€å…¥ GPU è©•ä¼°
  â†’ è¿”å› 32 å€‹çµæœ
  GPU åˆ©ç”¨ç‡é«˜
```

å‘¢å€‹éœ€è¦æ›´è¤‡é›œå˜…èª¿åº¦ï¼Œä½†å¤§å¹…æé«˜å’—ååé‡ã€‚

---

## æº«åº¦èˆ‡æœ€çµ‚æ€é¸

### è¨“ç·´æ™‚å˜…æº«åº¦

å–ºè‡ªæˆ‘å°å¼ˆè¨“ç·´å—°é™£ï¼ŒAlphaGo ä½¿ç”¨**æº«åº¦**åšŸæ§åˆ¶æ¢ç´¢ï¼š

$$\pi(a) = \frac{N(s,a)^{1/\tau}}{\sum_{a'} N(s,a')^{1/\tau}}$$

å…¶ä¸­ $\tau$ ä¿‚æº«åº¦åƒæ•¸ã€‚

- $\tau = 1$ï¼šæ©Ÿç‡æ­£æ¯”æ–¼è¨ªå•æ¬¡æ•¸ï¼ˆä¿æŒå¤šæ¨£æ€§ï¼‰
- $\tau \to 0$ï¼šæ€é¸è¨ªå•æ¬¡æ•¸æœ€å¤šå˜…å‹•ä½œï¼ˆç¢ºå®šæ€§æ€é¸ï¼‰

AlphaGo Zero å˜…ç­–ç•¥ï¼š
- **é ­ 30 æ‰‹**ï¼š$\tau = 1$ï¼Œä¿æŒé–‹å±€å¤šæ¨£æ€§
- **ä¹‹å¾Œ**ï¼š$\tau \to 0$ï¼Œæ€é¸æœ€ä½³è‘—æ³•

### æ¯”è³½æ™‚å˜…æ€é¸

å–ºå¯¦éš›æ¯”è³½å…¥é¢ï¼Œæ€é¸é€šå¸¸ä¿‚ç¢ºå®šæ€§å˜…ï¼š

```python
def select_move(root, temperature=0):
    if temperature == 0:
        # æ€é¸è¨ªå•æ¬¡æ•¸æœ€å¤šå˜…å‹•ä½œ
        return argmax(root.visit_counts)
    else:
        # æŒ‰æº«åº¦èª¿æ•´å˜…æ©Ÿç‡åˆ†ä½ˆæ¡æ¨£
        probs = root.visit_counts ** (1 / temperature)
        probs = probs / probs.sum()
        return np.random.choice(actions, p=probs)
```

### è€ƒæ…®å‹ç‡

æœ‰æ™‚éƒ½æœƒè€ƒæ…®å¹³å‡åƒ¹å€¼è€Œå””ä¿‚æ·¨ä¿‚è¨ªå•æ¬¡æ•¸ï¼š

```python
def select_move_with_value(root, temperature=0):
    # æ··åˆè¨ªå•æ¬¡æ•¸åŒåƒ¹å€¼
    scores = root.visit_counts * (1 + root.Q_values)
    scores = scores / scores.sum()

    if temperature == 0:
        return argmax(scores)
    else:
        probs = scores ** (1 / temperature)
        probs = probs / probs.sum()
        return np.random.choice(actions, p=probs)
```

---

## èˆ‡ç´”ç¥ç¶“ç¶²çµ¡å˜…æ¯”è¼ƒ

### é»è§£éœ€è¦æœç´¢ï¼Ÿ

ä¸€å€‹è‡ªç„¶å˜…å•é¡Œä¿‚ï¼š**æ—¢ç„¶ç¥ç¶“ç¶²çµ¡å·²ç¶“å¯ä»¥é æ¸¬å¥½å˜…è‘—æ³•ï¼Œé»è§£ä»²éœ€è¦æœç´¢ï¼Ÿ**

ç­”æ¡ˆä¿‚ï¼š**æœç´¢å¯ä»¥ä¿®æ­£ç¥ç¶“ç¶²çµ¡å˜…éŒ¯èª¤ä¸¦ç™¼ç¾æ›´å¥½å˜…è‘—æ³•**ã€‚

| æ–¹æ³• | å„ªé» | ç¼ºé» |
|------|------|------|
| ç´”ç¥ç¶“ç¶²çµ¡ | å¿«é€Ÿã€ç›´è¦º | å¯èƒ½æœ‰ç›²é» |
| ç´” MCTS | å¯ä»¥æ·±å…¥åˆ†æ | æ…¢ã€éœ€è¦è©•ä¼° |
| ç¥ç¶“ç¶²çµ¡ + MCTS | çµåˆå…©è€…å„ªé» | è¨ˆç®—é‡å¤§ |

### å¯¦é©—è­‰æ“š

DeepMind å˜…å¯¦é©—é¡¯ç¤ºï¼š

```
ç´” Policy Networkï¼šç´„ 3000 Elo
Policy + å°‘é‡ MCTSï¼šç´„ 3500 Elo
Policy + Value + MCTSï¼šç´„ 4500 Elo
```

æœç´¢æä¾›å’—é¡¯è‘—å˜…æ£‹åŠ›æå‡ã€‚

### æœç´¢å˜…ä½œç”¨

æœç´¢å–ºä»¥ä¸‹æƒ…æ³ç‰¹åˆ¥æœ‰åƒ¹å€¼ï¼š

1. **æˆ°è¡“è¨ˆç®—**ï¼šè®€å‡ºè¤‡é›œå˜…æ”»æ®º
2. **ä¿®æ­£åè¦‹**ï¼šç³¾æ­£ç¥ç¶“ç¶²çµ¡å˜…ç³»çµ±æ€§éŒ¯èª¤
3. **è™•ç†ç½•è¦‹å±€é¢**ï¼šç¥ç¶“ç¶²çµ¡è¨“ç·´å—°é™£å¯èƒ½æœªè¦‹é
4. **é©—è­‰ç›´è¦º**ï¼šç¢ºèªã€Œç‡è½å¥½ã€å˜…æ£‹ç¢ºå¯¦ä¿‚å¥½æ£‹

---

## AlphaGo å„ç‰ˆæœ¬å˜…å·®ç•°

### AlphaGo Fan/Lee

```
æ¶æ§‹ï¼š
- SL Policy Networkï¼ˆç›£ç£å­¸ç¿’ï¼‰
- RL Policy Networkï¼ˆå¼·åŒ–å­¸ç¿’ï¼‰
- Value Network
- Fast Rollout Policy

æœç´¢æ™‚ï¼š
- ç”¨ SL Policy Network å˜…å…ˆé©—æ©Ÿç‡
- æ··åˆ Value Network åŒ Rollout è©•ä¼°
```

### AlphaGo Master

```
æ¶æ§‹ï¼š
- æ›´å¤§å˜…ç¥ç¶“ç¶²çµ¡
- æ›´å¤šå˜…è¨“ç·´è³‡æ–™
- æ”¹é€²å˜…ç‰¹å¾µ

æœç´¢æ™‚ï¼š
- é¡ä¼¼ AlphaGo Lee
- æ›´å¼·å˜…ç¶²çµ¡ = æ›´å°‘å˜…æœç´¢éœ€æ±‚
```

### AlphaGo Zero

```
æ¶æ§‹ï¼š
- å–®ä¸€é›™é ­ ResNet
- ç”±é›¶é–‹å§‹è¨“ç·´
- ç„¡ Rollout

æœç´¢æ™‚ï¼š
- ç­–ç•¥é ­æä¾›å…ˆé©—æ©Ÿç‡
- åƒ¹å€¼é ­ç›´æ¥è©•ä¼°
- æ›´ç°¡æ½”ã€æ›´å¼·
```

### æ¼”é€²ç¸½çµ

```mermaid
flowchart TB
    A["AlphaGo Fan (2015)"] -->|"+ æ›´å¤§ç¶²çµ¡ã€æ›´å¤šè¨“ç·´"| B["AlphaGo Lee (2016)"]
    B -->|"+ æ›´å¤šè‡ªæˆ‘å°å¼ˆ"| C["AlphaGo Master (2017)"]
    C -->|"+ ç§»é™¤äººé¡è³‡æ–™ã€çµ±ä¸€ç¶²çµ¡ã€ç§»é™¤ Rollout"| D["AlphaGo Zero (2017)"]
    D -->|"+ æ³›åŒ–åˆ°å…¶ä»–éŠæˆ²"| E["AlphaZero (2018)"]
```

---

## å¯¦ä½œè€ƒé‡

### è¨˜æ†¶é«”ç®¡ç†

MCTS æ¨¹å¯ä»¥è®Šå¾—å¥½å¤§ï¼š

```
å‡è¨­ï¼š
- æ¯æ­¥å¹³å‡ 200 å€‹åˆæ³•å‹•ä½œ
- æœç´¢æ·±åº¦ 10
- å®Œå…¨å±•é–‹ï¼š200^10 â‰ˆ 10^23 å€‹ç¯€é»ï¼ˆå””å¯èƒ½ï¼‰

å¯¦éš›åšæ³•ï¼š
- åªå±•é–‹è¢«è¨ªå•å˜…ç¯€é»
- å®šæœŸæ¸…ç†å¥½å°‘è¨ªå•å˜…ç¯€é»
- é‡ç”¨ä¸Šä¸€æ­¥å˜…æœç´¢æ¨¹
```

### æ¨¹å˜…é‡ç”¨

ç•¶å°æ‰‹è½æ£‹ä¹‹å¾Œï¼Œå¯ä»¥é‡ç”¨éƒ¨åˆ†æœç´¢æ¨¹ï¼š

```python
def reuse_tree(root, opponent_move):
    if opponent_move in root.children:
        new_root = root.children[opponent_move]
        # æ¸…ç†å””éœ€è¦å˜…å…¶ä»–åˆ†æ”¯
        for action in root.children:
            if action != opponent_move:
                delete_subtree(root.children[action])
        return new_root
    else:
        # å°æ‰‹è½å’—æ„å¤–å˜…æ£‹ï¼Œéœ€è¦é‡æ–°é–‹å§‹
        return create_new_root()
```

### ç¥ç¶“ç¶²çµ¡ç·©å­˜

åŒä¸€å±€é¢å¯èƒ½è¢«å¤šæ¬¡è©•ä¼°ï¼Œä½¿ç”¨ç·©å­˜é¿å…é‡è¤‡è¨ˆç®—ï¼š

```python
class NeuralNetworkCache:
    def __init__(self, max_size=100000):
        self.cache = LRUCache(max_size)

    def evaluate(self, state, network):
        state_hash = hash(state)
        if state_hash in self.cache:
            return self.cache[state_hash]
        else:
            result = network(state)
            self.cache[state_hash] = result
            return result
```

### å°ç¨±æ€§åˆ©ç”¨

åœæ£‹æœ‰ 8 é‡å°ç¨±æ€§ï¼Œå¯ä»¥ç”¨åšŸå¢å¼·æœç´¢ï¼š

```python
def evaluate_with_symmetry(state, network):
    # ç”Ÿæˆæ‰€æœ‰å°ç¨±è®Šæ›
    symmetries = generate_symmetries(state)  # 8 å€‹ç‰ˆæœ¬

    # è©•ä¼°æ‰€æœ‰ç‰ˆæœ¬
    values = [network(s) for s in symmetries]

    # å¹³å‡ï¼ˆæ›´ç©©å®šï¼‰
    return np.mean(values)
```

---

## æœç´¢æ·±åº¦èˆ‡å»£åº¦

### å‹•æ…‹èª¿æ•´

MCTS è‡ªå‹•å¹³è¡¡æ·±åº¦èˆ‡å»£åº¦ï¼š

- **å»£åº¦**ï¼šç”± Policy Network å˜…å…ˆé©—æ©Ÿç‡æ§åˆ¶
- **æ·±åº¦**ï¼šç”± Value Network å˜…æº–ç¢ºåº¦æ±ºå®š

ç•¶ç¥ç¶“ç¶²çµ¡å¥½å¥½å—°é™£ï¼š
- é«˜ç½®ä¿¡å˜…è‘—æ³•æœƒè¢«æ·±å…¥æ¢ç´¢
- ä½ç½®ä¿¡å˜…è‘—æ³•è¢«å¿«é€Ÿæ’é™¤
- æœç´¢è‡ªç„¶èšç„¦å–ºé‡è¦å˜…åˆ†æ”¯

### èˆ‡å‚³çµ±æœç´¢å˜…æ¯”è¼ƒ

| æ–¹æ³• | æ·±åº¦æ§åˆ¶ | å»£åº¦æ§åˆ¶ |
|------|---------|---------|
| Minimax | å›ºå®šæ·±åº¦ | Alpha-Beta å‰ªæ |
| å‚³çµ± MCTS | ç”±æ¨¡æ“¬æ±ºå®š | UCB1 |
| AlphaGo MCTS | Policy + Value å¼•å° | PUCT + Policy |

AlphaGo å˜…æœç´¢æ›´ã€Œæ™ºèƒ½ã€â€”â€”ä½¢çŸ¥é“é‚Šå•²åœ°æ–¹å€¼å¾—æ·±å…¥ï¼Œé‚Šå•²å¯ä»¥å¿«é€Ÿç•¥éã€‚

---

## å‹•ç•«å°æ‡‰

æœ¬æ–‡æ¶‰åŠå˜…æ ¸å¿ƒæ¦‚å¿µèˆ‡å‹•ç•«ç·¨è™Ÿï¼š

| ç·¨è™Ÿ | æ¦‚å¿µ | ç‰©ç†/æ•¸å­¸å°æ‡‰ |
|------|------|--------------|
| ğŸ¬ C5 | MCTS å››éšæ®µ | æ¨¹æœç´¢ |

---

## ç¸½çµ

MCTS èˆ‡ç¥ç¶“ç¶²çµ¡å˜…çµåˆä¿‚ AlphaGo å˜…æ ¸å¿ƒå‰µæ–°ã€‚æˆ‘å“‹å­¸ç¿’å’—ï¼š

1. **å‚³çµ± MCTS**ï¼šSelectionã€Expansionã€Simulationã€Backpropagation
2. **ç¥ç¶“ç¶²çµ¡æ”¹é€²**ï¼šPolicy Network å¼•å°å±•é–‹ã€Value Network å–ä»£ Rollout
3. **æœç´¢éç¨‹**ï¼šPUCT æ€é¸ã€æ‰¹æ¬¡è©•ä¼°ã€åå‘å‚³æ’­
4. **è³‡æºåˆ†é…**ï¼šæ¨¡æ“¬æ¬¡æ•¸ã€æ™‚é–“ç®¡ç†ã€ä¸¦è¡Œæœç´¢
5. **æº«åº¦æ€é¸**ï¼šè¨“ç·´åŒæ¯”è³½å˜…å””åŒç­–ç•¥
6. **å¯¦ä½œç´°ç¯€**ï¼šè¨˜æ†¶é«”ç®¡ç†ã€æ¨¹é‡ç”¨ã€ç·©å­˜

ä¸‹ä¸€ç¯‡ï¼Œæˆ‘å“‹å°‡æœƒæ·±å…¥æ¢è¨ PUCT å…¬å¼å˜…æ•¸å­¸ç´°ç¯€ã€‚

---

## å»¶ä¼¸é–±è®€

- **ä¸‹ä¸€ç¯‡**ï¼š[PUCT å…¬å¼è©³è§£](../puct-formula) â€” MCTS æ€é¸å˜…æ•¸å­¸åŸç†
- **ä¸Šä¸€ç¯‡**ï¼š[è‡ªæˆ‘å°å¼ˆ](../self-play) â€” è‡ªæˆ‘å°å¼ˆå˜…æ©Ÿåˆ¶èˆ‡æ•ˆæœ
- **ç›¸é—œ**ï¼š[Policy Network è©³è§£](../policy-network) â€” ç­–ç•¥ç¶²çµ¡å˜…æ¶æ§‹

---

## åƒè€ƒè³‡æ–™

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
3. Coulom, R. (2006). "Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search." *Computers and Games*.
4. Kocsis, L., & SzepesvÃ¡ri, C. (2006). "Bandit based Monte-Carlo Planning." *ECML*.
5. Browne, C., et al. (2012). "A Survey of Monte Carlo Tree Search Methods." *IEEE TCIAIG*.
6. Rosin, C. D. (2011). "Multi-armed bandits with episode context." *Annals of Mathematics and Artificial Intelligence*.
