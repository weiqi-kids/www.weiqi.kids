---
sidebar_position: 21
title: AlphaGo 嘅遺產
description: 從圍棋到蛋白質結構，AlphaGo 嘅技術遺產點樣改變 AI 研究與整個科學界
keywords: [AlphaGo, AlphaZero, MuZero, AlphaFold, 深度強化學習, AI 影響, 圍棋 AI]
---

# AlphaGo 嘅遺產

2016 年 3 月，AlphaGo 擊敗李世乭嗰一刻，唔單止係圍棋歷史嘅轉捩點，更加係人工智能發展嘅里程碑。從嗰陣開始，AlphaGo 嘅技術核心被應用到越來越多嘅領域，從遊戲到科學發現，從基礎研究到實際應用。

本文將回顧 AlphaGo 對圍棋界、AI 研究、以及更廣泛科學領域嘅深遠影響。

---

## 對圍棋界嘅影響

### 震驚與接受

AlphaGo 擊敗李世乭之前，職業棋手普遍認為 AI 仲差得遠：

> "我會 5:0 獲勝。"
> — **李世乭**，賽前預測

但比賽結果係 4:1。更加衝擊嘅係，AlphaGo 展現嘅下法令職業棋手意識到：**我哋對圍棋嘅理解可能係錯嘅**。

### 棋理嘅革新

AlphaGo 帶嚟咗一系列棋理革新：

| 傳統觀點 | AlphaGo 嘅挑戰 |
|----------|----------------|
| 點三三要喺適當時機 | 開局直接點三三可行 |
| 定式要嚴格遵守 | 可以主動脫離定式 |
| 實地與外勢要平衡 | 勝率先係唯一標準 |
| 愚形必須避免 | 某啲「愚形」其實係好棋 |
| 序盤要搶大場 | 局部戰鬥可能更重要 |

呢啲變化唔係因為 AlphaGo「話」人類應該點落，而係人類喺研究 AI 棋譜之後，主動學習並驗證嘅結果。

### AI 訓練成為常態

2024 年嘅職業圍棋界，AI 訓練已經係標配：

| 變化 | 描述 |
|------|------|
| 覆盤方式 | 用 AI 分析每一手嘅勝率同建議 |
| 開局準備 | 研究 AI 推薦嘅開局變化 |
| 戰術訓練 | 用 AI 產生嘅死活題同手筋題練習 |
| 實戰應用 | 某啲職業比賽允許休息時查 AI |

### 對職業棋手嘅影響

唔同棋手對 AI 嘅態度：

> "AI 令我重新愛上圍棋。原來圍棋仲有咁多我唔知嘅嘢。"
> — **柯潔**，2017

> "同 AI 落棋令我感到絕望，但研究 AI 令我搵到新嘅方向。"
> — **李世乭**，2019（退役前）

> "AI 唔係對手，係老師。"
> — 好多職業棋手嘅共識

### 新一代棋手

2016 年之後出道嘅職業棋手，從細個就接受 AI 訓練：

- 開局更加多樣化
- 戰術更加精確
- 對「傳統棋理」更加靈活
- 整體水平可能比上一代更高

呢個係圍棋歷史上從未有過嘅學習資源——一個永遠可用、永不疲倦、棋力超人嘅老師。

---

## AlphaZero：通用遊戲 AI

### 從圍棋到三種棋類

2017 年 12 月，DeepMind 發表 **AlphaZero**，將 AlphaGo Zero 嘅技術推廣到三種唔同嘅棋類遊戲：

| 遊戲 | 訓練時間 | 對手 | 戰績 |
|------|----------|------|------|
| 圍棋 | 8 小時 | AlphaGo Zero | 60:40 |
| 西洋棋 | 4 小時 | Stockfish | 155:6（含和棋） |
| 將棋 | 2 小時 | Elmo | 90:8:2 |

**同一套演算法，三種唔同嘅遊戲，都達到超人水平。**

### 對西洋棋界嘅衝擊

西洋棋有超過一百年嘅 AI 研究歷史，Stockfish 係數十年工程優化嘅結晶。AlphaZero 用 4 小時從零開始訓練，就擊敗咗呢一切。

更重要嘅係 AlphaZero 嘅下棋風格：

> "AlphaZero 嘅棋好似來自另一個星球。佢願意犧牲棋子換取長期嘅位置優勢，呢個喺傳統西洋棋入面係唔可以想像嘅。"
> — **Garry Kasparov**，前西洋棋世界冠軍

### 技術上嘅意義

AlphaZero 證明咗：

1. **通用性**：同一套方法適用於唔同領域
2. **第一性原理學習**：唔需要領域專家知識
3. **效率**：訓練時間從月縮短到小時

呢個為 AI 嘅通用化邁出咗關鍵一步。

---

## MuZero：唔需要規則嘅學習

### 更進一步嘅突破

2019 年，DeepMind 發表 **MuZero**，比 AlphaZero 更進一步：

> **AlphaZero 需要知道遊戲規則，MuZero 連規則都唔需要。**

MuZero 透過同環境互動，自己學習環境嘅動態模型（dynamics model），然後用呢個學習到嘅模型進行規劃。

### 工作原理

```
AlphaGo/AlphaZero:
環境規則（已知）→ MCTS 搜索 → 最佳動作

MuZero:
環境觀察 → 學習動態模型 → 用學習到嘅模型進行 MCTS → 最佳動作
```

MuZero 學習三個模型：
- **表示函數（Representation）**：將觀察轉換為隱狀態
- **動態函數（Dynamics）**：預測下一個隱狀態同獎勵
- **預測函數（Prediction）**：預測策略同價值

### 應用範圍擴大

因為唔需要明確嘅規則，MuZero 可以應用於更多領域：

| 領域 | 描述 |
|------|------|
| Atari 遊戲 | 57 個遊戲，大部分超越人類 |
| 棋類遊戲 | 同 AlphaZero 同等水平 |
| 視訊壓縮 | 用於 YouTube 視訊編碼，節省 4% 頻寬 |
| 資料中心冷卻 | 優化 Google 資料中心能源效率 |

### 對 AI 研究嘅啟示

MuZero 展示咗**模型學習（Model-based RL）** 嘅威力：

- 唔需要手動定義環境規則
- 可以處理連續狀態空間
- 可以處理部分可觀察環境
- 更加接近人類嘅學習方式

---

## AlphaFold：改變生物學嘅 AI

### 蛋白質結構預測

2020 年，DeepMind 發表 **AlphaFold 2**，喺蛋白質結構預測競賽（CASP14）中取得驚人成績：

| 指標 | AlphaFold 2 | 第二名 |
|------|-------------|--------|
| GDT-TS 分數 | 92.4 | 67.0 |
| 中位誤差 | 0.96 Å | ~2.5 Å |

呢個精度已經接近實驗測量嘅水平，解決咗生物學領域 50 年嘅難題。

### 同 AlphaGo 嘅技術聯繫

AlphaFold 唔係直接使用 AlphaGo 嘅程式碼，但繼承咗核心理念：

| AlphaGo 技術 | AlphaFold 中嘅對應 |
|--------------|-------------------|
| 深度神經網絡 | Transformer + Attention |
| 迭代優化 | 迭代細化結構預測 |
| 端到端學習 | 從序列直接預測結構 |
| 大規模訓練 | 利用大量已知結構訓練 |

### 科學界嘅反應

> "呢個會改變一切。我哋唔再需要等幾年進行實驗，就可以知道蛋白質嘅結構。"
> — **結構生物學家**

AlphaFold 嘅影響：

- **藥物開發**：加速新藥設計
- **疾病研究**：理解疾病機制
- **合成生物學**：設計新蛋白質
- **基礎研究**：促進生命科學發展

2024 年，AlphaFold 嘅創造者 Demis Hassabis 同 John Jumper 因此獲得**諾貝爾化學獎**。

### 開放科學

DeepMind 將 AlphaFold 預測嘅**2 億+蛋白質結構**開放畀全球研究者免費使用。呢個係 AI 促進開放科學嘅典範。

---

## 對 AI 領域嘅啟示

### 方法論嘅轉變

AlphaGo 代表咗 AI 研究方法論嘅轉變：

| 傳統方法 | AlphaGo 方法 |
|----------|-------------|
| 手工設計特徵 | 端到端學習 |
| 專家規則 | 從資料學習 |
| 分步驟優化 | 聯合優化 |
| 人類知識編碼 | 從零開始學習 |

呢種「少啲人類設計，多啲學習」嘅理念，影響咗 AI 嘅各個子領域。

### 強化學習嘅復興

AlphaGo 令強化學習重新受到關注：

| 時期 | 強化學習地位 |
|------|-------------|
| 2010 年前 | 理論有趣，實用困難 |
| 2013 年 DQN | 開始展現潛力 |
| 2016 年 AlphaGo | 證明可以解決複雜問題 |
| 2017 年後 | 成為 AI 研究熱點 |

而家，強化學習被應用於：
- 機械人控制
- 自動駕駛
- 推薦系統
- 大型語言模型對齊（RLHF）

### 計算與演算法嘅權衡

AlphaGo 系列嘅演進展示咗計算與演算法嘅權衡：

```
AlphaGo Fan:  大量人類知識 + 大量計算
AlphaGo Lee:  人類知識 + 更多計算
AlphaGo Zero: 零人類知識 + 中等計算 + 更好嘅演算法
AlphaZero:    零人類知識 + 少量計算 + 最佳演算法
```

更好嘅演算法可以減少對計算資源嘅需求。呢個對 AI 民主化好重要。

---

## 技術遺產嘅擴散

### 開源社群

AlphaGo 嘅技術被開源社群快速複現同改進：

| 項目 | 特點 | 狀態 |
|------|------|------|
| **Leela Zero** | 分散式社群訓練 | 活躍 |
| **KataGo** | 單 GPU 高效訓練 | 非常活躍 |
| **ELF OpenGo** | Facebook 開源 | 維護中 |
| **Minigo** | Google 開源教學項目 | 完成 |
| **Pachi** | 傳統 MCTS，AI 時代前嘅王者 | 歷史意義 |

### 研究論文引用

AlphaGo 相關論文嘅影響力：

| 論文 | 引用數（約） |
|------|-------------|
| AlphaGo（2016, Nature） | 20,000+ |
| AlphaGo Zero（2017, Nature） | 15,000+ |
| AlphaZero（2018, Science） | 10,000+ |

呢啲論文被 AI、神經科學、認知科學、遊戲研究等多個領域引用。

### 教育影響

AlphaGo 成為 AI 教育嘅經典案例：

- 大學課程嘅必讀材料
- 強化學習教科書嘅重要章節
- 科普文章同紀錄片嘅熱門主題
- 激勵新一代研究者進入 AI 領域

---

## 對社會嘅更廣泛影響

### AI 意識嘅提升

AlphaGo 令公眾意識到 AI 嘅能力：

| 面向 | 影響 |
|------|------|
| 媒體報導 | AI 成為主流新聞話題 |
| 投資熱潮 | AI 創業同投資大幅增加 |
| 政策討論 | 各國開始制定 AI 戰略 |
| 公眾認知 | 更多人了解 AI 嘅可能性同風險 |

### 人機關係嘅思考

AlphaGo 引發咗關於人機關係嘅深層思考：

> "如果機器喺圍棋上超越人類，咁人類嘅價值喺邊度？"

圍棋界畀咗一個答案：
- AI 係工具，唔係對手
- 人類嘅價值唔在於同機器比賽
- 圍棋嘅樂趣唔會因為 AI 而消失

呢種思考方式，對其他 AI 可能超越人類嘅領域都有借鑑意義。

### 倫理考量

DeepMind 喺 AlphaGo 項目中都面對咗倫理問題：

- **比賽公平性**：AI 對人類係咪公平？
- **職業棋手嘅未來**：AI 會唔會取代人類？
- **技術責任**：強大嘅 AI 應該點樣被使用？

DeepMind 成立咗倫理委員會，並喺收購協議中加入咗 AI 安全條款。呢種做法影響咗後來嘅 AI 公司。

---

## 未來展望

### AI 嘅下一個挑戰

AlphaGo 之後，AI 研究者喺度問：下一個「圍棋」係咩？

| 候選領域 | 難度 | 進展 |
|----------|------|------|
| 即時戰略遊戲（如 StarCraft） | 極高 | AlphaStar 達到宗師水平 |
| 開放世界遊戲（如 Minecraft） | 好高 | 正喺度研究中 |
| 科學發現 | 極高 | AlphaFold 喺蛋白質領域突破 |
| 數學定理證明 | 極高 | AlphaProof 取得進展 |
| 通用人工智能（AGI） | 未知 | 長期目標 |

### 從專用到通用

AlphaGo 系列嘅演進方向：

```
AlphaGo（圍棋專用）
    ↓
AlphaZero（棋類通用）
    ↓
MuZero（遊戲通用）
    ↓
?（領域通用）
    ↓
AGI（完全通用）
```

每一步都喺度減少對特定領域知識嘅依賴，增加通用性。

### DeepMind 嘅願景

DeepMind 嘅使命依然係：

> "Solve intelligence, and then use that to solve everything else."

AlphaGo 係呢個願景嘅第一個重要里程碑。AlphaFold 係第二個。未來會有更多。

---

## 結語

回顧 AlphaGo 嘅故事，我哋睇到嘅唔單止係一個打敗人類嘅 AI，而係：

- **技術突破**：深度學習 + 強化學習 + 樹搜索嘅強大組合
- **方法論革新**：從零開始學習，超越人類知識
- **工程成就**：分散式系統同專用硬件嘅完美配合
- **科學應用**：從遊戲到蛋白質結構嘅跨越
- **文化影響**：改變人類對 AI 同自身嘅認識

AlphaGo 證明咗：**正確嘅方法 + 足夠嘅計算，可以解決曾被認為唔可能嘅問題**。

呢個教訓將繼續指引未來嘅 AI 研究。而圍棋——呢個有數千年歷史嘅遊戲——將永遠係呢段歷史嘅見證者。

---

## 動畫對應

本文涉及嘅核心概念與動畫編號：

| 編號 | 概念 | 物理/數學對應 |
|------|------|--------------|
| F8 | 湧現能力 | 相變 |
| E7 | 從零開始 | 自組織 |
| F1 | 通用智能 | 普適性 |
| F5 | 遷移學習 | 知識轉移 |

---

## 延伸閱讀

- **返去開始**：[AlphaGo 嘅誕生](../birth-of-alphago) — 呢一切係點樣開始嘅
- **技術總結**：[AlphaGo 完整解析](../) — 系列文章總覽
- **動手實作**：[30 分鐘跑起第一個圍棋 AI](../../../hands-on/) — 親自體驗

---

## 參考資料

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
3. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
4. Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." *Nature*, 588, 604-609.
5. Jumper, J., et al. (2021). "Highly accurate protein structure prediction with AlphaFold." *Nature*, 596, 583-589.
6. 《AlphaGo》紀錄片（2017），導演 Greg Kohs。
7. Hassabis, D. (2017). "Artificial Intelligence: Chess match of the century." *Nature*, 544, 413-414.
8. Kasparov, G. (2018). "Chess, a Drosophila of reasoning." *Science*, 362(6419), 1087.
