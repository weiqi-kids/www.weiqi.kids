<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-for-engineers/how-it-works/alphago-explained/self-play" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Self-Play | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/en/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/en/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/self-play/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="圍棋, Go, 好棋寶寶, AI, KataGo, AlphaGo, 圍棋教學, 圍棋入門"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Self-Play | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="Deep understanding of how AlphaGo surpasses human strength through self-play"><meta data-rh="true" property="og:description" content="Deep understanding of how AlphaGo surpasses human strength through self-play"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/self-play/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"給工程師的圍棋 AI 指南","item":"https://www.weiqi.kids/en/docs/for-engineers/"},{"@type":"ListItem","position":2,"name":"一篇文章搞懂圍棋 AI","item":"https://www.weiqi.kids/en/docs/for-engineers/how-it-works/"},{"@type":"ListItem","position":3,"name":"AlphaGo 完整解析","item":"https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/"},{"@type":"ListItem","position":4,"name":"Self-Play","item":"https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/self-play"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.f23bf74b.css">
<script src="/en/assets/js/runtime~main.51eae05f.js" defer="defer"></script>
<script src="/en/assets/js/main.74a0125b.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/en/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/logo.svg" alt="Good Go Baby Association Logo" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/en/img/logo.svg" alt="Good Go Baby Association Logo" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">Weiqi.Kids</b></a><a class="navbar__item navbar__link" href="/en/docs/for-players/">For Go Players</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/docs/for-engineers/">For Engineers</a><a class="navbar__item navbar__link" href="/en/docs/about/">About Us</a><a class="navbar__item navbar__link" href="/en/docs/activities/">Activities</a><a class="navbar__item navbar__link" href="/en/docs/references/">References</a><a class="navbar__item navbar__link" href="/en/docs/sop/">Standard Procedures</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/ja/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/docs/intro/"><span title="User Guide" class="linkLabel_REp1">User Guide</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/about/"><span title="關於協會" class="categoryLinkLabel_ezQx">關於協會</span></a><button aria-label="Expand sidebar category &#x27;關於協會&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/activities/"><span title="活動實績" class="categoryLinkLabel_ezQx">活動實績</span></a><button aria-label="Expand sidebar category &#x27;活動實績&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/for-players/"><span title="For Go Players" class="categoryLinkLabel_ezQx">For Go Players</span></a><button aria-label="Expand sidebar category &#x27;For Go Players&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/references/"><span title="參考資料" class="categoryLinkLabel_ezQx">參考資料</span></a><button aria-label="Expand sidebar category &#x27;參考資料&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/sop/"><span title="標準作業流程" class="categoryLinkLabel_ezQx">標準作業流程</span></a><button aria-label="Expand sidebar category &#x27;標準作業流程&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/en/docs/for-engineers/"><span title="給工程師的圍棋 AI 指南" class="categoryLinkLabel_ezQx">給工程師的圍棋 AI 指南</span></a><button aria-label="Collapse sidebar category &#x27;給工程師的圍棋 AI 指南&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/en/docs/for-engineers/deep-dive/"><span title="For Deep Learners" class="categoryLinkLabel_ezQx">For Deep Learners</span></a><button aria-label="Expand sidebar category &#x27;For Deep Learners&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/en/docs/for-engineers/hands-on/"><span title="30 分鐘跑起第一個圍棋 AI" class="categoryLinkLabel_ezQx">30 分鐘跑起第一個圍棋 AI</span></a><button aria-label="Expand sidebar category &#x27;30 分鐘跑起第一個圍棋 AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/en/docs/for-engineers/how-it-works/"><span title="一篇文章搞懂圍棋 AI" class="categoryLinkLabel_ezQx">一篇文章搞懂圍棋 AI</span></a><button aria-label="Collapse sidebar category &#x27;一篇文章搞懂圍棋 AI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/"><span title="AlphaGo 完整解析" class="categoryLinkLabel_ezQx">AlphaGo 完整解析</span></a><button aria-label="Collapse sidebar category &#x27;AlphaGo 完整解析&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/birth-of-alphago/"><span title="The Birth of AlphaGo" class="linkLabel_REp1">The Birth of AlphaGo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/key-matches/"><span title="Key Matches Review" class="linkLabel_REp1">Key Matches Review</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/move-37/"><span title="Deep Analysis of &quot;The Divine Move&quot;" class="linkLabel_REp1">Deep Analysis of &quot;The Divine Move&quot;</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/why-go-is-hard/"><span title="Why Is Go Hard?" class="linkLabel_REp1">Why Is Go Hard?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/traditional-limits/"><span title="Limits of Traditional Methods" class="linkLabel_REp1">Limits of Traditional Methods</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/board-representation/"><span title="Board State Representation" class="linkLabel_REp1">Board State Representation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/policy-network/"><span title="Policy Network Deep Dive" class="linkLabel_REp1">Policy Network Deep Dive</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/value-network/"><span title="Value Network Explained" class="linkLabel_REp1">Value Network Explained</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/input-features/"><span title="Input Feature Design" class="linkLabel_REp1">Input Feature Design</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/cnn-and-go/"><span title="CNN and Go" class="linkLabel_REp1">CNN and Go</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/supervised-learning/"><span title="Supervised Learning Phase" class="linkLabel_REp1">Supervised Learning Phase</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/"><span title="Introduction to Reinforcement Learning" class="linkLabel_REp1">Introduction to Reinforcement Learning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/self-play/"><span title="Self-Play" class="linkLabel_REp1">Self-Play</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/"><span title="MCTS and Neural Network Integration" class="linkLabel_REp1">MCTS and Neural Network Integration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/puct-formula/"><span title="PUCT Formula Explained" class="linkLabel_REp1">PUCT Formula Explained</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><span title="AlphaGo Zero Overview" class="linkLabel_REp1">AlphaGo Zero Overview</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/"><span title="Dual-Head Network and Residual Network" class="linkLabel_REp1">Dual-Head Network and Residual Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/training-from-scratch/"><span title="Training from Scratch" class="linkLabel_REp1">Training from Scratch</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/distributed-systems/"><span title="Distributed Systems and TPU" class="linkLabel_REp1">Distributed Systems and TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/legacy-and-impact/"><span title="AlphaGo&#x27;s Legacy" class="linkLabel_REp1">AlphaGo&#x27;s Legacy</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/katago-innovations/"><span title="KataGo 的關鍵創新" class="linkLabel_REp1">KataGo 的關鍵創新</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/concepts/"><span title="概念速查表" class="linkLabel_REp1">概念速查表</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/en/docs/for-engineers/industry/"><span title="圍棋 AI 產業現況" class="categoryLinkLabel_ezQx">圍棋 AI 產業現況</span></a><button aria-label="Expand sidebar category &#x27;圍棋 AI 產業現況&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/en/docs/for-engineers/overview/"><span title="圍棋 AI 能做什麼？" class="categoryLinkLabel_ezQx">圍棋 AI 能做什麼？</span></a><button aria-label="Expand sidebar category &#x27;圍棋 AI 能做什麼？&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/en/docs/for-engineers/"><span>給工程師的圍棋 AI 指南</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/en/docs/for-engineers/how-it-works/"><span>一篇文章搞懂圍棋 AI</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/en/docs/for-engineers/how-it-works/alphago-explained/"><span>AlphaGo 完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Self-Play</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Self-Play</h1></header>
<p>In the previous article, we introduced the basic concepts of reinforcement learning. Now, let&#x27;s explore one of the keys to AlphaGo&#x27;s success - <strong>Self-Play</strong>.</p>
<p>This is a seemingly contradictory concept: <strong>How can an AI become stronger by playing against itself?</strong></p>
<p>The answer is both profound and elegant, involving game theory, evolutionary dynamics, and the nature of learning.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="why-does-self-play-work">Why Does Self-Play Work?<a href="#why-does-self-play-work" class="hash-link" aria-label="Direct link to Why Does Self-Play Work?" title="Direct link to Why Does Self-Play Work?" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="intuitive-explanation">Intuitive Explanation<a href="#intuitive-explanation" class="hash-link" aria-label="Direct link to Intuitive Explanation" title="Direct link to Intuitive Explanation" translate="no">​</a></h3>
<p>Imagine you&#x27;re a Go beginner, practicing alone on a deserted island:</p>
<ol>
<li class="">You play a game, acting as both Black and White</li>
<li class="">After the game, you analyze which moves were good and which were bad</li>
<li class="">In the next game, you try to avoid previous mistakes</li>
<li class="">You repeat this process millions of times</li>
</ol>
<p>Intuitively, this seems problematic:</p>
<ul>
<li class="">If your level is poor, both Black and White play badly - what can you learn?</li>
<li class="">Could you fall into a &quot;wrong equilibrium&quot; - both sides play wrong but cancel each other out?</li>
</ul>
<p>But actually, self-play can produce continuous improvement. Here&#x27;s why:</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="progressive-discovery-of-weaknesses">Progressive Discovery of Weaknesses<a href="#progressive-discovery-of-weaknesses" class="hash-link" aria-label="Direct link to Progressive Discovery of Weaknesses" title="Direct link to Progressive Discovery of Weaknesses" translate="no">​</a></h3>
<p>The key insight is: <strong>Even when both sides are the same AI, each game&#x27;s result still contains information.</strong></p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Position A: AI chose move X, eventually won</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Position A: AI chose move Y, eventually lost</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">→ Conclusion: In position A, X is better than Y</span><br></span></code></pre></div></div>
<p>Through statistical analysis of many games, AI can learn which choices are better in each position. This is the essence of <strong>policy gradient</strong>: good choices get reinforced, bad choices get suppressed.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="adversarial-learning">Adversarial Learning<a href="#adversarial-learning" class="hash-link" aria-label="Direct link to Adversarial Learning" title="Direct link to Adversarial Learning" translate="no">​</a></h3>
<p>Self-play has a special property: <strong>The training opponent automatically adapts to your level.</strong></p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Training cycle 1: AI discovers effective tactic T</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Training cycle 2: As opponent, AI learns to defend against T</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Training cycle 3: Original AI is forced to find better tactic T&#x27;</span><br></span></code></pre></div></div>
<p>This forms an <strong>arms race</strong>, where both sides continuously discover and overcome each other&#x27;s weaknesses.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="comparison-with-human-games">Comparison with Human Games<a href="#comparison-with-human-games" class="hash-link" aria-label="Direct link to Comparison with Human Games" title="Direct link to Comparison with Human Games" translate="no">​</a></h3>
<table><thead><tr><th>Training Method</th><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td><strong>Human games</strong></td><td>Learn from accumulated human wisdom</td><td>Limited by human level</td></tr><tr><td><strong>Self-play</strong></td><td>Unlimited improvement potential</td><td>May get stuck in local optima</td></tr><tr><td><strong>Both combined</strong></td><td>Quick start + continuous improvement</td><td>Best strategy</td></tr></tbody></table>
<p>Original AlphaGo first used human games for supervised learning, then self-play for reinforcement learning. AlphaGo Zero proved that self-play alone can achieve superhuman level.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="game-theory-perspective">Game Theory Perspective<a href="#game-theory-perspective" class="hash-link" aria-label="Direct link to Game Theory Perspective" title="Direct link to Game Theory Perspective" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="nash-equilibrium">Nash Equilibrium<a href="#nash-equilibrium" class="hash-link" aria-label="Direct link to Nash Equilibrium" title="Direct link to Nash Equilibrium" translate="no">​</a></h3>
<p>In game theory, <strong>Nash Equilibrium</strong> is a stable state: at this state, no player has incentive to unilaterally change strategy.</p>
<p>For <strong>zero-sum, perfect-information games</strong> like Go, Nash equilibrium has special meaning:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>π</mi><mo>∗</mo></msup><mo>=</mo><mi>arg</mi><mo>⁡</mo><msub><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>π</mi></msub><msub><mrow><mi>min</mi><mo>⁡</mo></mrow><msup><mi>π</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mi>V</mi><mo stretchy="false">(</mo><mi>π</mi><mo separator="true">,</mo><msup><mi>π</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi^* = \arg\max_\pi \min_{\pi&#x27;} V(\pi, \pi&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mop">ar<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop">min</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em"><span style="top:-2.786em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><mi>π</mi><mo separator="true">,</mo><msup><mi>π</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(\pi, \pi&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> is the expected value when strategy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">π</span></span></span></span> plays against strategy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>π</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">\pi&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>.</p>
<p>This is the famous <strong>Minimax principle</strong>: the optimal strategy is the one that performs best in the worst case.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="self-play-and-nash-equilibrium">Self-Play and Nash Equilibrium<a href="#self-play-and-nash-equilibrium" class="hash-link" aria-label="Direct link to Self-Play and Nash Equilibrium" title="Direct link to Self-Play and Nash Equilibrium" translate="no">​</a></h3>
<p>Theoretically, if self-play converges, it should converge to Nash equilibrium. For deterministic games like Go, Nash equilibrium is <strong>perfect play</strong>.</p>
<p>But Go&#x27;s state space is too large (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>10</mn><mn>170</mn></msup></mrow><annotation encoding="application/x-tex">10^{170}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">170</span></span></span></span></span></span></span></span></span></span></span></span>), so we cannot find the true Nash equilibrium. Self-play actually <strong>approximates</strong> this equilibrium.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="fictitious-play">Fictitious Play<a href="#fictitious-play" class="hash-link" aria-label="Direct link to Fictitious Play" title="Direct link to Fictitious Play" translate="no">​</a></h3>
<p>Self-play is related to the game theory concept of <strong>fictitious play</strong>:</p>
<ol>
<li class="">Each player observes opponent&#x27;s historical strategies</li>
<li class="">Calculates average distribution of opponent strategies</li>
<li class="">Chooses best response against this average distribution</li>
</ol>
<p>Under certain conditions, fictitious play provably converges to Nash equilibrium.</p>
<p>AlphaGo&#x27;s self-play can be seen as a neural network implementation of this concept.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="mechanism-of-self-play">Mechanism of Self-Play<a href="#mechanism-of-self-play" class="hash-link" aria-label="Direct link to Mechanism of Self-Play" title="Direct link to Mechanism of Self-Play" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="basic-process">Basic Process<a href="#basic-process" class="hash-link" aria-label="Direct link to Basic Process" title="Direct link to Basic Process" translate="no">​</a></h3>
<p>AlphaGo&#x27;s self-play process:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Algorithm: Self-Play Training</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Initialize: Policy Network π_θ (can start from supervised learning or random initialization)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Repeat until convergence:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1. Generate game data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   For i = 1 to N (in parallel):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     a. Self-play one game with current policy π_θ</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     b. Collect trajectory: τ_i = (s_0, a_0, r_1, s_1, a_1, ...)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     c. Record final result z_i ∈ {-1, +1}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Update policy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   a. Compute policy gradient:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ∇J = (1/N) Σ_i Σ_t ∇_θ log π_θ(a_t|s_t) · z_i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   b. Update parameters: θ ← θ + α · ∇J</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Update value network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   a. Train Value Network with (s, z) pairs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   b. Minimize: L = E[(V_φ(s) - z)²]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">4. Optional: Evaluate and save checkpoint</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   a. Let new policy play against old versions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   b. If win rate &gt; 55%, update opponent pool</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="training-data-generation">Training Data Generation<a href="#training-data-generation" class="hash-link" aria-label="Direct link to Training Data Generation" title="Direct link to Training Data Generation" translate="no">​</a></h3>
<p>Each self-play game generates a <strong>trajectory</strong>:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>a</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>s</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>a</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>s</mi><mi>T</mi></msub><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose">)</span></span></span></span></p>
<p>Where:</p>
<ul>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">s_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>: Board state at time step <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></li>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>: Action chosen at time step <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></li>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span>: Final result (+1 win, -1 loss)</li>
</ul>
<p>A 200-move game generates 200 training samples. With hundreds of thousands of self-play games daily, training data volume is staggering.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="policy-update">Policy Update<a href="#policy-update" class="hash-link" aria-label="Direct link to Policy Update" title="Direct link to Policy Update" translate="no">​</a></h3>
<p>Update Policy Network using policy gradient:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>+</mo><mi>α</mi><mo>⋅</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi mathvariant="double-struck">E</mi><mrow><mo fence="true">[</mo><msub><mo>∑</mo><mi>t</mi></msub><mi>log</mi><mo>⁡</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>⋅</mo><mi>z</mi><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\theta \leftarrow \theta + \alpha \cdot \nabla_\theta \mathbb{E}\left[\sum_t \log \pi_\theta(a_t|s_t) \cdot z\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathbb">E</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">[</span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1308em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose delimcenter" style="top:0em">]</span></span></span></span></span></p>
<p>This update&#x27;s effect:</p>
<ul>
<li class="">If final win (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">z = +1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">+</span><span class="mord">1</span></span></span></span>): increase probability of all moves</li>
<li class="">If final loss (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">z = -1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">−</span><span class="mord">1</span></span></span></span>): decrease probability of all moves</li>
</ul>
<p>This seems crude - winning games may have some bad moves, losing games may have some good moves. But through statistics of many games, this &quot;noise&quot; averages out, and truly good moves are identified.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="value-network-training">Value Network Training<a href="#value-network-training" class="hash-link" aria-label="Direct link to Value Network Training" title="Direct link to Value Network Training" translate="no">​</a></h3>
<p>Value Network uses <strong>regression</strong> training:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mo>←</mo><mi>ϕ</mi><mo>−</mo><mi>β</mi><mo>⋅</mo><msub><mi mathvariant="normal">∇</mi><mi>ϕ</mi></msub><mi mathvariant="double-struck">E</mi><mrow><mo fence="true">[</mo><mo stretchy="false">(</mo><msub><mi>V</mi><mi>ϕ</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>−</mo><mi>z</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\phi \leftarrow \phi - \beta \cdot \nabla_\phi \mathbb{E}\left[(V_\phi(s) - z)^2\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">ϕ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">ϕ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord mathbb">E</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size1">[</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size1">]</span></span></span></span></span></span></p>
<p>This teaches Value Network to predict: from current position, what&#x27;s the probability of winning?</p>
<p>Value Network serves to:</p>
<ol>
<li class="">Provide leaf node evaluation in MCTS</li>
<li class="">Serve as baseline for policy gradient</li>
<li class="">Directly evaluate positions</li>
</ol>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="importance-of-randomization">Importance of Randomization<a href="#importance-of-randomization" class="hash-link" aria-label="Direct link to Importance of Randomization" title="Direct link to Importance of Randomization" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="avoiding-deterministic-loops">Avoiding Deterministic Loops<a href="#avoiding-deterministic-loops" class="hash-link" aria-label="Direct link to Avoiding Deterministic Loops" title="Direct link to Avoiding Deterministic Loops" translate="no">​</a></h3>
<p>If self-play is completely deterministic, it might loop:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Policy A always plays fixed opening</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Policy A vs Policy A always produces same game</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Only one game is repeatedly learned</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">AI cannot explore other possibilities</span><br></span></code></pre></div></div>
<p>This is why <strong>randomness</strong> is crucial in self-play.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="sources-of-randomization">Sources of Randomization<a href="#sources-of-randomization" class="hash-link" aria-label="Direct link to Sources of Randomization" title="Direct link to Sources of Randomization" translate="no">​</a></h3>
<p>Ways AlphaGo introduces randomness in self-play:</p>
<p><strong>1. Policy network itself is stochastic</strong></p>
<p>Policy Network outputs probability distribution, not deterministic choices:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a \sim \pi_\theta(a|s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span></p>
<p>Same position may yield different moves each time.</p>
<p><strong>2. Temperature parameter</strong></p>
<p>Use higher temperature during training for more diversity:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>τ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><msup><mo stretchy="false">)</mo><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>τ</mi></mrow></msup></mrow><mrow><msub><mo>∑</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mi mathvariant="normal">∣</mi><mi>s</mi><msup><mo stretchy="false">)</mo><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>τ</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\pi_\tau(a|s) = \frac{\pi_\theta(a|s)^{1/\tau}}{\sum_{a&#x27;} \pi_\theta(a&#x27;|s)^{1/\tau}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.1132em">τ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.7721em;vertical-align:-0.6104em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1617em"><span style="top:-2.6146em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2854em"><span style="top:-2.2854em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.6068em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8496em"><span style="top:-2.8496em;margin-right:0.1em"><span class="pstrut" style="height:2.5556em"></span><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3214em"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em"></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em"><span style="top:-2.786em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.822em"><span style="top:-2.822em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1/</span><span class="mord mathnormal mtight" style="margin-right:0.1132em">τ</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.485em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">a</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9667em"><span style="top:-2.9667em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1/</span><span class="mord mathnormal mtight" style="margin-right:0.1132em">τ</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6104em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<ul>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau &gt; 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>: More random, more exploration</li>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau &lt; 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>: More deterministic, more exploitation</li>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>: Original distribution</li>
</ul>
<p><strong>3. Dirichlet Noise</strong></p>
<p>AlphaGo Zero adds Dirichlet noise to root node prior probabilities during self-play:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>ε</mi><mo stretchy="false">)</mo><mo>⋅</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo><mo>+</mo><mi>ε</mi><mo>⋅</mo><msub><mi>η</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">P(s, a) = (1 - \varepsilon) \cdot \pi_\theta(a|s) + \varepsilon \cdot \eta_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">ε</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal">ε</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></p>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>∼</mo><mtext>Dir</mtext><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\eta \sim \text{Dir}(\alpha)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">Dir</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mclose">)</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ε</mi><mo>=</mo><mn>0.25</mn></mrow><annotation encoding="application/x-tex">\varepsilon = 0.25</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">ε</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.25</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.03</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.03</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.03</span></span></span></span> (for Go&#x27;s 361 actions).</p>
<p>This ensures even very low probability moves have chance to be explored.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="population-method">Population Method<a href="#population-method" class="hash-link" aria-label="Direct link to Population Method" title="Direct link to Population Method" translate="no">​</a></h3>
<p>Another way to increase diversity is maintaining an <strong>opponent pool</strong>:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Pool = [π_1, π_2, π_3, ..., π_k] (different policy versions)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Each game:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1. Randomly select opponent from pool</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Play against that opponent</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Update current policy with result</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">4. Periodically add improved policy to pool</span><br></span></code></pre></div></div>
<p>Benefits of this approach:</p>
<ul>
<li class=""><strong>Diversity</strong>: Different opponent styles</li>
<li class=""><strong>Stability</strong>: Avoid overfitting to specific opponent</li>
<li class=""><strong>Robustness</strong>: Learn to handle various strategies</li>
</ul>
<p>Both original AlphaGo and AlphaGo Zero used similar techniques.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="strength-growth-curve">Strength Growth Curve<a href="#strength-growth-curve" class="hash-link" aria-label="Direct link to Strength Growth Curve" title="Direct link to Strength Growth Curve" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="elo-rating-system">Elo Rating System<a href="#elo-rating-system" class="hash-link" aria-label="Direct link to Elo Rating System" title="Direct link to Elo Rating System" translate="no">​</a></h3>
<p>To track AI strength changes, AlphaGo uses the <strong>Elo rating system</strong>.</p>
<p>Elo system&#x27;s basic principle:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>A wins</mtext><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mn>10</mn><mrow><mo stretchy="false">(</mo><msub><mi>R</mi><mi>B</mi></msub><mo>−</mo><msub><mi>R</mi><mi>A</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mn>400</mn></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(\text{A wins}) = \frac{1}{1 + 10^{(R_B - R_A)/400}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">A wins</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.3331em;vertical-align:-0.488em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.5703em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mord mtight"><span class="mord mtight">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8853em"><span style="top:-2.8853em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3448em;margin-left:-0.0077em;margin-right:0.1em"><span class="pstrut" style="height:2.6833em"></span><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3385em"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3448em;margin-left:-0.0077em;margin-right:0.1em"><span class="pstrut" style="height:2.6833em"></span><span class="mord mathnormal mtight">A</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3385em"><span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mord mtight">/400</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.488em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">R_A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">R_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> are both sides&#x27; Elo scores.</p>
<ul>
<li class="">200 point difference: Stronger side expected to win 75%</li>
<li class="">400 point difference: Stronger side expected to win 90%</li>
<li class="">800 point difference: Stronger side expected to win 99%</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphagos-strength-growth">AlphaGo&#x27;s Strength Growth<a href="#alphagos-strength-growth" class="hash-link" aria-label="Direct link to AlphaGo&#x27;s Strength Growth" title="Direct link to AlphaGo&#x27;s Strength Growth" translate="no">​</a></h3>
<p>Let&#x27;s visualize strength growth of different AlphaGo versions:</p>
<div>載入中...</div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="growth-rate-analysis">Growth Rate Analysis<a href="#growth-rate-analysis" class="hash-link" aria-label="Direct link to Growth Rate Analysis" title="Direct link to Growth Rate Analysis" translate="no">​</a></h3>
<p>Several interesting phenomena observable from the curve:</p>
<p><strong>1. Rapid early growth</strong></p>
<p>In first few hours of training, AI learns basic rules and simple tactics. This is the <strong>low-hanging fruit</strong> phase - too many obvious mistakes to fix.</p>
<p><strong>2. Steady middle-phase growth</strong></p>
<p>As basic mistakes are eliminated, AI starts learning more sophisticated tactics and joseki. Growth rate slows but remains steady.</p>
<p><strong>3. Later growth slowdown</strong></p>
<p>When AI is already very strong, further improvement becomes difficult. May need to discover entirely new strategies, not just fix mistakes.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="moment-of-surpassing-humans">Moment of Surpassing Humans<a href="#moment-of-surpassing-humans" class="hash-link" aria-label="Direct link to Moment of Surpassing Humans" title="Direct link to Moment of Surpassing Humans" translate="no">​</a></h3>
<p>Key milestones in AlphaGo&#x27;s training curve:</p>
<table><thead><tr><th>Milestone</th><th>Equivalent to</th><th>Time to Achieve</th></tr></thead><tbody><tr><td>Surpass strong amateurs</td><td>Elo ~2700</td><td>~3 hours</td></tr><tr><td>Surpass Fan Hui</td><td>Elo ~3500</td><td>~36 hours</td></tr><tr><td>Surpass Lee Sedol</td><td>Elo ~4500</td><td>~60 hours</td></tr><tr><td>Surpass original AlphaGo</td><td>Elo ~5000</td><td>~72 hours</td></tr></tbody></table>
<p>These numbers (from AlphaGo Zero) are stunning: <strong>AI surpassed thousands of years of human Go wisdom in 3 days, starting from scratch.</strong></p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="convergence-analysis">Convergence Analysis<a href="#convergence-analysis" class="hash-link" aria-label="Direct link to Convergence Analysis" title="Direct link to Convergence Analysis" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="does-self-play-converge">Does Self-Play Converge?<a href="#does-self-play-converge" class="hash-link" aria-label="Direct link to Does Self-Play Converge?" title="Direct link to Does Self-Play Converge?" translate="no">​</a></h3>
<p>This is an important theoretical question. Short answer: <strong>Under certain conditions yes, but Go is too complex for rigorous proof.</strong></p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="theoretical-guarantees">Theoretical Guarantees<a href="#theoretical-guarantees" class="hash-link" aria-label="Direct link to Theoretical Guarantees" title="Direct link to Theoretical Guarantees" translate="no">​</a></h3>
<p>For simpler games (like tic-tac-toe), it can be proven:</p>
<ol>
<li class=""><strong>Existence</strong>: Nash equilibrium exists (Minimax theorem)</li>
<li class=""><strong>Convergence</strong>: Certain algorithms (like fictitious play) converge to Nash equilibrium</li>
</ol>
<p>For Go, we lack rigorous convergence guarantees, but experimental evidence shows:</p>
<ul>
<li class="">Strength continues to improve</li>
<li class="">No obvious oscillation or degradation</li>
<li class="">Final strength surpasses all known humans</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="possible-failure-modes">Possible Failure Modes<a href="#possible-failure-modes" class="hash-link" aria-label="Direct link to Possible Failure Modes" title="Direct link to Possible Failure Modes" translate="no">​</a></h3>
<p>Problems self-play might encounter:</p>
<p><strong>1. Strategy Cycling</strong></p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Strategy A beats Strategy B</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Strategy B beats Strategy C</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Strategy C beats Strategy A</span><br></span></code></pre></div></div>
<p>This does happen in some games (like rock-paper-scissors). But Go&#x27;s complexity seems to prevent pure cycling.</p>
<p><strong>2. Overfitting to Self</strong></p>
<p>AI might learn strategies only targeting its own style, unable to handle other styles&#x27; opponents. This is why AlphaGo plays against different versions of itself, and ultimately tests against human players.</p>
<p><strong>3. Local Optima</strong></p>
<p>AI might get stuck in local optima - a strategy that&#x27;s &quot;okay but not best.&quot; Randomization and massive game volume help avoid this.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="practical-observations">Practical Observations<a href="#practical-observations" class="hash-link" aria-label="Direct link to Practical Observations" title="Direct link to Practical Observations" translate="no">​</a></h3>
<p>Observations from AlphaGo&#x27;s training process:</p>
<ol>
<li class=""><strong>Continuous improvement</strong>: Elo score keeps rising with training</li>
<li class=""><strong>No degradation</strong>: No sudden strength drops</li>
<li class=""><strong>Style evolution</strong>: AI&#x27;s playing style gradually changes with training</li>
<li class=""><strong>New joseki discovered</strong>: AI discovers openings and tactics humans never used</li>
</ol>
<p>These observations show that while we lack theoretical guarantees, self-play is effective in practice.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="implementation-details">Implementation Details<a href="#implementation-details" class="hash-link" aria-label="Direct link to Implementation Details" title="Direct link to Implementation Details" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="parallel-self-play">Parallel Self-Play<a href="#parallel-self-play" class="hash-link" aria-label="Direct link to Parallel Self-Play" title="Direct link to Parallel Self-Play" translate="no">​</a></h3>
<p>To accelerate training, AlphaGo uses large-scale parallel self-play:</p>
<!-- -->
<p><strong>Key design decisions</strong>:</p>
<ul>
<li class=""><strong>Sync vs. Async</strong>: AlphaGo uses asynchronous updates, Workers don&#x27;t need to wait for each other</li>
<li class=""><strong>Update frequency</strong>: Update parameters every N games completed</li>
<li class=""><strong>Opponent selection</strong>: Randomly select one of recent versions as opponent</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="checkpoint-strategy">Checkpoint Strategy<a href="#checkpoint-strategy" class="hash-link" aria-label="Direct link to Checkpoint Strategy" title="Direct link to Checkpoint Strategy" translate="no">​</a></h3>
<p>Periodically save model checkpoints for:</p>
<ol>
<li class=""><strong>Opponent pool</strong>: Maintain different version opponents</li>
<li class=""><strong>Evaluation</strong>: Track strength changes</li>
<li class=""><strong>Failure recovery</strong>: Resume from interruption</li>
</ol>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Pseudocode</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">training_loop</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> iteration </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_iterations</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Generate game data</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        trajectories </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> parallel_self_play</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">current_policy</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_games</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1000</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Update policy</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        update_policy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">trajectories</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Periodic evaluation and save</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> iteration </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">100</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            elo </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> evaluate_against_pool</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">current_policy</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            save_checkpoint</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">current_policy</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> elo</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> elo </span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> best_elo</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                add_to_pool</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">current_policy</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                best_elo </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> elo</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="training-resource-requirements">Training Resource Requirements<a href="#training-resource-requirements" class="hash-link" aria-label="Direct link to Training Resource Requirements" title="Direct link to Training Resource Requirements" translate="no">​</a></h3>
<p>AlphaGo&#x27;s training scale is impressive:</p>
<table><thead><tr><th>Version</th><th>Hardware</th><th>Training Time</th><th>Self-Play Games</th></tr></thead><tbody><tr><td>AlphaGo Fan</td><td>176 GPU</td><td>Several months</td><td>~30M</td></tr><tr><td>AlphaGo Lee</td><td>48 TPU</td><td>Several weeks</td><td>~30M</td></tr><tr><td>AlphaGo Zero</td><td>4 TPU</td><td>3 days</td><td>~5M</td></tr><tr><td>AlphaGo Zero (40-day)</td><td>4 TPU</td><td>40 days</td><td>~30M</td></tr></tbody></table>
<p>Note AlphaGo Zero achieved stronger play with less hardware and shorter time - this is algorithmic efficiency improvement.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="hyperparameter-settings">Hyperparameter Settings<a href="#hyperparameter-settings" class="hash-link" aria-label="Direct link to Hyperparameter Settings" title="Direct link to Hyperparameter Settings" translate="no">​</a></h3>
<p>Some key hyperparameters:</p>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Self-play settings</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_PARALLEL_GAMES </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">5000</span><span class="token plain">      </span><span class="token comment" style="color:#999988;font-style:italic"># Concurrent games</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">GAMES_PER_ITERATION </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">25000</span><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Games per iteration</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">MCTS_SIMULATIONS </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1600</span><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># MCTS simulations per move</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Training settings</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">BATCH_SIZE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2048</span><span class="token plain">              </span><span class="token comment" style="color:#999988;font-style:italic"># Training batch size</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">LEARNING_RATE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.01</span><span class="token plain">           </span><span class="token comment" style="color:#999988;font-style:italic"># Initial learning rate</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">L2_REGULARIZATION </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1e-4</span><span class="token plain">       </span><span class="token comment" style="color:#999988;font-style:italic"># Weight decay</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Exploration settings</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">TEMPERATURE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.0</span><span class="token plain">              </span><span class="token comment" style="color:#999988;font-style:italic"># Temperature for first 30 moves</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">DIRICHLET_ALPHA </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.03</span><span class="token plain">         </span><span class="token comment" style="color:#999988;font-style:italic"># Dirichlet noise parameter</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">EXPLORATION_FRACTION </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.25</span><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Noise ratio</span><br></span></code></pre></div></div>
<p>These hyperparameters were tuned through extensive experimentation and significantly affect training results.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="self-play-variants">Self-Play Variants<a href="#self-play-variants" class="hash-link" aria-label="Direct link to Self-Play Variants" title="Direct link to Self-Play Variants" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="original-alphago">Original AlphaGo<a href="#original-alphago" class="hash-link" aria-label="Direct link to Original AlphaGo" title="Direct link to Original AlphaGo" translate="no">​</a></h3>
<p>Original AlphaGo&#x27;s training process:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. Supervised Learning (SL): Learn from human games</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   → Produces SL Policy Network (π_SL)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Reinforcement Learning (RL): Self-play</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Initialize π_RL = π_SL</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Opponent pool = [π_SL]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Repeat:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     a. π_RL plays against policies in pool</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     b. Update π_RL with policy gradient</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     c. If π_RL improves, add to pool</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   → Produces RL Policy Network (π_RL)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Value Network training:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Use π_RL self-play to generate positions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Train V(s) to predict win rate</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero">AlphaGo Zero<a href="#alphago-zero" class="hash-link" aria-label="Direct link to AlphaGo Zero" title="Direct link to AlphaGo Zero" translate="no">​</a></h3>
<p>AlphaGo Zero simplified this process:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. Pure self-play (no human data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Initialize random network f_θ</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Repeat:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     a. Self-play with MCTS + f_θ</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     b. Train policy head and value head together</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     c. Update f_θ</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   → Single network outputs both policy and value</span><br></span></code></pre></div></div>
<p>Key improvements:</p>
<ul>
<li class=""><strong>No human data</strong>: Start from scratch</li>
<li class=""><strong>Single network</strong>: Policy and value share features</li>
<li class=""><strong>Simpler training</strong>: End-to-end learning</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphazero">AlphaZero<a href="#alphazero" class="hash-link" aria-label="Direct link to AlphaZero" title="Direct link to AlphaZero" translate="no">​</a></h3>
<p>AlphaZero further generalized:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Same algorithm, different games:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Go: Surpasses AlphaGo Zero level</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Chess: Surpasses Stockfish</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Shogi: Surpasses Elmo</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Only game-specific part: Rule encoding</span><br></span></code></pre></div></div>
<p>This proves self-play is a <strong>general learning paradigm</strong>, not limited to Go.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="what-humans-learned">What Humans Learned<a href="#what-humans-learned" class="hash-link" aria-label="Direct link to What Humans Learned" title="Direct link to What Humans Learned" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="new-joseki-discovered-by-ai">New Joseki Discovered by AI<a href="#new-joseki-discovered-by-ai" class="hash-link" aria-label="Direct link to New Joseki Discovered by AI" title="Direct link to New Joseki Discovered by AI" translate="no">​</a></h3>
<p>Self-play produced many moves humans never used:</p>
<p><strong>1. Opening innovations</strong></p>
<p>Some openings AlphaGo prefers:</p>
<ul>
<li class="">3-3 invasion: Invading corner early</li>
<li class="">High plays: Traditionally considered &quot;unstable&quot;</li>
<li class="">Large avalanche variations: Humans thought too complex to calculate</li>
</ul>
<p><strong>2. New position evaluations</strong></p>
<p>AI&#x27;s evaluation of some positions differs greatly from humans:</p>
<ul>
<li class="">Some seemingly &quot;thin&quot; shapes are actually solid</li>
<li class="">Some &quot;thickness&quot; value is overestimated</li>
<li class="">Reassessment of &quot;sente&quot; and &quot;gote&quot;</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="impact-on-human-go">Impact on Human Go<a href="#impact-on-human-go" class="hash-link" aria-label="Direct link to Impact on Human Go" title="Direct link to Impact on Human Go" translate="no">​</a></h3>
<p>After AlphaGo, professional Go changed significantly:</p>
<ol>
<li class=""><strong>Opening diversification</strong>: Professionals began using AI-discovered openings</li>
<li class=""><strong>Training methods changed</strong>: AI became main training tool for professionals</li>
<li class=""><strong>Go theory rethought</strong>: Many traditional &quot;principles&quot; questioned and revised</li>
<li class=""><strong>New aesthetics</strong>: Began appreciating AI-style play</li>
</ol>
<p>Ke Jie said after losing to AlphaGo:</p>
<blockquote>
<p>&quot;AlphaGo made me understand Go anew. I thought humans understood Go, now I know we only scratched the surface.&quot;</p>
</blockquote>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="philosophical-reflection">Philosophical Reflection<a href="#philosophical-reflection" class="hash-link" aria-label="Direct link to Philosophical Reflection" title="Direct link to Philosophical Reflection" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="nature-of-learning">Nature of Learning<a href="#nature-of-learning" class="hash-link" aria-label="Direct link to Nature of Learning" title="Direct link to Nature of Learning" translate="no">​</a></h3>
<p>Self-play raises profound questions about learning:</p>
<p><strong>Where does knowledge come from?</strong></p>
<ul>
<li class="">Human learning depends on external information (teachers, books, experience)</li>
<li class="">Self-play AI only has rules, no external knowledge</li>
<li class="">Yet it can &quot;discover&quot; knowledge - where does this knowledge come from?</li>
</ul>
<p>The answer might be: <strong>Knowledge is implicit in game rules and structure.</strong> Go&#x27;s rules define what&#x27;s good and bad play; self-play just reveals these implicit structures.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="creativity-and-discovery">Creativity and Discovery<a href="#creativity-and-discovery" class="hash-link" aria-label="Direct link to Creativity and Discovery" title="Direct link to Creativity and Discovery" translate="no">​</a></h3>
<p>When AI plays &quot;divine move&quot; (Move 37), is that creation or discovery?</p>
<p>One view: That move always &quot;existed&quot; in Go&#x27;s rules; AI just &quot;discovered&quot; it.
Another view: AI &quot;created&quot; that move, since no one (including AI itself) knew it beforehand.</p>
<p>This question has no standard answer, but it challenges our traditional understanding of creativity.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="place-of-human-intelligence">Place of Human Intelligence<a href="#place-of-human-intelligence" class="hash-link" aria-label="Direct link to Place of Human Intelligence" title="Direct link to Place of Human Intelligence" translate="no">​</a></h3>
<p>If AI can start from scratch and surpass thousands of years of human wisdom through self-play, what does this mean for humans?</p>
<p>Optimistic view:</p>
<ul>
<li class="">AI is a tool created by humans</li>
<li class="">AI&#x27;s discoveries can enhance human understanding</li>
<li class="">Humans can collaborate with AI to reach higher levels</li>
</ul>
<p>Cautious view:</p>
<ul>
<li class="">In some domains, pure computation may surpass human intuition</li>
<li class="">Need to rethink value of &quot;expertise&quot;</li>
<li class="">Education and training methods may need to change</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="animation-reference">Animation Reference<a href="#animation-reference" class="hash-link" aria-label="Direct link to Animation Reference" title="Direct link to Animation Reference" translate="no">​</a></h2>
<p>Core concepts covered in this article with animation numbers:</p>
<table><thead><tr><th>Number</th><th>Concept</th><th>Physics/Math Correspondence</th></tr></thead><tbody><tr><td>Animation E5</td><td>Self-play loop</td><td>Fixed-point iteration</td></tr><tr><td>Animation E6</td><td>Strategy evolution</td><td>Evolutionary dynamics</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary" translate="no">​</a></h2>
<p>Self-play is one of AlphaGo&#x27;s key technologies. We learned:</p>
<ol>
<li class=""><strong>Why it works</strong>: Adversarial learning, progressive weakness discovery</li>
<li class=""><strong>Mechanism</strong>: Trajectory collection, policy gradient, value network training</li>
<li class=""><strong>Randomization</strong>: Temperature parameter, Dirichlet noise, opponent pool</li>
<li class=""><strong>Strength growth</strong>: Elo system, growth curve analysis</li>
<li class=""><strong>Convergence</strong>: Theoretical guarantees and practical observations</li>
<li class=""><strong>Implementation details</strong>: Parallel training, checkpoint strategy, hyperparameters</li>
</ol>
<p>In the next article, we&#x27;ll explore how AlphaGo combines neural networks with MCTS, leveraging both strengths.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="further-reading">Further Reading<a href="#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading" translate="no">​</a></h2>
<ul>
<li class=""><strong>Next</strong>: <a class="" href="/en/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/">MCTS and Neural Network Integration</a> - Perfect combination of intuition and reasoning</li>
<li class=""><strong>Previous</strong>: <a class="" href="/en/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/">Introduction to Reinforcement Learning</a> - Basic RL concepts</li>
<li class=""><strong>Related</strong>: <a class="" href="/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/">AlphaGo Zero Overview</a> - Breakthrough starting from scratch</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2016). &quot;Mastering the game of Go with deep neural networks and tree search.&quot; <em>Nature</em>, 529, 484-489.</li>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">Silver, D., et al. (2018). &quot;A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.&quot; <em>Science</em>, 362(6419), 1140-1144.</li>
<li class="">Heinrich, J., &amp; Silver, D. (2016). &quot;Deep Reinforcement Learning from Self-Play in Imperfect-Information Games.&quot; <em>arXiv preprint</em>.</li>
<li class="">Lanctot, M., et al. (2017). &quot;A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.&quot; <em>NeurIPS</em>.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/13-self-play.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Introduction to Reinforcement Learning</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">MCTS and Neural Network Integration</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#why-does-self-play-work" class="table-of-contents__link toc-highlight">Why Does Self-Play Work?</a><ul><li><a href="#intuitive-explanation" class="table-of-contents__link toc-highlight">Intuitive Explanation</a></li><li><a href="#progressive-discovery-of-weaknesses" class="table-of-contents__link toc-highlight">Progressive Discovery of Weaknesses</a></li><li><a href="#adversarial-learning" class="table-of-contents__link toc-highlight">Adversarial Learning</a></li><li><a href="#comparison-with-human-games" class="table-of-contents__link toc-highlight">Comparison with Human Games</a></li></ul></li><li><a href="#game-theory-perspective" class="table-of-contents__link toc-highlight">Game Theory Perspective</a><ul><li><a href="#nash-equilibrium" class="table-of-contents__link toc-highlight">Nash Equilibrium</a></li><li><a href="#self-play-and-nash-equilibrium" class="table-of-contents__link toc-highlight">Self-Play and Nash Equilibrium</a></li><li><a href="#fictitious-play" class="table-of-contents__link toc-highlight">Fictitious Play</a></li></ul></li><li><a href="#mechanism-of-self-play" class="table-of-contents__link toc-highlight">Mechanism of Self-Play</a><ul><li><a href="#basic-process" class="table-of-contents__link toc-highlight">Basic Process</a></li><li><a href="#training-data-generation" class="table-of-contents__link toc-highlight">Training Data Generation</a></li><li><a href="#policy-update" class="table-of-contents__link toc-highlight">Policy Update</a></li><li><a href="#value-network-training" class="table-of-contents__link toc-highlight">Value Network Training</a></li></ul></li><li><a href="#importance-of-randomization" class="table-of-contents__link toc-highlight">Importance of Randomization</a><ul><li><a href="#avoiding-deterministic-loops" class="table-of-contents__link toc-highlight">Avoiding Deterministic Loops</a></li><li><a href="#sources-of-randomization" class="table-of-contents__link toc-highlight">Sources of Randomization</a></li><li><a href="#population-method" class="table-of-contents__link toc-highlight">Population Method</a></li></ul></li><li><a href="#strength-growth-curve" class="table-of-contents__link toc-highlight">Strength Growth Curve</a><ul><li><a href="#elo-rating-system" class="table-of-contents__link toc-highlight">Elo Rating System</a></li><li><a href="#alphagos-strength-growth" class="table-of-contents__link toc-highlight">AlphaGo&#39;s Strength Growth</a></li><li><a href="#growth-rate-analysis" class="table-of-contents__link toc-highlight">Growth Rate Analysis</a></li><li><a href="#moment-of-surpassing-humans" class="table-of-contents__link toc-highlight">Moment of Surpassing Humans</a></li></ul></li><li><a href="#convergence-analysis" class="table-of-contents__link toc-highlight">Convergence Analysis</a><ul><li><a href="#does-self-play-converge" class="table-of-contents__link toc-highlight">Does Self-Play Converge?</a></li><li><a href="#theoretical-guarantees" class="table-of-contents__link toc-highlight">Theoretical Guarantees</a></li><li><a href="#possible-failure-modes" class="table-of-contents__link toc-highlight">Possible Failure Modes</a></li><li><a href="#practical-observations" class="table-of-contents__link toc-highlight">Practical Observations</a></li></ul></li><li><a href="#implementation-details" class="table-of-contents__link toc-highlight">Implementation Details</a><ul><li><a href="#parallel-self-play" class="table-of-contents__link toc-highlight">Parallel Self-Play</a></li><li><a href="#checkpoint-strategy" class="table-of-contents__link toc-highlight">Checkpoint Strategy</a></li><li><a href="#training-resource-requirements" class="table-of-contents__link toc-highlight">Training Resource Requirements</a></li><li><a href="#hyperparameter-settings" class="table-of-contents__link toc-highlight">Hyperparameter Settings</a></li></ul></li><li><a href="#self-play-variants" class="table-of-contents__link toc-highlight">Self-Play Variants</a><ul><li><a href="#original-alphago" class="table-of-contents__link toc-highlight">Original AlphaGo</a></li><li><a href="#alphago-zero" class="table-of-contents__link toc-highlight">AlphaGo Zero</a></li><li><a href="#alphazero" class="table-of-contents__link toc-highlight">AlphaZero</a></li></ul></li><li><a href="#what-humans-learned" class="table-of-contents__link toc-highlight">What Humans Learned</a><ul><li><a href="#new-joseki-discovered-by-ai" class="table-of-contents__link toc-highlight">New Joseki Discovered by AI</a></li><li><a href="#impact-on-human-go" class="table-of-contents__link toc-highlight">Impact on Human Go</a></li></ul></li><li><a href="#philosophical-reflection" class="table-of-contents__link toc-highlight">Philosophical Reflection</a><ul><li><a href="#nature-of-learning" class="table-of-contents__link toc-highlight">Nature of Learning</a></li><li><a href="#creativity-and-discovery" class="table-of-contents__link toc-highlight">Creativity and Discovery</a></li><li><a href="#place-of-human-intelligence" class="table-of-contents__link toc-highlight">Place of Human Intelligence</a></li></ul></li><li><a href="#animation-reference" class="table-of-contents__link toc-highlight">Animation Reference</a></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li><li><a href="#further-reading" class="table-of-contents__link toc-highlight">Further Reading</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>