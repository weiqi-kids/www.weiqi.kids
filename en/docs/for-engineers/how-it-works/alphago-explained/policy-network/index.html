<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-for-engineers/how-it-works/alphago-explained/policy-network" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Policy Network Deep Dive | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/en/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/en/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/policy-network/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="圍棋, Go, 好棋寶寶, AI, KataGo, AlphaGo, 圍棋教學, 圍棋入門"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Policy Network Deep Dive | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="Deep understanding of AlphaGo&#x27;s policy network architecture, training methods, and practical applications, from 13 convolutional layers to Softmax output"><meta data-rh="true" property="og:description" content="Deep understanding of AlphaGo&#x27;s policy network architecture, training methods, and practical applications, from 13 convolutional layers to Softmax output"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/policy-network/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/policy-network/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/for-engineers/how-it-works/alphago-explained/policy-network/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/for-engineers/how-it-works/alphago-explained/policy-network/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/policy-network/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/for-engineers/how-it-works/alphago-explained/policy-network/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/for-engineers/how-it-works/alphago-explained/policy-network/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/for-engineers/how-it-works/alphago-explained/policy-network/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/for-engineers/how-it-works/alphago-explained/policy-network/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/for-engineers/how-it-works/alphago-explained/policy-network/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/for-engineers/how-it-works/alphago-explained/policy-network/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/for-engineers/how-it-works/alphago-explained/policy-network/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/policy-network/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"給工程師的圍棋 AI 指南","item":"https://www.weiqi.kids/en/docs/for-engineers/"},{"@type":"ListItem","position":2,"name":"一篇文章搞懂圍棋 AI","item":"https://www.weiqi.kids/en/docs/for-engineers/how-it-works/"},{"@type":"ListItem","position":3,"name":"AlphaGo 完整解析","item":"https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/"},{"@type":"ListItem","position":4,"name":"Policy Network Deep Dive","item":"https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/policy-network"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.f23bf74b.css">
<script src="/en/assets/js/runtime~main.51eae05f.js" defer="defer"></script>
<script src="/en/assets/js/main.74a0125b.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/en/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/logo.svg" alt="Good Go Baby Association Logo" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/en/img/logo.svg" alt="Good Go Baby Association Logo" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">Weiqi.Kids</b></a><a class="navbar__item navbar__link" href="/en/docs/for-players/">For Go Players</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/docs/for-engineers/">For Engineers</a><a class="navbar__item navbar__link" href="/en/docs/about/">About Us</a><a class="navbar__item navbar__link" href="/en/docs/activities/">Activities</a><a class="navbar__item navbar__link" href="/en/docs/references/">References</a><a class="navbar__item navbar__link" href="/en/docs/sop/">Standard Procedures</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/docs/for-engineers/how-it-works/alphago-explained/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/for-engineers/how-it-works/alphago-explained/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/for-engineers/how-it-works/alphago-explained/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/for-engineers/how-it-works/alphago-explained/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/ja/docs/for-engineers/how-it-works/alphago-explained/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/for-engineers/how-it-works/alphago-explained/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/for-engineers/how-it-works/alphago-explained/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/for-engineers/how-it-works/alphago-explained/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/for-engineers/how-it-works/alphago-explained/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/for-engineers/how-it-works/alphago-explained/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/for-engineers/how-it-works/alphago-explained/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/docs/intro/"><span title="User Guide" class="linkLabel_REp1">User Guide</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/about/"><span title="關於協會" class="categoryLinkLabel_ezQx">關於協會</span></a><button aria-label="Expand sidebar category &#x27;關於協會&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/activities/"><span title="活動實績" class="categoryLinkLabel_ezQx">活動實績</span></a><button aria-label="Expand sidebar category &#x27;活動實績&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/for-players/"><span title="For Go Players" class="categoryLinkLabel_ezQx">For Go Players</span></a><button aria-label="Expand sidebar category &#x27;For Go Players&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/references/"><span title="參考資料" class="categoryLinkLabel_ezQx">參考資料</span></a><button aria-label="Expand sidebar category &#x27;參考資料&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/sop/"><span title="標準作業流程" class="categoryLinkLabel_ezQx">標準作業流程</span></a><button aria-label="Expand sidebar category &#x27;標準作業流程&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/en/docs/for-engineers/"><span title="給工程師的圍棋 AI 指南" class="categoryLinkLabel_ezQx">給工程師的圍棋 AI 指南</span></a><button aria-label="Collapse sidebar category &#x27;給工程師的圍棋 AI 指南&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/en/docs/for-engineers/deep-dive/"><span title="For Deep Learners" class="categoryLinkLabel_ezQx">For Deep Learners</span></a><button aria-label="Expand sidebar category &#x27;For Deep Learners&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/en/docs/for-engineers/hands-on/"><span title="30 分鐘跑起第一個圍棋 AI" class="categoryLinkLabel_ezQx">30 分鐘跑起第一個圍棋 AI</span></a><button aria-label="Expand sidebar category &#x27;30 分鐘跑起第一個圍棋 AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/en/docs/for-engineers/how-it-works/"><span title="一篇文章搞懂圍棋 AI" class="categoryLinkLabel_ezQx">一篇文章搞懂圍棋 AI</span></a><button aria-label="Collapse sidebar category &#x27;一篇文章搞懂圍棋 AI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/"><span title="AlphaGo 完整解析" class="categoryLinkLabel_ezQx">AlphaGo 完整解析</span></a><button aria-label="Collapse sidebar category &#x27;AlphaGo 完整解析&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/birth-of-alphago/"><span title="The Birth of AlphaGo" class="linkLabel_REp1">The Birth of AlphaGo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/key-matches/"><span title="Key Matches Review" class="linkLabel_REp1">Key Matches Review</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/move-37/"><span title="Deep Analysis of &quot;The Divine Move&quot;" class="linkLabel_REp1">Deep Analysis of &quot;The Divine Move&quot;</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/why-go-is-hard/"><span title="Why Is Go Hard?" class="linkLabel_REp1">Why Is Go Hard?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/traditional-limits/"><span title="Limits of Traditional Methods" class="linkLabel_REp1">Limits of Traditional Methods</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/board-representation/"><span title="Board State Representation" class="linkLabel_REp1">Board State Representation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/policy-network/"><span title="Policy Network Deep Dive" class="linkLabel_REp1">Policy Network Deep Dive</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/value-network/"><span title="Value Network Explained" class="linkLabel_REp1">Value Network Explained</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/input-features/"><span title="Input Feature Design" class="linkLabel_REp1">Input Feature Design</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/cnn-and-go/"><span title="CNN and Go" class="linkLabel_REp1">CNN and Go</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/supervised-learning/"><span title="Supervised Learning Phase" class="linkLabel_REp1">Supervised Learning Phase</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/"><span title="Introduction to Reinforcement Learning" class="linkLabel_REp1">Introduction to Reinforcement Learning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/self-play/"><span title="Self-Play" class="linkLabel_REp1">Self-Play</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/"><span title="MCTS and Neural Network Integration" class="linkLabel_REp1">MCTS and Neural Network Integration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/puct-formula/"><span title="PUCT Formula Explained" class="linkLabel_REp1">PUCT Formula Explained</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><span title="AlphaGo Zero Overview" class="linkLabel_REp1">AlphaGo Zero Overview</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/"><span title="Dual-Head Network and Residual Network" class="linkLabel_REp1">Dual-Head Network and Residual Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/training-from-scratch/"><span title="Training from Scratch" class="linkLabel_REp1">Training from Scratch</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/distributed-systems/"><span title="Distributed Systems and TPU" class="linkLabel_REp1">Distributed Systems and TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/legacy-and-impact/"><span title="AlphaGo&#x27;s Legacy" class="linkLabel_REp1">AlphaGo&#x27;s Legacy</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/katago-innovations/"><span title="KataGo 的關鍵創新" class="linkLabel_REp1">KataGo 的關鍵創新</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/concepts/"><span title="概念速查表" class="linkLabel_REp1">概念速查表</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/en/docs/for-engineers/industry/"><span title="圍棋 AI 產業現況" class="categoryLinkLabel_ezQx">圍棋 AI 產業現況</span></a><button aria-label="Expand sidebar category &#x27;圍棋 AI 產業現況&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/en/docs/for-engineers/overview/"><span title="圍棋 AI 能做什麼？" class="categoryLinkLabel_ezQx">圍棋 AI 能做什麼？</span></a><button aria-label="Expand sidebar category &#x27;圍棋 AI 能做什麼？&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/en/docs/for-engineers/"><span>給工程師的圍棋 AI 指南</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/en/docs/for-engineers/how-it-works/"><span>一篇文章搞懂圍棋 AI</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/en/docs/for-engineers/how-it-works/alphago-explained/"><span>AlphaGo 完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Policy Network Deep Dive</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Policy Network Deep Dive</h1></header>
<p>In any Go position, there are on average 250 legal moves. If a computer chose randomly, it would never play good moves.</p>
<p>AlphaGo&#x27;s breakthrough was this: it learned to &quot;glance at the board and know which positions are worth considering.&quot;</p>
<p>This ability comes from the <strong>Policy Network</strong>.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="what-is-the-policy-network">What is the Policy Network?<a href="#what-is-the-policy-network" class="hash-link" aria-label="Direct link to What is the Policy Network?" title="Direct link to What is the Policy Network?" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="core-function">Core Function<a href="#core-function" class="hash-link" aria-label="Direct link to Core Function" title="Direct link to Core Function" translate="no">​</a></h3>
<p>The Policy Network is a deep convolutional neural network with the task of:</p>
<blockquote>
<p><strong>Given the current board state, output the probability of playing at each position</strong></p>
</blockquote>
<p>In mathematical terms:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">p = f_θ(s)</span><br></span></code></pre></div></div>
<p>Where:</p>
<ul>
<li class=""><code>s</code>: Current board state (19×19 board + other features)</li>
<li class=""><code>f_θ</code>: Policy Network (θ is the network parameters)</li>
<li class=""><code>p</code>: Probability distribution over 361 positions (including pass)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="intuitive-understanding">Intuitive Understanding<a href="#intuitive-understanding" class="hash-link" aria-label="Direct link to Intuitive Understanding" title="Direct link to Intuitive Understanding" translate="no">​</a></h3>
<p>Imagine you&#x27;re a professional player. When you see a position, your brain automatically &quot;lights up&quot; several important locations—these are the points you intuitively consider worth examining.</p>
<p>The Policy Network simulates this process.</p>
<div>載入中...</div>
<p>The heatmap above shows the Policy Network&#x27;s output. Brighter positions are what the model considers more worth playing.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="why-do-we-need-a-policy-network">Why Do We Need a Policy Network?<a href="#why-do-we-need-a-policy-network" class="hash-link" aria-label="Direct link to Why Do We Need a Policy Network?" title="Direct link to Why Do We Need a Policy Network?" translate="no">​</a></h3>
<p>Go&#x27;s search space is too large. If we search all possible moves without filtering:</p>
<table><thead><tr><th>Strategy</th><th>Moves considered per turn</th><th>Nodes for 10-move search</th></tr></thead><tbody><tr><td>Consider all</td><td>361</td><td>361^10 ≈ 10^25</td></tr><tr><td>Policy Network filtering</td><td>~20</td><td>20^10 ≈ 10^13</td></tr></tbody></table>
<p>The Policy Network reduces the search space by <strong>10^12 times</strong> (one trillion times).</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="network-architecture">Network Architecture<a href="#network-architecture" class="hash-link" aria-label="Direct link to Network Architecture" title="Direct link to Network Architecture" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="overall-structure">Overall Structure<a href="#overall-structure" class="hash-link" aria-label="Direct link to Overall Structure" title="Direct link to Overall Structure" translate="no">​</a></h3>
<p>AlphaGo&#x27;s Policy Network uses a deep convolutional neural network (CNN) architecture:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Input layer → Conv layers ×12 → Output conv layer → Softmax</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     ↓            ↓                  ↓                ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">19×19×48      19×19×192          19×19×1         362 probabilities</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="input-layer">Input Layer<a href="#input-layer" class="hash-link" aria-label="Direct link to Input Layer" title="Direct link to Input Layer" translate="no">​</a></h3>
<p>Input is a <strong>19×19×48</strong> feature tensor:</p>
<ul>
<li class=""><strong>19×19</strong>: Board size</li>
<li class=""><strong>48</strong>: 48 feature planes (see <a class="" href="/en/docs/for-engineers/how-it-works/alphago-explained/input-features/">Input Features Design</a>)</li>
</ul>
<p>These 48 planes include:</p>
<ul>
<li class="">Black stone positions, white stone positions</li>
<li class="">History of the last 8 moves</li>
<li class="">Liberties, atari, ladder features</li>
<li class="">Legality (which positions can be played)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="convolutional-layers">Convolutional Layers<a href="#convolutional-layers" class="hash-link" aria-label="Direct link to Convolutional Layers" title="Direct link to Convolutional Layers" translate="no">​</a></h3>
<p>The network contains <strong>12 convolutional layers</strong>, each with this configuration:</p>
<table><thead><tr><th>Parameter</th><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>Number of filters</td><td>192</td><td>Each layer outputs 192 feature maps</td></tr><tr><td>Kernel size</td><td>3×3 (5×5 for first layer)</td><td>Each convolution looks at a 3×3 region</td></tr><tr><td>Padding</td><td>same</td><td>Maintains 19×19 size</td></tr><tr><td>Activation</td><td>ReLU</td><td>max(0, x)</td></tr></tbody></table>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="why-192-filters">Why 192 Filters?<a href="#why-192-filters" class="hash-link" aria-label="Direct link to Why 192 Filters?" title="Direct link to Why 192 Filters?" translate="no">​</a></h4>
<p>This is an empirical value. Too few limits model capacity, too many increases computation and overfitting risk. The DeepMind team determined through experiments that 192 is a good balance point.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="why-33-kernels">Why 3×3 Kernels?<a href="#why-33-kernels" class="hash-link" aria-label="Direct link to Why 3×3 Kernels?" title="Direct link to Why 3×3 Kernels?" translate="no">​</a></h4>
<p>3×3 is the most common size in CNNs, because:</p>
<ol>
<li class=""><strong>Sufficient to capture local patterns</strong>: Go patterns like eyes, connections, and cuts all fit within 3×3 regions</li>
<li class=""><strong>Computationally efficient</strong>: Compared to larger kernels, 3×3 has fewer parameters</li>
<li class=""><strong>Stackable</strong>: Multiple 3×3 convolutions can achieve a large receptive field</li>
</ol>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="why-55-for-the-first-layer">Why 5×5 for the First Layer?<a href="#why-55-for-the-first-layer" class="hash-link" aria-label="Direct link to Why 5×5 for the First Layer?" title="Direct link to Why 5×5 for the First Layer?" translate="no">​</a></h4>
<p>The first layer uses a larger 5×5 kernel to capture slightly larger patterns (like knight&#x27;s moves, one-point jumps) at the input layer. This is a design choice; later, AlphaGo Zero unified to use 3×3 throughout.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="relu-activation-function">ReLU Activation Function<a href="#relu-activation-function" class="hash-link" aria-label="Direct link to ReLU Activation Function" title="Direct link to ReLU Activation Function" translate="no">​</a></h3>
<p>Each convolutional layer is followed by a ReLU (Rectified Linear Unit) activation function:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">ReLU(x) = max(0, x)</span><br></span></code></pre></div></div>
<p>Why use ReLU?</p>
<ol>
<li class=""><strong>Simple computation</strong>: Just taking the maximum, much faster than sigmoid</li>
<li class=""><strong>Mitigates vanishing gradient</strong>: Gradient is always 1 in the positive region</li>
<li class=""><strong>Sparse activation</strong>: Negative values become zero, creating sparse representations</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="output-layer">Output Layer<a href="#output-layer" class="hash-link" aria-label="Direct link to Output Layer" title="Direct link to Output Layer" translate="no">​</a></h3>
<p>The final layer is a special convolutional layer:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">19×19×192 → Conv(1×1, 1 filter) → 19×19×1 → Flatten → 362-dim vector → Softmax</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="11-convolution">1×1 Convolution<a href="#11-convolution" class="hash-link" aria-label="Direct link to 1×1 Convolution" title="Direct link to 1×1 Convolution" translate="no">​</a></h4>
<p>The output layer uses 1×1 convolution to compress 192 channels into 1. This is equivalent to a linear combination of the 192-dimensional features at each position.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="softmax-output">Softmax Output<a href="#softmax-output" class="hash-link" aria-label="Direct link to Softmax Output" title="Direct link to Softmax Output" translate="no">​</a></h4>
<p>The 362-dimensional vector (361 board positions + 1 pass) goes through the Softmax function:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Softmax(z_i) = exp(z_i) / Σ_j exp(z_j)</span><br></span></code></pre></div></div>
<p>Softmax ensures the output is a valid probability distribution:</p>
<ul>
<li class="">All values are between 0 and 1</li>
<li class="">All values sum to 1</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="parameter-count">Parameter Count<a href="#parameter-count" class="hash-link" aria-label="Direct link to Parameter Count" title="Direct link to Parameter Count" translate="no">​</a></h3>
<p>Let&#x27;s calculate the total number of parameters:</p>
<table><thead><tr><th>Layer</th><th>Calculation</th><th>Parameters</th></tr></thead><tbody><tr><td>First conv layer</td><td>5×5×48×192 + 192</td><td>230,592</td></tr><tr><td>Middle conv layers ×11</td><td>(3×3×192×192 + 192) × 11</td><td>3,633,792</td></tr><tr><td>Output conv layer</td><td>1×1×192×1 + 1</td><td>193</td></tr><tr><td><strong>Total</strong></td><td></td><td><strong>~3.9M</strong></td></tr></tbody></table>
<p>Approximately <strong>3.9 million parameters</strong>, which by today&#x27;s standards is a small network.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="training-objective-and-methods">Training Objective and Methods<a href="#training-objective-and-methods" class="hash-link" aria-label="Direct link to Training Objective and Methods" title="Direct link to Training Objective and Methods" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="training-data">Training Data<a href="#training-data" class="hash-link" aria-label="Direct link to Training Data" title="Direct link to Training Data" translate="no">​</a></h3>
<p>The Policy Network uses <strong>supervised learning</strong>, learning from human game records.</p>
<p>Data sources:</p>
<ul>
<li class=""><strong>KGS Go Server</strong>: Games from amateur and professional players</li>
<li class=""><strong>About 30 million positions</strong>: Sampled from 160,000 games</li>
<li class=""><strong>Labels</strong>: The human&#x27;s next move for each position</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="cross-entropy-loss-function">Cross-Entropy Loss Function<a href="#cross-entropy-loss-function" class="hash-link" aria-label="Direct link to Cross-Entropy Loss Function" title="Direct link to Cross-Entropy Loss Function" translate="no">​</a></h3>
<p>The training objective is to maximize the probability of predicting human moves. Using cross-entropy loss:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L(θ) = -Σ log p_θ(a | s)</span><br></span></code></pre></div></div>
<p>Where:</p>
<ul>
<li class=""><code>s</code>: Board state</li>
<li class=""><code>a</code>: Position where the human actually played</li>
<li class=""><code>p_θ(a | s)</code>: Model&#x27;s predicted probability for that position</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="intuitive-understanding-1">Intuitive Understanding<a href="#intuitive-understanding-1" class="hash-link" aria-label="Direct link to Intuitive Understanding" title="Direct link to Intuitive Understanding" translate="no">​</a></h4>
<p>Cross-entropy loss has a simple meaning:</p>
<blockquote>
<p><strong>When the model predicts higher probability for the correct position, the loss is lower</strong></p>
</blockquote>
<p>If a human plays at K10, and the model&#x27;s probability for K10 is:</p>
<ul>
<li class="">0.9 → Loss = -log(0.9) ≈ 0.1 (very low, good)</li>
<li class="">0.1 → Loss = -log(0.1) ≈ 2.3 (high, bad)</li>
<li class="">0.01 → Loss = -log(0.01) ≈ 4.6 (very high, very bad)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="training-process">Training Process<a href="#training-process" class="hash-link" aria-label="Direct link to Training Process" title="Direct link to Training Process" translate="no">​</a></h3>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Pseudocode</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> epoch </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_epochs</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> batch </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> dataloader</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        states</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> actions </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> batch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Forward pass</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> network</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">states</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 361-dimensional probability vector</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Calculate loss (cross-entropy)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cross_entropy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">policy</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> actions</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Backward pass</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        loss</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">backward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p>Training details:</p>
<ul>
<li class=""><strong>Optimizer</strong>: SGD with momentum</li>
<li class=""><strong>Learning rate</strong>: Initial 0.003, gradually decayed</li>
<li class=""><strong>Batch size</strong>: 16</li>
<li class=""><strong>Training time</strong>: About 3 weeks (50 GPUs)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="data-augmentation">Data Augmentation<a href="#data-augmentation" class="hash-link" aria-label="Direct link to Data Augmentation" title="Direct link to Data Augmentation" translate="no">​</a></h3>
<p>The Go board has 8-fold symmetry (4 rotations × 2 reflections). Each training sample can be transformed into 8 equivalent samples:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Original → Rotate 90° → Rotate 180° → Rotate 270°</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ↓          ↓            ↓            ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Flip horizontal → ...</span><br></span></code></pre></div></div>
<p>This increases effective training data by 8×, and ensures the model learns patterns that don&#x27;t depend on orientation.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="training-results">Training Results<a href="#training-results" class="hash-link" aria-label="Direct link to Training Results" title="Direct link to Training Results" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="57-accuracy">57% Accuracy<a href="#57-accuracy" class="hash-link" aria-label="Direct link to 57% Accuracy" title="Direct link to 57% Accuracy" translate="no">​</a></h3>
<p>After training, the Policy Network achieved <strong>57% top-1 accuracy</strong>.</p>
<p>This means: Given any position, the model has a 57% chance of predicting the exact move the human expert played.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="is-this-accuracy-high">Is This Accuracy High?<a href="#is-this-accuracy-high" class="hash-link" aria-label="Direct link to Is This Accuracy High?" title="Direct link to Is This Accuracy High?" translate="no">​</a></h4>
<p>Considering that each position has on average 250 legal moves, random guessing has only 0.4% accuracy.</p>
<table><thead><tr><th>Method</th><th>Top-1 Accuracy</th></tr></thead><tbody><tr><td>Random guessing</td><td>0.4%</td></tr><tr><td>Previous strongest computer Go</td><td>~44%</td></tr><tr><td>AlphaGo Policy Network</td><td><strong>57%</strong></td></tr></tbody></table>
<p>A 13 percentage point improvement may not seem like much, but it&#x27;s highly significant.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="playing-strength-improvement">Playing Strength Improvement<a href="#playing-strength-improvement" class="hash-link" aria-label="Direct link to Playing Strength Improvement" title="Direct link to Playing Strength Improvement" translate="no">​</a></h3>
<p>What playing strength can be achieved using only the Policy Network (without search)?</p>
<table><thead><tr><th>Configuration</th><th>Elo Rating</th><th>Approximate Level</th></tr></thead><tbody><tr><td>Previous strongest program (Pachi)</td><td>2,500</td><td>Amateur 4-5 dan</td></tr><tr><td>Policy Network alone</td><td>2,800</td><td>Amateur 6-7 dan</td></tr><tr><td>+ MCTS 1600 simulations</td><td>3,200+</td><td>Professional level</td></tr></tbody></table>
<p>The Policy Network alone is already strong amateur level, and with MCTS it jumps to professional level.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="why-only-57">Why Only 57%?<a href="#why-only-57" class="hash-link" aria-label="Direct link to Why Only 57%?" title="Direct link to Why Only 57%?" translate="no">​</a></h3>
<p>Human game records have the following characteristics that limit accuracy:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-multiple-good-moves">1. Multiple Good Moves<a href="#1-multiple-good-moves" class="hash-link" aria-label="Direct link to 1. Multiple Good Moves" title="Direct link to 1. Multiple Good Moves" translate="no">​</a></h4>
<p>Many positions have multiple good moves. For example, both &quot;approach&quot; and &quot;defend corner&quot; might be correct choices. If the model chooses a different good move, it&#x27;s counted as &quot;wrong.&quot;</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-style-differences">2. Style Differences<a href="#2-style-differences" class="hash-link" aria-label="Direct link to 2. Style Differences" title="Direct link to 2. Style Differences" translate="no">​</a></h4>
<p>Different players have different styles. Aggressive players and steady players might play different moves in the same position. The model learns an &quot;average&quot; style.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-humans-make-mistakes-too">3. Humans Make Mistakes Too<a href="#3-humans-make-mistakes-too" class="hash-link" aria-label="Direct link to 3. Humans Make Mistakes Too" title="Direct link to 3. Humans Make Mistakes Too" translate="no">​</a></h4>
<p>KGS data includes amateur player games, whose choices aren&#x27;t always optimal. The model learning some &quot;mistakes&quot; is normal.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="role-in-mcts">Role in MCTS<a href="#role-in-mcts" class="hash-link" aria-label="Direct link to Role in MCTS" title="Direct link to Role in MCTS" translate="no">​</a></h2>
<p>The Policy Network plays two key roles in AlphaGo&#x27;s MCTS:</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="1-guiding-search-direction">1. Guiding Search Direction<a href="#1-guiding-search-direction" class="hash-link" aria-label="Direct link to 1. Guiding Search Direction" title="Direct link to 1. Guiding Search Direction" translate="no">​</a></h3>
<p>In the MCTS <strong>Selection</strong> phase, Policy Network output is used to calculate UCB (Upper Confidence Bound):</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">UCB(s, a) = Q(s, a) + c_puct × P(s, a) × √(N(s)) / (1 + N(s, a))</span><br></span></code></pre></div></div>
<p>Where <code>P(s, a)</code> is the probability given by the Policy Network.</p>
<p>This means:</p>
<ul>
<li class=""><strong>High-probability moves are explored first</strong></li>
<li class=""><strong>Low-probability moves also have a chance to be explored</strong> (because of the exploration term)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="2-priors-for-expanding-nodes">2. Priors for Expanding Nodes<a href="#2-priors-for-expanding-nodes" class="hash-link" aria-label="Direct link to 2. Priors for Expanding Nodes" title="Direct link to 2. Priors for Expanding Nodes" translate="no">​</a></h3>
<p>When MCTS expands a new node, the Policy Network provides <strong>prior probabilities</strong> for all child nodes.</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Expand node s:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  for each action a:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    child = Node()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    child.prior = policy_network(s)[a]  # Prior probability</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    child.value = 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    child.visits = 0</span><br></span></code></pre></div></div>
<p>These prior probabilities let MCTS &quot;know&quot; which child nodes are more worth exploring, even if they haven&#x27;t been visited yet.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="lightweight-vs-full-version">Lightweight vs Full Version<a href="#lightweight-vs-full-version" class="hash-link" aria-label="Direct link to Lightweight vs Full Version" title="Direct link to Lightweight vs Full Version" translate="no">​</a></h2>
<p>AlphaGo actually has two Policy Networks:</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="full-version-sl-policy-network">Full Version (SL Policy Network)<a href="#full-version-sl-policy-network" class="hash-link" aria-label="Direct link to Full Version (SL Policy Network)" title="Direct link to Full Version (SL Policy Network)" translate="no">​</a></h3>
<ul>
<li class=""><strong>Architecture</strong>: 13-layer CNN, 192 filters</li>
<li class=""><strong>Accuracy</strong>: 57%</li>
<li class=""><strong>Inference time</strong>: About 3 milliseconds/position</li>
<li class=""><strong>Use</strong>: Selection and Expansion in MCTS</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="lightweight-version-rollout-policy-network">Lightweight Version (Rollout Policy Network)<a href="#lightweight-version-rollout-policy-network" class="hash-link" aria-label="Direct link to Lightweight Version (Rollout Policy Network)" title="Direct link to Lightweight Version (Rollout Policy Network)" translate="no">​</a></h3>
<ul>
<li class=""><strong>Architecture</strong>: Linear model + handcrafted features</li>
<li class=""><strong>Accuracy</strong>: 24%</li>
<li class=""><strong>Inference time</strong>: About 2 microseconds/position (1500× faster)</li>
<li class=""><strong>Use</strong>: Fast simulation (rollout)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="why-a-lightweight-version">Why a Lightweight Version?<a href="#why-a-lightweight-version" class="hash-link" aria-label="Direct link to Why a Lightweight Version?" title="Direct link to Why a Lightweight Version?" translate="no">​</a></h3>
<p>In the MCTS <strong>Simulation</strong> phase, we need to play from the current node all the way to the end of the game, potentially playing 100+ moves. If every move used the full Policy Network, it would be too slow.</p>
<p>The lightweight version has only 24% accuracy, but is 1500× faster. In rollouts, speed matters more than precision.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="lightweight-version-features">Lightweight Version Features<a href="#lightweight-version-features" class="hash-link" aria-label="Direct link to Lightweight Version Features" title="Direct link to Lightweight Version Features" translate="no">​</a></h3>
<p>The lightweight version uses handcrafted features, including:</p>
<table><thead><tr><th>Feature Type</th><th>Examples</th></tr></thead><tbody><tr><td>Local patterns</td><td>Stone configurations in 3×3 regions</td></tr><tr><td>Global features</td><td>Whether on edge/corner, big points</td></tr><tr><td>Tactical features</td><td>Atari, ladder, connection</td></tr></tbody></table>
<p>These features are input to a linear model (no hidden layers), making computation extremely fast.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zeros-improvement">AlphaGo Zero&#x27;s Improvement<a href="#alphago-zeros-improvement" class="hash-link" aria-label="Direct link to AlphaGo Zero&#x27;s Improvement" title="Direct link to AlphaGo Zero&#x27;s Improvement" translate="no">​</a></h3>
<p>Later, AlphaGo Zero completely abandoned the lightweight version and rollouts. It directly used the Value Network to evaluate leaf nodes, eliminating the need for fast simulation. This was a major simplification.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="reinforcement-learning-fine-tuning-rl-policy-network">Reinforcement Learning Fine-Tuning (RL Policy Network)<a href="#reinforcement-learning-fine-tuning-rl-policy-network" class="hash-link" aria-label="Direct link to Reinforcement Learning Fine-Tuning (RL Policy Network)" title="Direct link to Reinforcement Learning Fine-Tuning (RL Policy Network)" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="limitations-of-supervised-learning">Limitations of Supervised Learning<a href="#limitations-of-supervised-learning" class="hash-link" aria-label="Direct link to Limitations of Supervised Learning" title="Direct link to Limitations of Supervised Learning" translate="no">​</a></h3>
<p>The supervised learning-trained Policy Network has a fundamental problem:</p>
<blockquote>
<p><strong>It learns to &quot;imitate humans,&quot; not to &quot;win games&quot;</strong></p>
</blockquote>
<p>This means it will learn humans&#x27; bad habits and also perform poorly in positions humans have never encountered.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="self-play-reinforcement">Self-Play Reinforcement<a href="#self-play-reinforcement" class="hash-link" aria-label="Direct link to Self-Play Reinforcement" title="Direct link to Self-Play Reinforcement" translate="no">​</a></h3>
<p>DeepMind&#x27;s solution was to use <strong>Policy Gradient</strong> methods for reinforcement learning:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. Have the Policy Network play against itself</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Record all moves in each game</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Adjust parameters based on outcome:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Won → Increase probability of these moves</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Lost → Decrease probability of these moves</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="reinforce-algorithm">REINFORCE Algorithm<a href="#reinforce-algorithm" class="hash-link" aria-label="Direct link to REINFORCE Algorithm" title="Direct link to REINFORCE Algorithm" translate="no">​</a></h3>
<p>Specifically, using the REINFORCE algorithm:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">∇J(θ) = E[Σ_t ∇log π_θ(a_t | s_t) × z]</span><br></span></code></pre></div></div>
<p>Where:</p>
<ul>
<li class=""><code>z</code>: Game outcome (+1 win, -1 loss)</li>
<li class=""><code>π_θ(a_t | s_t)</code>: Probability of choosing action <code>a_t</code> in state <code>s_t</code></li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="results">Results<a href="#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results" translate="no">​</a></h3>
<p>After about 1 day of self-play training (1.28 million games), the RL Policy Network:</p>
<table><thead><tr><th>Metric</th><th>SL Policy</th><th>RL Policy</th></tr></thead><tbody><tr><td>Win rate vs SL Policy</td><td>50%</td><td><strong>80%</strong></td></tr><tr><td>Elo improvement</td><td>-</td><td>+100</td></tr></tbody></table>
<p>Accuracy may drop slightly (since it no longer fully imitates humans), but actual game win rate significantly improved.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="from-imitation-to-innovation">From &quot;Imitation&quot; to &quot;Innovation&quot;<a href="#from-imitation-to-innovation" class="hash-link" aria-label="Direct link to From &quot;Imitation&quot; to &quot;Innovation&quot;" title="Direct link to From &quot;Imitation&quot; to &quot;Innovation&quot;" translate="no">​</a></h3>
<p>Reinforcement learning let the Policy Network learn some moves humans had never thought of. These moves never appeared in training data, but they&#x27;re effective.</p>
<p>This is why AlphaGo could play the &quot;Divine Move&quot;—it&#x27;s not limited by human experience.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="visual-analysis">Visual Analysis<a href="#visual-analysis" class="hash-link" aria-label="Direct link to Visual Analysis" title="Direct link to Visual Analysis" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="probability-distribution-for-different-positions">Probability Distribution for Different Positions<a href="#probability-distribution-for-different-positions" class="hash-link" aria-label="Direct link to Probability Distribution for Different Positions" title="Direct link to Probability Distribution for Different Positions" translate="no">​</a></h3>
<p>Let&#x27;s look at the Policy Network&#x27;s output in different positions:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="opening-fuseki-stage">Opening (Fuseki Stage)<a href="#opening-fuseki-stage" class="hash-link" aria-label="Direct link to Opening (Fuseki Stage)" title="Direct link to Opening (Fuseki Stage)" translate="no">​</a></h4>
<div>載入中...</div>
<p>During the opening, probability is mainly concentrated on:</p>
<ul>
<li class="">Corners (taking corners)</li>
<li class="">Edges (approaching, defending corners)</li>
<li class="">&quot;Big point&quot; positions</li>
</ul>
<p>This matches basic Go principles: corners are gold, edges are silver, center is grass.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="fighting-position">Fighting Position<a href="#fighting-position" class="hash-link" aria-label="Direct link to Fighting Position" title="Direct link to Fighting Position" translate="no">​</a></h4>
<div>載入中...</div>
<p>During fighting, probability concentrates on:</p>
<ul>
<li class="">Key cutting points</li>
<li class="">Atari, connections</li>
<li class="">Making eyes, destroying eyes</li>
</ul>
<p>This shows the model learned local tactics.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="endgame-stage">Endgame Stage<a href="#endgame-stage" class="hash-link" aria-label="Direct link to Endgame Stage" title="Direct link to Endgame Stage" translate="no">​</a></h4>
<div>載入中...</div>
<p>During the endgame, probability is scattered across various endgame points, requiring precise point calculation.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="what-do-hidden-layers-learn">What Do Hidden Layers Learn?<a href="#what-do-hidden-layers-learn" class="hash-link" aria-label="Direct link to What Do Hidden Layers Learn?" title="Direct link to What Do Hidden Layers Learn?" translate="no">​</a></h3>
<p>By visualizing convolutional layer outputs, we can see the &quot;features&quot; the model learned:</p>
<ul>
<li class=""><strong>Low layers</strong>: Basic shapes (eyes, cutting points)</li>
<li class=""><strong>Middle layers</strong>: Tactical patterns (atari, ladders)</li>
<li class=""><strong>High layers</strong>: Global concepts (influence, thickness)</li>
</ul>
<p>This closely resembles the hierarchical structure of how humans understand Go.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="implementation-notes">Implementation Notes<a href="#implementation-notes" class="hash-link" aria-label="Direct link to Implementation Notes" title="Direct link to Implementation Notes" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="pytorch-implementation">PyTorch Implementation<a href="#pytorch-implementation" class="hash-link" aria-label="Direct link to PyTorch Implementation" title="Direct link to PyTorch Implementation" translate="no">​</a></h3>
<p>Here&#x27;s a simplified Policy Network implementation:</p>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> nn</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">functional </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> F</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">PolicyNetwork</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Module</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> input_channels</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">48</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_filters</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">192</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_layers</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">12</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># First convolutional layer (5×5)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv1 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Conv2d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">input_channels</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_filters</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                               kernel_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> padding</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Middle convolutional layers (3×3) ×11</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv_layers </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ModuleList</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Conv2d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_filters</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_filters</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                     kernel_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> padding</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> _ </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_layers </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Output convolutional layer (1×1)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv_out </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Conv2d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_filters</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> kernel_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">forward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># x: (batch, 48, 19, 19)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># First layer</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> F</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">relu</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv1</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Middle layers</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> conv </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv_layers</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> F</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">relu</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">conv</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Output layer</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv_out</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># (batch, 1, 19, 19)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Flatten + Softmax</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">view</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">size</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># (batch, 361)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> F</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">softmax</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dim</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> x</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="training-loop">Training Loop<a href="#training-loop" class="hash-link" aria-label="Direct link to Training Loop" title="Direct link to Training Loop" translate="no">​</a></h3>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">train_step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> optimizer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> states</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> actions</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    states: (batch, 48, 19, 19) - Board features</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    actions: (batch,) - Positions where humans played (0-360)</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Forward pass</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">states</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># (batch, 361)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Cross-entropy loss</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> F</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cross_entropy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">log</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">policy </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1e-8</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># Prevent log(0)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        actions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Backward pass</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zero_grad</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    loss</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">backward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Calculate accuracy</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    predictions </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> policy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">argmax</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">dim</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    accuracy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">predictions </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> actions</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">float</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">mean</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> loss</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">item</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> accuracy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">item</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="notes-for-inference">Notes for Inference<a href="#notes-for-inference" class="hash-link" aria-label="Direct link to Notes for Inference" title="Direct link to Notes for Inference" translate="no">​</a></h3>
<p>When actually playing, note:</p>
<ol>
<li class=""><strong>Filter illegal moves</strong>: Set probability of illegal positions to 0, then renormalize</li>
<li class=""><strong>Temperature adjustment</strong>: Use a temperature parameter to control the &quot;sharpness&quot; of the probability distribution</li>
<li class=""><strong>Batch inference</strong>: In MCTS, multiple positions can be processed in batches</li>
</ol>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">get_move_probabilities</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> state</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> legal_moves</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> temperature</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1.0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;Get probability distribution over legal moves&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">state</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># (361,)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Keep only legal moves</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mask </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zeros</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">361</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mask</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">legal_moves</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> policy </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> mask</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Temperature adjustment</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> temperature </span><span class="token operator" style="color:#393A34">!=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.0</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> policy </span><span class="token operator" style="color:#393A34">**</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> temperature</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Renormalize</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> policy </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> policy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">sum</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> policy</span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="animation-mapping">Animation Mapping<a href="#animation-mapping" class="hash-link" aria-label="Direct link to Animation Mapping" title="Direct link to Animation Mapping" translate="no">​</a></h2>
<p>Core concepts covered in this article and their animation numbers:</p>
<table><thead><tr><th>Number</th><th>Concept</th><th>Physics/Math Correspondence</th></tr></thead><tbody><tr><td>Animation E1</td><td>Policy Network</td><td>Probability field</td></tr><tr><td>Animation D9</td><td>CNN feature extraction</td><td>Filter response</td></tr><tr><td>Animation D3</td><td>Supervised learning</td><td>Maximum likelihood estimation</td></tr><tr><td>Animation H4</td><td>Policy gradient</td><td>Stochastic optimization</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="further-reading">Further Reading<a href="#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading" translate="no">​</a></h2>
<ul>
<li class=""><strong>Next article</strong>: <a class="" href="/en/docs/for-engineers/how-it-works/alphago-explained/value-network/">Value Network Deep Dive</a> — How AlphaGo evaluates positions</li>
<li class=""><strong>Related topic</strong>: <a class="" href="/en/docs/for-engineers/how-it-works/alphago-explained/input-features/">Input Features Design</a> — Detailed explanation of 48 feature planes</li>
<li class=""><strong>Deep dive</strong>: <a class="" href="/en/docs/for-engineers/how-it-works/alphago-explained/cnn-and-go/">CNN and Go</a> — Why CNNs are suitable for board games</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="key-takeaways">Key Takeaways<a href="#key-takeaways" class="hash-link" aria-label="Direct link to Key Takeaways" title="Direct link to Key Takeaways" translate="no">​</a></h2>
<ol>
<li class=""><strong>Policy Network is a probability distribution generator</strong>: Input board, output probabilities for 361 positions</li>
<li class=""><strong>13-layer CNN + Softmax</strong>: Deep convolutions extract features, Softmax outputs probabilities</li>
<li class=""><strong>57% accuracy</strong>: Far exceeding previous computer Go programs</li>
<li class=""><strong>Two versions</strong>: Full version for MCTS decisions, lightweight version for fast simulation</li>
<li class=""><strong>Reinforcement learning fine-tuning</strong>: Evolving from &quot;imitating humans&quot; to &quot;pursuing victory&quot;</li>
</ol>
<p>The Policy Network is AlphaGo&#x27;s &quot;intuition&quot;—it allows the AI to quickly identify moves worth considering, just like a human.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2016). &quot;Mastering the game of Go with deep neural networks and tree search.&quot; <em>Nature</em>, 529, 484-489.</li>
<li class="">Maddison, C. J., et al. (2014). &quot;Move Evaluation in Go Using Deep Convolutional Neural Networks.&quot; <em>arXiv:1412.6564</em>.</li>
<li class="">Sutton, R. S., &amp; Barto, A. G. (2018). <em>Reinforcement Learning: An Introduction</em>. MIT Press.</li>
<li class="">LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). &quot;Deep learning.&quot; <em>Nature</em>, 521, 436-444.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/07-policy-network.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/docs/for-engineers/how-it-works/alphago-explained/board-representation/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Board State Representation</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/docs/for-engineers/how-it-works/alphago-explained/value-network/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Value Network Explained</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-the-policy-network" class="table-of-contents__link toc-highlight">What is the Policy Network?</a><ul><li><a href="#core-function" class="table-of-contents__link toc-highlight">Core Function</a></li><li><a href="#intuitive-understanding" class="table-of-contents__link toc-highlight">Intuitive Understanding</a></li><li><a href="#why-do-we-need-a-policy-network" class="table-of-contents__link toc-highlight">Why Do We Need a Policy Network?</a></li></ul></li><li><a href="#network-architecture" class="table-of-contents__link toc-highlight">Network Architecture</a><ul><li><a href="#overall-structure" class="table-of-contents__link toc-highlight">Overall Structure</a></li><li><a href="#input-layer" class="table-of-contents__link toc-highlight">Input Layer</a></li><li><a href="#convolutional-layers" class="table-of-contents__link toc-highlight">Convolutional Layers</a></li><li><a href="#relu-activation-function" class="table-of-contents__link toc-highlight">ReLU Activation Function</a></li><li><a href="#output-layer" class="table-of-contents__link toc-highlight">Output Layer</a></li><li><a href="#parameter-count" class="table-of-contents__link toc-highlight">Parameter Count</a></li></ul></li><li><a href="#training-objective-and-methods" class="table-of-contents__link toc-highlight">Training Objective and Methods</a><ul><li><a href="#training-data" class="table-of-contents__link toc-highlight">Training Data</a></li><li><a href="#cross-entropy-loss-function" class="table-of-contents__link toc-highlight">Cross-Entropy Loss Function</a></li><li><a href="#training-process" class="table-of-contents__link toc-highlight">Training Process</a></li><li><a href="#data-augmentation" class="table-of-contents__link toc-highlight">Data Augmentation</a></li></ul></li><li><a href="#training-results" class="table-of-contents__link toc-highlight">Training Results</a><ul><li><a href="#57-accuracy" class="table-of-contents__link toc-highlight">57% Accuracy</a></li><li><a href="#playing-strength-improvement" class="table-of-contents__link toc-highlight">Playing Strength Improvement</a></li><li><a href="#why-only-57" class="table-of-contents__link toc-highlight">Why Only 57%?</a></li></ul></li><li><a href="#role-in-mcts" class="table-of-contents__link toc-highlight">Role in MCTS</a><ul><li><a href="#1-guiding-search-direction" class="table-of-contents__link toc-highlight">1. Guiding Search Direction</a></li><li><a href="#2-priors-for-expanding-nodes" class="table-of-contents__link toc-highlight">2. Priors for Expanding Nodes</a></li></ul></li><li><a href="#lightweight-vs-full-version" class="table-of-contents__link toc-highlight">Lightweight vs Full Version</a><ul><li><a href="#full-version-sl-policy-network" class="table-of-contents__link toc-highlight">Full Version (SL Policy Network)</a></li><li><a href="#lightweight-version-rollout-policy-network" class="table-of-contents__link toc-highlight">Lightweight Version (Rollout Policy Network)</a></li><li><a href="#why-a-lightweight-version" class="table-of-contents__link toc-highlight">Why a Lightweight Version?</a></li><li><a href="#lightweight-version-features" class="table-of-contents__link toc-highlight">Lightweight Version Features</a></li><li><a href="#alphago-zeros-improvement" class="table-of-contents__link toc-highlight">AlphaGo Zero&#39;s Improvement</a></li></ul></li><li><a href="#reinforcement-learning-fine-tuning-rl-policy-network" class="table-of-contents__link toc-highlight">Reinforcement Learning Fine-Tuning (RL Policy Network)</a><ul><li><a href="#limitations-of-supervised-learning" class="table-of-contents__link toc-highlight">Limitations of Supervised Learning</a></li><li><a href="#self-play-reinforcement" class="table-of-contents__link toc-highlight">Self-Play Reinforcement</a></li><li><a href="#reinforce-algorithm" class="table-of-contents__link toc-highlight">REINFORCE Algorithm</a></li><li><a href="#results" class="table-of-contents__link toc-highlight">Results</a></li><li><a href="#from-imitation-to-innovation" class="table-of-contents__link toc-highlight">From &quot;Imitation&quot; to &quot;Innovation&quot;</a></li></ul></li><li><a href="#visual-analysis" class="table-of-contents__link toc-highlight">Visual Analysis</a><ul><li><a href="#probability-distribution-for-different-positions" class="table-of-contents__link toc-highlight">Probability Distribution for Different Positions</a></li><li><a href="#what-do-hidden-layers-learn" class="table-of-contents__link toc-highlight">What Do Hidden Layers Learn?</a></li></ul></li><li><a href="#implementation-notes" class="table-of-contents__link toc-highlight">Implementation Notes</a><ul><li><a href="#pytorch-implementation" class="table-of-contents__link toc-highlight">PyTorch Implementation</a></li><li><a href="#training-loop" class="table-of-contents__link toc-highlight">Training Loop</a></li><li><a href="#notes-for-inference" class="table-of-contents__link toc-highlight">Notes for Inference</a></li></ul></li><li><a href="#animation-mapping" class="table-of-contents__link toc-highlight">Animation Mapping</a></li><li><a href="#further-reading" class="table-of-contents__link toc-highlight">Further Reading</a></li><li><a href="#key-takeaways" class="table-of-contents__link toc-highlight">Key Takeaways</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>