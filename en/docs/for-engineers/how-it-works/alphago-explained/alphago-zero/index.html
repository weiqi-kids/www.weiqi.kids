<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-for-engineers/how-it-works/alphago-explained/alphago-zero" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">AlphaGo Zero Overview | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/en/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/en/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="AlphaGo Zero Overview | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="Starting from scratch, completely self-taught - how AlphaGo Zero surpassed all previous versions without any human game records"><meta data-rh="true" property="og:description" content="Starting from scratch, completely self-taught - how AlphaGo Zero surpassed all previous versions without any human game records"><meta data-rh="true" name="keywords" content="AlphaGo Zero,self-play,reinforcement learning,deep learning,Go AI,unsupervised learning"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"給工程師的圍棋 AI 指南","item":"https://www.weiqi.kids/en/docs/for-engineers/"},{"@type":"ListItem","position":2,"name":"一篇文章搞懂圍棋 AI","item":"https://www.weiqi.kids/en/docs/for-engineers/how-it-works/"},{"@type":"ListItem","position":3,"name":"AlphaGo 完整解析","item":"https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/"},{"@type":"ListItem","position":4,"name":"AlphaGo Zero Overview","item":"https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.f23bf74b.css">
<script src="/en/assets/js/runtime~main.51eae05f.js" defer="defer"></script>
<script src="/en/assets/js/main.74a0125b.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/en/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/logo.svg" alt="Good Go Baby Association Logo" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/en/img/logo.svg" alt="Good Go Baby Association Logo" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">Weiqi.Kids</b></a><a class="navbar__item navbar__link" href="/en/docs/for-players/">For Go Players</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/docs/for-engineers/">For Engineers</a><a class="navbar__item navbar__link" href="/en/docs/about/">About Us</a><a class="navbar__item navbar__link" href="/en/docs/activities/">Activities</a><a class="navbar__item navbar__link" href="/en/docs/references/">References</a><a class="navbar__item navbar__link" href="/en/docs/sop/">Standard Procedures</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/ja/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/docs/intro/"><span title="User Guide" class="linkLabel_REp1">User Guide</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/about/"><span title="關於協會" class="categoryLinkLabel_ezQx">關於協會</span></a><button aria-label="Expand sidebar category &#x27;關於協會&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/activities/"><span title="活動實績" class="categoryLinkLabel_ezQx">活動實績</span></a><button aria-label="Expand sidebar category &#x27;活動實績&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/for-players/"><span title="For Go Players" class="categoryLinkLabel_ezQx">For Go Players</span></a><button aria-label="Expand sidebar category &#x27;For Go Players&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/references/"><span title="參考資料" class="categoryLinkLabel_ezQx">參考資料</span></a><button aria-label="Expand sidebar category &#x27;參考資料&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/sop/"><span title="標準作業流程" class="categoryLinkLabel_ezQx">標準作業流程</span></a><button aria-label="Expand sidebar category &#x27;標準作業流程&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/en/docs/for-engineers/"><span title="給工程師的圍棋 AI 指南" class="categoryLinkLabel_ezQx">給工程師的圍棋 AI 指南</span></a><button aria-label="Collapse sidebar category &#x27;給工程師的圍棋 AI 指南&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/en/docs/for-engineers/deep-dive/"><span title="For Deep Learners" class="categoryLinkLabel_ezQx">For Deep Learners</span></a><button aria-label="Expand sidebar category &#x27;For Deep Learners&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/en/docs/for-engineers/hands-on/"><span title="30 分鐘跑起第一個圍棋 AI" class="categoryLinkLabel_ezQx">30 分鐘跑起第一個圍棋 AI</span></a><button aria-label="Expand sidebar category &#x27;30 分鐘跑起第一個圍棋 AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/en/docs/for-engineers/how-it-works/"><span title="一篇文章搞懂圍棋 AI" class="categoryLinkLabel_ezQx">一篇文章搞懂圍棋 AI</span></a><button aria-label="Collapse sidebar category &#x27;一篇文章搞懂圍棋 AI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/"><span title="AlphaGo 完整解析" class="categoryLinkLabel_ezQx">AlphaGo 完整解析</span></a><button aria-label="Collapse sidebar category &#x27;AlphaGo 完整解析&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/birth-of-alphago/"><span title="The Birth of AlphaGo" class="linkLabel_REp1">The Birth of AlphaGo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/key-matches/"><span title="Key Matches Review" class="linkLabel_REp1">Key Matches Review</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/move-37/"><span title="Deep Analysis of &quot;The Divine Move&quot;" class="linkLabel_REp1">Deep Analysis of &quot;The Divine Move&quot;</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/why-go-is-hard/"><span title="Why Is Go Hard?" class="linkLabel_REp1">Why Is Go Hard?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/traditional-limits/"><span title="Limits of Traditional Methods" class="linkLabel_REp1">Limits of Traditional Methods</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/board-representation/"><span title="Board State Representation" class="linkLabel_REp1">Board State Representation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/policy-network/"><span title="Policy Network Deep Dive" class="linkLabel_REp1">Policy Network Deep Dive</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/value-network/"><span title="Value Network Explained" class="linkLabel_REp1">Value Network Explained</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/input-features/"><span title="Input Feature Design" class="linkLabel_REp1">Input Feature Design</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/cnn-and-go/"><span title="CNN and Go" class="linkLabel_REp1">CNN and Go</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/supervised-learning/"><span title="Supervised Learning Phase" class="linkLabel_REp1">Supervised Learning Phase</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/"><span title="Introduction to Reinforcement Learning" class="linkLabel_REp1">Introduction to Reinforcement Learning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/self-play/"><span title="Self-Play" class="linkLabel_REp1">Self-Play</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/"><span title="MCTS and Neural Network Integration" class="linkLabel_REp1">MCTS and Neural Network Integration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/puct-formula/"><span title="PUCT Formula Explained" class="linkLabel_REp1">PUCT Formula Explained</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><span title="AlphaGo Zero Overview" class="linkLabel_REp1">AlphaGo Zero Overview</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/"><span title="Dual-Head Network and Residual Network" class="linkLabel_REp1">Dual-Head Network and Residual Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/training-from-scratch/"><span title="Training from Scratch" class="linkLabel_REp1">Training from Scratch</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/distributed-systems/"><span title="Distributed Systems and TPU" class="linkLabel_REp1">Distributed Systems and TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/alphago-explained/legacy-and-impact/"><span title="AlphaGo&#x27;s Legacy" class="linkLabel_REp1">AlphaGo&#x27;s Legacy</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/katago-innovations/"><span title="KataGo 的關鍵創新" class="linkLabel_REp1">KataGo 的關鍵創新</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/for-engineers/how-it-works/concepts/"><span title="概念速查表" class="linkLabel_REp1">概念速查表</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/en/docs/for-engineers/industry/"><span title="圍棋 AI 產業現況" class="categoryLinkLabel_ezQx">圍棋 AI 產業現況</span></a><button aria-label="Expand sidebar category &#x27;圍棋 AI 產業現況&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/en/docs/for-engineers/overview/"><span title="圍棋 AI 能做什麼？" class="categoryLinkLabel_ezQx">圍棋 AI 能做什麼？</span></a><button aria-label="Expand sidebar category &#x27;圍棋 AI 能做什麼？&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/en/docs/for-engineers/"><span>給工程師的圍棋 AI 指南</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/en/docs/for-engineers/how-it-works/"><span>一篇文章搞懂圍棋 AI</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/en/docs/for-engineers/how-it-works/alphago-explained/"><span>AlphaGo 完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">AlphaGo Zero Overview</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>AlphaGo Zero Overview</h1></header>
<p>In October 2017, DeepMind announced a result that shocked the AI world: <strong>AlphaGo Zero</strong>, without using any human game records, starting from a completely random state, surpassed the original AlphaGo that defeated Lee Sedol in just three days, winning <strong>100:0</strong>.</p>
<p>This isn&#x27;t just numerical progress. It represents a completely new paradigm: <strong>AI doesn&#x27;t need human knowledge; it can discover everything from scratch.</strong></p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="why-no-human-games-needed">Why No Human Games Needed?<a href="#why-no-human-games-needed" class="hash-link" aria-label="Direct link to Why No Human Games Needed?" title="Direct link to Why No Human Games Needed?" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="limitations-of-human-games">Limitations of Human Games<a href="#limitations-of-human-games" class="hash-link" aria-label="Direct link to Limitations of Human Games" title="Direct link to Limitations of Human Games" translate="no">​</a></h3>
<p>Original AlphaGo&#x27;s training process had two phases:</p>
<ol>
<li class=""><strong>Supervised Learning</strong>: Train Policy Network with 30 million human games</li>
<li class=""><strong>Reinforcement Learning</strong>: Further improve through self-play</li>
</ol>
<p>This approach has several fundamental problems:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-human-games-have-upper-limits">1. Human Games Have Upper Limits<a href="#1-human-games-have-upper-limits" class="hash-link" aria-label="Direct link to 1. Human Games Have Upper Limits" title="Direct link to 1. Human Games Have Upper Limits" translate="no">​</a></h4>
<p>Human players have finite strength; game records contain human understanding but also human errors and biases. When AI learns from human games, it learns:</p>
<ul>
<li class="">What humans think are good moves (but not necessarily optimal)</li>
<li class="">Human thought patterns (but might limit innovation)</li>
<li class="">Human mistakes (learned as if they were correct)</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-supervised-learning-bottleneck">2. Supervised Learning Bottleneck<a href="#2-supervised-learning-bottleneck" class="hash-link" aria-label="Direct link to 2. Supervised Learning Bottleneck" title="Direct link to 2. Supervised Learning Bottleneck" translate="no">​</a></h4>
<p>Supervised learning&#x27;s goal is to &quot;imitate humans&quot; - predict which move a human player would make. This means AI&#x27;s capability ceiling is limited by human player capability.</p>
<p>Like an apprentice who can only imitate the master, never surpassing them.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-data-collection-costs">3. Data Collection Costs<a href="#3-data-collection-costs" class="hash-link" aria-label="Direct link to 3. Data Collection Costs" title="Direct link to 3. Data Collection Costs" translate="no">​</a></h4>
<p>High-quality human game records take years to accumulate and only exist for games with long histories like Go. If we want to apply AI to new domains (like protein structure prediction), there simply are no &quot;expert game records&quot; available.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="zeros-breakthrough">Zero&#x27;s Breakthrough<a href="#zeros-breakthrough" class="hash-link" aria-label="Direct link to Zero&#x27;s Breakthrough" title="Direct link to Zero&#x27;s Breakthrough" translate="no">​</a></h3>
<p>AlphaGo Zero completely skips the supervised learning phase, starting directly from <strong>random initialization</strong> for self-play. This solves all the above problems:</p>
<table><thead><tr><th>Problem</th><th>Original AlphaGo</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Human knowledge ceiling</td><td>Limited by game quality</td><td>No such limitation</td></tr><tr><td>Learning objective</td><td>Imitate humans</td><td>Maximize win rate</td></tr><tr><td>Data requirement</td><td>30 million games</td><td>0</td></tr><tr><td>Generalizability</td><td>Go only</td><td>Can extend to other domains</td></tr></tbody></table>
<p>This is a fundamental paradigm shift: from &quot;learning human knowledge&quot; to &quot;discovering knowledge from first principles.&quot;</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="comparison-with-original-alphago-1000">Comparison with Original AlphaGo: 100:0<a href="#comparison-with-original-alphago-1000" class="hash-link" aria-label="Direct link to Comparison with Original AlphaGo: 100:0" title="Direct link to Comparison with Original AlphaGo: 100:0" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="crushing-victory">Crushing Victory<a href="#crushing-victory" class="hash-link" aria-label="Direct link to Crushing Victory" title="Direct link to Crushing Victory" translate="no">​</a></h3>
<p>DeepMind had trained AlphaGo Zero play against various AlphaGo versions:</p>
<table><thead><tr><th>Opponent</th><th>AlphaGo Zero Record</th></tr></thead><tbody><tr><td>AlphaGo Fan (defeated Fan Hui version)</td><td>100:0</td></tr><tr><td>AlphaGo Lee (defeated Lee Sedol version)</td><td>100:0</td></tr><tr><td>AlphaGo Master (60-game winning streak version)</td><td>89:11</td></tr></tbody></table>
<p><strong>100:0</strong> - this means in 100 games, original AlphaGo couldn&#x27;t win a single one.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="less-resources-stronger-play">Less Resources, Stronger Play<a href="#less-resources-stronger-play" class="hash-link" aria-label="Direct link to Less Resources, Stronger Play" title="Direct link to Less Resources, Stronger Play" translate="no">​</a></h3>
<p>Not only winning, AlphaGo Zero achieved stronger play with fewer resources:</p>
<table><thead><tr><th>Metric</th><th>AlphaGo Lee</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Training time</td><td>Months</td><td>40 days (3 days to surpass AlphaGo Lee)</td></tr><tr><td>Training games</td><td>30 million human games + self-play</td><td>4.9 million self-play games</td></tr><tr><td>TPUs (training)</td><td>50+</td><td>4</td></tr><tr><td>TPUs (inference)</td><td>48</td><td>4</td></tr><tr><td>Input features</td><td>48 planes</td><td>17 planes</td></tr><tr><td>Neural network</td><td>SL + RL dual networks</td><td>Single dual-head network</td></tr></tbody></table>
<p>This is stunning efficiency improvement: <strong>10× fewer resources, yet much stronger play.</strong></p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="why-is-zero-stronger">Why Is Zero Stronger?<a href="#why-is-zero-stronger" class="hash-link" aria-label="Direct link to Why Is Zero Stronger?" title="Direct link to Why Is Zero Stronger?" translate="no">​</a></h3>
<p>AlphaGo Zero&#x27;s superior strength can be understood from several angles:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-unbiased-learning">1. Unbiased Learning<a href="#1-unbiased-learning" class="hash-link" aria-label="Direct link to 1. Unbiased Learning" title="Direct link to 1. Unbiased Learning" translate="no">​</a></h4>
<p>Original AlphaGo learned from human games, inheriting human biases. For example, human players might overvalue certain joseki, or have wrong evaluations of some positions.</p>
<p>AlphaGo Zero has no such baggage. It starts from blank slate, learning what&#x27;s good only from win/loss results. This lets it discover moves humans never thought of.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-consistent-learning-objective">2. Consistent Learning Objective<a href="#2-consistent-learning-objective" class="hash-link" aria-label="Direct link to 2. Consistent Learning Objective" title="Direct link to 2. Consistent Learning Objective" translate="no">​</a></h4>
<p>Original AlphaGo&#x27;s training had two different objectives:</p>
<ul>
<li class="">Supervised learning: Maximize prediction accuracy of human moves</li>
<li class="">Reinforcement learning: Maximize win rate</li>
</ul>
<p>These two objectives might conflict. AlphaGo Zero has only one objective: <strong>maximize win rate</strong>. This makes learning more consistent and effective.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-simpler-architecture">3. Simpler Architecture<a href="#3-simpler-architecture" class="hash-link" aria-label="Direct link to 3. Simpler Architecture" title="Direct link to 3. Simpler Architecture" translate="no">​</a></h4>
<p>Original AlphaGo used separate Policy Network and Value Network. AlphaGo Zero uses a single dual-head network (see <a class="" href="/en/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/">next article: Dual-Head Network and Residual Network</a>), allowing feature representations to be shared, improving learning efficiency.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="simplified-input-features-from-48-to-17">Simplified Input Features: From 48 to 17<a href="#simplified-input-features-from-48-to-17" class="hash-link" aria-label="Direct link to Simplified Input Features: From 48 to 17" title="Direct link to Simplified Input Features: From 48 to 17" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="original-alphagos-48-feature-planes">Original AlphaGo&#x27;s 48 Feature Planes<a href="#original-alphagos-48-feature-planes" class="hash-link" aria-label="Direct link to Original AlphaGo&#x27;s 48 Feature Planes" title="Direct link to Original AlphaGo&#x27;s 48 Feature Planes" translate="no">​</a></h3>
<p>Original AlphaGo&#x27;s neural network input included 48 19×19 feature planes, encoding many human-designed features:</p>
<table><thead><tr><th>Category</th><th>Count</th><th>Content</th></tr></thead><tbody><tr><td>Stone positions</td><td>3</td><td>Black, white, empty</td></tr><tr><td>Liberties</td><td>8</td><td>Strings with 1-8 liberties</td></tr><tr><td>Captures</td><td>8</td><td>Can capture 1-8 stones</td></tr><tr><td>Ko</td><td>1</td><td>Ko position</td></tr><tr><td>Edge distance</td><td>4</td><td>1st to 4th line</td></tr><tr><td>Move legality</td><td>1</td><td>Which positions can be played</td></tr><tr><td>History</td><td>8</td><td>Past 8 moves&#x27; positions</td></tr><tr><td>Turn</td><td>1</td><td>Black or White</td></tr><tr><td>Other</td><td>14</td><td>Ladders, eyes, etc.</td></tr></tbody></table>
<p>These 48 features were carefully designed by Go experts, containing extensive domain knowledge.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zeros-17-feature-planes">AlphaGo Zero&#x27;s 17 Feature Planes<a href="#alphago-zeros-17-feature-planes" class="hash-link" aria-label="Direct link to AlphaGo Zero&#x27;s 17 Feature Planes" title="Direct link to AlphaGo Zero&#x27;s 17 Feature Planes" translate="no">​</a></h3>
<p>AlphaGo Zero dramatically simplified input to just 17 feature planes:</p>
<table><thead><tr><th>Plane Number</th><th>Content</th><th>Count</th></tr></thead><tbody><tr><td>1-8</td><td>Black positions (last 8 moves)</td><td>8</td></tr><tr><td>9-16</td><td>White positions (last 8 moves)</td><td>8</td></tr><tr><td>17</td><td>Current turn (all 1s or all 0s)</td><td>1</td></tr></tbody></table>
<p>These 17 features only include:</p>
<ul>
<li class=""><strong>Current board state</strong>: Black, white, or empty at each position</li>
<li class=""><strong>History information</strong>: Board states of past 8 moves</li>
<li class=""><strong>Turn information</strong>: Whose turn to play</li>
</ul>
<p>No liberty counts, no ladder detection, no edge distance - all this &quot;Go knowledge&quot; is left for the neural network to learn itself.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="why-is-simplification-good">Why Is Simplification Good?<a href="#why-is-simplification-good" class="hash-link" aria-label="Direct link to Why Is Simplification Good?" title="Direct link to Why Is Simplification Good?" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-let-network-discover-features">1. Let Network Discover Features<a href="#1-let-network-discover-features" class="hash-link" aria-label="Direct link to 1. Let Network Discover Features" title="Direct link to 1. Let Network Discover Features" translate="no">​</a></h4>
<p>Complex handcrafted features might miss important information or encode wrong assumptions. Letting neural network learn from raw data, it might discover better feature representations.</p>
<p>In fact, AlphaGo Zero learned all features humans designed (liberties, ladders, etc.), and also learned patterns humans weren&#x27;t explicitly aware of.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-better-generalizability">2. Better Generalizability<a href="#2-better-generalizability" class="hash-link" aria-label="Direct link to 2. Better Generalizability" title="Direct link to 2. Better Generalizability" translate="no">​</a></h4>
<p>Many of the 48 features are Go-specific (like ladders, edge distance). The 17 simplified features are universal - any board game can be encoded similarly.</p>
<p>This laid foundation for later <strong>AlphaZero</strong> (general game AI).</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-reduce-human-error">3. Reduce Human Error<a href="#3-reduce-human-error" class="hash-link" aria-label="Direct link to 3. Reduce Human Error" title="Direct link to 3. Reduce Human Error" translate="no">​</a></h4>
<p>Handcrafted features may contain errors or incomplete definitions. Simplified input eliminates this possibility.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="single-network-architecture">Single Network Architecture<a href="#single-network-architecture" class="hash-link" aria-label="Direct link to Single Network Architecture" title="Direct link to Single Network Architecture" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="original-dual-network-design">Original Dual-Network Design<a href="#original-dual-network-design" class="hash-link" aria-label="Direct link to Original Dual-Network Design" title="Direct link to Original Dual-Network Design" translate="no">​</a></h3>
<p>Original AlphaGo used two independent neural networks:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Policy Network:  Input → CNN → 19×19 move probabilities</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Value Network:   Input → CNN → Win rate estimate (-1 to 1)</span><br></span></code></pre></div></div>
<p>These two networks:</p>
<ul>
<li class="">Have different architectures (slightly different layers and channels)</li>
<li class="">Train independently (first Policy, then Value)</li>
<li class="">Share no parameters</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="zeros-dual-head-network">Zero&#x27;s Dual-Head Network<a href="#zeros-dual-head-network" class="hash-link" aria-label="Direct link to Zero&#x27;s Dual-Head Network" title="Direct link to Zero&#x27;s Dual-Head Network" translate="no">​</a></h3>
<p>AlphaGo Zero uses a single network with two output heads:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Input → ResNet shared backbone → Policy Head → 19×19 move probabilities</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                              → Value Head  → Win rate estimate</span><br></span></code></pre></div></div>
<p>Two Heads share the same ResNet backbone (see <a class="" href="/en/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/">next article: Dual-Head Network and Residual Network</a>), bringing several benefits:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-parameter-efficiency">1. Parameter Efficiency<a href="#1-parameter-efficiency" class="hash-link" aria-label="Direct link to 1. Parameter Efficiency" title="Direct link to 1. Parameter Efficiency" translate="no">​</a></h4>
<p>Shared backbone means most parameters are used by both tasks. This reduces total parameters, lowering overfitting risk.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-feature-sharing">2. Feature Sharing<a href="#2-feature-sharing" class="hash-link" aria-label="Direct link to 2. Feature Sharing" title="Direct link to 2. Feature Sharing" translate="no">​</a></h4>
<p>&quot;Where to play&quot; (Policy) and &quot;who will win&quot; (Value) need to understand similar board patterns. Shared backbone lets these features be learned and used by both tasks simultaneously.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-training-stability">3. Training Stability<a href="#3-training-stability" class="hash-link" aria-label="Direct link to 3. Training Stability" title="Direct link to 3. Training Stability" translate="no">​</a></h4>
<p>Joint training lets gradient signals come from two sources, providing richer supervision signal, making training more stable.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="power-of-residual-networks">Power of Residual Networks<a href="#power-of-residual-networks" class="hash-link" aria-label="Direct link to Power of Residual Networks" title="Direct link to Power of Residual Networks" translate="no">​</a></h3>
<p>AlphaGo Zero&#x27;s backbone uses <strong>40-layer Residual Network (ResNet)</strong>, much deeper than original AlphaGo&#x27;s 13-layer CNN.</p>
<p>Residual connections (skip connections) enable effective training of deep networks, avoiding vanishing gradient problem. This was a breakthrough technology from 2015 ImageNet competition, successfully applied by AlphaGo Zero to Go.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="training-efficiency-improvement">Training Efficiency Improvement<a href="#training-efficiency-improvement" class="hash-link" aria-label="Direct link to Training Efficiency Improvement" title="Direct link to Training Efficiency Improvement" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="exponential-growth-of-self-play">Exponential Growth of Self-Play<a href="#exponential-growth-of-self-play" class="hash-link" aria-label="Direct link to Exponential Growth of Self-Play" title="Direct link to Exponential Growth of Self-Play" translate="no">​</a></h3>
<p>AlphaGo Zero&#x27;s training process shows stunning efficiency:</p>
<table><thead><tr><th>Training Time</th><th>Elo Rating</th><th>Equivalent to</th></tr></thead><tbody><tr><td>0 hours</td><td>0</td><td>Random moves</td></tr><tr><td>3 hours</td><td>~1000</td><td>Discovers basic rules</td></tr><tr><td>12 hours</td><td>~3000</td><td>Discovers joseki</td></tr><tr><td>36 hours</td><td>~4500</td><td>Surpasses Fan Hui version</td></tr><tr><td>60 hours</td><td>~5200</td><td>Surpasses Lee Sedol version</td></tr><tr><td>72 hours</td><td>~5400</td><td>Surpasses original AlphaGo</td></tr><tr><td>40 days</td><td>~5600</td><td>Strongest version</td></tr></tbody></table>
<p><strong>Three days to surpass humans, three days to surpass AI that took months to train</strong> - this is exponential efficiency improvement.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="why-so-fast">Why So Fast?<a href="#why-so-fast" class="hash-link" aria-label="Direct link to Why So Fast?" title="Direct link to Why So Fast?" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-stronger-search-guidance">1. Stronger Search Guidance<a href="#1-stronger-search-guidance" class="hash-link" aria-label="Direct link to 1. Stronger Search Guidance" title="Direct link to 1. Stronger Search Guidance" translate="no">​</a></h4>
<p>AlphaGo Zero&#x27;s MCTS is completely guided by neural network, no longer using fast rollout policy. This makes search more efficient and accurate.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-faster-self-play">2. Faster Self-Play<a href="#2-faster-self-play" class="hash-link" aria-label="Direct link to 2. Faster Self-Play" title="Direct link to 2. Faster Self-Play" translate="no">​</a></h4>
<p>Since only one network is needed (not two), computational cost per self-play game decreases. This means more training data in same time.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-more-effective-learning">3. More Effective Learning<a href="#3-more-effective-learning" class="hash-link" aria-label="Direct link to 3. More Effective Learning" title="Direct link to 3. More Effective Learning" translate="no">​</a></h4>
<p>Dual-head network&#x27;s joint training lets each game&#x27;s information be used more effectively. Policy and Value gradients reinforce each other, accelerating convergence.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="comparison-with-human-learning">Comparison with Human Learning<a href="#comparison-with-human-learning" class="hash-link" aria-label="Direct link to Comparison with Human Learning" title="Direct link to Comparison with Human Learning" translate="no">​</a></h3>
<p>How long does it take human players to reach different levels?</p>
<table><thead><tr><th>Level</th><th>Human Time</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Beginner</td><td>Weeks</td><td>Minutes</td></tr><tr><td>Amateur 1-dan</td><td>Years</td><td>Hours</td></tr><tr><td>Professional</td><td>10-20 years</td><td>1-2 days</td></tr><tr><td>World Champion</td><td>20+ years full-time</td><td>3 days</td></tr><tr><td>Surpass humans</td><td>Impossible</td><td>3 days</td></tr></tbody></table>
<p>This comparison isn&#x27;t to diminish human players - they use biological neurons while AlphaGo Zero uses specially designed TPUs and kilowatts of electricity. But it does show how efficient the right learning method can be.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="generality-chess-shogi">Generality: Chess, Shogi<a href="#generality-chess-shogi" class="hash-link" aria-label="Direct link to Generality: Chess, Shogi" title="Direct link to Generality: Chess, Shogi" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="birth-of-alphazero">Birth of AlphaZero<a href="#birth-of-alphazero" class="hash-link" aria-label="Direct link to Birth of AlphaZero" title="Direct link to Birth of AlphaZero" translate="no">​</a></h3>
<p>In December 2017, DeepMind announced <strong>AlphaZero</strong> - the general version of AlphaGo Zero. Same algorithm, just changing game rules, achieved world-class level in three different board games:</p>
<table><thead><tr><th>Game</th><th>Training Time</th><th>Opponent</th><th>Record</th></tr></thead><tbody><tr><td>Go</td><td>8 hours</td><td>AlphaGo Zero</td><td>60:40</td></tr><tr><td>Chess</td><td>4 hours</td><td>Stockfish 8</td><td>28 wins 72 draws 0 losses</td></tr><tr><td>Shogi</td><td>2 hours</td><td>Elmo</td><td>90:8:2</td></tr></tbody></table>
<p>Note the opponents:</p>
<ul>
<li class=""><strong>Stockfish</strong> was then the strongest chess engine, using decades of human knowledge and optimization</li>
<li class=""><strong>Elmo</strong> was then the strongest Shogi AI</li>
</ul>
<p>AlphaZero, with hours of training, surpassed these systems that took years to develop.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="significance-of-generality">Significance of Generality<a href="#significance-of-generality" class="hash-link" aria-label="Direct link to Significance of Generality" title="Direct link to Significance of Generality" translate="no">​</a></h3>
<p>AlphaGo Zero / AlphaZero proved something important:</p>
<blockquote>
<p><strong>The same learning algorithm can achieve superhuman level in different domains.</strong></p>
</blockquote>
<p>This isn&#x27;t three different AIs, but one general learning framework:</p>
<ol>
<li class=""><strong>Self-play</strong> generates experience</li>
<li class=""><strong>Monte Carlo Tree Search</strong> explores possibilities</li>
<li class=""><strong>Neural network</strong> learns policy and value functions</li>
<li class=""><strong>Reinforcement learning</strong> optimizes objective function</li>
</ol>
<p>This framework doesn&#x27;t depend on domain-specific knowledge, taking an important step toward AI generalization.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="impact-on-traditional-ai">Impact on Traditional AI<a href="#impact-on-traditional-ai" class="hash-link" aria-label="Direct link to Impact on Traditional AI" title="Direct link to Impact on Traditional AI" translate="no">​</a></h3>
<p>Before AlphaZero, the strongest chess and shogi AIs were &quot;expert system&quot; style:</p>
<ul>
<li class=""><strong>Extensive human knowledge</strong>: Opening books, endgame tables, evaluation functions</li>
<li class=""><strong>Decades of optimization</strong>: Countless players&#x27; and engineers&#x27; work</li>
<li class=""><strong>Extreme specialization</strong>: Stockfish can&#x27;t play Go, Elmo can&#x27;t play chess</li>
</ul>
<p>AlphaZero surpassed all this with one general algorithm in hours. This made many AI researchers rethink:</p>
<blockquote>
<p>Should we invest more in &quot;general learning algorithms&quot; or &quot;expert knowledge encoding&quot;?</p>
</blockquote>
<p>The answer seems increasingly clear: letting machines learn themselves is more effective than teaching them knowledge.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zeros-playing-style">AlphaGo Zero&#x27;s Playing Style<a href="#alphago-zeros-playing-style" class="hash-link" aria-label="Direct link to AlphaGo Zero&#x27;s Playing Style" title="Direct link to AlphaGo Zero&#x27;s Playing Style" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="beyond-human-aesthetics">Beyond Human Aesthetics<a href="#beyond-human-aesthetics" class="hash-link" aria-label="Direct link to Beyond Human Aesthetics" title="Direct link to Beyond Human Aesthetics" translate="no">​</a></h3>
<p>Go community&#x27;s common evaluation of AlphaGo Zero&#x27;s play: <strong>more elegant.</strong></p>
<p>AlphaGo Lee&#x27;s moves sometimes seemed &quot;strange&quot; - like Move 37, humans needed post-game analysis to understand its brilliance. But AlphaGo Zero&#x27;s moves are often evaluated as &quot;obviously good at first glance.&quot;</p>
<p>This might be because:</p>
<ol>
<li class=""><strong>Stronger play</strong>: Zero sees deeper, plays more calmly</li>
<li class=""><strong>No human bias</strong>: Not bound by traditional joseki</li>
<li class=""><strong>Consistent objective</strong>: Only pursues win rate, doesn&#x27;t imitate humans</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="rediscovering-human-go-principles">Rediscovering Human Go Principles<a href="#rediscovering-human-go-principles" class="hash-link" aria-label="Direct link to Rediscovering Human Go Principles" title="Direct link to Rediscovering Human Go Principles" translate="no">​</a></h3>
<p>Interestingly, AlphaGo Zero &quot;rediscovered&quot; thousands of years of accumulated Go knowledge during training:</p>
<ul>
<li class=""><strong>Joseki</strong>: Zero discovered many common joseki on its own, because these are indeed optimal solutions for both sides</li>
<li class=""><strong>Opening principles</strong>: Order of importance of corners, edges, center</li>
<li class=""><strong>Shape knowledge</strong>: Difference between bad and good shapes</li>
</ul>
<p>This validates the reasonableness of human Go principles - this knowledge isn&#x27;t coincidental, but reflects Go&#x27;s essence.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="beyond-human-innovation">Beyond Human Innovation<a href="#beyond-human-innovation" class="hash-link" aria-label="Direct link to Beyond Human Innovation" title="Direct link to Beyond Human Innovation" translate="no">​</a></h3>
<p>But Zero also discovered moves humans never thought of:</p>
<ul>
<li class=""><strong>Unconventional openings</strong>: Variations on traditional openings</li>
<li class=""><strong>Aggressive sacrifice</strong>: More willing than humans to give up locally for global advantage</li>
<li class=""><strong>Counter-intuitive shapes</strong>: Seemingly &quot;bad shapes&quot; that are actually optimal</li>
</ul>
<p>These innovations are changing human understanding of Go. Many professional players say studying AlphaGo Zero&#x27;s games gave them a completely new understanding of Go.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="technical-details-summary">Technical Details Summary<a href="#technical-details-summary" class="hash-link" aria-label="Direct link to Technical Details Summary" title="Direct link to Technical Details Summary" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="complete-comparison-with-original-alphago">Complete Comparison with Original AlphaGo<a href="#complete-comparison-with-original-alphago" class="hash-link" aria-label="Direct link to Complete Comparison with Original AlphaGo" title="Direct link to Complete Comparison with Original AlphaGo" translate="no">​</a></h3>
<table><thead><tr><th>Aspect</th><th>AlphaGo (Original)</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td><strong>Training data</strong></td><td>Human games + self-play</td><td>Pure self-play</td></tr><tr><td><strong>Learning method</strong></td><td>Supervised + reinforcement</td><td>Pure reinforcement</td></tr><tr><td><strong>Input features</strong></td><td>48 planes</td><td>17 planes</td></tr><tr><td><strong>Network architecture</strong></td><td>Separate Policy/Value</td><td>Dual-head ResNet</td></tr><tr><td><strong>Network depth</strong></td><td>13 layers</td><td>40 layers (or more)</td></tr><tr><td><strong>MCTS evaluation</strong></td><td>Neural network + Rollout</td><td>Pure neural network</td></tr><tr><td><strong>Simulations/move</strong></td><td>~100,000</td><td>~1,600</td></tr><tr><td><strong>Training TPUs</strong></td><td>50+</td><td>4</td></tr><tr><td><strong>Inference TPUs</strong></td><td>48</td><td>4 (scalable)</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="core-algorithm">Core Algorithm<a href="#core-algorithm" class="hash-link" aria-label="Direct link to Core Algorithm" title="Direct link to Core Algorithm" translate="no">​</a></h3>
<p>AlphaGo Zero&#x27;s training loop is very concise:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. Self-play</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Perform MCTS with current network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Select moves by MCTS search probabilities</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Record each move&#x27;s (position, MCTS probabilities, win/loss result)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Train network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Sample from experience buffer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Policy Head: Minimize cross-entropy with MCTS probabilities</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Value Head: Minimize MSE with actual win/loss</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Jointly optimize both objectives</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Update network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Replace old network with new (verify new is stronger through play)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Return to step 1</span><br></span></code></pre></div></div>
<p>This loop runs continuously, network keeps improving. No human data, no human knowledge, just game rules and win/loss objective.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="insights-for-ai-research">Insights for AI Research<a href="#insights-for-ai-research" class="hash-link" aria-label="Direct link to Insights for AI Research" title="Direct link to Insights for AI Research" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="first-principles-learning">First-Principles Learning<a href="#first-principles-learning" class="hash-link" aria-label="Direct link to First-Principles Learning" title="Direct link to First-Principles Learning" translate="no">​</a></h3>
<p>AlphaGo Zero demonstrates a &quot;first-principles&quot; learning approach:</p>
<blockquote>
<p>Don&#x27;t tell AI how to do it, only tell it what the goal is, let it discover methods itself.</p>
</blockquote>
<p>This contrasts sharply with traditional expert system approaches. Expert systems try to encode human knowledge into AI, while AlphaGo Zero lets AI discover knowledge itself.</p>
<p>Result: AI-discovered knowledge may be more complete and accurate than human knowledge.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="power-of-self-play">Power of Self-Play<a href="#power-of-self-play" class="hash-link" aria-label="Direct link to Power of Self-Play" title="Direct link to Power of Self-Play" translate="no">​</a></h3>
<p>AlphaGo Zero proved self-play can generate unlimited training data, and this data&#x27;s quality improves as network improves.</p>
<p>This is a &quot;positive feedback loop&quot;:</p>
<ul>
<li class="">Stronger network → Better self-play data</li>
<li class="">Better data → Stronger network</li>
</ul>
<p>This loop can continue until reaching the game&#x27;s theoretical limit (if one exists).</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="importance-of-simplification">Importance of Simplification<a href="#importance-of-simplification" class="hash-link" aria-label="Direct link to Importance of Simplification" title="Direct link to Importance of Simplification" translate="no">​</a></h3>
<p>AlphaGo Zero&#x27;s success proves the importance of &quot;simplification&quot;:</p>
<ul>
<li class="">Simplified input (48 → 17)</li>
<li class="">Simplified architecture (dual network → single network)</li>
<li class="">Simplified training (supervised + reinforcement → pure reinforcement)</li>
</ul>
<p>Each simplification made the system more powerful. This tells us: complex doesn&#x27;t equal good, the simplest solution is often the best.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="animation-reference">Animation Reference<a href="#animation-reference" class="hash-link" aria-label="Direct link to Animation Reference" title="Direct link to Animation Reference" translate="no">​</a></h2>
<p>Core concepts covered in this article with animation numbers:</p>
<table><thead><tr><th>Number</th><th>Concept</th><th>Physics/Math Correspondence</th></tr></thead><tbody><tr><td>Animation E7</td><td>Training from scratch</td><td>Self-organization</td></tr><tr><td>Animation E5</td><td>Self-play</td><td>Fixed-point convergence</td></tr><tr><td>Animation E12</td><td>Strength growth curve</td><td>S-curve growth</td></tr><tr><td>Animation D12</td><td>Residual network</td><td>Gradient highway</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="further-reading">Further Reading<a href="#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading" translate="no">​</a></h2>
<ul>
<li class=""><strong>Next</strong>: <a class="" href="/en/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/">Dual-Head Network and Residual Network</a> - AlphaGo Zero&#x27;s neural network architecture in detail</li>
<li class=""><strong>Related Article</strong>: <a class="" href="/en/docs/for-engineers/how-it-works/alphago-explained/self-play/">Self-Play</a> - Why self-play produces superhuman level</li>
<li class=""><strong>Technical Deep Dive</strong>: <a class="" href="/en/docs/for-engineers/how-it-works/alphago-explained/training-from-scratch/">Training from Scratch</a> - Detailed Day 0-3 evolution</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">Silver, D., et al. (2018). &quot;A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.&quot; <em>Science</em>, 362(6419), 1140-1144.</li>
<li class="">DeepMind. (2017). &quot;AlphaGo Zero: Starting from scratch.&quot; <em>DeepMind Blog</em>.</li>
<li class="">Schrittwieser, J., et al. (2020). &quot;Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.&quot; <em>Nature</em>, 588, 604-609.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/16-alphago-zero.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/docs/for-engineers/how-it-works/alphago-explained/puct-formula/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">PUCT Formula Explained</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Dual-Head Network and Residual Network</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#why-no-human-games-needed" class="table-of-contents__link toc-highlight">Why No Human Games Needed?</a><ul><li><a href="#limitations-of-human-games" class="table-of-contents__link toc-highlight">Limitations of Human Games</a></li><li><a href="#zeros-breakthrough" class="table-of-contents__link toc-highlight">Zero&#39;s Breakthrough</a></li></ul></li><li><a href="#comparison-with-original-alphago-1000" class="table-of-contents__link toc-highlight">Comparison with Original AlphaGo: 100:0</a><ul><li><a href="#crushing-victory" class="table-of-contents__link toc-highlight">Crushing Victory</a></li><li><a href="#less-resources-stronger-play" class="table-of-contents__link toc-highlight">Less Resources, Stronger Play</a></li><li><a href="#why-is-zero-stronger" class="table-of-contents__link toc-highlight">Why Is Zero Stronger?</a></li></ul></li><li><a href="#simplified-input-features-from-48-to-17" class="table-of-contents__link toc-highlight">Simplified Input Features: From 48 to 17</a><ul><li><a href="#original-alphagos-48-feature-planes" class="table-of-contents__link toc-highlight">Original AlphaGo&#39;s 48 Feature Planes</a></li><li><a href="#alphago-zeros-17-feature-planes" class="table-of-contents__link toc-highlight">AlphaGo Zero&#39;s 17 Feature Planes</a></li><li><a href="#why-is-simplification-good" class="table-of-contents__link toc-highlight">Why Is Simplification Good?</a></li></ul></li><li><a href="#single-network-architecture" class="table-of-contents__link toc-highlight">Single Network Architecture</a><ul><li><a href="#original-dual-network-design" class="table-of-contents__link toc-highlight">Original Dual-Network Design</a></li><li><a href="#zeros-dual-head-network" class="table-of-contents__link toc-highlight">Zero&#39;s Dual-Head Network</a></li><li><a href="#power-of-residual-networks" class="table-of-contents__link toc-highlight">Power of Residual Networks</a></li></ul></li><li><a href="#training-efficiency-improvement" class="table-of-contents__link toc-highlight">Training Efficiency Improvement</a><ul><li><a href="#exponential-growth-of-self-play" class="table-of-contents__link toc-highlight">Exponential Growth of Self-Play</a></li><li><a href="#why-so-fast" class="table-of-contents__link toc-highlight">Why So Fast?</a></li><li><a href="#comparison-with-human-learning" class="table-of-contents__link toc-highlight">Comparison with Human Learning</a></li></ul></li><li><a href="#generality-chess-shogi" class="table-of-contents__link toc-highlight">Generality: Chess, Shogi</a><ul><li><a href="#birth-of-alphazero" class="table-of-contents__link toc-highlight">Birth of AlphaZero</a></li><li><a href="#significance-of-generality" class="table-of-contents__link toc-highlight">Significance of Generality</a></li><li><a href="#impact-on-traditional-ai" class="table-of-contents__link toc-highlight">Impact on Traditional AI</a></li></ul></li><li><a href="#alphago-zeros-playing-style" class="table-of-contents__link toc-highlight">AlphaGo Zero&#39;s Playing Style</a><ul><li><a href="#beyond-human-aesthetics" class="table-of-contents__link toc-highlight">Beyond Human Aesthetics</a></li><li><a href="#rediscovering-human-go-principles" class="table-of-contents__link toc-highlight">Rediscovering Human Go Principles</a></li><li><a href="#beyond-human-innovation" class="table-of-contents__link toc-highlight">Beyond Human Innovation</a></li></ul></li><li><a href="#technical-details-summary" class="table-of-contents__link toc-highlight">Technical Details Summary</a><ul><li><a href="#complete-comparison-with-original-alphago" class="table-of-contents__link toc-highlight">Complete Comparison with Original AlphaGo</a></li><li><a href="#core-algorithm" class="table-of-contents__link toc-highlight">Core Algorithm</a></li></ul></li><li><a href="#insights-for-ai-research" class="table-of-contents__link toc-highlight">Insights for AI Research</a><ul><li><a href="#first-principles-learning" class="table-of-contents__link toc-highlight">First-Principles Learning</a></li><li><a href="#power-of-self-play" class="table-of-contents__link toc-highlight">Power of Self-Play</a></li><li><a href="#importance-of-simplification" class="table-of-contents__link toc-highlight">Importance of Simplification</a></li></ul></li><li><a href="#animation-reference" class="table-of-contents__link toc-highlight">Animation Reference</a></li><li><a href="#further-reading" class="table-of-contents__link toc-highlight">Further Reading</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>