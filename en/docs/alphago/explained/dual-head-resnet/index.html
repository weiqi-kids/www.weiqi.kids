<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-alphago/explained/dual-head-resnet" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Dual-Head Network and Residual Network | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/en/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/en/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/en/docs/alphago/explained/dual-head-resnet/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Dual-Head Network and Residual Network | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="Deep dive into AlphaGo Zero&#x27;s neural network architecture - shared backbone, Policy Head, Value Head, and 40-layer ResNet"><meta data-rh="true" property="og:description" content="Deep dive into AlphaGo Zero&#x27;s neural network architecture - shared backbone, Policy Head, Value Head, and 40-layer ResNet"><meta data-rh="true" name="keywords" content="dual-head network,residual network,ResNet,Policy Head,Value Head,deep learning,neural network architecture"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/en/docs/alphago/explained/dual-head-resnet/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/explained/dual-head-resnet/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/alphago/explained/dual-head-resnet/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/alphago/explained/dual-head-resnet/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/alphago/explained/dual-head-resnet/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/alphago/explained/dual-head-resnet/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/alphago/explained/dual-head-resnet/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/alphago/explained/dual-head-resnet/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/alphago/explained/dual-head-resnet/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/alphago/explained/dual-head-resnet/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/alphago/explained/dual-head-resnet/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/alphago/explained/dual-head-resnet/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/explained/dual-head-resnet/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AlphaGo","item":"https://www.weiqi.kids/en/docs/alphago/"},{"@type":"ListItem","position":2,"name":"完整解析","item":"https://www.weiqi.kids/en/docs/alphago/explained/"},{"@type":"ListItem","position":3,"name":"Dual-Head Network and Residual Network","item":"https://www.weiqi.kids/en/docs/alphago/explained/dual-head-resnet"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.f23bf74b.css">
<script src="/en/assets/js/runtime~main.412c6837.js" defer="defer"></script>
<script src="/en/assets/js/main.37a937f4.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/en/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/logo.svg" alt="Good Go Baby Association Logo" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/en/img/logo.svg" alt="Good Go Baby Association Logo" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">Weiqi.Kids</b></a><a class="navbar__item navbar__link" href="/en/docs/learn/">Learn Go</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/docs/alphago/">AlphaGo</a><a class="navbar__item navbar__link" href="/en/docs/animations/">Animation Studio</a><a class="navbar__item navbar__link" href="/en/docs/tech/">Technical Docs</a><a class="navbar__item navbar__link" href="/en/docs/about/">About Us</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/ja/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/docs/intro/"><span title="User Guide" class="linkLabel_REp1">User Guide</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/learn/"><span title="學圍棋" class="categoryLinkLabel_ezQx">學圍棋</span></a><button aria-label="Expand sidebar category &#x27;學圍棋&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/en/docs/alphago/"><span title="AlphaGo" class="categoryLinkLabel_ezQx">AlphaGo</span></a><button aria-label="Collapse sidebar category &#x27;AlphaGo&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/en/docs/alphago/explained/"><span title="完整解析" class="categoryLinkLabel_ezQx">完整解析</span></a><button aria-label="Collapse sidebar category &#x27;完整解析&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/birth-of-alphago/"><span title="The Birth of AlphaGo" class="linkLabel_REp1">The Birth of AlphaGo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/key-matches/"><span title="Key Matches Review" class="linkLabel_REp1">Key Matches Review</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/move-37/"><span title="Deep Analysis of &quot;The Divine Move&quot;" class="linkLabel_REp1">Deep Analysis of &quot;The Divine Move&quot;</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/why-go-is-hard/"><span title="Why Is Go Hard?" class="linkLabel_REp1">Why Is Go Hard?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/traditional-limits/"><span title="Limits of Traditional Methods" class="linkLabel_REp1">Limits of Traditional Methods</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/board-representation/"><span title="Board State Representation" class="linkLabel_REp1">Board State Representation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/policy-network/"><span title="Policy Network Deep Dive" class="linkLabel_REp1">Policy Network Deep Dive</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/value-network/"><span title="Value Network Explained" class="linkLabel_REp1">Value Network Explained</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/input-features/"><span title="Input Feature Design" class="linkLabel_REp1">Input Feature Design</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/cnn-and-go/"><span title="CNN and Go" class="linkLabel_REp1">CNN and Go</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/supervised-learning/"><span title="Supervised Learning Phase" class="linkLabel_REp1">Supervised Learning Phase</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/reinforcement-intro/"><span title="Introduction to Reinforcement Learning" class="linkLabel_REp1">Introduction to Reinforcement Learning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/self-play/"><span title="Self-Play" class="linkLabel_REp1">Self-Play</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/mcts-neural-combo/"><span title="MCTS and Neural Network Integration" class="linkLabel_REp1">MCTS and Neural Network Integration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/puct-formula/"><span title="PUCT Formula Explained" class="linkLabel_REp1">PUCT Formula Explained</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/alphago-zero/"><span title="AlphaGo Zero Overview" class="linkLabel_REp1">AlphaGo Zero Overview</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/docs/alphago/explained/dual-head-resnet/"><span title="Dual-Head Network and Residual Network" class="linkLabel_REp1">Dual-Head Network and Residual Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/training-from-scratch/"><span title="Training from Scratch" class="linkLabel_REp1">Training from Scratch</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/distributed-systems/"><span title="Distributed Systems and TPU" class="linkLabel_REp1">Distributed Systems and TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/docs/alphago/explained/legacy-and-impact/"><span title="AlphaGo&#x27;s Legacy" class="linkLabel_REp1">AlphaGo&#x27;s Legacy</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/docs/animations/"><span title="動畫教室" class="linkLabel_REp1">動畫教室</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/tech/"><span title="技術文件" class="categoryLinkLabel_ezQx">技術文件</span></a><button aria-label="Expand sidebar category &#x27;技術文件&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/en/docs/about/"><span title="關於我們" class="categoryLinkLabel_ezQx">關於我們</span></a><button aria-label="Expand sidebar category &#x27;關於我們&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/en/docs/alphago/"><span>AlphaGo</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/en/docs/alphago/explained/"><span>完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Dual-Head Network and Residual Network</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Dual-Head Network and Residual Network</h1></header>
<p>One of AlphaGo Zero&#x27;s most important architectural innovations is using a <strong>Dual-Head Network</strong> to replace the original AlphaGo&#x27;s dual-network design. This seemingly simple change brought significant performance improvements and a more elegant learning process.</p>
<p>This article will deeply analyze the design principles, mathematical foundations, and why this architecture is so effective.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="dual-head-network-design">Dual-Head Network Design<a href="#dual-head-network-design" class="hash-link" aria-label="Direct link to Dual-Head Network Design" title="Direct link to Dual-Head Network Design" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="overall-architecture">Overall Architecture<a href="#overall-architecture" class="hash-link" aria-label="Direct link to Overall Architecture" title="Direct link to Overall Architecture" translate="no">​</a></h3>
<p>AlphaGo Zero&#x27;s neural network can be divided into three parts:</p>
<!-- -->
<p>Let&#x27;s analyze each part in detail.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="shared-backbone">Shared Backbone<a href="#shared-backbone" class="hash-link" aria-label="Direct link to Shared Backbone" title="Direct link to Shared Backbone" translate="no">​</a></h3>
<p>The shared backbone is a deep <strong>Residual Network (ResNet)</strong> responsible for extracting features from the board state.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="architecture-details">Architecture Details<a href="#architecture-details" class="hash-link" aria-label="Direct link to Architecture Details" title="Direct link to Architecture Details" translate="no">​</a></h4>
<table><thead><tr><th>Component</th><th>Specification</th></tr></thead><tbody><tr><td>Input layer</td><td>3×3 convolution, 256 channels</td></tr><tr><td>Residual blocks</td><td>40 (or 20 for compact version)</td></tr><tr><td>Each residual block</td><td>2 layers of 3×3 convolution, 256 channels</td></tr><tr><td>Activation function</td><td>ReLU</td></tr><tr><td>Normalization</td><td>Batch Normalization</td></tr></tbody></table>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="mathematical-representation">Mathematical Representation<a href="#mathematical-representation" class="hash-link" aria-label="Direct link to Mathematical Representation" title="Direct link to Mathematical Representation" translate="no">​</a></h4>
<p>Let input be x (dimension 17 × 19 × 19), the shared backbone output is:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">f(x) = ResNet_40(Conv_3x3(x))</span><br></span></code></pre></div></div>
<p>where f(x) (dimension 256 × 19 × 19) is a high-dimensional feature representation.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="policy-head">Policy Head<a href="#policy-head" class="hash-link" aria-label="Direct link to Policy Head" title="Direct link to Policy Head" translate="no">​</a></h3>
<p>The Policy Head is responsible for predicting the move probability for each position.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="architecture-details-1">Architecture Details<a href="#architecture-details-1" class="hash-link" aria-label="Direct link to Architecture Details" title="Direct link to Architecture Details" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Shared backbone output (256 × 19 × 19)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1×1 convolution (2 channels)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Batch Normalization</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ReLU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Flatten (2 × 19 × 19 = 722)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Fully connected layer (362)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Softmax</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Output: 362 probabilities (361 positions + Pass)</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="mathematical-representation-1">Mathematical Representation<a href="#mathematical-representation-1" class="hash-link" aria-label="Direct link to Mathematical Representation" title="Direct link to Mathematical Representation" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">π = Softmax(FC(Flatten(ReLU(BN(Conv_1x1(f(x)))))))</span><br></span></code></pre></div></div>
<p>Output π is a 362-dimensional vector where all elements are non-negative and sum to 1.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="value-head">Value Head<a href="#value-head" class="hash-link" aria-label="Direct link to Value Head" title="Direct link to Value Head" translate="no">​</a></h3>
<p>The Value Head is responsible for predicting the win rate of the current position.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="architecture-details-2">Architecture Details<a href="#architecture-details-2" class="hash-link" aria-label="Direct link to Architecture Details" title="Direct link to Architecture Details" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Shared backbone output (256 × 19 × 19)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1×1 convolution (1 channel)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Batch Normalization</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ReLU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Flatten (1 × 19 × 19 = 361)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Fully connected layer (256)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ReLU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Fully connected layer (1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Tanh</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Output: Win rate [-1, 1]</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="mathematical-representation-2">Mathematical Representation<a href="#mathematical-representation-2" class="hash-link" aria-label="Direct link to Mathematical Representation" title="Direct link to Mathematical Representation" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">v = Tanh(FC_1(ReLU(FC_2(Flatten(ReLU(BN(Conv_1x1(f(x)))))))))</span><br></span></code></pre></div></div>
<p>Output v ranges from [-1, 1]:</p>
<ul>
<li class="">v = 1: Current side will definitely win</li>
<li class="">v = -1: Current side will definitely lose</li>
<li class="">v = 0: Even game</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="why-share-the-backbone">Why Share the Backbone?<a href="#why-share-the-backbone" class="hash-link" aria-label="Direct link to Why Share the Backbone?" title="Direct link to Why Share the Backbone?" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="intuitive-understanding">Intuitive Understanding<a href="#intuitive-understanding" class="hash-link" aria-label="Direct link to Intuitive Understanding" title="Direct link to Intuitive Understanding" translate="no">​</a></h3>
<p>The two questions &quot;where should the next move be&quot; (Policy) and &quot;who will win&quot; (Value) actually require understanding the same board patterns:</p>
<ul>
<li class=""><strong>Shapes</strong>: Which shapes are good, which are bad</li>
<li class=""><strong>Influence</strong>: Which side is bigger, where there&#x27;s still space</li>
<li class=""><strong>Life and death</strong>: Which groups are alive, which are still in ko</li>
<li class=""><strong>Fighting</strong>: Where are the attacks, what&#x27;s the local outcome</li>
</ul>
<p>If using two independent networks, these features need to be learned twice. A shared backbone lets these underlying features be learned once and used by both tasks.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="multi-task-learning-perspective">Multi-task Learning Perspective<a href="#multi-task-learning-perspective" class="hash-link" aria-label="Direct link to Multi-task Learning Perspective" title="Direct link to Multi-task Learning Perspective" translate="no">​</a></h3>
<p>From a machine learning perspective, this is a form of <strong>Multi-task Learning</strong>:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L = L_policy + L_value</span><br></span></code></pre></div></div>
<p>Two tasks sharing the underlying representation brings several benefits:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-regularization-effect">1. Regularization Effect<a href="#1-regularization-effect" class="hash-link" aria-label="Direct link to 1. Regularization Effect" title="Direct link to 1. Regularization Effect" translate="no">​</a></h4>
<p>Shared parameters act as implicit regularization. If a feature is only useful for Policy but not Value (or vice versa), it&#x27;s harder to be overly amplified.</p>
<p>The effective parameter count is smaller than two independent networks.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-data-efficiency">2. Data Efficiency<a href="#2-data-efficiency" class="hash-link" aria-label="Direct link to 2. Data Efficiency" title="Direct link to 2. Data Efficiency" translate="no">​</a></h4>
<p>Each game simultaneously produces Policy labels (MCTS search probabilities) and Value labels (final outcome). The shared backbone uses both labels to train shared features, improving data utilization efficiency.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-rich-gradient-signal">3. Rich Gradient Signal<a href="#3-rich-gradient-signal" class="hash-link" aria-label="Direct link to 3. Rich Gradient Signal" title="Direct link to 3. Rich Gradient Signal" translate="no">​</a></h4>
<p>Gradients from both tasks flow to the shared backbone:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">∂L/∂θ_shared = ∂L_policy/∂θ_shared + ∂L_value/∂θ_shared</span><br></span></code></pre></div></div>
<p>This provides richer supervision signal, making shared features more robust.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="experimental-evidence">Experimental Evidence<a href="#experimental-evidence" class="hash-link" aria-label="Direct link to Experimental Evidence" title="Direct link to Experimental Evidence" translate="no">​</a></h3>
<p>DeepMind&#x27;s ablation experiments showed that dual-head networks significantly outperform separate dual networks:</p>
<table><thead><tr><th>Configuration</th><th>ELO Rating</th><th>Relative Gap</th></tr></thead><tbody><tr><td>Separate Policy + Value networks</td><td>Baseline</td><td>-</td></tr><tr><td>Dual-head network (shared backbone)</td><td>+300 ELO</td><td>~65% win rate difference</td></tr></tbody></table>
<p>A 300 ELO gap means the dual-head network has about 65% win rate against separated networks. This is a significant improvement.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="residual-network-principles">Residual Network Principles<a href="#residual-network-principles" class="hash-link" aria-label="Direct link to Residual Network Principles" title="Direct link to Residual Network Principles" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="the-deep-network-dilemma">The Deep Network Dilemma<a href="#the-deep-network-dilemma" class="hash-link" aria-label="Direct link to The Deep Network Dilemma" title="Direct link to The Deep Network Dilemma" translate="no">​</a></h3>
<p>Before ResNet was invented, deep neural networks faced a paradox:</p>
<blockquote>
<p>In theory, deeper networks should be at least as good as shallow networks (in the worst case, extra layers can learn identity mapping). But in practice, deeper networks often performed worse.</p>
</blockquote>
<p>This is the <strong>Degradation Problem</strong>:</p>
<ul>
<li class="">Training error increases with depth (not overfitting, but optimization difficulty)</li>
<li class="">Gradients vanish during backpropagation (Vanishing Gradient)</li>
<li class="">Parameters in deep layers can hardly be effectively updated</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="residual-block-design">Residual Block Design<a href="#residual-block-design" class="hash-link" aria-label="Direct link to Residual Block Design" title="Direct link to Residual Block Design" translate="no">​</a></h3>
<p>In 2015, Kaiming He and colleagues proposed a simple and elegant solution: <strong>Skip Connection</strong>.</p>
<!-- -->
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="mathematical-representation-3">Mathematical Representation<a href="#mathematical-representation-3" class="hash-link" aria-label="Direct link to Mathematical Representation" title="Direct link to Mathematical Representation" translate="no">​</a></h4>
<p>Traditional network: Learn target mapping H(x)</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">y = H(x)</span><br></span></code></pre></div></div>
<p>Residual network: Learn <strong>residual mapping</strong> F(x) = H(x) - x</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">y = F(x) + x</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="why-skip-connections-work">Why Skip Connections Work?<a href="#why-skip-connections-work" class="hash-link" aria-label="Direct link to Why Skip Connections Work?" title="Direct link to Why Skip Connections Work?" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-gradient-highway">1. Gradient Highway<a href="#1-gradient-highway" class="hash-link" aria-label="Direct link to 1. Gradient Highway" title="Direct link to 1. Gradient Highway" translate="no">​</a></h4>
<p>Consider the backpropagation gradient:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">∂L/∂x = ∂L/∂y × ∂y/∂x = ∂L/∂y × (1 + ∂F(x)/∂x)</span><br></span></code></pre></div></div>
<p>The key is that <strong>+1</strong>. Even if ∂F(x)/∂x is very small or zero, gradients can still flow directly back through +1.</p>
<p>It&#x27;s like building a &quot;gradient highway&quot; that lets gradients flow smoothly from output layer back to input layer.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-identity-mapping-is-easier-to-learn">2. Identity Mapping is Easier to Learn<a href="#2-identity-mapping-is-easier-to-learn" class="hash-link" aria-label="Direct link to 2. Identity Mapping is Easier to Learn" title="Direct link to 2. Identity Mapping is Easier to Learn" translate="no">​</a></h4>
<p>If the optimal solution is close to identity mapping (H(x) ≈ x), then:</p>
<ul>
<li class="">Traditional network: Needs to learn H(x) = x, which can be difficult</li>
<li class="">Residual network: Just needs to learn F(x) ≈ 0, relatively easy</li>
</ul>
<p>Initializing weights to zero or near zero, residual blocks naturally tend toward identity mapping.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-ensemble-effect">3. Ensemble Effect<a href="#3-ensemble-effect" class="hash-link" aria-label="Direct link to 3. Ensemble Effect" title="Direct link to 3. Ensemble Effect" translate="no">​</a></h4>
<p>Deep ResNet can be viewed as an <strong>implicit ensemble</strong> of many shallow networks. With n residual blocks, information can flow through 2^n different paths.</p>
<p>This ensemble effect increases model robustness.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="resnets-breakthrough-on-imagenet">ResNet&#x27;s Breakthrough on ImageNet<a href="#resnets-breakthrough-on-imagenet" class="hash-link" aria-label="Direct link to ResNet&#x27;s Breakthrough on ImageNet" title="Direct link to ResNet&#x27;s Breakthrough on ImageNet" translate="no">​</a></h3>
<p>ResNet achieved stunning results in the 2015 ImageNet competition:</p>
<table><thead><tr><th>Depth</th><th>Top-5 Error Rate</th></tr></thead><tbody><tr><td>VGG-19 (no residual)</td><td>7.3%</td></tr><tr><td>ResNet-34</td><td>5.7%</td></tr><tr><td>ResNet-152</td><td>4.5%</td></tr><tr><td>Human level</td><td>~5.1%</td></tr></tbody></table>
<p><strong>152-layer</strong> ResNet not only trains successfully but also performs much better than 19-layer VGG. This proves skip connections truly solve the deep network training problem.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zeros-40-layer-resnet">AlphaGo Zero&#x27;s 40-Layer ResNet<a href="#alphago-zeros-40-layer-resnet" class="hash-link" aria-label="Direct link to AlphaGo Zero&#x27;s 40-Layer ResNet" title="Direct link to AlphaGo Zero&#x27;s 40-Layer ResNet" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="why-choose-40-layers">Why Choose 40 Layers?<a href="#why-choose-40-layers" class="hash-link" aria-label="Direct link to Why Choose 40 Layers?" title="Direct link to Why Choose 40 Layers?" translate="no">​</a></h3>
<p>DeepMind tested ResNets of different depths:</p>
<table><thead><tr><th>Residual Blocks</th><th>Total Layers</th><th>ELO Rating</th></tr></thead><tbody><tr><td>5</td><td>11</td><td>Baseline</td></tr><tr><td>10</td><td>21</td><td>+200</td></tr><tr><td>20</td><td>41</td><td>+400</td></tr><tr><td>40</td><td>81</td><td>+500</td></tr></tbody></table>
<p>Deeper networks are indeed stronger, but with diminishing returns. AlphaGo Zero uses 20 or 40 residual blocks:</p>
<ul>
<li class=""><strong>AlphaGo Zero (paper version)</strong>: 40 residual blocks, 256 channels</li>
<li class=""><strong>Compact version</strong>: 20 residual blocks, 256 channels</li>
</ul>
<p>The 40-layer configuration achieves a good balance between playing strength and training cost.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="specific-configuration">Specific Configuration<a href="#specific-configuration" class="hash-link" aria-label="Direct link to Specific Configuration" title="Direct link to Specific Configuration" translate="no">​</a></h3>
<p>AlphaGo Zero&#x27;s ResNet configuration:</p>
<!-- -->
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="parameter-count-estimate">Parameter Count Estimate<a href="#parameter-count-estimate" class="hash-link" aria-label="Direct link to Parameter Count Estimate" title="Direct link to Parameter Count Estimate" translate="no">​</a></h4>
<table><thead><tr><th>Component</th><th>Parameters (approx.)</th></tr></thead><tbody><tr><td>Input convolution</td><td>17 × 3 × 3 × 256 ≈ 39K</td></tr><tr><td>Each residual block</td><td>2 × 256 × 3 × 3 × 256 ≈ 1.2M</td></tr><tr><td>40 residual blocks</td><td>40 × 1.2M ≈ 47M</td></tr><tr><td>Policy Head</td><td>~1M</td></tr><tr><td>Value Head</td><td>~0.2M</td></tr><tr><td><strong>Total</strong></td><td><strong>~48M</strong></td></tr></tbody></table>
<p>About 48 million parameters, a medium-sized neural network by modern standards.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="role-of-batch-normalization">Role of Batch Normalization<a href="#role-of-batch-normalization" class="hash-link" aria-label="Direct link to Role of Batch Normalization" title="Direct link to Role of Batch Normalization" translate="no">​</a></h3>
<p>Every convolutional layer is followed by <strong>Batch Normalization (BN)</strong>, which is crucial for training stability:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-normalize-activations">1. Normalize Activations<a href="#1-normalize-activations" class="hash-link" aria-label="Direct link to 1. Normalize Activations" title="Direct link to 1. Normalize Activations" translate="no">​</a></h4>
<p>BN normalizes each layer&#x27;s activations to mean 0 and variance 1:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">x_hat = (x - μ_B) / sqrt(σ_B² + ε)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">y = γ × x_hat + β</span><br></span></code></pre></div></div>
<p>where γ and β are learnable parameters.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-mitigate-internal-covariate-shift">2. Mitigate Internal Covariate Shift<a href="#2-mitigate-internal-covariate-shift" class="hash-link" aria-label="Direct link to 2. Mitigate Internal Covariate Shift" title="Direct link to 2. Mitigate Internal Covariate Shift" translate="no">​</a></h4>
<p>In deep networks, each layer&#x27;s input distribution changes as parameters in previous layers update. BN keeps each layer&#x27;s input distribution stable, accelerating training convergence.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-regularization-effect">3. Regularization Effect<a href="#3-regularization-effect" class="hash-link" aria-label="Direct link to 3. Regularization Effect" title="Direct link to 3. Regularization Effect" translate="no">​</a></h4>
<p>BN uses mini-batch statistics during training, introducing randomness and providing a mild regularization effect.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="comparison-with-other-architectures">Comparison with Other Architectures<a href="#comparison-with-other-architectures" class="hash-link" aria-label="Direct link to Comparison with Other Architectures" title="Direct link to Comparison with Other Architectures" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="vs-original-alphagos-cnn">vs. Original AlphaGo&#x27;s CNN<a href="#vs-original-alphagos-cnn" class="hash-link" aria-label="Direct link to vs. Original AlphaGo&#x27;s CNN" title="Direct link to vs. Original AlphaGo&#x27;s CNN" translate="no">​</a></h3>
<table><thead><tr><th>Feature</th><th>Original AlphaGo</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Architecture type</td><td>Standard CNN</td><td>ResNet</td></tr><tr><td>Depth</td><td>13 layers</td><td>41-81 layers</td></tr><tr><td>Skip connections</td><td>No</td><td>Yes</td></tr><tr><td>Number of networks</td><td>2 (separate)</td><td>1 (shared)</td></tr><tr><td>BN</td><td>No</td><td>Yes</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="vs-vgg-style-networks">vs. VGG-style Networks<a href="#vs-vgg-style-networks" class="hash-link" aria-label="Direct link to vs. VGG-style Networks" title="Direct link to vs. VGG-style Networks" translate="no">​</a></h3>
<p>VGG was the runner-up architecture in 2014 ImageNet, using stacked 3×3 convolutions:</p>
<table><thead><tr><th>Feature</th><th>VGG</th><th>ResNet</th></tr></thead><tbody><tr><td>Maximum trainable depth</td><td>~19 layers</td><td>152+ layers</td></tr><tr><td>Gradient flow</td><td>Decreases layer by layer</td><td>Has highway</td></tr><tr><td>Training difficulty</td><td>Deep is difficult</td><td>Deep is trainable</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="vs-inception--googlenet">vs. Inception / GoogLeNet<a href="#vs-inception--googlenet" class="hash-link" aria-label="Direct link to vs. Inception / GoogLeNet" title="Direct link to vs. Inception / GoogLeNet" translate="no">​</a></h3>
<p>Inception uses multi-scale convolutions in parallel:</p>
<table><thead><tr><th>Feature</th><th>Inception</th><th>ResNet</th></tr></thead><tbody><tr><td>Characteristic</td><td>Multi-scale features</td><td>Deep stacking</td></tr><tr><td>Complexity</td><td>Higher</td><td>Simple</td></tr><tr><td>Go suitability</td><td>Average</td><td>Excellent</td></tr></tbody></table>
<p>ResNet&#x27;s simple design is more suitable for tasks like Go that require deep reasoning.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="vs-transformer">vs. Transformer<a href="#vs-transformer" class="hash-link" aria-label="Direct link to vs. Transformer" title="Direct link to vs. Transformer" translate="no">​</a></h3>
<p>The Transformer architecture proposed in 2017 achieved great success in NLP. Some have attempted to apply Transformers to Go:</p>
<table><thead><tr><th>Feature</th><th>ResNet</th><th>Transformer</th></tr></thead><tbody><tr><td>Inductive bias</td><td>Locality (convolution)</td><td>Global attention</td></tr><tr><td>Position encoding</td><td>Implicit (convolution)</td><td>Explicit</td></tr><tr><td>Go performance</td><td>Excellent</td><td>Feasible but not better than ResNet</td></tr><tr><td>Computational efficiency</td><td>Higher</td><td>Lower (O(n²))</td></tr></tbody></table>
<p>For problems with obvious spatial structure like Go, CNN/ResNet&#x27;s inductive bias is more appropriate.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="deep-analysis-of-design-choices">Deep Analysis of Design Choices<a href="#deep-analysis-of-design-choices" class="hash-link" aria-label="Direct link to Deep Analysis of Design Choices" title="Direct link to Deep Analysis of Design Choices" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="why-use-33-convolutions">Why Use 3×3 Convolutions?<a href="#why-use-33-convolutions" class="hash-link" aria-label="Direct link to Why Use 3×3 Convolutions?" title="Direct link to Why Use 3×3 Convolutions?" translate="no">​</a></h3>
<p>AlphaGo Zero uses 3×3 convolutions throughout, rather than larger kernels:</p>
<ol>
<li class=""><strong>Parameter efficiency</strong>: Two 3×3 convolutions have the same receptive field as one 5×5, but fewer parameters (18 vs 25)</li>
<li class=""><strong>Deeper networks</strong>: Same parameter count allows stacking more layers</li>
<li class=""><strong>More nonlinearity</strong>: ReLU between each layer increases expressiveness</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="why-use-256-channels">Why Use 256 Channels?<a href="#why-use-256-channels" class="hash-link" aria-label="Direct link to Why Use 256 Channels?" title="Direct link to Why Use 256 Channels?" translate="no">​</a></h3>
<p>256 channels is an empirical choice:</p>
<ul>
<li class=""><strong>Too few</strong> (like 64): Insufficient expressiveness, can&#x27;t capture complex patterns</li>
<li class=""><strong>Too many</strong> (like 512): Parameter count doubles, training cost increases greatly, but strength improvement is limited</li>
</ul>
<p>Later KataGo experiments showed channel count can be adjusted based on training resources:</p>
<ul>
<li class="">Low resources: 128 channels, 20 blocks</li>
<li class="">High resources: 256 channels, 40 blocks</li>
<li class="">Higher resources: 384 channels, 60 blocks</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="why-does-policy-head-use-softmax-value-head-use-tanh">Why Does Policy Head Use Softmax, Value Head Use Tanh?<a href="#why-does-policy-head-use-softmax-value-head-use-tanh" class="hash-link" aria-label="Direct link to Why Does Policy Head Use Softmax, Value Head Use Tanh?" title="Direct link to Why Does Policy Head Use Softmax, Value Head Use Tanh?" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="policy-head-softmax">Policy Head: Softmax<a href="#policy-head-softmax" class="hash-link" aria-label="Direct link to Policy Head: Softmax" title="Direct link to Policy Head: Softmax" translate="no">​</a></h4>
<p>Move selection is a <strong>classification problem</strong> - choosing one from 361 positions (plus Pass). Softmax output satisfies:</p>
<ul>
<li class="">All probabilities non-negative: π_i &gt;= 0</li>
<li class="">Probabilities sum to 1: Σπ_i = 1</li>
</ul>
<p>This matches the definition of a probability distribution.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="value-head-tanh">Value Head: Tanh<a href="#value-head-tanh" class="hash-link" aria-label="Direct link to Value Head: Tanh" title="Direct link to Value Head: Tanh" translate="no">​</a></h4>
<p>Win rate is a <strong>regression problem</strong> - predicting a continuous value. Tanh output range is [-1, 1]:</p>
<ul>
<li class="">Bounded: Won&#x27;t produce extreme values</li>
<li class="">Symmetric: Treats wins and losses symmetrically</li>
<li class="">Differentiable: Convenient for gradient computation</li>
</ul>
<p>Using Tanh instead of unbounded output (like a linear layer) prevents training instability.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="training-details">Training Details<a href="#training-details" class="hash-link" aria-label="Direct link to Training Details" title="Direct link to Training Details" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="loss-function">Loss Function<a href="#loss-function" class="hash-link" aria-label="Direct link to Loss Function" title="Direct link to Loss Function" translate="no">​</a></h3>
<p>AlphaGo Zero&#x27;s total loss is the sum of three terms:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L = L_policy + L_value + L_reg</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="policy-loss">Policy Loss<a href="#policy-loss" class="hash-link" aria-label="Direct link to Policy Loss" title="Direct link to Policy Loss" translate="no">​</a></h4>
<p>Uses <strong>cross-entropy loss</strong> to make network output approach MCTS search probabilities:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L_policy = -Σ π_MCTS(a) × log(π_net(a))</span><br></span></code></pre></div></div>
<p>where:</p>
<ul>
<li class="">π_MCTS(a) is MCTS search probability for action a</li>
<li class="">π_net(a) is network output probability</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="value-loss">Value Loss<a href="#value-loss" class="hash-link" aria-label="Direct link to Value Loss" title="Direct link to Value Loss" translate="no">​</a></h4>
<p>Uses <strong>Mean Squared Error (MSE)</strong> to make network output approach actual game outcome:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L_value = (v_net - z)²</span><br></span></code></pre></div></div>
<p>where:</p>
<ul>
<li class="">v_net is network predicted win rate</li>
<li class="">z is actual game result (+1 or -1)</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="regularization-loss">Regularization Loss<a href="#regularization-loss" class="hash-link" aria-label="Direct link to Regularization Loss" title="Direct link to Regularization Loss" translate="no">​</a></h4>
<p>Uses <strong>L2 regularization</strong> to prevent overfitting:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L_reg = c × ||θ||²</span><br></span></code></pre></div></div>
<p>where c is regularization coefficient and θ is network parameters.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="optimizer-configuration">Optimizer Configuration<a href="#optimizer-configuration" class="hash-link" aria-label="Direct link to Optimizer Configuration" title="Direct link to Optimizer Configuration" translate="no">​</a></h3>
<table><thead><tr><th>Parameter</th><th>Value</th></tr></thead><tbody><tr><td>Optimizer</td><td>SGD + Momentum</td></tr><tr><td>Momentum</td><td>0.9</td></tr><tr><td>Initial learning rate</td><td>0.01</td></tr><tr><td>Learning rate decay</td><td>Halve every X steps</td></tr><tr><td>Batch Size</td><td>32 × 2048 = 64K (distributed)</td></tr><tr><td>L2 regularization coefficient</td><td>1e-4</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="data-augmentation">Data Augmentation<a href="#data-augmentation" class="hash-link" aria-label="Direct link to Data Augmentation" title="Direct link to Data Augmentation" translate="no">​</a></h3>
<p>The Go board has 8-fold symmetry (4 rotations × 2 flips). During training, each position can produce 8 equivalent training samples.</p>
<p>This increases effective training data 8-fold without additional self-play.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="implementation-considerations">Implementation Considerations<a href="#implementation-considerations" class="hash-link" aria-label="Direct link to Implementation Considerations" title="Direct link to Implementation Considerations" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="memory-optimization">Memory Optimization<a href="#memory-optimization" class="hash-link" aria-label="Direct link to Memory Optimization" title="Direct link to Memory Optimization" translate="no">​</a></h3>
<p>Training a 40-layer ResNet requires substantial memory:</p>
<ul>
<li class=""><strong>Forward pass</strong>: Need to store activations from each layer (for backpropagation)</li>
<li class=""><strong>Backward pass</strong>: Need to store gradients</li>
</ul>
<p>Optimization strategies:</p>
<ol>
<li class=""><strong>Gradient Checkpointing</strong>: Only store some activations, recompute when needed</li>
<li class=""><strong>Mixed precision training</strong>: Use FP16 to reduce memory footprint</li>
<li class=""><strong>Distributed training</strong>: Distribute batch across multiple GPUs/TPUs</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="inference-optimization">Inference Optimization<a href="#inference-optimization" class="hash-link" aria-label="Direct link to Inference Optimization" title="Direct link to Inference Optimization" translate="no">​</a></h3>
<p>During inference, BN doesn&#x27;t need mini-batch statistics; it can use moving averages accumulated during training:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">x_hat = (x - μ_moving) / sqrt(σ_moving² + ε)</span><br></span></code></pre></div></div>
<p>This makes inference faster and deterministic.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="quantization-and-compression">Quantization and Compression<a href="#quantization-and-compression" class="hash-link" aria-label="Direct link to Quantization and Compression" title="Direct link to Quantization and Compression" translate="no">​</a></h3>
<p>Networks can be further compressed for deployment:</p>
<ul>
<li class=""><strong>Weight quantization</strong>: FP32 → INT8, 4× memory reduction</li>
<li class=""><strong>Pruning</strong>: Remove small weight connections</li>
<li class=""><strong>Knowledge distillation</strong>: Train small network using large network</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="animation-reference">Animation Reference<a href="#animation-reference" class="hash-link" aria-label="Direct link to Animation Reference" title="Direct link to Animation Reference" translate="no">​</a></h2>
<p>Core concepts covered in this article with animation numbers:</p>
<table><thead><tr><th>Number</th><th>Concept</th><th>Physics/Math Correspondence</th></tr></thead><tbody><tr><td>Animation E3</td><td>Dual-head network</td><td>Multi-task learning</td></tr><tr><td>Animation D12</td><td>Skip connections</td><td>Gradient highway</td></tr><tr><td>Animation D8</td><td>CNN</td><td>Local receptive field</td></tr><tr><td>Animation D10</td><td>Batch Normalization</td><td>Distribution normalization</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="further-reading">Further Reading<a href="#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading" translate="no">​</a></h2>
<ul>
<li class=""><strong>Previous</strong>: <a class="" href="/en/docs/alphago/explained/alphago-zero/">AlphaGo Zero Overview</a> - Why human game records aren&#x27;t needed</li>
<li class=""><strong>Next</strong>: <a class="" href="/en/docs/alphago/explained/training-from-scratch/">Training from Scratch</a> - Detailed Day 0-3 evolution</li>
<li class=""><strong>Technical Deep Dive</strong>: <a class="" href="/en/docs/alphago/explained/cnn-and-go/">CNN and Go</a> - Why CNN suits the board</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">He, K., et al. (2016). &quot;Deep Residual Learning for Image Recognition.&quot; <em>CVPR 2016</em>.</li>
<li class="">Ioffe, S., &amp; Szegedy, C. (2015). &quot;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.&quot; <em>ICML 2015</em>.</li>
<li class="">Caruana, R. (1997). &quot;Multitask Learning.&quot; <em>Machine Learning</em>, 28(1), 41-75.</li>
<li class="">Veit, A., et al. (2016). &quot;Residual Networks Behave Like Ensembles of Relatively Shallow Networks.&quot; <em>NeurIPS 2016</em>.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/explained/17-dual-head-resnet.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/docs/alphago/explained/alphago-zero/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">AlphaGo Zero Overview</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/docs/alphago/explained/training-from-scratch/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Training from Scratch</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#dual-head-network-design" class="table-of-contents__link toc-highlight">Dual-Head Network Design</a><ul><li><a href="#overall-architecture" class="table-of-contents__link toc-highlight">Overall Architecture</a></li><li><a href="#shared-backbone" class="table-of-contents__link toc-highlight">Shared Backbone</a></li><li><a href="#policy-head" class="table-of-contents__link toc-highlight">Policy Head</a></li><li><a href="#value-head" class="table-of-contents__link toc-highlight">Value Head</a></li></ul></li><li><a href="#why-share-the-backbone" class="table-of-contents__link toc-highlight">Why Share the Backbone?</a><ul><li><a href="#intuitive-understanding" class="table-of-contents__link toc-highlight">Intuitive Understanding</a></li><li><a href="#multi-task-learning-perspective" class="table-of-contents__link toc-highlight">Multi-task Learning Perspective</a></li><li><a href="#experimental-evidence" class="table-of-contents__link toc-highlight">Experimental Evidence</a></li></ul></li><li><a href="#residual-network-principles" class="table-of-contents__link toc-highlight">Residual Network Principles</a><ul><li><a href="#the-deep-network-dilemma" class="table-of-contents__link toc-highlight">The Deep Network Dilemma</a></li><li><a href="#residual-block-design" class="table-of-contents__link toc-highlight">Residual Block Design</a></li><li><a href="#why-skip-connections-work" class="table-of-contents__link toc-highlight">Why Skip Connections Work?</a></li><li><a href="#resnets-breakthrough-on-imagenet" class="table-of-contents__link toc-highlight">ResNet&#39;s Breakthrough on ImageNet</a></li></ul></li><li><a href="#alphago-zeros-40-layer-resnet" class="table-of-contents__link toc-highlight">AlphaGo Zero&#39;s 40-Layer ResNet</a><ul><li><a href="#why-choose-40-layers" class="table-of-contents__link toc-highlight">Why Choose 40 Layers?</a></li><li><a href="#specific-configuration" class="table-of-contents__link toc-highlight">Specific Configuration</a></li><li><a href="#role-of-batch-normalization" class="table-of-contents__link toc-highlight">Role of Batch Normalization</a></li></ul></li><li><a href="#comparison-with-other-architectures" class="table-of-contents__link toc-highlight">Comparison with Other Architectures</a><ul><li><a href="#vs-original-alphagos-cnn" class="table-of-contents__link toc-highlight">vs. Original AlphaGo&#39;s CNN</a></li><li><a href="#vs-vgg-style-networks" class="table-of-contents__link toc-highlight">vs. VGG-style Networks</a></li><li><a href="#vs-inception--googlenet" class="table-of-contents__link toc-highlight">vs. Inception / GoogLeNet</a></li><li><a href="#vs-transformer" class="table-of-contents__link toc-highlight">vs. Transformer</a></li></ul></li><li><a href="#deep-analysis-of-design-choices" class="table-of-contents__link toc-highlight">Deep Analysis of Design Choices</a><ul><li><a href="#why-use-33-convolutions" class="table-of-contents__link toc-highlight">Why Use 3×3 Convolutions?</a></li><li><a href="#why-use-256-channels" class="table-of-contents__link toc-highlight">Why Use 256 Channels?</a></li><li><a href="#why-does-policy-head-use-softmax-value-head-use-tanh" class="table-of-contents__link toc-highlight">Why Does Policy Head Use Softmax, Value Head Use Tanh?</a></li></ul></li><li><a href="#training-details" class="table-of-contents__link toc-highlight">Training Details</a><ul><li><a href="#loss-function" class="table-of-contents__link toc-highlight">Loss Function</a></li><li><a href="#optimizer-configuration" class="table-of-contents__link toc-highlight">Optimizer Configuration</a></li><li><a href="#data-augmentation" class="table-of-contents__link toc-highlight">Data Augmentation</a></li></ul></li><li><a href="#implementation-considerations" class="table-of-contents__link toc-highlight">Implementation Considerations</a><ul><li><a href="#memory-optimization" class="table-of-contents__link toc-highlight">Memory Optimization</a></li><li><a href="#inference-optimization" class="table-of-contents__link toc-highlight">Inference Optimization</a></li><li><a href="#quantization-and-compression" class="table-of-contents__link toc-highlight">Quantization and Compression</a></li></ul></li><li><a href="#animation-reference" class="table-of-contents__link toc-highlight">Animation Reference</a></li><li><a href="#further-reading" class="table-of-contents__link toc-highlight">Further Reading</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>