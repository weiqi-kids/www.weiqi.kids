"use strict";(self.webpackChunktemp_docusaurus=self.webpackChunktemp_docusaurus||[]).push([[396],{1587:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"for-engineers/background-info/alphago","title":"AlphaGo Paper Analysis","description":"This article provides an in-depth analysis of DeepMind\'s classic paper published in Nature: \\"Mastering the game of Go with deep neural networks and tree search,\\" as well as subsequent AlphaGo Zero and AlphaZero papers.","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/for-engineers/background-info/alphago.md","sourceDirName":"for-engineers/background-info","slug":"/for-engineers/background-info/alphago","permalink":"/www.weiqi.kids/en/docs/for-engineers/background-info/alphago","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/background-info/alphago.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"AlphaGo Paper Analysis"},"sidebar":"tutorialSidebar","previous":{"title":"Background Knowledge","permalink":"/www.weiqi.kids/en/docs/for-engineers/background-info/"},"next":{"title":"KataGo Paper Analysis","permalink":"/www.weiqi.kids/en/docs/for-engineers/background-info/katago-paper"}}');var s=i(3420),l=i(5521);const t={sidebar_position:1,title:"AlphaGo Paper Analysis"},a="AlphaGo Paper Analysis",o={},d=[{value:"Historical Significance of AlphaGo",id:"historical-significance-of-alphago",level:2},{value:"Milestone Events",id:"milestone-events",level:3},{value:"Core Technical Architecture",id:"core-technical-architecture",level:2},{value:"Policy Network",id:"policy-network",level:3},{value:"Network Architecture",id:"network-architecture",level:4},{value:"Input Features",id:"input-features",level:4},{value:"Training Method",id:"training-method",level:4},{value:"Value Network",id:"value-network",level:3},{value:"Network Architecture",id:"network-architecture-1",level:4},{value:"Training Method",id:"training-method-1",level:4},{value:"Monte Carlo Tree Search (MCTS)",id:"monte-carlo-tree-search-mcts",level:2},{value:"MCTS Four Steps",id:"mcts-four-steps",level:3},{value:"Selection Formula (PUCT)",id:"selection-formula-puct",level:3},{value:"Search Process Details",id:"search-process-details",level:3},{value:"Rollout (Fast Playouts)",id:"rollout-fast-playouts",level:3},{value:"Self-play Training Method",id:"self-play-training-method",level:2},{value:"Training Loop",id:"training-loop",level:3},{value:"Why Self-play Works",id:"why-self-play-works",level:3},{value:"AlphaGo Zero Improvements",id:"alphago-zero-improvements",level:2},{value:"Main Differences",id:"main-differences",level:3},{value:"Architecture Simplification",id:"architecture-simplification",level:3},{value:"Simplified Input Features",id:"simplified-input-features",level:3},{value:"Training Improvements",id:"training-improvements",level:3},{value:"AlphaZero Generalization",id:"alphazero-generalization",level:2},{value:"Key Features",id:"key-features",level:3},{value:"Differences from AlphaGo Zero",id:"differences-from-alphago-zero",level:3},{value:"Implementation Key Points",id:"implementation-key-points",level:2},{value:"Computational Resources",id:"computational-resources",level:3},{value:"Key Hyperparameters",id:"key-hyperparameters",level:3},{value:"Common Issues",id:"common-issues",level:3},{value:"Further Reading",id:"further-reading",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"alphago-paper-analysis",children:"AlphaGo Paper Analysis"})}),"\n",(0,s.jsx)(n.p,{children:'This article provides an in-depth analysis of DeepMind\'s classic paper published in Nature: "Mastering the game of Go with deep neural networks and tree search," as well as subsequent AlphaGo Zero and AlphaZero papers.'}),"\n",(0,s.jsx)(n.h2,{id:"historical-significance-of-alphago",children:"Historical Significance of AlphaGo"}),"\n",(0,s.jsx)(n.p,{children:'Go has long been considered the "holy grail" challenge for artificial intelligence. Unlike chess, Go\'s search space is extremely vast:'}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Game"}),(0,s.jsx)(n.th,{children:"Average Branching Factor"}),(0,s.jsx)(n.th,{children:"Average Game Length"}),(0,s.jsx)(n.th,{children:"State Space"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Chess"}),(0,s.jsx)(n.td,{children:"~35"}),(0,s.jsx)(n.td,{children:"~80"}),(0,s.jsx)(n.td,{children:"~10^47"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Go"}),(0,s.jsx)(n.td,{children:"~250"}),(0,s.jsx)(n.td,{children:"~150"}),(0,s.jsx)(n.td,{children:"~10^170"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"Traditional brute-force search methods are completely infeasible for Go. AlphaGo's 2016 victory over Lee Sedol demonstrated the powerful combination of deep learning and reinforcement learning."}),"\n",(0,s.jsx)(n.h3,{id:"milestone-events",children:"Milestone Events"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"October 2015"}),": AlphaGo Fan defeats European champion Fan Hui (professional 2-dan) 5:0"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"March 2016"}),": AlphaGo Lee defeats world champion Lee Sedol (professional 9-dan) 4:1"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"May 2017"}),": AlphaGo Master defeats world #1 ranked Ke Jie 3:0"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"October 2017"}),": AlphaGo Zero published, pure self-play training, surpasses all previous versions"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"core-technical-architecture",children:"Core Technical Architecture"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo's core innovation lies in combining three key technologies:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    AlphaGo Architecture                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502   \u2502   Policy    \u2502    \u2502    Value    \u2502                   \u2502\n\u2502   \u2502   Network   \u2502    \u2502   Network   \u2502                   \u2502\n\u2502   \u2502(Move Policy)\u2502    \u2502(Win Rate Est)\u2502                  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502          \u2502                  \u2502                          \u2502\n\u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2502                   \u2502                                    \u2502\n\u2502                   \u25bc                                    \u2502\n\u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u2502\n\u2502          \u2502      MCTS       \u2502                          \u2502\n\u2502          \u2502(Monte Carlo Tree\u2502                          \u2502\n\u2502          \u2502    Search)      \u2502                          \u2502\n\u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"policy-network",children:"Policy Network"}),"\n",(0,s.jsx)(n.p,{children:"The Policy Network predicts the probability of playing at each position, guiding search direction."}),"\n",(0,s.jsx)(n.h4,{id:"network-architecture",children:"Network Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Input layer: 19\xd719\xd748 feature planes\n    \u2502\n    \u25bc\nConv layer 1: 5\xd75 kernel, 192 filters\n    \u2502\n    \u25bc\nConv layers 2-12: 3\xd73 kernel, 192 filters\n    \u2502\n    \u25bc\nOutput layer: 19\xd719 probability distribution (softmax)\n"})}),"\n",(0,s.jsx)(n.h4,{id:"input-features",children:"Input Features"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo uses 48 feature planes as input:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"Planes"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Stone color"}),(0,s.jsx)(n.td,{children:"3"}),(0,s.jsx)(n.td,{children:"Black stones, white stones, empty"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Liberties"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"1, 2, ..., 8 or more liberties"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Liberties after capture"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"How many liberties after capturing"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Capture size"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"How many stones can be captured at position"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Ko"}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"Whether position is ko"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Move legality"}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"Whether position is legal"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Previous 1-8 moves"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"Positions of last several moves"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Turn to play"}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"Currently Black or White to play"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"training-method",children:"Training Method"}),"\n",(0,s.jsx)(n.p,{children:"Policy Network training has two phases:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Phase 1: Supervised Learning (SL Policy Network)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Uses 30 million games from KGS Go server"}),"\n",(0,s.jsx)(n.li,{children:"Objective: Predict human player's next move"}),"\n",(0,s.jsx)(n.li,{children:"Achieves 57% prediction accuracy"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Phase 2: Reinforcement Learning (RL Policy Network)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Starts from SL Policy Network"}),"\n",(0,s.jsx)(n.li,{children:"Self-play against previous versions"}),"\n",(0,s.jsx)(n.li,{children:"Optimizes using REINFORCE algorithm"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Simplified Policy Gradient update\n# reward: +1 win, -1 loss\nloss = -log(policy[action]) * reward\n"})}),"\n",(0,s.jsx)(n.h3,{id:"value-network",children:"Value Network"}),"\n",(0,s.jsx)(n.p,{children:"The Value Network evaluates the win rate of the current position, reducing search depth needed."}),"\n",(0,s.jsx)(n.h4,{id:"network-architecture-1",children:"Network Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Input layer: 19\xd719\xd748 feature planes (same as Policy Network)\n    \u2502\n    \u25bc\nConv layers 1-12: Similar to Policy Network\n    \u2502\n    \u25bc\nFully connected layer: 256 neurons\n    \u2502\n    \u25bc\nOutput layer: 1 neuron (tanh, range [-1, 1])\n"})}),"\n",(0,s.jsx)(n.h4,{id:"training-method-1",children:"Training Method"}),"\n",(0,s.jsx)(n.p,{children:"Value Network trains on 30 million positions generated by RL Policy Network self-play:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Randomly sample one position from each game"}),"\n",(0,s.jsx)(n.li,{children:"Use final game outcome as label"}),"\n",(0,s.jsx)(n.li,{children:"Use MSE loss function"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Value Network training\nvalue_prediction = value_network(position)\nloss = (value_prediction - game_outcome) ** 2\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why sample only one position per game?"})}),"\n",(0,s.jsx)(n.p,{children:"Taking multiple samples would make adjacent positions from the same game highly correlated, causing overfitting. Random sampling ensures training data diversity."}),"\n",(0,s.jsx)(n.h2,{id:"monte-carlo-tree-search-mcts",children:"Monte Carlo Tree Search (MCTS)"}),"\n",(0,s.jsx)(n.p,{children:"MCTS is AlphaGo's decision-making core, combining neural networks for efficient best-move search."}),"\n",(0,s.jsx)(n.h3,{id:"mcts-four-steps",children:"MCTS Four Steps"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"    (1) Selection          (2) Expansion\n         \u2502                      \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502  Select \u2502            \u2502  Expand \u2502\n    \u2502Best Path\u2502            \u2502New Node \u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n         \u2502                      \u2502\n         \u25bc                      \u25bc\n    (3) Evaluation         (4) Backpropagation\n         \u2502                      \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502 Neural  \u2502            \u2502 Backprop\u2502\n    \u2502 Network \u2502            \u2502 Update  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"selection-formula-puct",children:"Selection Formula (PUCT)"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo uses PUCT (Predictor + UCT) formula to select which branch to explore:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"a = argmax[Q(s,a) + u(s,a)]\n\nu(s,a) = c_puct * P(s,a) * sqrt(N(s)) / (1 + N(s,a))\n"})}),"\n",(0,s.jsx)(n.p,{children:"Where:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Q(s,a)"}),": Average value of action a (exploitation)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"P(s,a)"}),": Prior probability from Policy Network"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"N(s)"}),": Visit count of parent node"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"N(s,a)"}),": Visit count of this action"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"c_puct"}),": Exploration constant, balances exploration and exploitation"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"search-process-details",children:"Search Process Details"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Selection"}),": From root node, use PUCT formula to select actions until reaching leaf node"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Expansion"}),": Expand new child nodes at leaf, initialize prior probabilities with Policy Network"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Evaluation"}),": Combine Value Network evaluation and rollout simulation to evaluate value"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Backpropagation"}),": Pass evaluation value back along path, update Q values and N values"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"rollout-fast-playouts",children:"Rollout (Fast Playouts)"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo (non-Zero version) also uses a small fast policy network for simulation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Leaf node \u2192 Fast random playout to game end \u2192 Calculate outcome\n"})}),"\n",(0,s.jsx)(n.p,{children:"Final evaluation combines Value Network and Rollout:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"V = \u03bb * v_network + (1-\u03bb) * v_rollout\n"})}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo uses \u03bb = 0.5, giving equal weight to both."}),"\n",(0,s.jsx)(n.h2,{id:"self-play-training-method",children:"Self-play Training Method"}),"\n",(0,s.jsx)(n.p,{children:"Self-play is AlphaGo's core training strategy, letting AI continuously improve by playing against itself."}),"\n",(0,s.jsx)(n.h3,{id:"training-loop",children:"Training Loop"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Self-play Training Loop                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n\u2502   \u2502 Current \u2502 \u2192  \u2502Self-play\u2502 \u2192  \u2502Generate \u2502           \u2502\n\u2502   \u2502  Model  \u2502    \u2502         \u2502    \u2502  Data   \u2502           \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n\u2502        \u25b2                              \u2502                \u2502\n\u2502        \u2502                              \u25bc                \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n\u2502   \u2502   New   \u2502 \u2190  \u2502  Train  \u2502 \u2190  \u2502  Data   \u2502           \u2502\n\u2502   \u2502  Model  \u2502    \u2502         \u2502    \u2502  Buffer \u2502           \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"why-self-play-works",children:"Why Self-play Works"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Unlimited data"}),": Not limited by number of human games"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Adaptive difficulty"}),": Opponent strength improves together with self"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Explore innovations"}),": Not constrained by human thinking patterns"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Clear objective"}),": Directly optimizes win rate, not imitating humans"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"alphago-zero-improvements",children:"AlphaGo Zero Improvements"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero published in 2017 brought revolutionary improvements:"}),"\n",(0,s.jsx)(n.h3,{id:"main-differences",children:"Main Differences"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"AlphaGo"}),(0,s.jsx)(n.th,{children:"AlphaGo Zero"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Initial training"}),(0,s.jsx)(n.td,{children:"Supervised learning on human games"}),(0,s.jsx)(n.td,{children:"Starts completely from zero"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Network architecture"}),(0,s.jsx)(n.td,{children:"Separate Policy/Value"}),(0,s.jsx)(n.td,{children:"Single dual-head network"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Network structure"}),(0,s.jsx)(n.td,{children:"Regular CNN"}),(0,s.jsx)(n.td,{children:"ResNet"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Feature engineering"}),(0,s.jsx)(n.td,{children:"48 hand-crafted features"}),(0,s.jsx)(n.td,{children:"17 simple features"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Rollout"}),(0,s.jsx)(n.td,{children:"Required"}),(0,s.jsx)(n.td,{children:"Not needed"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Training time"}),(0,s.jsx)(n.td,{children:"Months"}),(0,s.jsx)(n.td,{children:"3 days to surpass humans"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"architecture-simplification",children:"Architecture Simplification"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              AlphaGo Zero Dual-Head Network              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502   Input: 19\xd719\xd717 (simplified features)                 \u2502\n\u2502                      \u2502                                  \u2502\n\u2502                      \u25bc                                  \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n\u2502              \u2502  ResNet Trunk  \u2502                         \u2502\n\u2502              \u2502 (40 res blocks)\u2502                         \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2502                      \u2502                                  \u2502\n\u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502          \u25bc                       \u25bc                     \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502   \u2502 Policy Head \u2502         \u2502 Value Head  \u2502             \u2502\n\u2502   \u2502 (19\xd719+1)  \u2502         \u2502   ([-1,1])   \u2502             \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"simplified-input-features",children:"Simplified Input Features"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero uses only 17 feature planes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"8 planes: Own stone positions for last 8 moves"}),"\n",(0,s.jsx)(n.li,{children:"8 planes: Opponent stone positions for last 8 moves"}),"\n",(0,s.jsx)(n.li,{children:"1 plane: Current player to move (all 0s or all 1s)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"training-improvements",children:"Training Improvements"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pure self-play"}),": No human data used"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"MCTS probabilities as training targets"}),": Instead of binary win/loss"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"No rollout"}),": Relies entirely on Value Network"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Single network training"}),": Policy and Value share parameters, mutually enhance"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"alphazero-generalization",children:"AlphaZero Generalization"}),"\n",(0,s.jsx)(n.p,{children:"AlphaZero published late 2017 applied the same architecture to Go, chess, and shogi:"}),"\n",(0,s.jsx)(n.h3,{id:"key-features",children:"Key Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Zero domain knowledge"}),": Uses no domain-specific knowledge besides game rules"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Unified architecture"}),": Same algorithm works for different games"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Faster training"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Go: 8 hours to surpass AlphaGo Lee"}),"\n",(0,s.jsx)(n.li,{children:"Chess: 4 hours to surpass Stockfish"}),"\n",(0,s.jsx)(n.li,{children:"Shogi: 2 hours to surpass Elmo"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"differences-from-alphago-zero",children:"Differences from AlphaGo Zero"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"AlphaGo Zero"}),(0,s.jsx)(n.th,{children:"AlphaZero"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Target games"}),(0,s.jsx)(n.td,{children:"Go only"}),(0,s.jsx)(n.td,{children:"Go, chess, shogi"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Symmetry usage"}),(0,s.jsx)(n.td,{children:"Uses Go's 8-fold symmetry"}),(0,s.jsx)(n.td,{children:"Doesn't assume symmetry"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Hyperparameter tuning"}),(0,s.jsx)(n.td,{children:"Optimized for Go"}),(0,s.jsx)(n.td,{children:"Generic settings"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Training method"}),(0,s.jsx)(n.td,{children:"Best model self-play"}),(0,s.jsx)(n.td,{children:"Latest model self-play"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"implementation-key-points",children:"Implementation Key Points"}),"\n",(0,s.jsx)(n.p,{children:"If you want to implement a similar system, here are key considerations:"}),"\n",(0,s.jsx)(n.h3,{id:"computational-resources",children:"Computational Resources"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo training required enormous computational resources:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AlphaGo Lee"}),": 176 GPUs + 48 TPUs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AlphaGo Zero"}),": 4 TPUs (training) + 1 TPU (self-play)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AlphaZero"}),": 5000 TPUs (training)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"key-hyperparameters",children:"Key Hyperparameters"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# MCTS related\nnum_simulations = 800     # Simulations per move\nc_puct = 1.5              # Exploration constant\ntemperature = 1.0         # Temperature for move selection\n\n# Training related\nbatch_size = 2048\nlearning_rate = 0.01      # With decay\nl2_regularization = 1e-4\n"})}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Training instability"}),": Use smaller learning rate, increase batch size"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Overfitting"}),": Ensure training data diversity, use regularization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Search efficiency"}),": Optimize GPU batch inference, parallelize MCTS"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.nature.com/articles/nature16961",children:"Original paper: Mastering the game of Go with deep neural networks and tree search"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.nature.com/articles/nature24270",children:"AlphaGo Zero paper: Mastering the game of Go without human knowledge"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.science.org/doi/10.1126/science.aar6404",children:"AlphaZero paper: A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["After understanding AlphaGo's technology, let's see ",(0,s.jsx)(n.a,{href:"/www.weiqi.kids/en/docs/for-engineers/background-info/katago-paper",children:"how KataGo improves upon this foundation"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},5521:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var r=i(6672);const s={},l=r.createContext(s);function t(e){const n=r.useContext(l);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(l.Provider,{value:n},e.children)}}}]);