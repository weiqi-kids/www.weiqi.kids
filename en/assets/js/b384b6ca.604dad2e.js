"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[4398],{89613(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"tech/deep-dive/quantization-deploy","title":"Model Quantization & Deployment","description":"KataGo model quantization techniques, export formats, and cross-platform deployment solutions","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/tech/deep-dive/quantization-deploy.md","sourceDirName":"tech/deep-dive","slug":"/tech/deep-dive/quantization-deploy","permalink":"/en/docs/tech/deep-dive/quantization-deploy","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tech/deep-dive/quantization-deploy.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"title":"Model Quantization & Deployment","description":"KataGo model quantization techniques, export formats, and cross-platform deployment solutions"},"sidebar":"tutorialSidebar","previous":{"title":"Distributed Training Architecture","permalink":"/en/docs/tech/deep-dive/distributed-training"},"next":{"title":"Evaluation & Benchmarking","permalink":"/en/docs/tech/deep-dive/evaluation"}}');var r=t(62615),s=t(30416);const o={sidebar_position:8,title:"Model Quantization & Deployment",description:"KataGo model quantization techniques, export formats, and cross-platform deployment solutions"},a="Model Quantization & Deployment",l={},d=[{value:"Quantization Techniques Overview",id:"quantization-techniques-overview",level:2},{value:"Why Quantization?",id:"why-quantization",level:3},{value:"Quantization Types",id:"quantization-types",level:3},{value:"FP16 Half Precision",id:"fp16-half-precision",level:2},{value:"Concept",id:"concept",level:3},{value:"KataGo Settings",id:"katago-settings",level:3},{value:"Performance Impact",id:"performance-impact",level:3},{value:"INT8 Quantization",id:"int8-quantization",level:2},{value:"Quantization Process",id:"quantization-process",level:3},{value:"Calibration Data",id:"calibration-data",level:3},{value:"Notes",id:"notes",level:3},{value:"TensorRT Deployment",id:"tensorrt-deployment",level:2},{value:"Conversion Process",id:"conversion-process",level:3},{value:"Using TensorRT Engine",id:"using-tensorrt-engine",level:3},{value:"ONNX Export",id:"onnx-export",level:2},{value:"PyTorch \u2192 ONNX",id:"pytorch--onnx",level:3},{value:"Validate ONNX Model",id:"validate-onnx-model",level:3},{value:"Cross-Platform Deployment",id:"cross-platform-deployment",level:2},{value:"Server Deployment",id:"server-deployment",level:3},{value:"Desktop Application Integration",id:"desktop-application-integration",level:3},{value:"Mobile Deployment",id:"mobile-deployment",level:3},{value:"iOS (Core ML)",id:"ios-core-ml",level:4},{value:"Android (TensorFlow Lite)",id:"android-tensorflow-lite",level:4},{value:"Embedded Systems",id:"embedded-systems",level:3},{value:"Raspberry Pi",id:"raspberry-pi",level:4},{value:"NVIDIA Jetson",id:"nvidia-jetson",level:4},{value:"Performance Comparison",id:"performance-comparison",level:2},{value:"Performance by Deployment Method",id:"performance-by-deployment-method",level:3},{value:"Model Size Comparison",id:"model-size-comparison",level:3},{value:"Deployment Checklist",id:"deployment-checklist",level:2},{value:"Further Reading",id:"further-reading",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",input:"input",li:"li",mermaid:"mermaid",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"model-quantization--deployment",children:"Model Quantization & Deployment"})}),"\n",(0,r.jsx)(n.p,{children:"This article introduces how to quantize KataGo models to reduce resource requirements, and deployment solutions for various platforms."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"quantization-techniques-overview",children:"Quantization Techniques Overview"}),"\n",(0,r.jsx)(n.h3,{id:"why-quantization",children:"Why Quantization?"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Precision"}),(0,r.jsx)(n.th,{children:"Size"}),(0,r.jsx)(n.th,{children:"Speed"}),(0,r.jsx)(n.th,{children:"Accuracy Loss"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FP32"}),(0,r.jsx)(n.td,{children:"100%"}),(0,r.jsx)(n.td,{children:"Baseline"}),(0,r.jsx)(n.td,{children:"0%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FP16"}),(0,r.jsx)(n.td,{children:"50%"}),(0,r.jsx)(n.td,{children:"+50%"}),(0,r.jsx)(n.td,{children:"~0%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"INT8"}),(0,r.jsx)(n.td,{children:"25%"}),(0,r.jsx)(n.td,{children:"+100%"}),(0,r.jsx)(n.td,{children:"<1%"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"quantization-types",children:"Quantization Types"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart TB\n    subgraph PTQ["Post-Training Quantization (PTQ)"]\n        P1["Simple and fast"]\n        P2["No retraining needed"]\n        P3["May have accuracy loss"]\n    end\n\n    subgraph QAT["Quantization-Aware Training (QAT)"]\n        Q1["Higher accuracy"]\n        Q2["Requires retraining"]\n        Q3["More complex"]\n    end'}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"fp16-half-precision",children:"FP16 Half Precision"}),"\n",(0,r.jsx)(n.h3,{id:"concept",children:"Concept"}),"\n",(0,r.jsx)(n.p,{children:"Convert 32-bit floating point to 16-bit:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# FP32 \u2192 FP16 conversion\nmodel_fp16 = model.half()\n\n# Inference\nwith torch.cuda.amp.autocast():\n    output = model_fp16(input.half())\n"})}),"\n",(0,r.jsx)(n.h3,{id:"katago-settings",children:"KataGo Settings"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-ini",children:"# config.cfg\nuseFP16 = true           # Enable FP16 inference\nuseFP16Storage = true    # FP16 intermediate storage\n"})}),"\n",(0,r.jsx)(n.h3,{id:"performance-impact",children:"Performance Impact"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"GPU Series"}),(0,r.jsx)(n.th,{children:"FP16 Speedup"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"GTX 10xx"}),(0,r.jsx)(n.td,{children:"None (no Tensor Core)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"RTX 20xx"}),(0,r.jsx)(n.td,{children:"+30-50%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"RTX 30xx"}),(0,r.jsx)(n.td,{children:"+50-80%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"RTX 40xx"}),(0,r.jsx)(n.td,{children:"+80-100%"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"int8-quantization",children:"INT8 Quantization"}),"\n",(0,r.jsx)(n.h3,{id:"quantization-process",children:"Quantization Process"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch.quantization as quant\n\n# 1. Prepare model\nmodel.eval()\nmodel.qconfig = quant.get_default_qconfig('fbgemm')\n\n# 2. Prepare for quantization\nmodel_prepared = quant.prepare(model)\n\n# 3. Calibration (use representative data)\nwith torch.no_grad():\n    for data in calibration_loader:\n        model_prepared(data)\n\n# 4. Convert to quantized model\nmodel_quantized = quant.convert(model_prepared)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"calibration-data",children:"Calibration Data"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def create_calibration_dataset(num_samples=1000):\n    """Create calibration dataset"""\n    samples = []\n\n    # Sample from actual games\n    for game in random_games(num_samples):\n        position = random_position(game)\n        features = encode_state(position)\n        samples.append(features)\n\n    return samples\n'})}),"\n",(0,r.jsx)(n.h3,{id:"notes",children:"Notes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"INT8 quantization requires calibration data"}),"\n",(0,r.jsx)(n.li,{children:"Some layers may not be suitable for quantization"}),"\n",(0,r.jsx)(n.li,{children:"Need to test accuracy loss"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"tensorrt-deployment",children:"TensorRT Deployment"}),"\n",(0,r.jsx)(n.h3,{id:"conversion-process",children:"Conversion Process"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import tensorrt as trt\n\ndef convert_to_tensorrt(onnx_path, engine_path):\n    logger = trt.Logger(trt.Logger.WARNING)\n    builder = trt.Builder(logger)\n    network = builder.create_network(\n        1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n    )\n    parser = trt.OnnxParser(network, logger)\n\n    # Parse ONNX model\n    with open(onnx_path, 'rb') as f:\n        parser.parse(f.read())\n\n    # Set optimization options\n    config = builder.create_builder_config()\n    config.max_workspace_size = 1 << 30  # 1GB\n\n    # Enable FP16\n    config.set_flag(trt.BuilderFlag.FP16)\n\n    # Build engine\n    engine = builder.build_engine(network, config)\n\n    # Save\n    with open(engine_path, 'wb') as f:\n        f.write(engine.serialize())\n"})}),"\n",(0,r.jsx)(n.h3,{id:"using-tensorrt-engine",children:"Using TensorRT Engine"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def inference_with_tensorrt(engine_path, input_data):\n    # Load engine\n    with open(engine_path, 'rb') as f:\n        engine = trt.Runtime(logger).deserialize_cuda_engine(f.read())\n\n    context = engine.create_execution_context()\n\n    # Allocate memory\n    d_input = cuda.mem_alloc(input_data.nbytes)\n    d_output = cuda.mem_alloc(output_size)\n\n    # Copy input\n    cuda.memcpy_htod(d_input, input_data)\n\n    # Execute inference\n    context.execute_v2([int(d_input), int(d_output)])\n\n    # Get output\n    output = np.empty(output_shape, dtype=np.float32)\n    cuda.memcpy_dtoh(output, d_output)\n\n    return output\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"onnx-export",children:"ONNX Export"}),"\n",(0,r.jsx)(n.h3,{id:"pytorch--onnx",children:"PyTorch \u2192 ONNX"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch.onnx\n\ndef export_to_onnx(model, output_path):\n    model.eval()\n\n    # Create sample input\n    dummy_input = torch.randn(1, 22, 19, 19)\n\n    # Export\n    torch.onnx.export(\n        model,\n        dummy_input,\n        output_path,\n        input_names=['input'],\n        output_names=['policy', 'value', 'ownership'],\n        dynamic_axes={\n            'input': {0: 'batch_size'},\n            'policy': {0: 'batch_size'},\n            'value': {0: 'batch_size'},\n            'ownership': {0: 'batch_size'}\n        },\n        opset_version=13\n    )\n"})}),"\n",(0,r.jsx)(n.h3,{id:"validate-onnx-model",children:"Validate ONNX Model"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import onnx\nimport onnxruntime as ort\n\n# Validate model structure\nmodel = onnx.load("model.onnx")\nonnx.checker.check_model(model)\n\n# Test inference\nsession = ort.InferenceSession("model.onnx")\noutput = session.run(None, {\'input\': input_data})\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"cross-platform-deployment",children:"Cross-Platform Deployment"}),"\n",(0,r.jsx)(n.h3,{id:"server-deployment",children:"Server Deployment"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# docker-compose.yml\nversion: '3'\nservices:\n  katago:\n    image: katago/katago:latest\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n    volumes:\n      - ./models:/models\n      - ./config:/config\n    command: >\n      katago analysis\n      -model /models/kata-b18c384.bin.gz\n      -config /config/analysis.cfg\n"})}),"\n",(0,r.jsx)(n.h3,{id:"desktop-application-integration",children:"Desktop Application Integration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Embed KataGo in Python application\nimport subprocess\nimport json\n\nclass KataGoProcess:\n    def __init__(self, katago_path, model_path):\n        self.process = subprocess.Popen(\n            [katago_path, 'analysis', '-model', model_path],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            text=True\n        )\n\n    def analyze(self, moves):\n        query = {\n            'id': 'query1',\n            'moves': moves,\n            'rules': 'chinese',\n            'komi': 7.5,\n            'boardXSize': 19,\n            'boardYSize': 19\n        }\n        self.process.stdin.write(json.dumps(query) + '\\n')\n        self.process.stdin.flush()\n\n        response = self.process.stdout.readline()\n        return json.loads(response)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"mobile-deployment",children:"Mobile Deployment"}),"\n",(0,r.jsx)(n.h4,{id:"ios-core-ml",children:"iOS (Core ML)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import coremltools as ct\n\n# Convert to Core ML\nmlmodel = ct.convert(\n    model,\n    inputs=[ct.TensorType(shape=(1, 22, 19, 19))],\n    minimum_deployment_target=ct.target.iOS15\n)\n\nmlmodel.save("KataGo.mlmodel")\n'})}),"\n",(0,r.jsx)(n.h4,{id:"android-tensorflow-lite",children:"Android (TensorFlow Lite)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import tensorflow as tf\n\n# Convert to TFLite\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_path)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.float16]\n\ntflite_model = converter.convert()\n\nwith open('katago.tflite', 'wb') as f:\n    f.write(tflite_model)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"embedded-systems",children:"Embedded Systems"}),"\n",(0,r.jsx)(n.h4,{id:"raspberry-pi",children:"Raspberry Pi"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Use Eigen backend (CPU only)\n./katago gtp -model kata-b10c128.bin.gz -config rpi.cfg\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-ini",children:"# rpi.cfg - Raspberry Pi optimized settings\nnumSearchThreads = 4\nmaxVisits = 100\nnnMaxBatchSize = 1\n"})}),"\n",(0,r.jsx)(n.h4,{id:"nvidia-jetson",children:"NVIDIA Jetson"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Use CUDA backend\n./katago gtp -model kata-b18c384.bin.gz -config jetson.cfg\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"performance-comparison",children:"Performance Comparison"}),"\n",(0,r.jsx)(n.h3,{id:"performance-by-deployment-method",children:"Performance by Deployment Method"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Deployment"}),(0,r.jsx)(n.th,{children:"Hardware"}),(0,r.jsx)(n.th,{children:"Playouts/sec"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"CUDA FP32"}),(0,r.jsx)(n.td,{children:"RTX 3080"}),(0,r.jsx)(n.td,{children:"~3000"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"CUDA FP16"}),(0,r.jsx)(n.td,{children:"RTX 3080"}),(0,r.jsx)(n.td,{children:"~5000"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"TensorRT FP16"}),(0,r.jsx)(n.td,{children:"RTX 3080"}),(0,r.jsx)(n.td,{children:"~6500"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"OpenCL"}),(0,r.jsx)(n.td,{children:"M1 Pro"}),(0,r.jsx)(n.td,{children:"~1500"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Core ML"}),(0,r.jsx)(n.td,{children:"M1 Pro"}),(0,r.jsx)(n.td,{children:"~1800"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"TFLite"}),(0,r.jsx)(n.td,{children:"Pixel 7"}),(0,r.jsx)(n.td,{children:"~50"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Eigen"}),(0,r.jsx)(n.td,{children:"RPi 4"}),(0,r.jsx)(n.td,{children:"~15"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"model-size-comparison",children:"Model Size Comparison"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Format"}),(0,r.jsx)(n.th,{children:"b18c384 Size"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Original (.bin.gz)"}),(0,r.jsx)(n.td,{children:"~140 MB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ONNX FP32"}),(0,r.jsx)(n.td,{children:"~280 MB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ONNX FP16"}),(0,r.jsx)(n.td,{children:"~140 MB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"TensorRT FP16"}),(0,r.jsx)(n.td,{children:"~100 MB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"TFLite FP16"}),(0,r.jsx)(n.td,{children:"~140 MB"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"deployment-checklist",children:"Deployment Checklist"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Select appropriate quantization precision"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Prepare calibration data (INT8)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Export to target format"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Validate acceptable accuracy loss"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Test performance on target platform"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Optimize memory usage"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up automated deployment pipeline"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"../gpu-optimization",children:"GPU Backend & Optimization"})," \u2014 Basic performance optimization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"../evaluation",children:"Evaluation & Benchmarking"})," \u2014 Verify post-deployment performance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"../../hands-on/integration",children:"Integrate into Your Project"})," \u2014 API integration examples"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},30416(e,n,t){t.d(n,{R:()=>o,x:()=>a});var i=t(59471);const r={},s=i.createContext(r);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);