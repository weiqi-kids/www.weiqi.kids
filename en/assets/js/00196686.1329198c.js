"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[475],{75473(e,n,i){i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"tech/deep-dive/papers","title":"Key Papers Guide","description":"Highlights from milestone Go AI papers including AlphaGo, AlphaZero, and KataGo","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/tech/deep-dive/papers.md","sourceDirName":"tech/deep-dive","slug":"/tech/deep-dive/papers","permalink":"/en/docs/tech/deep-dive/papers","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tech/deep-dive/papers.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11,"title":"Key Papers Guide","description":"Highlights from milestone Go AI papers including AlphaGo, AlphaZero, and KataGo"},"sidebar":"tutorialSidebar","previous":{"title":"Custom Rules & Variants","permalink":"/en/docs/tech/deep-dive/custom-rules"},"next":{"title":"Build Go AI from Scratch","permalink":"/en/docs/tech/deep-dive/build-from-scratch"}}');var l=i(62615),s=i(30416);const t={sidebar_position:11,title:"Key Papers Guide",description:"Highlights from milestone Go AI papers including AlphaGo, AlphaZero, and KataGo"},a="Key Papers Guide",o={},d=[{value:"Papers Overview",id:"papers-overview",level:2},{value:"Timeline",id:"timeline",level:3},{value:"Reading Recommendations",id:"reading-recommendations",level:3},{value:"1. Birth of MCTS (2006)",id:"1-birth-of-mcts-2006",level:2},{value:"Paper Information",id:"paper-information",level:3},{value:"Core Contribution",id:"core-contribution",level:3},{value:"Key Concepts",id:"key-concepts",level:3},{value:"UCB1 Formula",id:"ucb1-formula",level:4},{value:"MCTS Four Steps",id:"mcts-four-steps",level:4},{value:"Impact",id:"impact",level:3},{value:"2. AlphaGo (2016)",id:"2-alphago-2016",level:2},{value:"Paper Information",id:"paper-information-1",level:3},{value:"Core Contribution",id:"core-contribution-1",level:3},{value:"System Architecture",id:"system-architecture",level:3},{value:"Technical Highlights",id:"technical-highlights",level:3},{value:"1. Supervised Learning Policy Network",id:"1-supervised-learning-policy-network",level:4},{value:"2. Reinforcement Learning Improvement",id:"2-reinforcement-learning-improvement",level:4},{value:"3. Value Network Training",id:"3-value-network-training",level:4},{value:"4. MCTS Integration",id:"4-mcts-integration",level:4},{value:"Key Numbers",id:"key-numbers",level:3},{value:"3. AlphaGo Zero (2017)",id:"3-alphago-zero-2017",level:2},{value:"Paper Information",id:"paper-information-2",level:3},{value:"Core Contribution",id:"core-contribution-2",level:3},{value:"Differences from AlphaGo",id:"differences-from-alphago",level:3},{value:"Key Innovations",id:"key-innovations",level:3},{value:"1. Single Dual-Head Network",id:"1-single-dual-head-network",level:4},{value:"2. Simplified Input Features",id:"2-simplified-input-features",level:4},{value:"3. Pure Value Network Evaluation",id:"3-pure-value-network-evaluation",level:4},{value:"4. Training Process",id:"4-training-process",level:4},{value:"Learning Curve",id:"learning-curve",level:3},{value:"4. AlphaZero (2017)",id:"4-alphazero-2017",level:2},{value:"Paper Information",id:"paper-information-3",level:3},{value:"Core Contribution",id:"core-contribution-3",level:3},{value:"General Architecture",id:"general-architecture",level:3},{value:"Cross-Game Adaptation",id:"cross-game-adaptation",level:3},{value:"MCTS Improvements",id:"mcts-improvements",level:3},{value:"PUCT Formula",id:"puct-formula",level:4},{value:"Exploration Noise",id:"exploration-noise",level:4},{value:"5. KataGo (2019)",id:"5-katago-2019",level:2},{value:"Paper Information",id:"paper-information-4",level:3},{value:"Core Contribution",id:"core-contribution-4",level:3},{value:"Key Innovations",id:"key-innovations-1",level:3},{value:"1. Auxiliary Training Targets",id:"1-auxiliary-training-targets",level:4},{value:"2. Global Features",id:"2-global-features",level:4},{value:"3. Playout Cap Randomization",id:"3-playout-cap-randomization",level:4},{value:"4. Progressive Board Sizes",id:"4-progressive-board-sizes",level:4},{value:"Efficiency Comparison",id:"efficiency-comparison",level:3},{value:"6. Extended Papers",id:"6-extended-papers",level:2},{value:"MuZero (2020)",id:"muzero-2020",level:3},{value:"EfficientZero (2021)",id:"efficientzero-2021",level:3},{value:"Gumbel AlphaZero (2022)",id:"gumbel-alphazero-2022",level:3},{value:"Paper Reading Suggestions",id:"paper-reading-suggestions",level:2},{value:"Beginner Order",id:"beginner-order",level:3},{value:"Advanced Order",id:"advanced-order",level:3},{value:"Reading Tips",id:"reading-tips",level:3},{value:"Resource Links",id:"resource-links",level:2},{value:"Paper PDFs",id:"paper-pdfs",level:3},{value:"Open Source Implementations",id:"open-source-implementations",level:3},{value:"Further Reading",id:"further-reading",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"key-papers-guide",children:"Key Papers Guide"})}),"\n",(0,l.jsx)(n.p,{children:"This article summarizes the most important papers in Go AI development history, providing quick summaries and technical highlights."}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"papers-overview",children:"Papers Overview"}),"\n",(0,l.jsx)(n.h3,{id:"timeline",children:"Timeline"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"2006  Coulom - MCTS first applied to Go\n2016  Silver et al. - AlphaGo (Nature)\n2017  Silver et al. - AlphaGo Zero (Nature)\n2017  Silver et al. - AlphaZero\n2019  Wu - KataGo\n2020+ Various improvements and applications\n"})}),"\n",(0,l.jsx)(n.h3,{id:"reading-recommendations",children:"Reading Recommendations"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Goal"}),(0,l.jsx)(n.th,{children:"Recommended Paper"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Understand basics"}),(0,l.jsx)(n.td,{children:"AlphaGo (2016)"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Understand self-play"}),(0,l.jsx)(n.td,{children:"AlphaGo Zero (2017)"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Understand general methods"}),(0,l.jsx)(n.td,{children:"AlphaZero (2017)"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Implementation reference"}),(0,l.jsx)(n.td,{children:"KataGo (2019)"})]})]})]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"1-birth-of-mcts-2006",children:"1. Birth of MCTS (2006)"}),"\n",(0,l.jsx)(n.h3,{id:"paper-information",children:"Paper Information"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Title: Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search\nAuthor: Remi Coulom\nPublished: Computers and Games 2006\n"})}),"\n",(0,l.jsx)(n.h3,{id:"core-contribution",children:"Core Contribution"}),"\n",(0,l.jsx)(n.p,{children:"First systematic application of Monte Carlo methods to Go:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Before: Pure random simulation, no tree structure\nAfter: Build search tree + UCB selection + backpropagation\n"})}),"\n",(0,l.jsx)(n.h3,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,l.jsx)(n.h4,{id:"ucb1-formula",children:"UCB1 Formula"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Selection score = Average win rate + C \xd7 \u221a(ln(N) / n)\n\nWhere:\n- N: Parent visit count\n- n: Child visit count\n- C: Exploration constant\n"})}),"\n",(0,l.jsx)(n.h4,{id:"mcts-four-steps",children:"MCTS Four Steps"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"1. Selection: Use UCB to select node\n2. Expansion: Expand new node\n3. Simulation: Random playout to end\n4. Backpropagation: Propagate win/loss\n"})}),"\n",(0,l.jsx)(n.h3,{id:"impact",children:"Impact"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Go AI reached amateur dan level"}),"\n",(0,l.jsx)(n.li,{children:"Became foundation for all subsequent Go AI"}),"\n",(0,l.jsx)(n.li,{children:"UCB concept influenced PUCT development"}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"2-alphago-2016",children:"2. AlphaGo (2016)"}),"\n",(0,l.jsx)(n.h3,{id:"paper-information-1",children:"Paper Information"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Title: Mastering the game of Go with deep neural networks and tree search\nAuthors: Silver, D., Huang, A., Maddison, C.J., et al.\nPublished: Nature, 2016\nDOI: 10.1038/nature16961\n"})}),"\n",(0,l.jsx)(n.h3,{id:"core-contribution-1",children:"Core Contribution"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"First combination of deep learning and MCTS"}),", defeating human world champion."]}),"\n",(0,l.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,l.jsx)(n.mermaid,{value:'flowchart TB\n    subgraph AlphaGo["AlphaGo Architecture"]\n        subgraph SL["Policy Network (SL)"]\n            SL1["Input: Board state (48 feature planes)"]\n            SL2["Architecture: 13-layer CNN"]\n            SL3["Output: 361 position probabilities"]\n            SL4["Training: 30 million human games"]\n        end\n\n        subgraph RL["Policy Network (RL)"]\n            RL1["Initialized from SL Policy"]\n            RL2["Self-play reinforcement learning"]\n        end\n\n        subgraph VN["Value Network"]\n            VN1["Input: Board state"]\n            VN2["Output: Single win rate value"]\n            VN3["Training: Self-play generated positions"]\n        end\n\n        subgraph MCTS["MCTS"]\n            MCTS1["Use Policy Network to guide search"]\n            MCTS2["Evaluate with Value Network + Rollout"]\n        end\n    end'}),"\n",(0,l.jsx)(n.h3,{id:"technical-highlights",children:"Technical Highlights"}),"\n",(0,l.jsx)(n.h4,{id:"1-supervised-learning-policy-network",children:"1. Supervised Learning Policy Network"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"# Input features (48 planes)\n- Own stone positions\n- Opponent stone positions\n- Liberty counts\n- Post-capture state\n- Legal move positions\n- Recent move positions\n...\n"})}),"\n",(0,l.jsx)(n.h4,{id:"2-reinforcement-learning-improvement",children:"2. Reinforcement Learning Improvement"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"SL Policy \u2192 Self-play \u2192 RL Policy\n\nRL Policy beats SL Policy with ~80% win rate\n"})}),"\n",(0,l.jsx)(n.h4,{id:"3-value-network-training",children:"3. Value Network Training"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Key to prevent overfitting:\n- Take only one position per game\n- Avoid similar positions repeating\n"})}),"\n",(0,l.jsx)(n.h4,{id:"4-mcts-integration",children:"4. MCTS Integration"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Leaf evaluation = 0.5 \xd7 Value Network + 0.5 \xd7 Rollout\n\nRollout uses fast Policy Network (lower accuracy but faster)\n"})}),"\n",(0,l.jsx)(n.h3,{id:"key-numbers",children:"Key Numbers"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Item"}),(0,l.jsx)(n.th,{children:"Value"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"SL Policy accuracy"}),(0,l.jsx)(n.td,{children:"57%"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"RL Policy vs SL Policy win rate"}),(0,l.jsx)(n.td,{children:"80%"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Training GPUs"}),(0,l.jsx)(n.td,{children:"176"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Playing TPUs"}),(0,l.jsx)(n.td,{children:"48"})]})]})]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"3-alphago-zero-2017",children:"3. AlphaGo Zero (2017)"}),"\n",(0,l.jsx)(n.h3,{id:"paper-information-2",children:"Paper Information"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Title: Mastering the game of Go without human knowledge\nAuthors: Silver, D., Schrittwieser, J., Simonyan, K., et al.\nPublished: Nature, 2017\nDOI: 10.1038/nature24270\n"})}),"\n",(0,l.jsx)(n.h3,{id:"core-contribution-2",children:"Core Contribution"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"No human games needed"}),", learning from scratch through self-play."]}),"\n",(0,l.jsx)(n.h3,{id:"differences-from-alphago",children:"Differences from AlphaGo"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Aspect"}),(0,l.jsx)(n.th,{children:"AlphaGo"}),(0,l.jsx)(n.th,{children:"AlphaGo Zero"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Human games"}),(0,l.jsx)(n.td,{children:"Required"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Not required"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Network count"}),(0,l.jsx)(n.td,{children:"4"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"1 dual-head"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Input features"}),(0,l.jsx)(n.td,{children:"48 planes"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"17 planes"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Rollout"}),(0,l.jsx)(n.td,{children:"Used"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Not used"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Residual network"}),(0,l.jsx)(n.td,{children:"No"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Yes"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Training time"}),(0,l.jsx)(n.td,{children:"Months"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"3 days"})})]})]})]}),"\n",(0,l.jsx)(n.h3,{id:"key-innovations",children:"Key Innovations"}),"\n",(0,l.jsx)(n.h4,{id:"1-single-dual-head-network",children:"1. Single Dual-Head Network"}),"\n",(0,l.jsx)(n.mermaid,{value:'flowchart TB\n    Input["Input (17 planes)"]\n    Input --\x3e ResNet\n    subgraph ResNet["Residual Tower (19 or 39 layers)"]\n        R[" "]\n    end\n    ResNet --\x3e Policy["Policy (361)"]\n    ResNet --\x3e Value["Value (1)"]'}),"\n",(0,l.jsx)(n.h4,{id:"2-simplified-input-features",children:"2. Simplified Input Features"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"# Only 17 feature planes needed\nfeatures = [\n    current_player_stones,      # Own stones\n    opponent_stones,            # Opponent stones\n    history_1_player,           # History state 1\n    history_1_opponent,\n    ...                         # History states 2-7\n    color_to_play               # Whose turn\n]\n"})}),"\n",(0,l.jsx)(n.h4,{id:"3-pure-value-network-evaluation",children:"3. Pure Value Network Evaluation"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"No more Rollout\nLeaf evaluation = Value Network output\n\nCleaner, faster\n"})}),"\n",(0,l.jsx)(n.h4,{id:"4-training-process",children:"4. Training Process"}),"\n",(0,l.jsx)(n.mermaid,{value:'flowchart TB\n    Init["Initialize random network"]\n    Init --\x3e SelfPlay\n    SelfPlay["Self-play generates games"]\n    SelfPlay --\x3e Train\n    Train["Train neural network<br/>- Policy: Minimize cross-entropy<br/>- Value: Minimize MSE"]\n    Train --\x3e Eval\n    Eval["Evaluate new network<br/>Replace if stronger"]\n    Eval --\x3e SelfPlay'}),"\n",(0,l.jsx)(n.h3,{id:"learning-curve",children:"Learning Curve"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Training time    Elo\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n3 hours          Beginner\n24 hours         Surpasses AlphaGo Lee\n72 hours         Surpasses AlphaGo Master\n"})}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"4-alphazero-2017",children:"4. AlphaZero (2017)"}),"\n",(0,l.jsx)(n.h3,{id:"paper-information-3",children:"Paper Information"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Title: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\nAuthors: Silver, D., Hubert, T., Schrittwieser, J., et al.\nPublished: arXiv:1712.01815 (later published in Science, 2018)\n"})}),"\n",(0,l.jsx)(n.h3,{id:"core-contribution-3",children:"Core Contribution"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Generalization"}),": Same algorithm applied to Go, Chess, and Shogi."]}),"\n",(0,l.jsx)(n.h3,{id:"general-architecture",children:"General Architecture"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Input encoding (game-specific) \u2192 Residual network (general) \u2192 Dual-head output (general)\n"})}),"\n",(0,l.jsx)(n.h3,{id:"cross-game-adaptation",children:"Cross-Game Adaptation"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Game"}),(0,l.jsx)(n.th,{children:"Input Planes"}),(0,l.jsx)(n.th,{children:"Action Space"}),(0,l.jsx)(n.th,{children:"Training Time"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Go"}),(0,l.jsx)(n.td,{children:"17"}),(0,l.jsx)(n.td,{children:"362"}),(0,l.jsx)(n.td,{children:"40 days"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Chess"}),(0,l.jsx)(n.td,{children:"119"}),(0,l.jsx)(n.td,{children:"4672"}),(0,l.jsx)(n.td,{children:"9 hours"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Shogi"}),(0,l.jsx)(n.td,{children:"362"}),(0,l.jsx)(n.td,{children:"11259"}),(0,l.jsx)(n.td,{children:"12 hours"})]})]})]}),"\n",(0,l.jsx)(n.h3,{id:"mcts-improvements",children:"MCTS Improvements"}),"\n",(0,l.jsx)(n.h4,{id:"puct-formula",children:"PUCT Formula"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Selection score = Q(s,a) + c(s) \xd7 P(s,a) \xd7 \u221aN(s) / (1 + N(s,a))\n\nc(s) = log((1 + N(s) + c_base) / c_base) + c_init\n"})}),"\n",(0,l.jsx)(n.h4,{id:"exploration-noise",children:"Exploration Noise"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"# Add Dirichlet noise at root\nP(s,a) = (1 - \u03b5) \xd7 p_a + \u03b5 \xd7 \u03b7_a\n\n\u03b7 ~ Dir(\u03b1)\n\u03b1 = 0.03 (Go), 0.3 (Chess), 0.15 (Shogi)\n"})}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"5-katago-2019",children:"5. KataGo (2019)"}),"\n",(0,l.jsx)(n.h3,{id:"paper-information-4",children:"Paper Information"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Title: Accelerating Self-Play Learning in Go\nAuthor: David J. Wu\nPublished: arXiv:1902.10565\n"})}),"\n",(0,l.jsx)(n.h3,{id:"core-contribution-4",children:"Core Contribution"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"50x efficiency improvement"}),", allowing individual developers to train strong Go AI."]}),"\n",(0,l.jsx)(n.h3,{id:"key-innovations-1",children:"Key Innovations"}),"\n",(0,l.jsx)(n.h4,{id:"1-auxiliary-training-targets",children:"1. Auxiliary Training Targets"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Total loss = Policy Loss + Value Loss +\n         Score Loss + Ownership Loss + ...\n\nAuxiliary targets help network converge faster\n"})}),"\n",(0,l.jsx)(n.h4,{id:"2-global-features",children:"2. Global Features"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"# Global pooling layer\nglobal_features = global_avg_pool(conv_features)\n# Combine with local features\ncombined = concat(conv_features, broadcast(global_features))\n"})}),"\n",(0,l.jsx)(n.h4,{id:"3-playout-cap-randomization",children:"3. Playout Cap Randomization"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Traditional: Fixed N searches each time\nKataGo: N sampled from a distribution\n\nLets network perform well at various search depths\n"})}),"\n",(0,l.jsx)(n.h4,{id:"4-progressive-board-sizes",children:"4. Progressive Board Sizes"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"if training_step < 1000000:\n    board_size = random.choice([9, 13, 19])\nelse:\n    board_size = 19\n"})}),"\n",(0,l.jsx)(n.h3,{id:"efficiency-comparison",children:"Efficiency Comparison"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Metric"}),(0,l.jsx)(n.th,{children:"AlphaZero"}),(0,l.jsx)(n.th,{children:"KataGo"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"GPU-days to superhuman"}),(0,l.jsx)(n.td,{children:"5000"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"100"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Efficiency improvement"}),(0,l.jsx)(n.td,{children:"Baseline"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"50x"})})]})]})]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"6-extended-papers",children:"6. Extended Papers"}),"\n",(0,l.jsx)(n.h3,{id:"muzero-2020",children:"MuZero (2020)"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Title: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\nContribution: Learn environment dynamics model, no game rules needed\n"})}),"\n",(0,l.jsx)(n.h3,{id:"efficientzero-2021",children:"EfficientZero (2021)"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Title: Mastering Atari Games with Limited Data\nContribution: Dramatically improved sample efficiency\n"})}),"\n",(0,l.jsx)(n.h3,{id:"gumbel-alphazero-2022",children:"Gumbel AlphaZero (2022)"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Title: Policy Improvement by Planning with Gumbel\nContribution: Improved policy improvement method\n"})}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"paper-reading-suggestions",children:"Paper Reading Suggestions"}),"\n",(0,l.jsx)(n.h3,{id:"beginner-order",children:"Beginner Order"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"1. AlphaGo (2016) - Understand basic architecture\n2. AlphaGo Zero (2017) - Understand self-play\n3. KataGo (2019) - Understand implementation details\n"})}),"\n",(0,l.jsx)(n.h3,{id:"advanced-order",children:"Advanced Order"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"4. AlphaZero (2017) - Generalization\n5. MuZero (2020) - Learn world models\n6. MCTS original paper - Understand foundations\n"})}),"\n",(0,l.jsx)(n.h3,{id:"reading-tips",children:"Reading Tips"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Read abstract and conclusion first"}),": Quickly grasp core contribution"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Look at figures"}),": Understand overall architecture"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Read methods section"}),": Understand technical details"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Check appendix"}),": Find implementation details and hyperparameters"]}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"resource-links",children:"Resource Links"}),"\n",(0,l.jsx)(n.h3,{id:"paper-pdfs",children:"Paper PDFs"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Paper"}),(0,l.jsx)(n.th,{children:"Link"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"AlphaGo"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.a,{href:"https://www.nature.com/articles/nature16961",children:"Nature"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"AlphaGo Zero"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.a,{href:"https://www.nature.com/articles/nature24270",children:"Nature"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"AlphaZero"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.a,{href:"https://www.science.org/doi/10.1126/science.aar6404",children:"Science"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"KataGo"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.a,{href:"https://arxiv.org/abs/1902.10565",children:"arXiv"})})]})]})]}),"\n",(0,l.jsx)(n.h3,{id:"open-source-implementations",children:"Open Source Implementations"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Project"}),(0,l.jsx)(n.th,{children:"Link"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"KataGo"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.a,{href:"https://github.com/lightvector/KataGo",children:"GitHub"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Leela Zero"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.a,{href:"https://github.com/leela-zero/leela-zero",children:"GitHub"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"MiniGo"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.a,{href:"https://github.com/tensorflow/minigo",children:"GitHub"})})]})]})]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"../neural-network",children:"Neural Network Architecture"})," \u2014 Deep dive into network design"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"../mcts-implementation",children:"MCTS Implementation Details"})," \u2014 Search algorithm implementation"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"../training",children:"KataGo Training Mechanism"})," \u2014 Training process details"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(c,{...e})}):c(e)}},30416(e,n,i){i.d(n,{R:()=>t,x:()=>a});var r=i(59471);const l={},s=r.createContext(l);function t(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:t(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);