"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[5780],{33529(e,n,s){s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>c,frontMatter:()=>l,metadata:()=>i,toc:()=>o});const i=JSON.parse('{"id":"alphago/input-features","title":"Input Feature Design","description":"From AlphaGo\'s 48 planes to AlphaGo Zero\'s 17 planes, exploring the evolution of Go AI feature engineering","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/alphago/09-input-features.mdx","sourceDirName":"alphago","slug":"/alphago/input-features","permalink":"/en/docs/alphago/input-features","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/09-input-features.mdx","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"sidebar_position":10,"title":"Input Feature Design","description":"From AlphaGo\'s 48 planes to AlphaGo Zero\'s 17 planes, exploring the evolution of Go AI feature engineering"},"sidebar":"tutorialSidebar","previous":{"title":"Value Network Explained","permalink":"/en/docs/alphago/value-network"},"next":{"title":"CNN and Go","permalink":"/en/docs/alphago/cnn-and-go"}}');var r=s(62615),t=s(30416);const l={sidebar_position:10,title:"Input Feature Design",description:"From AlphaGo's 48 planes to AlphaGo Zero's 17 planes, exploring the evolution of Go AI feature engineering"},a="Input Feature Design",d={},o=[{value:"What are Feature Planes?",id:"what-are-feature-planes",level:2},{value:"Basic Concept",id:"basic-concept",level:3},{value:"Multiple Feature Planes",id:"multiple-feature-planes",level:3},{value:"AlphaGo&#39;s 48 Feature Planes",id:"alphagos-48-feature-planes",level:2},{value:"Complete List",id:"complete-list",level:3},{value:"1. Stone Positions (3 planes)",id:"1-stone-positions-3-planes",level:4},{value:"2. History (16 planes)",id:"2-history-16-planes",level:4},{value:"3. Liberty Features (8 planes)",id:"3-liberty-features-8-planes",level:4},{value:"4. Capture Features (8 planes)",id:"4-capture-features-8-planes",level:4},{value:"5. Ladder Features (8 planes)",id:"5-ladder-features-8-planes",level:4},{value:"6. Legality Feature (1 plane)",id:"6-legality-feature-1-plane",level:4},{value:"7. Border Features (4 planes)",id:"7-border-features-4-planes",level:4},{value:"Why So Many Features?",id:"why-so-many-features",level:3},{value:"AlphaGo Zero&#39;s Simplification: 17 Feature Planes",id:"alphago-zeros-simplification-17-feature-planes",level:2},{value:"Revolutionary Change",id:"revolutionary-change",level:3},{value:"17 Planes Composition",id:"17-planes-composition",level:3},{value:"1. Stone Position History (16 planes)",id:"1-stone-position-history-16-planes",level:4},{value:"2. Color (1 plane)",id:"2-color-1-plane",level:4},{value:"Why Can It Be Simplified?",id:"why-can-it-be-simplified",level:3},{value:"Performance Comparison",id:"performance-comparison",level:3},{value:"Why Is Human Knowledge a Burden?",id:"why-is-human-knowledge-a-burden",level:3},{value:"1. Human Knowledge May Be Wrong",id:"1-human-knowledge-may-be-wrong",level:4},{value:"2. Feature Encoding Limits Representation",id:"2-feature-encoding-limits-representation",level:4},{value:"3. Representation Bottleneck",id:"3-representation-bottleneck",level:4},{value:"KataGo&#39;s Optimization: 22 Feature Planes",id:"katagos-optimization-22-feature-planes",level:2},{value:"Pragmatic Balance",id:"pragmatic-balance",level:3},{value:"KataGo&#39;s Feature List",id:"katagos-feature-list",level:3},{value:"Basic Features (5)",id:"basic-features-5",level:4},{value:"History Features (5)",id:"history-features-5",level:4},{value:"Ko Features (3)",id:"ko-features-3",level:4},{value:"Rule Features (9)",id:"rule-features-9",level:4},{value:"Why Add These Features?",id:"why-add-these-features",level:3},{value:"1. Ko Is Too Important",id:"1-ko-is-too-important",level:4},{value:"2. Rule Diversity",id:"2-rule-diversity",level:4},{value:"3. Training Efficiency",id:"3-training-efficiency",level:4},{value:"Philosophy of Feature Design",id:"philosophy-of-feature-design",level:2},{value:"Three Approaches",id:"three-approaches",level:3},{value:"Trade-off Considerations",id:"trade-off-considerations",level:3},{value:"Limited Resources",id:"limited-resources",level:4},{value:"Pursuing the Limit",id:"pursuing-the-limit",level:4},{value:"Insights",id:"insights",level:3},{value:"Implementation Examples",id:"implementation-examples",level:2},{value:"Feature Extraction (AlphaGo Style)",id:"feature-extraction-alphago-style",level:3},{value:"Feature Extraction (AlphaGo Zero Style)",id:"feature-extraction-alphago-zero-style",level:3},{value:"Performance Comparison",id:"performance-comparison-1",level:3},{value:"Visualizing Feature Planes",id:"visualizing-feature-planes",level:2},{value:"Real Position Example",id:"real-position-example",level:3},{value:"Insights from Feature Planes",id:"insights-from-feature-planes",level:3},{value:"Animation Reference",id:"animation-reference",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"References",id:"references",level:2}];function h(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"input-feature-design",children:"Input Feature Design"})}),"\n",(0,r.jsx)(n.p,{children:'Neural networks can only process numbers. To make them understand Go, we need a way to "translate" the board into numbers.'}),"\n",(0,r.jsxs)(n.p,{children:["This translation process is ",(0,r.jsx)(n.strong,{children:"input feature design"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo used 48 feature planes, AlphaGo Zero simplified to 17, and KataGo optimized to 22. This article will explain the considerations behind these design choices in detail."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"what-are-feature-planes",children:"What are Feature Planes?"}),"\n",(0,r.jsx)(n.h3,{id:"basic-concept",children:"Basic Concept"}),"\n",(0,r.jsxs)(n.p,{children:["A ",(0,r.jsx)(n.strong,{children:"feature plane"})," is a 19\xd719 matrix where each element represents a certain property of the corresponding board position."]}),"\n",(0,r.jsx)(n.p,{children:'For example, the "Black stone positions" feature plane:'}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Board state:                 Feature plane (Black stones):\n  A B C D E                   A B C D E\n1 . . . . .                1  0 0 0 0 0\n2 . \u25cf . . .                2  0 1 0 0 0\n3 . . \u25cb . .    \u2192           3  0 0 0 0 0\n4 . . . \u25cf .                4  0 0 0 1 0\n5 . . . . .                5  0 0 0 0 0\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Position with Black stone = 1"}),"\n",(0,r.jsx)(n.li,{children:"Position without Black stone = 0"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"multiple-feature-planes",children:"Multiple Feature Planes"}),"\n",(0,r.jsx)(n.p,{children:"Neural networks need various information, so we stack multiple feature planes:"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart TB\n    subgraph Tensor["Input tensor: 19 x 19 x N"]\n        P1["Plane 1: Black stone positions"]\n        P2["Plane 2: White stone positions"]\n        P3["Plane 3: Empty positions"]\n        P4["..."]\n        PN["Plane N: Other features"]\n    end\n\n    P1 --\x3e P2 --\x3e P3 --\x3e P4 --\x3e PN'}),"\n",(0,r.jsx)(n.p,{children:'This is similar to how color images have R, G, B channels. Go "images" have N channels.'}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"alphagos-48-feature-planes",children:"AlphaGo's 48 Feature Planes"}),"\n",(0,r.jsx)(n.h3,{id:"complete-list",children:"Complete List"}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo uses 48 feature planes, divided into several categories:"}),"\n",(0,r.jsx)(n.h4,{id:"1-stone-positions-3-planes",children:"1. Stone Positions (3 planes)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Plane"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1"}),(0,r.jsx)(n.td,{children:"Black"}),(0,r.jsx)(n.td,{children:"Black stone = 1, otherwise = 0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"2"}),(0,r.jsx)(n.td,{children:"White"}),(0,r.jsx)(n.td,{children:"White stone = 1, otherwise = 0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"3"}),(0,r.jsx)(n.td,{children:"Empty"}),(0,r.jsx)(n.td,{children:"Empty point = 1, otherwise = 0"})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"2-history-16-planes",children:"2. History (16 planes)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Plane"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"4-11"}),(0,r.jsx)(n.td,{children:"Black history"}),(0,r.jsx)(n.td,{children:"Black positions 1-8 moves ago"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"12-19"}),(0,r.jsx)(n.td,{children:"White history"}),(0,r.jsx)(n.td,{children:"White positions 1-8 moves ago"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Why is history needed?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ko detection"}),": Need to know if immediate recapture is allowed"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Playing intention"}),": Recent moves reveal both sides' plans"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Temporal information"}),": CNN itself doesn't handle time, history planes fill this gap"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"3-liberty-features-8-planes",children:"3. Liberty Features (8 planes)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Plane"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"20-23"}),(0,r.jsx)(n.td,{children:"1-4 liberties (own)"}),(0,r.jsx)(n.td,{children:"Own string has 1/2/3/4 liberties = 1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"24-27"}),(0,r.jsx)(n.td,{children:"1-4 liberties (opponent)"}),(0,r.jsx)(n.td,{children:"Opponent string has 1/2/3/4 liberties = 1"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Liberty count is the most important tactical concept in Go:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"1 liberty"}),": In atari, about to be captured"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"2 liberties"}),": Dangerous state"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"3 liberties"}),": Needs attention"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"4+ liberties"}),": Temporarily safe"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"4-capture-features-8-planes",children:"4. Capture Features (8 planes)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Plane"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"28-31"}),(0,r.jsx)(n.td,{children:"Capture positions (own)"}),(0,r.jsx)(n.td,{children:"Playing here can capture opponent's 1/2/3/4 stones"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"32-35"}),(0,r.jsx)(n.td,{children:"Capture positions (opponent)"}),(0,r.jsx)(n.td,{children:"Playing here can capture own 1/2/3/4 stones"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Atari is the most common tactic in Go:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Capturing more stones = bigger threat"}),"\n",(0,r.jsx)(n.li,{children:"Different capture sizes require different responses"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"5-ladder-features-8-planes",children:"5. Ladder Features (8 planes)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Plane"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"36-39"}),(0,r.jsx)(n.td,{children:"Ladder-related (own)"}),(0,r.jsx)(n.td,{children:"Positions related to own ladders"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"40-43"}),(0,r.jsx)(n.td,{children:"Ladder-related (opponent)"}),(0,r.jsx)(n.td,{children:"Positions related to opponent ladders"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"The ladder is a famous Go tactic:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Chasing opponent's stones along diagonal"}),"\n",(0,r.jsx)(n.li,{children:'Need to determine "ladder works" or "ladder fails"'}),"\n",(0,r.jsx)(n.li,{children:"Requires global vision, a challenge for traditional Go programs"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"6-legality-feature-1-plane",children:"6. Legality Feature (1 plane)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Plane"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"44"}),(0,r.jsx)(n.td,{children:"Legal positions"}),(0,r.jsx)(n.td,{children:"Can legally play here = 1"})]})})]}),"\n",(0,r.jsx)(n.p,{children:"This prevents the network from outputting illegal moves:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Cannot play where a stone already exists"}),"\n",(0,r.jsx)(n.li,{children:"Cannot play at suicide points (self-capture without capturing)"}),"\n",(0,r.jsx)(n.li,{children:"Cannot immediately recapture in ko"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"7-border-features-4-planes",children:"7. Border Features (4 planes)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Plane"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"45"}),(0,r.jsx)(n.td,{children:"Distance to edge 1"}),(0,r.jsx)(n.td,{children:"On 1st line = 1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"46"}),(0,r.jsx)(n.td,{children:"Distance to edge 2"}),(0,r.jsx)(n.td,{children:"On 2nd line = 1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"47"}),(0,r.jsx)(n.td,{children:"Distance to edge 3"}),(0,r.jsx)(n.td,{children:"On 3rd line = 1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"48"}),(0,r.jsx)(n.td,{children:"Distance to edge 4+"}),(0,r.jsx)(n.td,{children:"On 4th line or inner = 1"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Edges and corners have special meaning in Go:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"1st line"}),": Death line, stones easily captured"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"2nd line"}),": Survival line, but inefficient"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"3rd line"}),": Territory line, solid"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"4th line"}),": Influence line, pursuing influence"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"why-so-many-features",children:"Why So Many Features?"}),"\n",(0,r.jsxs)(n.p,{children:["DeepMind's design philosophy was to ",(0,r.jsx)(n.strong,{children:"provide as much information as possible"}),", letting the network decide what's useful:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Raw board \u2192 48 feature planes \u2192 Neural network \u2192 Decision\n\nFeature engineer's job: Encode Go knowledge as features\nNeural network's job: Learn to combine these features\n"})}),"\n",(0,r.jsx)(n.p,{children:'This is a strategy of "passing the ball to the neural network" - humans handle feature design, the network handles learning combinations.'}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"alphago-zeros-simplification-17-feature-planes",children:"AlphaGo Zero's Simplification: 17 Feature Planes"}),"\n",(0,r.jsx)(n.h3,{id:"revolutionary-change",children:"Revolutionary Change"}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo Zero dramatically simplified input features:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Version"}),(0,r.jsx)(n.th,{children:"Feature Planes"}),(0,r.jsx)(n.th,{children:"Human Knowledge Used"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaGo"}),(0,r.jsx)(n.td,{children:"48"}),(0,r.jsx)(n.td,{children:"Extensive (liberties, ladders, etc.)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaGo Zero"}),(0,r.jsx)(n.td,{children:"17"}),(0,r.jsx)(n.td,{children:"Almost none"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"17-planes-composition",children:"17 Planes Composition"}),"\n",(0,r.jsx)(n.h4,{id:"1-stone-position-history-16-planes",children:"1. Stone Position History (16 planes)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Plane"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1-8"}),(0,r.jsx)(n.td,{children:"Black T-0 to T-7"}),(0,r.jsx)(n.td,{children:"Black positions at current and past 7 moves"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"9-16"}),(0,r.jsx)(n.td,{children:"White T-0 to T-7"}),(0,r.jsx)(n.td,{children:"White positions at current and past 7 moves"})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"2-color-1-plane",children:"2. Color (1 plane)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Plane"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"17"}),(0,r.jsx)(n.td,{children:"Whose turn"}),(0,r.jsx)(n.td,{children:"Black's turn = all 1s, White's turn = all 0s"})]})})]}),"\n",(0,r.jsx)(n.h3,{id:"why-can-it-be-simplified",children:"Why Can It Be Simplified?"}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo Zero's core insight:"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Given enough computation and training time, neural networks can learn these features themselves"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:'Concepts like "liberties," "atari," "ladders" - humans took thousands of years to develop. But AlphaGo Zero proved that neural networks can learn them in days - and perhaps learn even better representations than humans.'}),"\n",(0,r.jsx)(n.h3,{id:"performance-comparison",children:"Performance Comparison"}),"\n",(0,r.jsx)(n.p,{children:"Surprisingly, AlphaGo Zero with fewer features is actually stronger:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Version"}),(0,r.jsx)(n.th,{children:"Features"}),(0,r.jsx)(n.th,{children:"Training Time"}),(0,r.jsx)(n.th,{children:"Final Strength"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaGo Master"}),(0,r.jsx)(n.td,{children:"48"}),(0,r.jsx)(n.td,{children:"Months"}),(0,r.jsx)(n.td,{children:"~5185 Elo"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaGo Zero"}),(0,r.jsx)(n.td,{children:"17"}),(0,r.jsx)(n.td,{children:"40 days"}),(0,r.jsx)(n.td,{children:"~5185 Elo"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaGo Zero (3 days)"}),(0,r.jsx)(n.td,{children:"17"}),(0,r.jsx)(n.td,{children:"3 days"}),(0,r.jsx)(n.td,{children:"Surpasses humans"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Less human knowledge actually leads to stronger performance."}),"\n",(0,r.jsx)(n.h3,{id:"why-is-human-knowledge-a-burden",children:"Why Is Human Knowledge a Burden?"}),"\n",(0,r.jsx)(n.h4,{id:"1-human-knowledge-may-be-wrong",children:"1. Human Knowledge May Be Wrong"}),"\n",(0,r.jsx)(n.p,{children:"Human-summarized Go principles are empirical and may not be optimal. For example:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"Golden corner, silver edge, grass belly" - but in some positions the center is more important'}),"\n",(0,r.jsx)(n.li,{children:'"Don\'t play ladder if it fails" - but sometimes you can deliberately sacrifice'}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"2-feature-encoding-limits-representation",children:"2. Feature Encoding Limits Representation"}),"\n",(0,r.jsx)(n.p,{children:'When we encode "liberty count" as four planes for 1-4 liberties, we implicitly assume "liberty count" is an important classification method. But perhaps there are better classifications, and this encoding prevents the network from discovering them.'}),"\n",(0,r.jsx)(n.h4,{id:"3-representation-bottleneck",children:"3. Representation Bottleneck"}),"\n",(0,r.jsx)(n.p,{children:"48 planes consume more computational resources. If some features are redundant, these resources are wasted."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"katagos-optimization-22-feature-planes",children:"KataGo's Optimization: 22 Feature Planes"}),"\n",(0,r.jsx)(n.h3,{id:"pragmatic-balance",children:"Pragmatic Balance"}),"\n",(0,r.jsx)(n.p,{children:"KataGo built on AlphaGo Zero's foundation, adding a small amount of carefully selected human knowledge:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Item"}),(0,r.jsx)(n.th,{children:"AlphaGo Zero"}),(0,r.jsx)(n.th,{children:"KataGo"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"History planes"}),(0,r.jsx)(n.td,{children:"16"}),(0,r.jsx)(n.td,{children:"5"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Stone positions"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Yes"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Whose turn"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Yes"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Ko state"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Yes"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Rule variants"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Yes (komi, suicide rules, etc.)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Total"})}),(0,r.jsx)(n.td,{children:"17"}),(0,r.jsx)(n.td,{children:"22"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"katagos-feature-list",children:"KataGo's Feature List"}),"\n",(0,r.jsx)(n.h4,{id:"basic-features-5",children:"Basic Features (5)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Plane"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1"}),(0,r.jsx)(n.td,{children:"Black"}),(0,r.jsx)(n.td,{children:"Current black stone positions"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"2"}),(0,r.jsx)(n.td,{children:"White"}),(0,r.jsx)(n.td,{children:"Current white stone positions"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"3"}),(0,r.jsx)(n.td,{children:"Empty"}),(0,r.jsx)(n.td,{children:"Current empty positions"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"4"}),(0,r.jsx)(n.td,{children:"Whose turn (1)"}),(0,r.jsx)(n.td,{children:"Constant plane always 1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"5"}),(0,r.jsx)(n.td,{children:"Whose turn (2)"}),(0,r.jsx)(n.td,{children:"Black's turn = 1, White's turn = 0"})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"history-features-5",children:"History Features (5)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Plane"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"6"}),(0,r.jsx)(n.td,{children:"Last move"}),(0,r.jsx)(n.td,{children:"Opponent's last move position"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"7"}),(0,r.jsx)(n.td,{children:"2nd last move"}),(0,r.jsx)(n.td,{children:"Own last move position"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"8"}),(0,r.jsx)(n.td,{children:"3rd last move"}),(0,r.jsx)(n.td,{children:"Opponent's 2nd last move"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"9"}),(0,r.jsx)(n.td,{children:"4th last move"}),(0,r.jsx)(n.td,{children:"Own 2nd last move"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"10"}),(0,r.jsx)(n.td,{children:"5th last move"}),(0,r.jsx)(n.td,{children:"Opponent's 3rd last move"})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"ko-features-3",children:"Ko Features (3)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Plane"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"11"}),(0,r.jsx)(n.td,{children:"Ko forbidden"}),(0,r.jsx)(n.td,{children:"Current ko forbidden point"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"12"}),(0,r.jsx)(n.td,{children:"Potential ko (own)"}),(0,r.jsx)(n.td,{children:"Playing here creates ko"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"13"}),(0,r.jsx)(n.td,{children:"Potential ko (opponent)"}),(0,r.jsx)(n.td,{children:"Opponent playing here creates ko"})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"rule-features-9",children:"Rule Features (9)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Plane"}),(0,r.jsx)(n.th,{children:"Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"14-22"}),(0,r.jsx)(n.td,{children:"Rule encoding"}),(0,r.jsx)(n.td,{children:"Komi, suicide rules, superko, etc."})]})})]}),"\n",(0,r.jsx)(n.h3,{id:"why-add-these-features",children:"Why Add These Features?"}),"\n",(0,r.jsx)(n.p,{children:"KataGo's author lightvector explains:"}),"\n",(0,r.jsx)(n.h4,{id:"1-ko-is-too-important",children:"1. Ko Is Too Important"}),"\n",(0,r.jsx)(n.p,{children:"Ko is one of the most complex concepts in Go. Learning ko rules purely from raw board states requires massive samples. Explicitly marking ko forbidden points accelerates learning."}),"\n",(0,r.jsx)(n.h4,{id:"2-rule-diversity",children:"2. Rule Diversity"}),"\n",(0,r.jsx)(n.p,{children:"Go has multiple rule sets:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Komi"}),": Chinese rules 7.5 points, Japanese rules 6.5 points"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Suicide rules"}),": Some rules allow suicide"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Superko"}),": Different ways to handle long cycles"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Explicitly encoding rules in input allows one network to handle all variants."}),"\n",(0,r.jsx)(n.h4,{id:"3-training-efficiency",children:"3. Training Efficiency"}),"\n",(0,r.jsx)(n.p,{children:"Adding a small amount of human knowledge can dramatically accelerate training. KataGo achieved with 50 GPU-days what AlphaGo Zero took 5000+ TPU-days to achieve."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"philosophy-of-feature-design",children:"Philosophy of Feature Design"}),"\n",(0,r.jsx)(n.h3,{id:"three-approaches",children:"Three Approaches"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Approach"}),(0,r.jsx)(n.th,{children:"Representative"}),(0,r.jsx)(n.th,{children:"Feature Count"}),(0,r.jsx)(n.th,{children:"Human Knowledge"}),(0,r.jsx)(n.th,{children:"Compute Required"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Heavy human knowledge"}),(0,r.jsx)(n.td,{children:"AlphaGo"}),(0,r.jsx)(n.td,{children:"48"}),(0,r.jsx)(n.td,{children:"Extensive"}),(0,r.jsx)(n.td,{children:"Medium"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Minimal human knowledge"}),(0,r.jsx)(n.td,{children:"AlphaGo Zero"}),(0,r.jsx)(n.td,{children:"17"}),(0,r.jsx)(n.td,{children:"Almost none"}),(0,r.jsx)(n.td,{children:"Very high"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Moderate human knowledge"}),(0,r.jsx)(n.td,{children:"KataGo"}),(0,r.jsx)(n.td,{children:"22"}),(0,r.jsx)(n.td,{children:"Small selection"}),(0,r.jsx)(n.td,{children:"Lower"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"trade-off-considerations",children:"Trade-off Considerations"}),"\n",(0,r.jsx)(n.h4,{id:"limited-resources",children:"Limited Resources"}),"\n",(0,r.jsx)(n.p,{children:"If computational resources are limited (most researchers' situation), adding some human knowledge is wise:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Accelerates training convergence"}),"\n",(0,r.jsx)(n.li,{children:"Reduces required training data"}),"\n",(0,r.jsx)(n.li,{children:"Avoids reinventing the wheel"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"pursuing-the-limit",children:"Pursuing the Limit"}),"\n",(0,r.jsx)(n.p,{children:"If computational resources are abundant, reducing human knowledge may achieve higher strength:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Avoids human biases"}),"\n",(0,r.jsx)(n.li,{children:"Discovers strategies unknown to humans"}),"\n",(0,r.jsx)(n.li,{children:'True "starting from scratch"'}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"insights",children:"Insights"}),"\n",(0,r.jsx)(n.p,{children:"The AlphaGo series evolution tells us:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feature engineering still matters"})," - but the form has changed"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"End-to-end learning is the trend"})," - let networks learn features themselves"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"No single correct answer"})," - depends on resources and goals"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"implementation-examples",children:"Implementation Examples"}),"\n",(0,r.jsx)(n.h3,{id:"feature-extraction-alphago-style",children:"Feature Extraction (AlphaGo Style)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\ndef extract_features_alphago(board, history, current_player):\n    """\n    Extract AlphaGo-style 48 feature planes\n\n    board: 19\xd719 board, 0=empty, 1=black, 2=white\n    history: Last 8 moves\' history\n    current_player: 1=black, 2=white\n    """\n    features = np.zeros((48, 19, 19))\n\n    # 1-3: Stone positions\n    features[0] = (board == 1)  # Black\n    features[1] = (board == 2)  # White\n    features[2] = (board == 0)  # Empty\n\n    # 4-19: History positions\n    for i, hist_board in enumerate(history[:8]):\n        features[3 + i] = (hist_board == 1)      # Black history\n        features[11 + i] = (hist_board == 2)     # White history\n\n    # 20-27: Liberty features\n    liberties = compute_liberties(board)\n    for i, lib_count in enumerate([1, 2, 3, 4]):\n        my_color = current_player\n        opp_color = 3 - current_player\n        features[19 + i] = (liberties == lib_count) & (board == my_color)\n        features[23 + i] = (liberties == lib_count) & (board == opp_color)\n\n    # 28-35: Capture features\n    capture_counts = compute_captures(board)\n    for i, cap_count in enumerate([1, 2, 3, 4]):\n        features[27 + i] = (capture_counts[current_player] == cap_count)\n        features[31 + i] = (capture_counts[3-current_player] == cap_count)\n\n    # 36-43: Ladder features (simplified)\n    ladder_status = compute_ladder(board)\n    # ... implementation omitted ...\n\n    # 44: Legal positions\n    features[43] = compute_legal_moves(board, current_player)\n\n    # 45-48: Border distance\n    for i in range(19):\n        for j in range(19):\n            dist = min(i, j, 18-i, 18-j)\n            if dist == 0:\n                features[44, i, j] = 1\n            elif dist == 1:\n                features[45, i, j] = 1\n            elif dist == 2:\n                features[46, i, j] = 1\n            else:\n                features[47, i, j] = 1\n\n    return features\n'})}),"\n",(0,r.jsx)(n.h3,{id:"feature-extraction-alphago-zero-style",children:"Feature Extraction (AlphaGo Zero Style)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def extract_features_zero(board_history, current_player):\n    """\n    Extract AlphaGo Zero-style 17 feature planes\n\n    board_history: List of last 8 board states\n    current_player: 1=black, 2=white\n    """\n    features = np.zeros((17, 19, 19))\n\n    # 1-8: Black positions at T-0 to T-7\n    for i, board in enumerate(board_history[:8]):\n        features[i] = (board == 1)\n\n    # 9-16: White positions at T-0 to T-7\n    for i, board in enumerate(board_history[:8]):\n        features[8 + i] = (board == 2)\n\n    # 17: Whose turn\n    if current_player == 1:  # Black\n        features[16] = np.ones((19, 19))\n    else:\n        features[16] = np.zeros((19, 19))\n\n    return features\n'})}),"\n",(0,r.jsx)(n.h3,{id:"performance-comparison-1",children:"Performance Comparison"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\n\n# Simulate 1000 feature extractions\nboard = np.random.randint(0, 3, (19, 19))\nhistory = [np.random.randint(0, 3, (19, 19)) for _ in range(8)]\n\n# AlphaGo style (with complex computations)\nstart = time.time()\nfor _ in range(1000):\n    features = extract_features_alphago(board, history, 1)\nalphago_time = time.time() - start\n\n# AlphaGo Zero style (simple)\nstart = time.time()\nfor _ in range(1000):\n    features = extract_features_zero(history, 1)\nzero_time = time.time() - start\n\nprint(f"AlphaGo style: {alphago_time:.2f}s")\nprint(f"AlphaGo Zero style: {zero_time:.2f}s")\n# Typical result: AlphaGo style 5-10x slower\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"visualizing-feature-planes",children:"Visualizing Feature Planes"}),"\n",(0,r.jsx)(n.h3,{id:"real-position-example",children:"Real Position Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Actual board:\n   A B C D E F G H J K L M N O P Q R S T\n19 . . . . . . . . . . . . . . . . . . .\n18 . . . . . . . . . . . . . . . . . . .\n17 . . . \u25cf . . . . . . . . . . . \u25cb . . .\n16 . . . . . . . . . . . . . . . . . . .\n15 . . . . . . . . . . . . . . . . . . .\n...\n\nFeature plane 1 (Black):\n   A B C D E F G H J K L M N O P Q R S T\n19 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n17 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n...\n\nFeature plane 2 (White):\n   A B C D E F G H J K L M N O P Q R S T\n19 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n...\n"})}),"\n",(0,r.jsx)(n.h3,{id:"insights-from-feature-planes",children:"Insights from Feature Planes"}),"\n",(0,r.jsx)(n.p,{children:'Observing different feature planes helps understand what the model "sees":'}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Feature"}),(0,r.jsx)(n.th,{children:"Intuitive Meaning"}),(0,r.jsx)(n.th,{children:"What Model Might Learn"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Black/White positions"}),(0,r.jsx)(n.td,{children:"Who is where"}),(0,r.jsx)(n.td,{children:"Shapes, connectivity"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"History"}),(0,r.jsx)(n.td,{children:"What happened recently"}),(0,r.jsx)(n.td,{children:"Playing intentions, fighting directions"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Liberties"}),(0,r.jsx)(n.td,{children:"Who is in danger"}),(0,r.jsx)(n.td,{children:"Attack/defense targets"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Captures"}),(0,r.jsx)(n.td,{children:"Tactical opportunities"}),(0,r.jsx)(n.td,{children:"Local tactics"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Border distance"}),(0,r.jsx)(n.td,{children:"Position importance"}),(0,r.jsx)(n.td,{children:"Opening moves, corner joseki"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"animation-reference",children:"Animation Reference"}),"\n",(0,r.jsx)(n.p,{children:"Core concepts covered in this article with animation numbers:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Number"}),(0,r.jsx)(n.th,{children:"Concept"}),(0,r.jsx)(n.th,{children:"Physics/Math Correspondence"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Animation A8"}),(0,r.jsx)(n.td,{children:"Feature encoding"}),(0,r.jsx)(n.td,{children:"Tensor representation"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Animation A10"}),(0,r.jsx)(n.td,{children:"Input normalization"}),(0,r.jsx)(n.td,{children:"Feature engineering"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Animation D1"}),(0,r.jsx)(n.td,{children:"Convolution input"}),(0,r.jsx)(n.td,{children:"Multi-channel images"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Animation E3"}),(0,r.jsx)(n.td,{children:"Zero's simplification"}),(0,r.jsx)(n.td,{children:"Minimal representation"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Previous"}),": ",(0,r.jsx)(n.a,{href:"../value-network",children:"Value Network Explained"})," - How to evaluate position value"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Next"}),": ",(0,r.jsx)(n.a,{href:"../cnn-and-go",children:"CNN and Go"})," - How CNNs process the board"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Related Topic"}),": ",(0,r.jsx)(n.a,{href:"../board-representation",children:"Board State Representation"})," - Lower-level data structures"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feature planes are digital board representations"}),": Each plane is a 19\xd719 matrix"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"AlphaGo uses 48 planes"}),": Contains extensive human Go knowledge"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"AlphaGo Zero simplifies to 17"}),": Proves networks can learn features themselves"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"KataGo optimizes to 22"}),": Balances efficiency and performance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feature design is a trade-off"}),": Human knowledge vs. computational resources"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:'Input feature design is the bridge connecting "human-understood Go" with "machine-processable numbers."'}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:['Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." ',(0,r.jsx)(n.em,{children:"Nature"}),", 529, 484-489."]}),"\n",(0,r.jsxs)(n.li,{children:['Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." ',(0,r.jsx)(n.em,{children:"Nature"}),", 551, 354-359."]}),"\n",(0,r.jsxs)(n.li,{children:['Wu, D. (2019). "Accelerating Self-Play Learning in Go." ',(0,r.jsx)(n.em,{children:"arXiv:1902.10565"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["KataGo Documentation: ",(0,r.jsx)(n.a,{href:"https://github.com/lightvector/KataGo",children:"https://github.com/lightvector/KataGo"})]}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},30416(e,n,s){s.d(n,{R:()=>l,x:()=>a});var i=s(59471);const r={},t=i.createContext(r);function l(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);