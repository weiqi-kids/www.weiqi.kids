"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[9138],{90621(e,n,i){i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"for-engineers/how-it-works/alphago-explained/value-network","title":"Value Network Explained","description":"Deep dive into AlphaGo\'s Value Network architecture, training challenges, and its critical role in MCTS","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/for-engineers/how-it-works/alphago-explained/08-value-network.mdx","sourceDirName":"for-engineers/how-it-works/alphago-explained","slug":"/for-engineers/how-it-works/alphago-explained/value-network","permalink":"/en/docs/for-engineers/how-it-works/alphago-explained/value-network","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/08-value-network.mdx","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9,"title":"Value Network Explained","description":"Deep dive into AlphaGo\'s Value Network architecture, training challenges, and its critical role in MCTS"},"sidebar":"tutorialSidebar","previous":{"title":"Policy Network Deep Dive","permalink":"/en/docs/for-engineers/how-it-works/alphago-explained/policy-network"},"next":{"title":"Input Feature Design","permalink":"/en/docs/for-engineers/how-it-works/alphago-explained/input-features"}}');var r=i(62615),l=i(30416);const s={sidebar_position:9,title:"Value Network Explained",description:"Deep dive into AlphaGo's Value Network architecture, training challenges, and its critical role in MCTS"},a="Value Network Explained",o={},d=[{value:"What is the Value Network?",id:"what-is-the-value-network",level:2},{value:"Core Function",id:"core-function",level:3},{value:"Output Interpretation",id:"output-interpretation",level:3},{value:"Why a Single Value?",id:"why-a-single-value",level:3},{value:"Comparing Different Options",id:"comparing-different-options",level:4},{value:"Replacing Extensive Simulations",id:"replacing-extensive-simulations",level:4},{value:"Network Architecture",id:"network-architecture",level:2},{value:"Similarity to Policy Network",id:"similarity-to-policy-network",level:3},{value:"Input Layer",id:"input-layer",level:3},{value:"Convolutional Layers",id:"convolutional-layers",level:3},{value:"Output Layer Differences",id:"output-layer-differences",level:3},{value:"Policy Network Output",id:"policy-network-output",level:4},{value:"Value Network Output",id:"value-network-output",level:4},{value:"Tanh Activation Function",id:"tanh-activation-function",level:3},{value:"Why Tanh Instead of Sigmoid?",id:"why-tanh-instead-of-sigmoid",level:4},{value:"Complete Architecture Diagram",id:"complete-architecture-diagram",level:3},{value:"Parameter Count",id:"parameter-count",level:3},{value:"Training Challenges",id:"training-challenges",level:2},{value:"Overfitting Problem",id:"overfitting-problem",level:3},{value:"What is Overfitting?",id:"what-is-overfitting",level:4},{value:"Why is Value Network Prone to Overfitting?",id:"why-is-value-network-prone-to-overfitting",level:4},{value:"Solution: Self-Play Data",id:"solution-self-play-data",level:3},{value:"Why Does This Solve Overfitting?",id:"why-does-this-solve-overfitting",level:4},{value:"Training Data Generation",id:"training-data-generation",level:3},{value:"Training Objective and Method",id:"training-objective-and-method",level:2},{value:"Mean Squared Error Loss",id:"mean-squared-error-loss",level:3},{value:"Why MSE Instead of Cross-Entropy?",id:"why-mse-instead-of-cross-entropy",level:4},{value:"Training Process",id:"training-process",level:3},{value:"Accuracy Analysis",id:"accuracy-analysis",level:2},{value:"Comparison with Random Rollouts",id:"comparison-with-random-rollouts",level:3},{value:"Accuracy at Different Stages",id:"accuracy-at-different-stages",level:3},{value:"Output Distribution",id:"output-distribution",level:3},{value:"Uncertain Positions",id:"uncertain-positions",level:3},{value:"Role in MCTS",id:"role-in-mcts",level:2},{value:"Leaf Node Evaluation",id:"leaf-node-evaluation",level:3},{value:"Why Combine Them?",id:"why-combine-them",level:4},{value:"AlphaGo Zero&#39;s Simplification",id:"alphago-zeros-simplification",level:3},{value:"Backpropagation Updates",id:"backpropagation-updates",level:3},{value:"Visual Analysis",id:"visual-analysis",level:2},{value:"Value Surface",id:"value-surface",level:3},{value:"Evolution During Training",id:"evolution-during-training",level:3},{value:"Identifying Difficult Positions",id:"identifying-difficult-positions",level:3},{value:"Implementation Notes",id:"implementation-notes",level:2},{value:"PyTorch Implementation",id:"pytorch-implementation",level:3},{value:"Training Loop",id:"training-loop",level:3},{value:"Tips to Avoid Overfitting",id:"tips-to-avoid-overfitting",level:3},{value:"Collaboration with Policy Network",id:"collaboration-with-policy-network",level:2},{value:"Complementary Relationship",id:"complementary-relationship",level:3},{value:"Unified Dual-Head Network",id:"unified-dual-head-network",level:3},{value:"Animation Reference",id:"animation-reference",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"References",id:"references",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"value-network-explained",children:"Value Network Explained"})}),"\n",(0,r.jsx)(n.p,{children:'If the Policy Network tells AlphaGo "where to play next," the Value Network answers a more fundamental question:'}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Will I win this game?"'})}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"what-is-the-value-network",children:"What is the Value Network?"}),"\n",(0,r.jsx)(n.h3,{id:"core-function",children:"Core Function"}),"\n",(0,r.jsx)(n.p,{children:"The Value Network is a deep convolutional neural network whose task is:"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Given the current board state, predict the final win probability"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Mathematically expressed:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"v = f_\u03b8(s)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"s"}),": Current board state"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"f_\u03b8"}),": Value Network (\u03b8 represents network parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"v"}),": A value between -1 and +1"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"output-interpretation",children:"Output Interpretation"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Output Value"}),(0,r.jsx)(n.th,{children:"Meaning"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"+1"}),(0,r.jsx)(n.td,{children:"Current player wins for certain"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"+0.5"}),(0,r.jsx)(n.td,{children:"Current player has ~75% win rate"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"0"}),(0,r.jsx)(n.td,{children:"Equal chances for both sides"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"-0.5"}),(0,r.jsx)(n.td,{children:"Current player has ~25% win rate"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"-1"}),(0,r.jsx)(n.td,{children:"Current player loses for certain"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"why-a-single-value",children:"Why a Single Value?"}),"\n",(0,r.jsx)(n.h4,{id:"comparing-different-options",children:"Comparing Different Options"}),"\n",(0,r.jsx)(n.p,{children:"When playing Go, we often need to choose among multiple options. The Value Network makes this comparison simple:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Option A position value: 0.3\nOption B position value: 0.5\nOption C position value: 0.2\n\n\u2192 Choose B (highest value)\n"})}),"\n",(0,r.jsx)(n.p,{children:'Without a single value, how would we compare "capturing an opponent\'s group" versus "surrounding a large territory"?'}),"\n",(0,r.jsx)(n.h4,{id:"replacing-extensive-simulations",children:"Replacing Extensive Simulations"}),"\n",(0,r.jsxs)(n.p,{children:["In traditional Monte Carlo Tree Search, evaluating a position requires ",(0,r.jsx)(n.strong,{children:"random rollouts"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Start from current position"}),"\n",(0,r.jsx)(n.li,{children:"Both sides play randomly until game ends"}),"\n",(0,r.jsx)(n.li,{children:"Record win/loss"}),"\n",(0,r.jsx)(n.li,{children:"Repeat thousands of times, calculate win rate"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["This is very slow. The Value Network can provide an evaluation with ",(0,r.jsx)(n.strong,{children:"a single forward pass"}),", orders of magnitude faster."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Method"}),(0,r.jsx)(n.th,{children:"Evaluation Time"}),(0,r.jsx)(n.th,{children:"Accuracy"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1000 random rollouts"}),(0,r.jsx)(n.td,{children:"~2000 ms"}),(0,r.jsx)(n.td,{children:"Lower"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"15000 random rollouts"}),(0,r.jsx)(n.td,{children:"~30000 ms"}),(0,r.jsx)(n.td,{children:"Medium"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Value Network"}),(0,r.jsx)(n.td,{children:"~3 ms"}),(0,r.jsx)(n.td,{children:"High (equivalent to 15000 rollouts)"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"network-architecture",children:"Network Architecture"}),"\n",(0,r.jsx)(n.h3,{id:"similarity-to-policy-network",children:"Similarity to Policy Network"}),"\n",(0,r.jsx)(n.p,{children:"The Value Network architecture is very similar to the Policy Network, both being deep convolutional neural networks:"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart LR\n    Input["Input Layer<br/>19\xd719\xd748"]\n    Conv["Conv Layers \xd712<br/>19\xd719\xd7192"]\n    FC["Fully Connected<br/>256-dim"]\n    Output["Output<br/>Single Value"]\n\n    Input --\x3e Conv --\x3e FC --\x3e Output'}),"\n",(0,r.jsx)(n.h3,{id:"input-layer",children:"Input Layer"}),"\n",(0,r.jsxs)(n.p,{children:["Same as Policy Network, input is a ",(0,r.jsx)(n.strong,{children:"19\xd719\xd749"})," feature tensor:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"19\xd719"}),": Board size"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"49"}),": 48 feature planes + 1 plane indicating whose turn"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The extra plane is important: Value Network needs to know whose turn it is, since the same position has opposite values for Black and White."}),"\n",(0,r.jsx)(n.h3,{id:"convolutional-layers",children:"Convolutional Layers"}),"\n",(0,r.jsx)(n.p,{children:"Same as Policy Network:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"12 convolutional layers"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"192 filters"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"3\xd73 kernels"})," (first layer 5\xd75)"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"ReLU activation function"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"output-layer-differences",children:"Output Layer Differences"}),"\n",(0,r.jsx)(n.p,{children:"This is the key difference between Value Network and Policy Network:"}),"\n",(0,r.jsx)(n.h4,{id:"policy-network-output",children:"Policy Network Output"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"19\xd719\xd7192 \u2192 1\xd71 conv \u2192 19\xd719\xd71 \u2192 Flatten \u2192 361-dim \u2192 Softmax \u2192 Probability distribution\n"})}),"\n",(0,r.jsx)(n.h4,{id:"value-network-output",children:"Value Network Output"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"19\xd719\xd7192 \u2192 1\xd71 conv \u2192 19\xd719\xd71 \u2192 Flatten \u2192 361-dim \u2192 FC 256 \u2192 ReLU \u2192 FC 1 \u2192 Tanh \u2192 Single value\n"})}),"\n",(0,r.jsx)(n.h3,{id:"tanh-activation-function",children:"Tanh Activation Function"}),"\n",(0,r.jsxs)(n.p,{children:["The Value Network's final layer uses ",(0,r.jsx)(n.strong,{children:"Tanh"})," (hyperbolic tangent) function:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Tanh's output range is ",(0,r.jsx)(n.strong,{children:"(-1, +1)"}),", exactly corresponding to win/loss."]}),"\n",(0,r.jsx)(n.h4,{id:"why-tanh-instead-of-sigmoid",children:"Why Tanh Instead of Sigmoid?"}),"\n",(0,r.jsx)(n.p,{children:"Sigmoid's output range is (0, 1), which could also represent win rate. But Tanh has several advantages:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Symmetry"}),": Centered at 0, output can be positive or negative"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Better gradients"}),": Gradient is close to 1 near 0"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Clear semantics"}),": Positive means win, negative means loss, zero is draw"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"complete-architecture-diagram",children:"Complete Architecture Diagram"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart TB\n    Input["Input: 19\xd719\xd749"]\n    Conv1["Conv 5\xd75, 192 filters"]\n    ReLU1["ReLU"]\n    Conv2["Conv 3\xd73, 192 filters (\xd711)"]\n    ReLU2["ReLU"]\n    Conv3["Conv 1\xd71, 1 filter"]\n    Flat["Flatten (361-dim)"]\n    FC1["Fully Connected (256-dim)"]\n    ReLU3["ReLU"]\n    FC2["Fully Connected (1-dim)"]\n    Tanh["Tanh"]\n    Output["Output: [-1, +1]"]\n\n    Input --\x3e Conv1 --\x3e ReLU1 --\x3e Conv2 --\x3e ReLU2 --\x3e Conv3 --\x3e Flat --\x3e FC1 --\x3e ReLU3 --\x3e FC2 --\x3e Tanh --\x3e Output'}),"\n",(0,r.jsx)(n.h3,{id:"parameter-count",children:"Parameter Count"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Layer"}),(0,r.jsx)(n.th,{children:"Calculation"}),(0,r.jsx)(n.th,{children:"Parameter Count"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Conv Layers"}),(0,r.jsx)(n.td,{children:"Same as Policy Network"}),(0,r.jsx)(n.td,{children:"~3.9M"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FC Layer 1"}),(0,r.jsx)(n.td,{children:"361\xd7256 + 256"}),(0,r.jsx)(n.td,{children:"92,672"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FC Layer 2"}),(0,r.jsx)(n.td,{children:"256\xd71 + 1"}),(0,r.jsx)(n.td,{children:"257"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Total"})}),(0,r.jsx)(n.td,{}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"~4.0M"})})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"About 4 million parameters, slightly more than Policy Network."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"training-challenges",children:"Training Challenges"}),"\n",(0,r.jsx)(n.h3,{id:"overfitting-problem",children:"Overfitting Problem"}),"\n",(0,r.jsxs)(n.p,{children:["Training the Value Network is much harder than the Policy Network. The main issue is ",(0,r.jsx)(n.strong,{children:"overfitting"}),"."]}),"\n",(0,r.jsx)(n.h4,{id:"what-is-overfitting",children:"What is Overfitting?"}),"\n",(0,r.jsx)(n.p,{children:'Overfitting means the model "memorizes" the training data rather than learning to generalize. Symptoms include:'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Great performance on training set"}),"\n",(0,r.jsx)(n.li,{children:"Poor performance on test set"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"why-is-value-network-prone-to-overfitting",children:"Why is Value Network Prone to Overfitting?"}),"\n",(0,r.jsx)(n.p,{children:"Consider data from one game:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Position 1 \u2192 Position 2 \u2192 Position 3 \u2192 ... \u2192 Position 200 \u2192 Result: Black wins\n"})}),"\n",(0,r.jsx)(n.p,{children:"If we train directly with this data:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"These 200 positions are highly correlated"}),"\n",(0,r.jsx)(n.li,{children:"They come from the same game, with the same outcome"}),"\n",(0,r.jsx)(n.li,{children:'The model might learn to "recognize" this game rather than understand positions'}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"DeepMind discovered: if you train Policy and Value Networks with the same human game records, the Value Network severely overfits."}),"\n",(0,r.jsx)(n.h3,{id:"solution-self-play-data",children:"Solution: Self-Play Data"}),"\n",(0,r.jsxs)(n.p,{children:["DeepMind's solution was to generate new training data through ",(0,r.jsx)(n.strong,{children:"self-play"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"1. Use trained RL Policy Network for self-play\n2. Take only one position from each game (avoiding correlation)\n3. Label this position with the game's final result\n4. Generate 30 million such samples\n"})}),"\n",(0,r.jsx)(n.h4,{id:"why-does-this-solve-overfitting",children:"Why Does This Solve Overfitting?"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Large data volume"}),": 30 million independent positions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"No correlation"}),": Only one position per game"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Different distribution"}),": Self-play positions differ from human games"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"training-data-generation",children:"Training Data Generation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Pseudocode\ntraining_data = []\n\nfor game_id in range(30_000_000):\n    # Self-play one game\n    states, result = self_play(rl_policy_network)\n\n    # Randomly select one position\n    random_index = random.randint(0, len(states) - 1)\n    state = states[random_index]\n\n    # Record position and result\n    training_data.append((state, result))\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"training-objective-and-method",children:"Training Objective and Method"}),"\n",(0,r.jsx)(n.h3,{id:"mean-squared-error-loss",children:"Mean Squared Error Loss"}),"\n",(0,r.jsxs)(n.p,{children:["The Value Network uses ",(0,r.jsx)(n.strong,{children:"Mean Squared Error (MSE)"})," as the loss function:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"L(\u03b8) = (1/n) \xd7 \u03a3 (v_\u03b8(s) - z)\xb2\n"})}),"\n",(0,r.jsx)(n.p,{children:"Where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"v_\u03b8(s)"}),": Model's predicted value"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"z"}),": Actual result (+1 or -1)"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"why-mse-instead-of-cross-entropy",children:"Why MSE Instead of Cross-Entropy?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cross-entropy"})," is suitable for classification problems (discrete labels)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"MSE"})," is suitable for regression problems (continuous values)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Although results are only +1 or -1, the model predicts continuous values (any number between -1 and +1). MSE trains the model to predict values close to +1 or -1."}),"\n",(0,r.jsx)(n.h3,{id:"training-process",children:"Training Process"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Pseudocode\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        states, outcomes = batch\n\n        # Forward pass\n        values = network(states)  # (batch, 1)\n\n        # Calculate loss (MSE)\n        loss = mse_loss(values, outcomes)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n"})}),"\n",(0,r.jsx)(n.p,{children:"Training details:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Optimizer"}),": SGD with momentum"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Learning rate"}),": 0.003"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Batch size"}),": 32"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Training time"}),": About 1 week (50 GPUs)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"accuracy-analysis",children:"Accuracy Analysis"}),"\n",(0,r.jsx)(n.h3,{id:"comparison-with-random-rollouts",children:"Comparison with Random Rollouts"}),"\n",(0,r.jsx)(n.p,{children:"DeepMind conducted detailed comparisons in their paper:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Evaluation Method"}),(0,r.jsx)(n.th,{children:"Prediction Error"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1000 random rollouts"}),(0,r.jsx)(n.td,{children:"Higher"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"15000 random rollouts"}),(0,r.jsx)(n.td,{children:"Medium"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Value Network"}),(0,r.jsx)(n.td,{children:"Comparable to 15000 rollouts"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"This means one Value Network evaluation \u2248 15000 random rollouts, but ~1000\xd7 faster."}),"\n",(0,r.jsx)(n.h3,{id:"accuracy-at-different-stages",children:"Accuracy at Different Stages"}),"\n",(0,r.jsx)(n.p,{children:"Value Network accuracy depends on game progress:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Stage"}),(0,r.jsx)(n.th,{children:"Remaining Moves"}),(0,r.jsx)(n.th,{children:"Prediction Difficulty"}),(0,r.jsx)(n.th,{children:"Accuracy"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Opening"}),(0,r.jsx)(n.td,{children:"~300"}),(0,r.jsx)(n.td,{children:"Very hard"}),(0,r.jsx)(n.td,{children:"Lower"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Middlegame"}),(0,r.jsx)(n.td,{children:"~150"}),(0,r.jsx)(n.td,{children:"Difficult"}),(0,r.jsx)(n.td,{children:"Medium"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Endgame"}),(0,r.jsx)(n.td,{children:"~50"}),(0,r.jsx)(n.td,{children:"Easier"}),(0,r.jsx)(n.td,{children:"Higher"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Final"}),(0,r.jsx)(n.td,{children:"~10"}),(0,r.jsx)(n.td,{children:"Simple"}),(0,r.jsx)(n.td,{children:"Very high"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"This is intuitively reasonable: the closer to game end, the more certain the outcome."}),"\n",(0,r.jsx)(n.h3,{id:"output-distribution",children:"Output Distribution"}),"\n",(0,r.jsx)(n.p,{children:"A well-trained Value Network's output distribution:"}),"\n",(0,r.jsx)(n.mermaid,{value:'xychart-beta\n    title "Output Distribution"\n    x-axis "Output Value" [-1, -0.5, 0, 0.5, 1]\n    y-axis "Frequency"\n    bar [0.4, 0.15, 0.05, 0.15, 0.4]'}),"\n",(0,r.jsx)(n.p,{children:"Most outputs concentrated near -1 and +1 (because most positions have clear win/loss tendency)."}),"\n",(0,r.jsx)(n.h3,{id:"uncertain-positions",children:"Uncertain Positions"}),"\n",(0,r.jsx)(n.p,{children:"When Value Network output is close to 0, it indicates a very complex position where the outcome is uncertain. These positions are typically:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"In the middle of large battles"}),"\n",(0,r.jsx)(n.li,{children:"Both sides evenly matched"}),"\n",(0,r.jsx)(n.li,{children:"Multiple possible variations exist"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"In MCTS, these nodes receive more search resources (due to high uncertainty)."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"role-in-mcts",children:"Role in MCTS"}),"\n",(0,r.jsx)(n.h3,{id:"leaf-node-evaluation",children:"Leaf Node Evaluation"}),"\n",(0,r.jsxs)(n.p,{children:["The Value Network plays a critical role in MCTS's ",(0,r.jsx)(n.strong,{children:"Evaluation"})," phase:"]}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart TB\n    Root["Root Node<br/>(current position)"]\n    A["A"]\n    B["B"]\n    A1["A1"]\n    A2["A2"]\n    B1["B1"]\n    B2["B2"]\n    E1["Eval"]\n    E2["Eval"]\n    E3["Eval"]\n    E4["Eval"]\n\n    Root --\x3e A\n    Root --\x3e B\n    A --\x3e A1\n    A --\x3e A2\n    B --\x3e B1\n    B --\x3e B2\n    A1 --\x3e E1\n    A2 --\x3e E2\n    B1 --\x3e E3\n    B2 --\x3e E4'}),"\n",(0,r.jsx)(n.p,{children:"When MCTS reaches a leaf node, it needs to evaluate this position's value. Two methods:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Random Rollout"}),": Play randomly from leaf node to game end"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Value Network Evaluation"}),": Directly predict with neural network"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo combines both:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"V(leaf) = (1-\u03bb) \xd7 V_network(leaf) + \u03bb \xd7 V_rollout(leaf)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Where \u03bb = 0.5, meaning equal weight for both."}),"\n",(0,r.jsx)(n.h4,{id:"why-combine-them",children:"Why Combine Them?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Value Network"})," is more accurate but may have systematic bias"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Random rollouts"})," are less accurate but provide independent estimates"]}),"\n",(0,r.jsx)(n.li,{children:"Combining both complements each other"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"alphago-zeros-simplification",children:"AlphaGo Zero's Simplification"}),"\n",(0,r.jsx)(n.p,{children:"Later, AlphaGo Zero completely removed random rollouts:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"V(leaf) = V_network(leaf)\n"})}),"\n",(0,r.jsx)(n.p,{children:'This greatly simplified the system while achieving stronger play. This proved the Value Network is reliable enough without the "insurance" of random rollouts.'}),"\n",(0,r.jsx)(n.h3,{id:"backpropagation-updates",children:"Backpropagation Updates"}),"\n",(0,r.jsx)(n.p,{children:"After evaluating a leaf node, this value propagates back along the path:"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart BT\n    Leaf["v3 = V(leaf) = 0.6"]\n    A2["A2\'s Q value updates"]\n    A["A\'s Q value updates"]\n    Root["Root node statistics update"]\n\n    Leaf --\x3e A2 --\x3e A --\x3e Root'}),"\n",(0,r.jsx)(n.p,{children:"Each node maintains a Q value that is the average of all leaf evaluations passing through it:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Q(s, a) = (1/N(s,a)) \xd7 \u03a3 V(leaf)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"visual-analysis",children:"Visual Analysis"}),"\n",(0,r.jsx)(n.h3,{id:"value-surface",children:"Value Surface"}),"\n",(0,r.jsx)(n.p,{children:'Imagine a simplified 3\xd73 board. What the Value Network learns is a "value surface":'}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{style:{textAlign:"center"}}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"White 1"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"White 2"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"White 3"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{style:{textAlign:"center"},children:(0,r.jsx)(n.strong,{children:"Black 1"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"+0.3"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"-0.1"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"+0.2"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{style:{textAlign:"center"},children:(0,r.jsx)(n.strong,{children:"Black 2"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"-0.2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"+0.5"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"-0.3"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{style:{textAlign:"center"},children:(0,r.jsx)(n.strong,{children:"Black 3"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"+0.1"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"-0.2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"+0.4"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"This surface tells us the value of each position combination. Positive values favor Black, negative values favor White."}),"\n",(0,r.jsx)(n.h3,{id:"evolution-during-training",children:"Evolution During Training"}),"\n",(0,r.jsx)(n.p,{children:"As training progresses, Value Network predictions become more accurate:"}),"\n",(0,r.jsx)(n.mermaid,{value:'xychart-beta\n    title "Prediction Error During Training"\n    x-axis "Training Steps" [0, "100K", "500K", "1M"]\n    y-axis "Error" 0 --\x3e 1\n    line [1.0, 0.5, 0.15, 0.1]'}),"\n",(0,r.jsx)(n.p,{children:"Error drops rapidly then stabilizes."}),"\n",(0,r.jsx)(n.h3,{id:"identifying-difficult-positions",children:"Identifying Difficult Positions"}),"\n",(0,r.jsx)(n.p,{children:"Value Network can help identify difficult positions:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Output"}),(0,r.jsx)(n.th,{children:"Meaning"}),(0,r.jsx)(n.th,{children:"Strategy"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Near +1"}),(0,r.jsx)(n.td,{children:"Big advantage"}),(0,r.jsx)(n.td,{children:"Play conservatively"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Near -1"}),(0,r.jsx)(n.td,{children:"Big disadvantage"}),(0,r.jsx)(n.td,{children:"Look for comeback opportunities"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Near 0"}),(0,r.jsx)(n.td,{children:"Complex position"}),(0,r.jsx)(n.td,{children:"Need deep calculation"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo invests more thinking time in positions near 0."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"implementation-notes",children:"Implementation Notes"}),"\n",(0,r.jsx)(n.h3,{id:"pytorch-implementation",children:"PyTorch Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ValueNetwork(nn.Module):\n    def __init__(self, input_channels=49, num_filters=192, num_layers=12):\n        super().__init__()\n\n        # First convolutional layer (5\xd75)\n        self.conv1 = nn.Conv2d(input_channels, num_filters,\n                               kernel_size=5, padding=2)\n\n        # Middle convolutional layers (3\xd73)\xd711\n        self.conv_layers = nn.ModuleList([\n            nn.Conv2d(num_filters, num_filters,\n                     kernel_size=3, padding=1)\n            for _ in range(num_layers - 1)\n        ])\n\n        # Output convolutional layer\n        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(361, 256)\n        self.fc2 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        # x: (batch, 49, 19, 19)\n\n        # Convolutional layers\n        x = F.relu(self.conv1(x))\n        for conv in self.conv_layers:\n            x = F.relu(conv(x))\n        x = self.conv_out(x)\n\n        # Flatten\n        x = x.view(x.size(0), -1)  # (batch, 361)\n\n        # Fully connected layers\n        x = F.relu(self.fc1(x))\n        x = torch.tanh(self.fc2(x))\n\n        return x.squeeze(-1)  # (batch,)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"training-loop",children:"Training Loop"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def train_value_network(model, optimizer, states, outcomes):\n    """\n    states: (batch, 49, 19, 19) - Board features\n    outcomes: (batch,) - Game results (+1 or -1)\n    """\n    # Forward pass\n    values = model(states)  # (batch,)\n\n    # MSE loss\n    loss = F.mse_loss(values, outcomes)\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Calculate accuracy (correct win/loss prediction)\n    predictions = (values > 0).float() * 2 - 1  # Convert to +1/-1\n    accuracy = (predictions == outcomes).float().mean()\n\n    return loss.item(), accuracy.item()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"tips-to-avoid-overfitting",children:"Tips to Avoid Overfitting"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# 1. Data augmentation (8-fold symmetry)\ndef augment(state, outcome):\n    augmented = []\n    for rotation in [0, 90, 180, 270]:\n        s = rotate(state, rotation)\n        augmented.append((s, outcome))\n        augmented.append((flip(s), outcome))\n    return augmented\n\n# 2. Dropout\nclass ValueNetworkWithDropout(ValueNetwork):\n    def __init__(self, *args, dropout_rate=0.5, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        # ... conv layers ...\n        x = self.dropout(x)  # Dropout before FC layers\n        # ... FC layers ...\n\n# 3. Early Stopping\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(max_epochs):\n    train_loss = train_one_epoch()\n    val_loss = evaluate()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        save_model()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping!\")\n            break\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"collaboration-with-policy-network",children:"Collaboration with Policy Network"}),"\n",(0,r.jsx)(n.h3,{id:"complementary-relationship",children:"Complementary Relationship"}),"\n",(0,r.jsx)(n.p,{children:"Policy Network and Value Network complement each other in AlphaGo:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Network"}),(0,r.jsx)(n.th,{children:"Question Answered"}),(0,r.jsx)(n.th,{children:"Output"}),(0,r.jsx)(n.th,{children:"MCTS Role"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Policy"}),(0,r.jsx)(n.td,{children:"Where to play next?"}),(0,r.jsx)(n.td,{children:"Probability distribution"}),(0,r.jsx)(n.td,{children:"Guide search direction"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Value"}),(0,r.jsx)(n.td,{children:"Will I win?"}),(0,r.jsx)(n.td,{children:"Single value"}),(0,r.jsx)(n.td,{children:"Evaluate leaf nodes"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"unified-dual-head-network",children:"Unified Dual-Head Network"}),"\n",(0,r.jsxs)(n.p,{children:["In AlphaGo Zero, these two networks were merged into a ",(0,r.jsx)(n.strong,{children:"dual-head network"}),":"]}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart TB\n    Shared["Shared Feature Extraction Layers"]\n    Policy["Policy Head"]\n    Value["Value Head"]\n    P_Out["361 probabilities"]\n    V_Out["Single value"]\n\n    Shared --\x3e Policy --\x3e P_Out\n    Shared --\x3e Value --\x3e V_Out'}),"\n",(0,r.jsx)(n.p,{children:"Advantages of this design:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Parameter sharing"}),": Reduces computation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feature sharing"}),": Policy and Value use same features"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"More stable training"}),": Two objectives regularize each other"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["See ",(0,r.jsx)(n.a,{href:"../dual-head-resnet",children:"Dual-Head Network and Residual Network"})," for details."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"animation-reference",children:"Animation Reference"}),"\n",(0,r.jsx)(n.p,{children:"Core concepts covered in this article with animation numbers:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Number"}),(0,r.jsx)(n.th,{children:"Concept"}),(0,r.jsx)(n.th,{children:"Physics/Math Correspondence"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Animation E2"}),(0,r.jsx)(n.td,{children:"Value Network"}),(0,r.jsx)(n.td,{children:"Potential energy surface"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Animation D4"}),(0,r.jsx)(n.td,{children:"Value function"}),(0,r.jsx)(n.td,{children:"Expected return"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Animation C6"}),(0,r.jsx)(n.td,{children:"Leaf node evaluation"}),(0,r.jsx)(n.td,{children:"Function approximation"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Animation H3"}),(0,r.jsx)(n.td,{children:"Temporal difference"}),(0,r.jsx)(n.td,{children:"Bootstrap learning"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Previous"}),": ",(0,r.jsx)(n.a,{href:"../policy-network",children:"Policy Network Explained"})," - How the policy network selects moves"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Next"}),": ",(0,r.jsx)(n.a,{href:"../input-features",children:"Input Feature Design"})," - 48 feature planes explained"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Advanced Topic"}),": ",(0,r.jsx)(n.a,{href:"../mcts-neural-combo",children:"MCTS and Neural Network Integration"})," - Complete search process"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Value Network predicts win rate"}),": Outputs a single value between -1 and +1"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tanh output"}),": Ensures output is in the correct range"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"MSE loss"}),": Drives predictions toward actual outcomes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Overfitting challenge"}),": Requires self-play data to avoid"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Replaces random rollouts"}),": One evaluation \u2248 15000 rollouts"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The Value Network is AlphaGo's \"judgment\" - it lets the AI evaluate any position's quality without exhaustively exploring all possibilities."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:['Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." ',(0,r.jsx)(n.em,{children:"Nature"}),", 529, 484-489."]}),"\n",(0,r.jsxs)(n.li,{children:['Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." ',(0,r.jsx)(n.em,{children:"Nature"}),", 551, 354-359."]}),"\n",(0,r.jsxs)(n.li,{children:["Sutton, R. S., & Barto, A. G. (2018). ",(0,r.jsx)(n.em,{children:"Reinforcement Learning: An Introduction"}),". MIT Press."]}),"\n",(0,r.jsxs)(n.li,{children:['Tesauro, G. (1995). "Temporal difference learning and TD-Gammon." ',(0,r.jsx)(n.em,{children:"Communications of the ACM"}),", 38(3), 58-68."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},30416(e,n,i){i.d(n,{R:()=>s,x:()=>a});var t=i(59471);const r={},l=t.createContext(r);function s(e){const n=t.useContext(l);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(l.Provider,{value:n},e.children)}}}]);