"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[2751],{71488(e,n,i){i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>c,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"for-engineers/how-it-works/alphago-explained/alphago-zero","title":"AlphaGo Zero Overview","description":"Starting from scratch, completely self-taught - how AlphaGo Zero surpassed all previous versions without any human game records","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/for-engineers/how-it-works/alphago-explained/16-alphago-zero.mdx","sourceDirName":"for-engineers/how-it-works/alphago-explained","slug":"/for-engineers/how-it-works/alphago-explained/alphago-zero","permalink":"/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/16-alphago-zero.mdx","tags":[],"version":"current","sidebarPosition":17,"frontMatter":{"sidebar_position":17,"title":"AlphaGo Zero Overview","description":"Starting from scratch, completely self-taught - how AlphaGo Zero surpassed all previous versions without any human game records","keywords":["AlphaGo Zero","self-play","reinforcement learning","deep learning","Go AI","unsupervised learning"]},"sidebar":"tutorialSidebar","previous":{"title":"PUCT Formula Explained","permalink":"/en/docs/for-engineers/how-it-works/alphago-explained/puct-formula"},"next":{"title":"Dual-Head Network and Residual Network","permalink":"/en/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet"}}');var s=i(62615),l=i(30416);const t={sidebar_position:17,title:"AlphaGo Zero Overview",description:"Starting from scratch, completely self-taught - how AlphaGo Zero surpassed all previous versions without any human game records",keywords:["AlphaGo Zero","self-play","reinforcement learning","deep learning","Go AI","unsupervised learning"]},a="AlphaGo Zero Overview",o={},d=[{value:"Why No Human Games Needed?",id:"why-no-human-games-needed",level:2},{value:"Limitations of Human Games",id:"limitations-of-human-games",level:3},{value:"1. Human Games Have Upper Limits",id:"1-human-games-have-upper-limits",level:4},{value:"2. Supervised Learning Bottleneck",id:"2-supervised-learning-bottleneck",level:4},{value:"3. Data Collection Costs",id:"3-data-collection-costs",level:4},{value:"Zero&#39;s Breakthrough",id:"zeros-breakthrough",level:3},{value:"Comparison with Original AlphaGo: 100:0",id:"comparison-with-original-alphago-1000",level:2},{value:"Crushing Victory",id:"crushing-victory",level:3},{value:"Less Resources, Stronger Play",id:"less-resources-stronger-play",level:3},{value:"Why Is Zero Stronger?",id:"why-is-zero-stronger",level:3},{value:"1. Unbiased Learning",id:"1-unbiased-learning",level:4},{value:"2. Consistent Learning Objective",id:"2-consistent-learning-objective",level:4},{value:"3. Simpler Architecture",id:"3-simpler-architecture",level:4},{value:"Simplified Input Features: From 48 to 17",id:"simplified-input-features-from-48-to-17",level:2},{value:"Original AlphaGo&#39;s 48 Feature Planes",id:"original-alphagos-48-feature-planes",level:3},{value:"AlphaGo Zero&#39;s 17 Feature Planes",id:"alphago-zeros-17-feature-planes",level:3},{value:"Why Is Simplification Good?",id:"why-is-simplification-good",level:3},{value:"1. Let Network Discover Features",id:"1-let-network-discover-features",level:4},{value:"2. Better Generalizability",id:"2-better-generalizability",level:4},{value:"3. Reduce Human Error",id:"3-reduce-human-error",level:4},{value:"Single Network Architecture",id:"single-network-architecture",level:2},{value:"Original Dual-Network Design",id:"original-dual-network-design",level:3},{value:"Zero&#39;s Dual-Head Network",id:"zeros-dual-head-network",level:3},{value:"1. Parameter Efficiency",id:"1-parameter-efficiency",level:4},{value:"2. Feature Sharing",id:"2-feature-sharing",level:4},{value:"3. Training Stability",id:"3-training-stability",level:4},{value:"Power of Residual Networks",id:"power-of-residual-networks",level:3},{value:"Training Efficiency Improvement",id:"training-efficiency-improvement",level:2},{value:"Exponential Growth of Self-Play",id:"exponential-growth-of-self-play",level:3},{value:"Why So Fast?",id:"why-so-fast",level:3},{value:"1. Stronger Search Guidance",id:"1-stronger-search-guidance",level:4},{value:"2. Faster Self-Play",id:"2-faster-self-play",level:4},{value:"3. More Effective Learning",id:"3-more-effective-learning",level:4},{value:"Comparison with Human Learning",id:"comparison-with-human-learning",level:3},{value:"Generality: Chess, Shogi",id:"generality-chess-shogi",level:2},{value:"Birth of AlphaZero",id:"birth-of-alphazero",level:3},{value:"Significance of Generality",id:"significance-of-generality",level:3},{value:"Impact on Traditional AI",id:"impact-on-traditional-ai",level:3},{value:"AlphaGo Zero&#39;s Playing Style",id:"alphago-zeros-playing-style",level:2},{value:"Beyond Human Aesthetics",id:"beyond-human-aesthetics",level:3},{value:"Rediscovering Human Go Principles",id:"rediscovering-human-go-principles",level:3},{value:"Beyond Human Innovation",id:"beyond-human-innovation",level:3},{value:"Technical Details Summary",id:"technical-details-summary",level:2},{value:"Complete Comparison with Original AlphaGo",id:"complete-comparison-with-original-alphago",level:3},{value:"Core Algorithm",id:"core-algorithm",level:3},{value:"Insights for AI Research",id:"insights-for-ai-research",level:2},{value:"First-Principles Learning",id:"first-principles-learning",level:3},{value:"Power of Self-Play",id:"power-of-self-play",level:3},{value:"Importance of Simplification",id:"importance-of-simplification",level:3},{value:"Animation Reference",id:"animation-reference",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"References",id:"references",level:2}];function h(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"alphago-zero-overview",children:"AlphaGo Zero Overview"})}),"\n",(0,s.jsxs)(n.p,{children:["In October 2017, DeepMind announced a result that shocked the AI world: ",(0,s.jsx)(n.strong,{children:"AlphaGo Zero"}),", without using any human game records, starting from a completely random state, surpassed the original AlphaGo that defeated Lee Sedol in just three days, winning ",(0,s.jsx)(n.strong,{children:"100:0"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["This isn't just numerical progress. It represents a completely new paradigm: ",(0,s.jsx)(n.strong,{children:"AI doesn't need human knowledge; it can discover everything from scratch."})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"why-no-human-games-needed",children:"Why No Human Games Needed?"}),"\n",(0,s.jsx)(n.h3,{id:"limitations-of-human-games",children:"Limitations of Human Games"}),"\n",(0,s.jsx)(n.p,{children:"Original AlphaGo's training process had two phases:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Supervised Learning"}),": Train Policy Network with 30 million human games"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reinforcement Learning"}),": Further improve through self-play"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This approach has several fundamental problems:"}),"\n",(0,s.jsx)(n.h4,{id:"1-human-games-have-upper-limits",children:"1. Human Games Have Upper Limits"}),"\n",(0,s.jsx)(n.p,{children:"Human players have finite strength; game records contain human understanding but also human errors and biases. When AI learns from human games, it learns:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"What humans think are good moves (but not necessarily optimal)"}),"\n",(0,s.jsx)(n.li,{children:"Human thought patterns (but might limit innovation)"}),"\n",(0,s.jsx)(n.li,{children:"Human mistakes (learned as if they were correct)"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"2-supervised-learning-bottleneck",children:"2. Supervised Learning Bottleneck"}),"\n",(0,s.jsx)(n.p,{children:"Supervised learning's goal is to \"imitate humans\" - predict which move a human player would make. This means AI's capability ceiling is limited by human player capability."}),"\n",(0,s.jsx)(n.p,{children:"Like an apprentice who can only imitate the master, never surpassing them."}),"\n",(0,s.jsx)(n.h4,{id:"3-data-collection-costs",children:"3. Data Collection Costs"}),"\n",(0,s.jsx)(n.p,{children:'High-quality human game records take years to accumulate and only exist for games with long histories like Go. If we want to apply AI to new domains (like protein structure prediction), there simply are no "expert game records" available.'}),"\n",(0,s.jsx)(n.h3,{id:"zeros-breakthrough",children:"Zero's Breakthrough"}),"\n",(0,s.jsxs)(n.p,{children:["AlphaGo Zero completely skips the supervised learning phase, starting directly from ",(0,s.jsx)(n.strong,{children:"random initialization"})," for self-play. This solves all the above problems:"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Problem"}),(0,s.jsx)(n.th,{children:"Original AlphaGo"}),(0,s.jsx)(n.th,{children:"AlphaGo Zero"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Human knowledge ceiling"}),(0,s.jsx)(n.td,{children:"Limited by game quality"}),(0,s.jsx)(n.td,{children:"No such limitation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Learning objective"}),(0,s.jsx)(n.td,{children:"Imitate humans"}),(0,s.jsx)(n.td,{children:"Maximize win rate"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Data requirement"}),(0,s.jsx)(n.td,{children:"30 million games"}),(0,s.jsx)(n.td,{children:"0"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Generalizability"}),(0,s.jsx)(n.td,{children:"Go only"}),(0,s.jsx)(n.td,{children:"Can extend to other domains"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:'This is a fundamental paradigm shift: from "learning human knowledge" to "discovering knowledge from first principles."'}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"comparison-with-original-alphago-1000",children:"Comparison with Original AlphaGo: 100:0"}),"\n",(0,s.jsx)(n.h3,{id:"crushing-victory",children:"Crushing Victory"}),"\n",(0,s.jsx)(n.p,{children:"DeepMind had trained AlphaGo Zero play against various AlphaGo versions:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Opponent"}),(0,s.jsx)(n.th,{children:"AlphaGo Zero Record"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"AlphaGo Fan (defeated Fan Hui version)"}),(0,s.jsx)(n.td,{children:"100:0"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"AlphaGo Lee (defeated Lee Sedol version)"}),(0,s.jsx)(n.td,{children:"100:0"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"AlphaGo Master (60-game winning streak version)"}),(0,s.jsx)(n.td,{children:"89:11"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"100:0"})," - this means in 100 games, original AlphaGo couldn't win a single one."]}),"\n",(0,s.jsx)(n.h3,{id:"less-resources-stronger-play",children:"Less Resources, Stronger Play"}),"\n",(0,s.jsx)(n.p,{children:"Not only winning, AlphaGo Zero achieved stronger play with fewer resources:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Metric"}),(0,s.jsx)(n.th,{children:"AlphaGo Lee"}),(0,s.jsx)(n.th,{children:"AlphaGo Zero"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Training time"}),(0,s.jsx)(n.td,{children:"Months"}),(0,s.jsx)(n.td,{children:"40 days (3 days to surpass AlphaGo Lee)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Training games"}),(0,s.jsx)(n.td,{children:"30 million human games + self-play"}),(0,s.jsx)(n.td,{children:"4.9 million self-play games"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"TPUs (training)"}),(0,s.jsx)(n.td,{children:"50+"}),(0,s.jsx)(n.td,{children:"4"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"TPUs (inference)"}),(0,s.jsx)(n.td,{children:"48"}),(0,s.jsx)(n.td,{children:"4"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Input features"}),(0,s.jsx)(n.td,{children:"48 planes"}),(0,s.jsx)(n.td,{children:"17 planes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Neural network"}),(0,s.jsx)(n.td,{children:"SL + RL dual networks"}),(0,s.jsx)(n.td,{children:"Single dual-head network"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:["This is stunning efficiency improvement: ",(0,s.jsx)(n.strong,{children:"10\xd7 fewer resources, yet much stronger play."})]}),"\n",(0,s.jsx)(n.h3,{id:"why-is-zero-stronger",children:"Why Is Zero Stronger?"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero's superior strength can be understood from several angles:"}),"\n",(0,s.jsx)(n.h4,{id:"1-unbiased-learning",children:"1. Unbiased Learning"}),"\n",(0,s.jsx)(n.p,{children:"Original AlphaGo learned from human games, inheriting human biases. For example, human players might overvalue certain joseki, or have wrong evaluations of some positions."}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero has no such baggage. It starts from blank slate, learning what's good only from win/loss results. This lets it discover moves humans never thought of."}),"\n",(0,s.jsx)(n.h4,{id:"2-consistent-learning-objective",children:"2. Consistent Learning Objective"}),"\n",(0,s.jsx)(n.p,{children:"Original AlphaGo's training had two different objectives:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Supervised learning: Maximize prediction accuracy of human moves"}),"\n",(0,s.jsx)(n.li,{children:"Reinforcement learning: Maximize win rate"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["These two objectives might conflict. AlphaGo Zero has only one objective: ",(0,s.jsx)(n.strong,{children:"maximize win rate"}),". This makes learning more consistent and effective."]}),"\n",(0,s.jsx)(n.h4,{id:"3-simpler-architecture",children:"3. Simpler Architecture"}),"\n",(0,s.jsxs)(n.p,{children:["Original AlphaGo used separate Policy Network and Value Network. AlphaGo Zero uses a single dual-head network (see ",(0,s.jsx)(n.a,{href:"../dual-head-resnet",children:"next article: Dual-Head Network and Residual Network"}),"), allowing feature representations to be shared, improving learning efficiency."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"simplified-input-features-from-48-to-17",children:"Simplified Input Features: From 48 to 17"}),"\n",(0,s.jsx)(n.h3,{id:"original-alphagos-48-feature-planes",children:"Original AlphaGo's 48 Feature Planes"}),"\n",(0,s.jsx)(n.p,{children:"Original AlphaGo's neural network input included 48 19\xd719 feature planes, encoding many human-designed features:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Category"}),(0,s.jsx)(n.th,{children:"Count"}),(0,s.jsx)(n.th,{children:"Content"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Stone positions"}),(0,s.jsx)(n.td,{children:"3"}),(0,s.jsx)(n.td,{children:"Black, white, empty"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Liberties"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"Strings with 1-8 liberties"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Captures"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"Can capture 1-8 stones"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Ko"}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"Ko position"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Edge distance"}),(0,s.jsx)(n.td,{children:"4"}),(0,s.jsx)(n.td,{children:"1st to 4th line"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Move legality"}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"Which positions can be played"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"History"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"Past 8 moves' positions"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Turn"}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"Black or White"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Other"}),(0,s.jsx)(n.td,{children:"14"}),(0,s.jsx)(n.td,{children:"Ladders, eyes, etc."})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"These 48 features were carefully designed by Go experts, containing extensive domain knowledge."}),"\n",(0,s.jsx)(n.h3,{id:"alphago-zeros-17-feature-planes",children:"AlphaGo Zero's 17 Feature Planes"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero dramatically simplified input to just 17 feature planes:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Plane Number"}),(0,s.jsx)(n.th,{children:"Content"}),(0,s.jsx)(n.th,{children:"Count"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1-8"}),(0,s.jsx)(n.td,{children:"Black positions (last 8 moves)"}),(0,s.jsx)(n.td,{children:"8"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"9-16"}),(0,s.jsx)(n.td,{children:"White positions (last 8 moves)"}),(0,s.jsx)(n.td,{children:"8"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"17"}),(0,s.jsx)(n.td,{children:"Current turn (all 1s or all 0s)"}),(0,s.jsx)(n.td,{children:"1"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"These 17 features only include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Current board state"}),": Black, white, or empty at each position"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"History information"}),": Board states of past 8 moves"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Turn information"}),": Whose turn to play"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:'No liberty counts, no ladder detection, no edge distance - all this "Go knowledge" is left for the neural network to learn itself.'}),"\n",(0,s.jsx)(n.h3,{id:"why-is-simplification-good",children:"Why Is Simplification Good?"}),"\n",(0,s.jsx)(n.h4,{id:"1-let-network-discover-features",children:"1. Let Network Discover Features"}),"\n",(0,s.jsx)(n.p,{children:"Complex handcrafted features might miss important information or encode wrong assumptions. Letting neural network learn from raw data, it might discover better feature representations."}),"\n",(0,s.jsx)(n.p,{children:"In fact, AlphaGo Zero learned all features humans designed (liberties, ladders, etc.), and also learned patterns humans weren't explicitly aware of."}),"\n",(0,s.jsx)(n.h4,{id:"2-better-generalizability",children:"2. Better Generalizability"}),"\n",(0,s.jsx)(n.p,{children:"Many of the 48 features are Go-specific (like ladders, edge distance). The 17 simplified features are universal - any board game can be encoded similarly."}),"\n",(0,s.jsxs)(n.p,{children:["This laid foundation for later ",(0,s.jsx)(n.strong,{children:"AlphaZero"})," (general game AI)."]}),"\n",(0,s.jsx)(n.h4,{id:"3-reduce-human-error",children:"3. Reduce Human Error"}),"\n",(0,s.jsx)(n.p,{children:"Handcrafted features may contain errors or incomplete definitions. Simplified input eliminates this possibility."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"single-network-architecture",children:"Single Network Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"original-dual-network-design",children:"Original Dual-Network Design"}),"\n",(0,s.jsx)(n.p,{children:"Original AlphaGo used two independent neural networks:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Policy Network:  Input \u2192 CNN \u2192 19\xd719 move probabilities\nValue Network:   Input \u2192 CNN \u2192 Win rate estimate (-1 to 1)\n"})}),"\n",(0,s.jsx)(n.p,{children:"These two networks:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Have different architectures (slightly different layers and channels)"}),"\n",(0,s.jsx)(n.li,{children:"Train independently (first Policy, then Value)"}),"\n",(0,s.jsx)(n.li,{children:"Share no parameters"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"zeros-dual-head-network",children:"Zero's Dual-Head Network"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero uses a single network with two output heads:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Input \u2192 ResNet shared backbone \u2192 Policy Head \u2192 19\xd719 move probabilities\n                              \u2192 Value Head  \u2192 Win rate estimate\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Two Heads share the same ResNet backbone (see ",(0,s.jsx)(n.a,{href:"../dual-head-resnet",children:"next article: Dual-Head Network and Residual Network"}),"), bringing several benefits:"]}),"\n",(0,s.jsx)(n.h4,{id:"1-parameter-efficiency",children:"1. Parameter Efficiency"}),"\n",(0,s.jsx)(n.p,{children:"Shared backbone means most parameters are used by both tasks. This reduces total parameters, lowering overfitting risk."}),"\n",(0,s.jsx)(n.h4,{id:"2-feature-sharing",children:"2. Feature Sharing"}),"\n",(0,s.jsx)(n.p,{children:'"Where to play" (Policy) and "who will win" (Value) need to understand similar board patterns. Shared backbone lets these features be learned and used by both tasks simultaneously.'}),"\n",(0,s.jsx)(n.h4,{id:"3-training-stability",children:"3. Training Stability"}),"\n",(0,s.jsx)(n.p,{children:"Joint training lets gradient signals come from two sources, providing richer supervision signal, making training more stable."}),"\n",(0,s.jsx)(n.h3,{id:"power-of-residual-networks",children:"Power of Residual Networks"}),"\n",(0,s.jsxs)(n.p,{children:["AlphaGo Zero's backbone uses ",(0,s.jsx)(n.strong,{children:"40-layer Residual Network (ResNet)"}),", much deeper than original AlphaGo's 13-layer CNN."]}),"\n",(0,s.jsx)(n.p,{children:"Residual connections (skip connections) enable effective training of deep networks, avoiding vanishing gradient problem. This was a breakthrough technology from 2015 ImageNet competition, successfully applied by AlphaGo Zero to Go."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"training-efficiency-improvement",children:"Training Efficiency Improvement"}),"\n",(0,s.jsx)(n.h3,{id:"exponential-growth-of-self-play",children:"Exponential Growth of Self-Play"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero's training process shows stunning efficiency:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Training Time"}),(0,s.jsx)(n.th,{children:"Elo Rating"}),(0,s.jsx)(n.th,{children:"Equivalent to"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"0 hours"}),(0,s.jsx)(n.td,{children:"0"}),(0,s.jsx)(n.td,{children:"Random moves"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"3 hours"}),(0,s.jsx)(n.td,{children:"~1000"}),(0,s.jsx)(n.td,{children:"Discovers basic rules"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"12 hours"}),(0,s.jsx)(n.td,{children:"~3000"}),(0,s.jsx)(n.td,{children:"Discovers joseki"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"36 hours"}),(0,s.jsx)(n.td,{children:"~4500"}),(0,s.jsx)(n.td,{children:"Surpasses Fan Hui version"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"60 hours"}),(0,s.jsx)(n.td,{children:"~5200"}),(0,s.jsx)(n.td,{children:"Surpasses Lee Sedol version"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"72 hours"}),(0,s.jsx)(n.td,{children:"~5400"}),(0,s.jsx)(n.td,{children:"Surpasses original AlphaGo"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"40 days"}),(0,s.jsx)(n.td,{children:"~5600"}),(0,s.jsx)(n.td,{children:"Strongest version"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Three days to surpass humans, three days to surpass AI that took months to train"})," - this is exponential efficiency improvement."]}),"\n",(0,s.jsx)(n.h3,{id:"why-so-fast",children:"Why So Fast?"}),"\n",(0,s.jsx)(n.h4,{id:"1-stronger-search-guidance",children:"1. Stronger Search Guidance"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero's MCTS is completely guided by neural network, no longer using fast rollout policy. This makes search more efficient and accurate."}),"\n",(0,s.jsx)(n.h4,{id:"2-faster-self-play",children:"2. Faster Self-Play"}),"\n",(0,s.jsx)(n.p,{children:"Since only one network is needed (not two), computational cost per self-play game decreases. This means more training data in same time."}),"\n",(0,s.jsx)(n.h4,{id:"3-more-effective-learning",children:"3. More Effective Learning"}),"\n",(0,s.jsx)(n.p,{children:"Dual-head network's joint training lets each game's information be used more effectively. Policy and Value gradients reinforce each other, accelerating convergence."}),"\n",(0,s.jsx)(n.h3,{id:"comparison-with-human-learning",children:"Comparison with Human Learning"}),"\n",(0,s.jsx)(n.p,{children:"How long does it take human players to reach different levels?"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Level"}),(0,s.jsx)(n.th,{children:"Human Time"}),(0,s.jsx)(n.th,{children:"AlphaGo Zero"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Beginner"}),(0,s.jsx)(n.td,{children:"Weeks"}),(0,s.jsx)(n.td,{children:"Minutes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Amateur 1-dan"}),(0,s.jsx)(n.td,{children:"Years"}),(0,s.jsx)(n.td,{children:"Hours"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Professional"}),(0,s.jsx)(n.td,{children:"10-20 years"}),(0,s.jsx)(n.td,{children:"1-2 days"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"World Champion"}),(0,s.jsx)(n.td,{children:"20+ years full-time"}),(0,s.jsx)(n.td,{children:"3 days"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Surpass humans"}),(0,s.jsx)(n.td,{children:"Impossible"}),(0,s.jsx)(n.td,{children:"3 days"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"This comparison isn't to diminish human players - they use biological neurons while AlphaGo Zero uses specially designed TPUs and kilowatts of electricity. But it does show how efficient the right learning method can be."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"generality-chess-shogi",children:"Generality: Chess, Shogi"}),"\n",(0,s.jsx)(n.h3,{id:"birth-of-alphazero",children:"Birth of AlphaZero"}),"\n",(0,s.jsxs)(n.p,{children:["In December 2017, DeepMind announced ",(0,s.jsx)(n.strong,{children:"AlphaZero"})," - the general version of AlphaGo Zero. Same algorithm, just changing game rules, achieved world-class level in three different board games:"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Game"}),(0,s.jsx)(n.th,{children:"Training Time"}),(0,s.jsx)(n.th,{children:"Opponent"}),(0,s.jsx)(n.th,{children:"Record"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Go"}),(0,s.jsx)(n.td,{children:"8 hours"}),(0,s.jsx)(n.td,{children:"AlphaGo Zero"}),(0,s.jsx)(n.td,{children:"60:40"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Chess"}),(0,s.jsx)(n.td,{children:"4 hours"}),(0,s.jsx)(n.td,{children:"Stockfish 8"}),(0,s.jsx)(n.td,{children:"28 wins 72 draws 0 losses"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Shogi"}),(0,s.jsx)(n.td,{children:"2 hours"}),(0,s.jsx)(n.td,{children:"Elmo"}),(0,s.jsx)(n.td,{children:"90:8:2"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"Note the opponents:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stockfish"})," was then the strongest chess engine, using decades of human knowledge and optimization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Elmo"})," was then the strongest Shogi AI"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"AlphaZero, with hours of training, surpassed these systems that took years to develop."}),"\n",(0,s.jsx)(n.h3,{id:"significance-of-generality",children:"Significance of Generality"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero / AlphaZero proved something important:"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"The same learning algorithm can achieve superhuman level in different domains."})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This isn't three different AIs, but one general learning framework:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Self-play"})," generates experience"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monte Carlo Tree Search"})," explores possibilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Neural network"})," learns policy and value functions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reinforcement learning"})," optimizes objective function"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This framework doesn't depend on domain-specific knowledge, taking an important step toward AI generalization."}),"\n",(0,s.jsx)(n.h3,{id:"impact-on-traditional-ai",children:"Impact on Traditional AI"}),"\n",(0,s.jsx)(n.p,{children:'Before AlphaZero, the strongest chess and shogi AIs were "expert system" style:'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Extensive human knowledge"}),": Opening books, endgame tables, evaluation functions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Decades of optimization"}),": Countless players' and engineers' work"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Extreme specialization"}),": Stockfish can't play Go, Elmo can't play chess"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"AlphaZero surpassed all this with one general algorithm in hours. This made many AI researchers rethink:"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:'Should we invest more in "general learning algorithms" or "expert knowledge encoding"?'}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The answer seems increasingly clear: letting machines learn themselves is more effective than teaching them knowledge."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"alphago-zeros-playing-style",children:"AlphaGo Zero's Playing Style"}),"\n",(0,s.jsx)(n.h3,{id:"beyond-human-aesthetics",children:"Beyond Human Aesthetics"}),"\n",(0,s.jsxs)(n.p,{children:["Go community's common evaluation of AlphaGo Zero's play: ",(0,s.jsx)(n.strong,{children:"more elegant."})]}),"\n",(0,s.jsx)(n.p,{children:'AlphaGo Lee\'s moves sometimes seemed "strange" - like Move 37, humans needed post-game analysis to understand its brilliance. But AlphaGo Zero\'s moves are often evaluated as "obviously good at first glance."'}),"\n",(0,s.jsx)(n.p,{children:"This might be because:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stronger play"}),": Zero sees deeper, plays more calmly"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"No human bias"}),": Not bound by traditional joseki"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Consistent objective"}),": Only pursues win rate, doesn't imitate humans"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"rediscovering-human-go-principles",children:"Rediscovering Human Go Principles"}),"\n",(0,s.jsx)(n.p,{children:'Interestingly, AlphaGo Zero "rediscovered" thousands of years of accumulated Go knowledge during training:'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Joseki"}),": Zero discovered many common joseki on its own, because these are indeed optimal solutions for both sides"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Opening principles"}),": Order of importance of corners, edges, center"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Shape knowledge"}),": Difference between bad and good shapes"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This validates the reasonableness of human Go principles - this knowledge isn't coincidental, but reflects Go's essence."}),"\n",(0,s.jsx)(n.h3,{id:"beyond-human-innovation",children:"Beyond Human Innovation"}),"\n",(0,s.jsx)(n.p,{children:"But Zero also discovered moves humans never thought of:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Unconventional openings"}),": Variations on traditional openings"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Aggressive sacrifice"}),": More willing than humans to give up locally for global advantage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Counter-intuitive shapes"}),': Seemingly "bad shapes" that are actually optimal']}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"These innovations are changing human understanding of Go. Many professional players say studying AlphaGo Zero's games gave them a completely new understanding of Go."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"technical-details-summary",children:"Technical Details Summary"}),"\n",(0,s.jsx)(n.h3,{id:"complete-comparison-with-original-alphago",children:"Complete Comparison with Original AlphaGo"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Aspect"}),(0,s.jsx)(n.th,{children:"AlphaGo (Original)"}),(0,s.jsx)(n.th,{children:"AlphaGo Zero"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Training data"})}),(0,s.jsx)(n.td,{children:"Human games + self-play"}),(0,s.jsx)(n.td,{children:"Pure self-play"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Learning method"})}),(0,s.jsx)(n.td,{children:"Supervised + reinforcement"}),(0,s.jsx)(n.td,{children:"Pure reinforcement"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Input features"})}),(0,s.jsx)(n.td,{children:"48 planes"}),(0,s.jsx)(n.td,{children:"17 planes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Network architecture"})}),(0,s.jsx)(n.td,{children:"Separate Policy/Value"}),(0,s.jsx)(n.td,{children:"Dual-head ResNet"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Network depth"})}),(0,s.jsx)(n.td,{children:"13 layers"}),(0,s.jsx)(n.td,{children:"40 layers (or more)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"MCTS evaluation"})}),(0,s.jsx)(n.td,{children:"Neural network + Rollout"}),(0,s.jsx)(n.td,{children:"Pure neural network"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Simulations/move"})}),(0,s.jsx)(n.td,{children:"~100,000"}),(0,s.jsx)(n.td,{children:"~1,600"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Training TPUs"})}),(0,s.jsx)(n.td,{children:"50+"}),(0,s.jsx)(n.td,{children:"4"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Inference TPUs"})}),(0,s.jsx)(n.td,{children:"48"}),(0,s.jsx)(n.td,{children:"4 (scalable)"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"core-algorithm",children:"Core Algorithm"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero's training loop is very concise:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"1. Self-play\n   - Perform MCTS with current network\n   - Select moves by MCTS search probabilities\n   - Record each move's (position, MCTS probabilities, win/loss result)\n\n2. Train network\n   - Sample from experience buffer\n   - Policy Head: Minimize cross-entropy with MCTS probabilities\n   - Value Head: Minimize MSE with actual win/loss\n   - Jointly optimize both objectives\n\n3. Update network\n   - Replace old network with new (verify new is stronger through play)\n   - Return to step 1\n"})}),"\n",(0,s.jsx)(n.p,{children:"This loop runs continuously, network keeps improving. No human data, no human knowledge, just game rules and win/loss objective."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"insights-for-ai-research",children:"Insights for AI Research"}),"\n",(0,s.jsx)(n.h3,{id:"first-principles-learning",children:"First-Principles Learning"}),"\n",(0,s.jsx)(n.p,{children:'AlphaGo Zero demonstrates a "first-principles" learning approach:'}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Don't tell AI how to do it, only tell it what the goal is, let it discover methods itself."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This contrasts sharply with traditional expert system approaches. Expert systems try to encode human knowledge into AI, while AlphaGo Zero lets AI discover knowledge itself."}),"\n",(0,s.jsx)(n.p,{children:"Result: AI-discovered knowledge may be more complete and accurate than human knowledge."}),"\n",(0,s.jsx)(n.h3,{id:"power-of-self-play",children:"Power of Self-Play"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero proved self-play can generate unlimited training data, and this data's quality improves as network improves."}),"\n",(0,s.jsx)(n.p,{children:'This is a "positive feedback loop":'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Stronger network \u2192 Better self-play data"}),"\n",(0,s.jsx)(n.li,{children:"Better data \u2192 Stronger network"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This loop can continue until reaching the game's theoretical limit (if one exists)."}),"\n",(0,s.jsx)(n.h3,{id:"importance-of-simplification",children:"Importance of Simplification"}),"\n",(0,s.jsx)(n.p,{children:'AlphaGo Zero\'s success proves the importance of "simplification":'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Simplified input (48 \u2192 17)"}),"\n",(0,s.jsx)(n.li,{children:"Simplified architecture (dual network \u2192 single network)"}),"\n",(0,s.jsx)(n.li,{children:"Simplified training (supervised + reinforcement \u2192 pure reinforcement)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Each simplification made the system more powerful. This tells us: complex doesn't equal good, the simplest solution is often the best."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"animation-reference",children:"Animation Reference"}),"\n",(0,s.jsx)(n.p,{children:"Core concepts covered in this article with animation numbers:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Number"}),(0,s.jsx)(n.th,{children:"Concept"}),(0,s.jsx)(n.th,{children:"Physics/Math Correspondence"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Animation E7"}),(0,s.jsx)(n.td,{children:"Training from scratch"}),(0,s.jsx)(n.td,{children:"Self-organization"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Animation E5"}),(0,s.jsx)(n.td,{children:"Self-play"}),(0,s.jsx)(n.td,{children:"Fixed-point convergence"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Animation E12"}),(0,s.jsx)(n.td,{children:"Strength growth curve"}),(0,s.jsx)(n.td,{children:"S-curve growth"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Animation D12"}),(0,s.jsx)(n.td,{children:"Residual network"}),(0,s.jsx)(n.td,{children:"Gradient highway"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Next"}),": ",(0,s.jsx)(n.a,{href:"../dual-head-resnet",children:"Dual-Head Network and Residual Network"})," - AlphaGo Zero's neural network architecture in detail"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Related Article"}),": ",(0,s.jsx)(n.a,{href:"../self-play",children:"Self-Play"})," - Why self-play produces superhuman level"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Technical Deep Dive"}),": ",(0,s.jsx)(n.a,{href:"../training-from-scratch",children:"Training from Scratch"})," - Detailed Day 0-3 evolution"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:['Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." ',(0,s.jsx)(n.em,{children:"Nature"}),", 550, 354-359."]}),"\n",(0,s.jsxs)(n.li,{children:['Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." ',(0,s.jsx)(n.em,{children:"Science"}),", 362(6419), 1140-1144."]}),"\n",(0,s.jsxs)(n.li,{children:['DeepMind. (2017). "AlphaGo Zero: Starting from scratch." ',(0,s.jsx)(n.em,{children:"DeepMind Blog"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:['Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." ',(0,s.jsx)(n.em,{children:"Nature"}),", 588, 604-609."]}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},30416(e,n,i){i.d(n,{R:()=>t,x:()=>a});var r=i(59471);const s={},l=r.createContext(s);function t(e){const n=r.useContext(l);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(l.Provider,{value:n},e.children)}}}]);