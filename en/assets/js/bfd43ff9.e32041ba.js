"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[91],{1928(e,t,n){n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>o,default:()=>p,frontMatter:()=>l,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"alphago/supervised-learning","title":"Supervised Learning Phase","description":"How AlphaGo learned from 30 million human game positions to achieve 57% prediction accuracy","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/alphago/11-supervised-learning.mdx","sourceDirName":"alphago","slug":"/alphago/supervised-learning","permalink":"/en/docs/alphago/supervised-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/11-supervised-learning.mdx","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12,"title":"Supervised Learning Phase","description":"How AlphaGo learned from 30 million human game positions to achieve 57% prediction accuracy"},"sidebar":"tutorialSidebar","previous":{"title":"CNN and Go","permalink":"/en/docs/alphago/cnn-and-go"},"next":{"title":"Introduction to Reinforcement Learning","permalink":"/en/docs/alphago/reinforcement-intro"}}');var s=n(62615),a=n(30416),i=n(45695);const l={sidebar_position:12,title:"Supervised Learning Phase",description:"How AlphaGo learned from 30 million human game positions to achieve 57% prediction accuracy"},o="Supervised Learning Phase",d={},c=[{value:"Why Start with Human Games?",id:"why-start-with-human-games",level:2},{value:"The Starting Point of Learning",id:"the-starting-point-of-learning",level:3},{value:"Value of Human Games",id:"value-of-human-games",level:3},{value:"Training Data Source",id:"training-data-source",level:2},{value:"KGS Go Server",id:"kgs-go-server",level:3},{value:"KGS Characteristics",id:"kgs-characteristics",level:4},{value:"Why Choose KGS?",id:"why-choose-kgs",level:4},{value:"30 Million Positions",id:"30-million-positions",level:3},{value:"Data Format",id:"data-format",level:3},{value:"Data Preprocessing",id:"data-preprocessing",level:2},{value:"SGF Parsing",id:"sgf-parsing",level:3},{value:"Feature Extraction",id:"feature-extraction",level:3},{value:"Data Augmentation",id:"data-augmentation",level:3},{value:"Loss Function",id:"loss-function",level:2},{value:"Cross-Entropy Loss",id:"cross-entropy-loss",level:3},{value:"Intuitive Understanding",id:"intuitive-understanding",level:3},{value:"Comparison with MSE",id:"comparison-with-mse",level:3},{value:"Training Process",id:"training-process",level:2},{value:"Hardware Configuration",id:"hardware-configuration",level:3},{value:"Optimizer",id:"optimizer",level:3},{value:"Why SGD Instead of Adam?",id:"why-sgd-instead-of-adam",level:4},{value:"Learning Rate Schedule",id:"learning-rate-schedule",level:3},{value:"Training Loop",id:"training-loop",level:3},{value:"Training Curves",id:"training-curves",level:3},{value:"Results Analysis",id:"results-analysis",level:2},{value:"57% Accuracy",id:"57-accuracy",level:3},{value:"What Is Top-1 Accuracy?",id:"what-is-top-1-accuracy",level:4},{value:"Comparison with Other Programs",id:"comparison-with-other-programs",level:3},{value:"Strength Evaluation",id:"strength-evaluation",level:3},{value:"Accuracy vs. Strength",id:"accuracy-vs-strength",level:3},{value:"Limitations of Supervised Learning",id:"limitations-of-supervised-learning",level:2},{value:"Problem 1: Ceiling Effect",id:"problem-1-ceiling-effect",level:3},{value:"Problem 2: Cannot Distinguish Good from Bad Moves",id:"problem-2-cannot-distinguish-good-from-bad-moves",level:3},{value:"Problem 3: Insufficient Exploration",id:"problem-3-insufficient-exploration",level:3},{value:"Solution: Reinforcement Learning",id:"solution-reinforcement-learning",level:3},{value:"Implementation Notes",id:"implementation-notes",level:2},{value:"Complete Training Code",id:"complete-training-code",level:3},{value:"Evaluation Code",id:"evaluation-code",level:3},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Animation Reference",id:"animation-reference",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"References",id:"references",level:2}];function h(e){const t={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"supervised-learning-phase",children:"Supervised Learning Phase"})}),"\n",(0,s.jsxs)(t.p,{children:['Before AlphaGo could play against itself, it needed to first "observe" massive amounts of human game records. This process is called ',(0,s.jsx)(t.strong,{children:"supervised learning"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["By analyzing 30 million human game positions, AlphaGo's Policy Network achieved ",(0,s.jsx)(t.strong,{children:"57% prediction accuracy"})," - able to guess the human expert's next move more than half the time."]}),"\n",(0,s.jsx)(t.p,{children:"This might not sound impressive, but considering each position averages 250 legal moves, this is a remarkable achievement."}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"why-start-with-human-games",children:"Why Start with Human Games?"}),"\n",(0,s.jsx)(t.h3,{id:"the-starting-point-of-learning",children:"The Starting Point of Learning"}),"\n",(0,s.jsx)(t.p,{children:"Imagine you're teaching someone who knows nothing about Go. What would you do?"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Option A: Random Exploration"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"Let them play randomly, slowly discovering what's good\n\u2192 Extremely inefficient, might never learn\n"})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Option B: Watch How Experts Play"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"Let them observe many professional games, imitating their moves\n\u2192 After getting basics, then explore on their own\n"})}),"\n",(0,s.jsx)(t.p,{children:'AlphaGo chose Option B. Supervised learning is the mathematical version of "watching how experts play."'}),"\n",(0,s.jsx)(t.h3,{id:"value-of-human-games",children:"Value of Human Games"}),"\n",(0,s.jsx)(t.p,{children:"Humans spent thousands of years developing Go theory. This knowledge is encoded in game records:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Opening joseki"}),": Time-tested opening patterns"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Middlegame tactics"}),": Wisdom of attack and defense transitions"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Endgame techniques"}),": Essence of point counting"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Whole-board vision"}),": Intuition for overall judgment"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:'Supervised learning let AlphaGo "inherit" this human wisdom without starting from scratch.'}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"training-data-source",children:"Training Data Source"}),"\n",(0,s.jsx)(t.h3,{id:"kgs-go-server",children:"KGS Go Server"}),"\n",(0,s.jsxs)(t.p,{children:["AlphaGo's training data came primarily from ",(0,s.jsx)(t.strong,{children:"KGS Go Server"})," (also known as Kiseido Go Server), a well-known online Go platform."]}),"\n",(0,s.jsx)(t.h4,{id:"kgs-characteristics",children:"KGS Characteristics"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Property"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Users"}),(0,s.jsx)(t.td,{children:"Mainly amateurs, some professionals"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Strength range"}),(0,s.jsx)(t.td,{children:"From beginner to professional 9 dan"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Game records"}),(0,s.jsx)(t.td,{children:"Complete SGF records saved"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Active period"}),(0,s.jsx)(t.td,{children:"2000 to present"})]})]})]}),"\n",(0,s.jsx)(t.h4,{id:"why-choose-kgs",children:"Why Choose KGS?"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Large data volume"}),": Millions of game records"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Uniform format"}),": SGF format easy to parse"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Strength labels"}),": Each user has rating"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Diversity"}),": Different playing styles"]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"30-million-positions",children:"30 Million Positions"}),"\n",(0,s.jsxs)(t.p,{children:["From KGS game records, DeepMind extracted approximately ",(0,s.jsx)(t.strong,{children:"30 million positions"}),":"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"Raw data:\n- About 160,000 games\n- About 200 moves per game\n- Total ~32 million positions\n\nData filtering:\n- Filter out low-ranked games\n- Filter out mid-game resignation positions\n- Final ~30 million high-quality positions\n"})}),"\n",(0,s.jsx)(t.h3,{id:"data-format",children:"Data Format"}),"\n",(0,s.jsx)(t.p,{children:"Each training sample contains:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'{\n    "board_state": [[0, 1, 2, ...], ...],  # 19\xd719 board\n    "features": [...],                      # 48 feature planes\n    "next_move": 123,                       # Human\'s move position (0-360)\n    "game_result": 1,                       # 1=Black wins, -1=White wins\n    "player_rank": "5d",                    # Rank of player who made this move\n}\n'})}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"data-preprocessing",children:"Data Preprocessing"}),"\n",(0,s.jsx)(t.h3,{id:"sgf-parsing",children:"SGF Parsing"}),"\n",(0,s.jsx)(t.p,{children:"SGF (Smart Game Format) is the standard format for Go game records:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"(;GM[1]FF[4]CA[UTF-8]AP[CGoban:3]ST[2]\nRU[Japanese]SZ[19]KM[6.50]\nPW[White]PB[Black]\n;B[pd];W[dd];B[pq];W[dp];B[qk];W[nc]...\n)\n"})}),"\n",(0,s.jsx)(t.p,{children:"Need to parse:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Board size (SZ[19])"}),"\n",(0,s.jsx)(t.li,{children:"Each move (B[pd], W[dd]...)"}),"\n",(0,s.jsx)(t.li,{children:"Game result (RE[B+2.5])"}),"\n"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"def parse_sgf(sgf_string):\n    \"\"\"Parse SGF game record\"\"\"\n    moves = []\n    # Extract all moves\n    pattern = r';([BW])\\[([a-s]{2})\\]'\n    for match in re.finditer(pattern, sgf_string):\n        color = match.group(1)  # 'B' or 'W'\n        coord = match.group(2)  # 'pd', 'dd', etc.\n\n        # Convert coordinates\n        x = ord(coord[0]) - ord('a')\n        y = ord(coord[1]) - ord('a')\n\n        moves.append((color, x, y))\n\n    return moves\n"})}),"\n",(0,s.jsx)(t.h3,{id:"feature-extraction",children:"Feature Extraction"}),"\n",(0,s.jsxs)(t.p,{children:["For each position, extract 48 feature planes (see ",(0,s.jsx)(t.a,{href:"../input-features",children:"Input Feature Design"}),"):"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'def extract_features(board, history, current_player):\n    """Extract 48 feature planes"""\n    features = np.zeros((48, 19, 19))\n\n    # Stone positions\n    features[0] = (board == 1)  # Black\n    features[1] = (board == 2)  # White\n    features[2] = (board == 0)  # Empty\n\n    # History\n    for i, hist in enumerate(history[:8]):\n        features[3+i] = (hist == 1)\n        features[11+i] = (hist == 2)\n\n    # Liberties, captures, ladders, etc...\n    # (detailed implementation omitted)\n\n    return features\n'})}),"\n",(0,s.jsx)(t.h3,{id:"data-augmentation",children:"Data Augmentation"}),"\n",(0,s.jsxs)(t.p,{children:["The Go board has ",(0,s.jsx)(t.strong,{children:"8-fold symmetry"})," (4 rotations \xd7 2 reflections). Each original sample can become 8:"]}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"Original"}),(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"Rotate 90"}),(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"Rotate 180"}),(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"Rotate 270"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsxs)(t.td,{style:{textAlign:"center"},children:[(0,s.jsx)(t.strong,{children:"X"})," . ."]}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:". . ."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:". . ."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:". . ."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"center"},children:". . ."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:". . ."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:". . ."}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:". . ."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"center"},children:". . ."}),(0,s.jsxs)(t.td,{style:{textAlign:"center"},children:[(0,s.jsx)(t.strong,{children:"X"})," . ."]}),(0,s.jsxs)(t.td,{style:{textAlign:"center"},children:[". . ",(0,s.jsx)(t.strong,{children:"X"})]}),(0,s.jsxs)(t.td,{style:{textAlign:"center"},children:[". . ",(0,s.jsx)(t.strong,{children:"X"})]})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"Each of these 4 rotations can then be flipped horizontally, giving 8 equivalent training samples."}),"\n",(0,s.jsx)(t.p,{children:"This effectively increases training data by 8\xd7 while ensuring learned patterns don't depend on specific orientation."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'def augment(state, action):\n    """8-fold symmetry augmentation"""\n    augmented = []\n\n    for rotation in [0, 1, 2, 3]:  # 0, 90, 180, 270 degrees\n        rotated_state = np.rot90(state, rotation, axes=(1, 2))\n        rotated_action = rotate_action(action, rotation)\n        augmented.append((rotated_state, rotated_action))\n\n        # Horizontal flip\n        flipped_state = np.flip(rotated_state, axis=2)\n        flipped_action = flip_action(rotated_action)\n        augmented.append((flipped_state, flipped_action))\n\n    return augmented\n'})}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"loss-function",children:"Loss Function"}),"\n",(0,s.jsx)(t.h3,{id:"cross-entropy-loss",children:"Cross-Entropy Loss"}),"\n",(0,s.jsxs)(t.p,{children:["Supervised learning uses ",(0,s.jsx)(t.strong,{children:"Cross-Entropy Loss"})," to train the Policy Network:"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"L(\u03b8) = -\u03a3 log p_\u03b8(a | s)\n"})}),"\n",(0,s.jsx)(t.p,{children:"Where:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"s"}),": Board state"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"a"}),": Position where human actually played (label)"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"p_\u03b8(a | s)"}),": Model's predicted probability for that position"]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"intuitive-understanding",children:"Intuitive Understanding"}),"\n",(0,s.jsx)(t.p,{children:'Cross-entropy loss measures "gap between model prediction and label":'}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Scenario"}),(0,s.jsx)(t.th,{children:"Model Prediction"}),(0,s.jsx)(t.th,{children:"Loss"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Perfect prediction"}),(0,s.jsx)(t.td,{children:"P(a) = 1.0"}),(0,s.jsx)(t.td,{children:"0"}),(0,s.jsx)(t.td,{children:"Best"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Confident and correct"}),(0,s.jsx)(t.td,{children:"P(a) = 0.9"}),(0,s.jsx)(t.td,{children:"0.1"}),(0,s.jsx)(t.td,{children:"Very good"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Uncertain but correct"}),(0,s.jsx)(t.td,{children:"P(a) = 0.5"}),(0,s.jsx)(t.td,{children:"0.7"}),(0,s.jsx)(t.td,{children:"OK"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Wrong prediction"}),(0,s.jsx)(t.td,{children:"P(a) = 0.1"}),(0,s.jsx)(t.td,{children:"2.3"}),(0,s.jsx)(t.td,{children:"Bad"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Completely wrong"}),(0,s.jsx)(t.td,{children:"P(a) = 0.01"}),(0,s.jsx)(t.td,{children:"4.6"}),(0,s.jsx)(t.td,{children:"Worst"})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"The loss function drives the model to increase probability of correct positions."}),"\n",(0,s.jsx)(t.h3,{id:"comparison-with-mse",children:"Comparison with MSE"}),"\n",(0,s.jsx)(t.p,{children:"Why not use Mean Squared Error (MSE)?"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"# MSE:\nloss_mse = (prediction - target)^2\n\n# Cross-Entropy:\nloss_ce = -log(prediction[target])\n"})}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Property"}),(0,s.jsx)(t.th,{children:"MSE"}),(0,s.jsx)(t.th,{children:"Cross-Entropy"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Target type"}),(0,s.jsx)(t.td,{children:"Regression (continuous)"}),(0,s.jsx)(t.td,{children:"Classification (probability distribution)"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Gradient behavior"}),(0,s.jsx)(t.td,{children:"Larger error, larger gradient"}),(0,s.jsx)(t.td,{children:"Confident but wrong gets larger gradient"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Suitable for"}),(0,s.jsx)(t.td,{children:"Value Network"}),(0,s.jsx)(t.td,{children:"Policy Network"})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"Policy Network outputs a probability distribution over 361 classes; cross-entropy is the natural choice."}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"training-process",children:"Training Process"}),"\n",(0,s.jsx)(t.h3,{id:"hardware-configuration",children:"Hardware Configuration"}),"\n",(0,s.jsx)(t.p,{children:"DeepMind used substantial computational resources:"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Resource"}),(0,s.jsx)(t.th,{children:"Quantity"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"GPUs"}),(0,s.jsx)(t.td,{children:"50"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Training time"}),(0,s.jsx)(t.td,{children:"About 3 weeks"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Batch size"}),(0,s.jsx)(t.td,{children:"16"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Total training steps"}),(0,s.jsx)(t.td,{children:"~340M"})]})]})]}),"\n",(0,s.jsx)(t.h3,{id:"optimizer",children:"Optimizer"}),"\n",(0,s.jsxs)(t.p,{children:["Used ",(0,s.jsx)(t.strong,{children:"Stochastic Gradient Descent (SGD) + momentum"}),":"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"optimizer = torch.optim.SGD(\n    model.parameters(),\n    lr=0.003,         # Initial learning rate\n    momentum=0.9,     # Momentum coefficient\n    weight_decay=1e-4 # L2 regularization\n)\n"})}),"\n",(0,s.jsx)(t.h4,{id:"why-sgd-instead-of-adam",children:"Why SGD Instead of Adam?"}),"\n",(0,s.jsx)(t.p,{children:"In 2016, SGD + momentum was still mainstream for image tasks. Actually, later research (including KataGo) found Adam-type optimizers may be better."}),"\n",(0,s.jsx)(t.h3,{id:"learning-rate-schedule",children:"Learning Rate Schedule"}),"\n",(0,s.jsx)(t.p,{children:"Learning rate decays during training:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=80_000_000,  # Every 80M steps\n    gamma=0.1              # Multiply learning rate by 0.1\n)\n"})}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Training Steps"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Learning Rate"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"0 - 80M"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"0.003"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"80M - 160M"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"0.0003"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"160M - 240M"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"0.00003"})]})]})]}),"\n",(0,s.jsx)(t.h3,{id:"training-loop",children:"Training Loop"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"def train_epoch(model, dataloader, optimizer):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    for batch in dataloader:\n        states, actions = batch\n\n        # Forward pass\n        policy = model(states)  # (batch, 361)\n\n        # Calculate loss\n        loss = F.cross_entropy(policy, actions)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Statistics\n        total_loss += loss.item()\n        predictions = policy.argmax(dim=1)\n        correct += (predictions == actions).sum().item()\n        total += actions.size(0)\n\n    accuracy = correct / total\n    avg_loss = total_loss / len(dataloader)\n\n    return avg_loss, accuracy\n"})}),"\n",(0,s.jsx)(t.h3,{id:"training-curves",children:"Training Curves"}),"\n",(0,s.jsx)(t.p,{children:"Typical training process:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"Accuracy\n60% |                    ......**********\n    |              ......*\n50% |        ......*\n    |    ....*\n40% |  ..*\n    |..*\n30% |*\n    +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Training Steps\n    0       100M     200M     300M     340M\n"})}),"\n",(0,s.jsx)(t.p,{children:"Loss and accuracy improve rapidly then stabilize."}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"results-analysis",children:"Results Analysis"}),"\n",(0,s.jsx)(t.h3,{id:"57-accuracy",children:"57% Accuracy"}),"\n",(0,s.jsxs)(t.p,{children:["After complete training, Policy Network achieved ",(0,s.jsx)(t.strong,{children:"57.0% top-1 accuracy"}),"."]}),"\n",(0,s.jsx)(t.h4,{id:"what-is-top-1-accuracy",children:"What Is Top-1 Accuracy?"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"Prediction: Model outputs 361 probabilities\nTop-1: Position with highest probability\nAccuracy: Proportion where this position equals human's actual move\n"})}),"\n",(0,s.jsx)(t.p,{children:"57% means: the model guesses the human expert's next move more than half the time."}),"\n",(0,s.jsx)(t.h3,{id:"comparison-with-other-programs",children:"Comparison with Other Programs"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Program"}),(0,s.jsx)(t.th,{children:"Top-1 Accuracy"}),(0,s.jsx)(t.th,{children:"Notes"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Random selection"}),(0,s.jsx)(t.td,{children:"0.4%"}),(0,s.jsx)(t.td,{children:"Baseline"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Traditional features + linear model"}),(0,s.jsx)(t.td,{children:"~24%"}),(0,s.jsx)(t.td,{children:"2008 level"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Shallow CNN"}),(0,s.jsx)(t.td,{children:"~44%"}),(0,s.jsx)(t.td,{children:"2014 level"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"AlphaGo Policy Network"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"57%"})}),(0,s.jsx)(t.td,{children:"2016 breakthrough"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"AlphaGo Zero"}),(0,s.jsx)(t.td,{children:"~60%"}),(0,s.jsx)(t.td,{children:"2017"})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"DeepMind's deep CNN improved by 13 percentage points over previous best methods."}),"\n",(0,s.jsx)(t.h3,{id:"strength-evaluation",children:"Strength Evaluation"}),"\n",(0,s.jsx)(t.p,{children:"Playing strength using Policy Network alone (no search):"}),"\n",(0,s.jsx)(i.$W,{mode:"training",width:600,height:350}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Configuration"}),(0,s.jsx)(t.th,{children:"Elo Rating"}),(0,s.jsx)(t.th,{children:"Approximate Level"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Traditional strongest (Pachi)"}),(0,s.jsx)(t.td,{children:"~2500"}),(0,s.jsx)(t.td,{children:"Amateur 4-5 dan"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"SL Policy Network"}),(0,s.jsx)(t.td,{children:"~2800"}),(0,s.jsx)(t.td,{children:"Amateur 6-7 dan"})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"Pure supervised learning already reached strong amateur level - a major breakthrough in 2016."}),"\n",(0,s.jsx)(t.h3,{id:"accuracy-vs-strength",children:"Accuracy vs. Strength"}),"\n",(0,s.jsx)(t.p,{children:"Interestingly, accuracy and strength are not linearly related:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"Accuracy:  44% \u2192 57% (13% improvement)\nElo:      ~2500 \u2192 ~2800 (~300 improvement)\n\nAccuracy improvement ratio: 13% / 44% \u2248 30%\nElo improvement ratio: 300 / 2500 \u2248 12%\n"})}),"\n",(0,s.jsx)(t.p,{children:"Small accuracy improvements can lead to significant strength gains because:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Correct choices in critical positions matter more"}),"\n",(0,s.jsx)(t.li,{children:"Avoiding obvious mistakes matters more than playing slightly better moves"}),"\n"]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"limitations-of-supervised-learning",children:"Limitations of Supervised Learning"}),"\n",(0,s.jsx)(t.h3,{id:"problem-1-ceiling-effect",children:"Problem 1: Ceiling Effect"}),"\n",(0,s.jsx)(t.p,{children:'Supervised learning can only reach "human level," cannot surpass it:'}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"SL Policy's goal: Imitate humans\n          \u2193\nIf humans have wrong habits\n          \u2193\nSL Policy will learn those mistakes too\n"})}),"\n",(0,s.jsx)(t.p,{children:'For example, if training data players rarely play moves like "Move 37," SL Policy won\'t learn them either.'}),"\n",(0,s.jsx)(t.h3,{id:"problem-2-cannot-distinguish-good-from-bad-moves",children:"Problem 2: Cannot Distinguish Good from Bad Moves"}),"\n",(0,s.jsx)(t.p,{children:'Supervised learning only sees "what humans played," not whether the move was good:'}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"Position A: Human played K10 (actually a bad move)\nPosition B: Human played Q4 (good move)\n\nSL Policy treats them equally, must learn both\n"})}),"\n",(0,s.jsx)(t.p,{children:"Training data includes amateur games with many mistakes. SL Policy learns these mistakes."}),"\n",(0,s.jsx)(t.h3,{id:"problem-3-insufficient-exploration",children:"Problem 3: Insufficient Exploration"}),"\n",(0,s.jsx)(t.p,{children:"SL Policy only learns moves humans already know:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"Human move set: {A, B, C, D, E}\n           \u2193\nSL Policy will only choose among these moves\n           \u2193\nBetter move F might exist but was never discovered\n"})}),"\n",(0,s.jsx)(t.p,{children:"This is a fundamental limitation of supervised learning: it can only learn what exists in training data."}),"\n",(0,s.jsx)(t.h3,{id:"solution-reinforcement-learning",children:"Solution: Reinforcement Learning"}),"\n",(0,s.jsxs)(t.p,{children:["To surpass humans, AlphaGo uses ",(0,s.jsx)(t.strong,{children:"reinforcement learning"})," after supervised learning:"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"SL Policy (human level)\n      \u2193 Self-play\nRL Policy (surpasses humans)\n"})}),"\n",(0,s.jsxs)(t.p,{children:["See ",(0,s.jsx)(t.a,{href:"../reinforcement-intro",children:"Introduction to Reinforcement Learning"})," and ",(0,s.jsx)(t.a,{href:"../self-play",children:"Self-Play"})," for details."]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"implementation-notes",children:"Implementation Notes"}),"\n",(0,s.jsx)(t.h3,{id:"complete-training-code",children:"Complete Training Code"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\nclass GoDataset(Dataset):\n    def __init__(self, data_path):\n        # Load preprocessed data\n        self.states = np.load(f"{data_path}/states.npy")\n        self.actions = np.load(f"{data_path}/actions.npy")\n\n    def __len__(self):\n        return len(self.states)\n\n    def __getitem__(self, idx):\n        state = torch.FloatTensor(self.states[idx])\n        action = torch.LongTensor([self.actions[idx]])[0]\n        return state, action\n\ndef train_policy_network():\n    # Model\n    model = PolicyNetwork(input_channels=48, num_filters=192, num_layers=12)\n    model = model.cuda()\n\n    # Data\n    dataset = GoDataset("data/kgs")\n    dataloader = DataLoader(\n        dataset, batch_size=16, shuffle=True, num_workers=4\n    )\n\n    # Optimizer\n    optimizer = optim.SGD(\n        model.parameters(),\n        lr=0.003,\n        momentum=0.9,\n        weight_decay=1e-4\n    )\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=80_000_000, gamma=0.1)\n\n    # Training loop\n    best_accuracy = 0\n\n    for epoch in range(100):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        for states, actions in dataloader:\n            states = states.cuda()\n            actions = actions.cuda()\n\n            # Forward pass\n            policy = model(states)\n            loss = nn.functional.cross_entropy(policy, actions)\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            # Statistics\n            total_loss += loss.item()\n            predictions = policy.argmax(dim=1)\n            correct += (predictions == actions).sum().item()\n            total += actions.size(0)\n\n        accuracy = correct / total\n        print(f"Epoch {epoch}: Loss={total_loss/len(dataloader):.4f}, Acc={accuracy:.4f}")\n\n        # Save best model\n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n            torch.save(model.state_dict(), "best_policy.pth")\n\n    print(f"Best accuracy: {best_accuracy:.4f}")\n'})}),"\n",(0,s.jsx)(t.h3,{id:"evaluation-code",children:"Evaluation Code"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'def evaluate_policy(model, test_dataloader):\n    model.eval()\n\n    correct_top1 = 0\n    correct_top5 = 0\n    total = 0\n\n    with torch.no_grad():\n        for states, actions in test_dataloader:\n            states = states.cuda()\n            actions = actions.cuda()\n\n            policy = model(states)\n\n            # Top-1 accuracy\n            top1_pred = policy.argmax(dim=1)\n            correct_top1 += (top1_pred == actions).sum().item()\n\n            # Top-5 accuracy\n            top5_pred = policy.topk(5, dim=1)[1]\n            for i, action in enumerate(actions):\n                if action in top5_pred[i]:\n                    correct_top5 += 1\n\n            total += actions.size(0)\n\n    print(f"Top-1 Accuracy: {correct_top1/total:.4f}")\n    print(f"Top-5 Accuracy: {correct_top5/total:.4f}")\n'})}),"\n",(0,s.jsx)(t.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Issue"}),(0,s.jsx)(t.th,{children:"Symptom"}),(0,s.jsx)(t.th,{children:"Solution"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Overfitting"}),(0,s.jsx)(t.td,{children:"High train accuracy, low test accuracy"}),(0,s.jsx)(t.td,{children:"More data augmentation, Dropout"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Unstable training"}),(0,s.jsx)(t.td,{children:"Loss fluctuates wildly"}),(0,s.jsx)(t.td,{children:"Lower learning rate, larger batch size"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Slow convergence"}),(0,s.jsx)(t.td,{children:"Accuracy plateaus"}),(0,s.jsx)(t.td,{children:"Adjust learning rate, check data"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Out of memory"}),(0,s.jsx)(t.td,{children:"OOM error"}),(0,s.jsx)(t.td,{children:"Smaller batch size, mixed precision"})]})]})]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"animation-reference",children:"Animation Reference"}),"\n",(0,s.jsx)(t.p,{children:"Core concepts covered in this article with animation numbers:"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Number"}),(0,s.jsx)(t.th,{children:"Concept"}),(0,s.jsx)(t.th,{children:"Physics/Math Correspondence"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Animation D3"}),(0,s.jsx)(t.td,{children:"Supervised learning"}),(0,s.jsx)(t.td,{children:"Maximum likelihood estimation"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Animation D5"}),(0,s.jsx)(t.td,{children:"Cross-entropy loss"}),(0,s.jsx)(t.td,{children:"KL divergence"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Animation D6"}),(0,s.jsx)(t.td,{children:"Gradient descent"}),(0,s.jsx)(t.td,{children:"Optimization"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Animation A6"}),(0,s.jsx)(t.td,{children:"Data preprocessing"}),(0,s.jsx)(t.td,{children:"Standardization"})]})]})]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Previous"}),": ",(0,s.jsx)(t.a,{href:"../cnn-and-go",children:"CNN and Go"})," - How CNNs process the board"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Next"}),": ",(0,s.jsx)(t.a,{href:"../reinforcement-intro",children:"Introduction to Reinforcement Learning"})," - The key to surpassing humans"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Related Topic"}),": ",(0,s.jsx)(t.a,{href:"../policy-network",children:"Policy Network Explained"})," - Network architecture details"]}),"\n"]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"KGS game records are the training data source"}),": About 30 million high-quality positions"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Cross-entropy loss drives learning"}),": Makes model increase probability of correct positions"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"57% accuracy is a major breakthrough"}),": 13 percentage points better than previous best"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"8-fold symmetry augmentation"}),": Effectively increases training data"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Supervised learning has a ceiling"}),": Cannot surpass training data level"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:'Supervised learning is AlphaGo\'s "starting point" - it inherited thousands of years of human Go wisdom, laying the foundation for subsequent reinforcement learning.'}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:['Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." ',(0,s.jsx)(t.em,{children:"Nature"}),", 529, 484-489."]}),"\n",(0,s.jsxs)(t.li,{children:['Maddison, C. J., et al. (2014). "Move Evaluation in Go Using Deep Convolutional Neural Networks." ',(0,s.jsx)(t.em,{children:"arXiv:1412.6564"}),"."]}),"\n",(0,s.jsxs)(t.li,{children:['Clark, C., & Storkey, A. (2015). "Training Deep Convolutional Neural Networks to Play Go." ',(0,s.jsx)(t.em,{children:"ICML"}),"."]}),"\n",(0,s.jsxs)(t.li,{children:["KGS Game Archives: ",(0,s.jsx)(t.a,{href:"https://www.gokgs.com/archives.jsp",children:"https://www.gokgs.com/archives.jsp"})]}),"\n"]})]})}function p(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},42948(e,t,n){n.d(t,{A:()=>a});n(59471);var r=n(61785),s=n(62615);function a({children:e,fallback:t}){return(0,r.A)()?(0,s.jsx)(s.Fragment,{children:e?.()}):t??null}},45695(e,t,n){n.d(t,{$W:()=>k,tO:()=>o,u8:()=>j,dW:()=>u});var r=n(59471),s=n(90989),a=n(62615);const i=19,l=[[3,3],[3,9],[3,15],[9,3],[9,9],[9,15],[15,3],[15,9],[15,15]];function o({size:e=400,stones:t=[],highlights:n=[],labels:o=[],onCellClick:d=null,showCoordinates:c=!0}){const h=(0,r.useRef)(null),p=c?30:15,x=e-2*p,u=x/18;return(0,r.useEffect)(()=>{if(!h.current)return;const e=s.Ltv(h.current);e.selectAll("*").remove();const r=e.append("g").attr("transform",`translate(${p}, ${p})`);r.append("rect").attr("x",-u/2).attr("y",-u/2).attr("width",x+u).attr("height",x+u).attr("fill","#dcb35c").attr("rx",4);const a=r.append("g").attr("class","grid");for(let t=0;t<i;t++)a.append("line").attr("class","grid-line").attr("x1",0).attr("y1",t*u).attr("x2",18*u).attr("y2",t*u);for(let t=0;t<i;t++)a.append("line").attr("class","grid-line").attr("x1",t*u).attr("y1",0).attr("x2",t*u).attr("y2",18*u);const m=r.append("g").attr("class","star-points");if(l.forEach(([e,t])=>{m.append("circle").attr("class","star-point").attr("cx",e*u).attr("cy",t*u).attr("r",u/8)}),n.length>0){const e=r.append("g").attr("class","highlights");n.forEach(({x:t,y:n,intensity:r})=>{e.append("rect").attr("class","heatmap-cell").attr("x",t*u-u/2).attr("y",n*u-u/2).attr("width",u).attr("height",u).attr("fill",s.Q3(r)).attr("opacity",.7*r)})}const g=r.append("g").attr("class","stones");if(t.forEach(({x:e,y:t,color:n})=>{const r="black"===n?"stone-black":"stone-white";g.append("circle").attr("cx",e*u+2).attr("cy",t*u+2).attr("r",.45*u).attr("fill","rgba(0,0,0,0.2)"),g.append("circle").attr("class",r).attr("cx",e*u).attr("cy",t*u).attr("r",.45*u)}),o.length>0){const e=r.append("g").attr("class","labels");o.forEach(({x:n,y:r,text:s})=>{const a=t.find(e=>e.x===n&&e.y===r),i="black"===a?.color?"#fff":"#000";e.append("text").attr("x",n*u).attr("y",r*u).attr("dy","0.35em").attr("text-anchor","middle").attr("fill",i).attr("font-size",.5*u).attr("font-weight","bold").text(s)})}if(c){const t=e.append("g").attr("class","coordinates"),n="ABCDEFGHJKLMNOPQRST";for(let e=0;e<i;e++)t.append("text").attr("x",p+e*u).attr("y",p/2).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(n[e]);for(let e=0;e<i;e++)t.append("text").attr("x",p/2).attr("y",p+e*u).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(i-e)}d&&r.append("g").attr("class","click-targets").selectAll("rect").data(s.y17(361)).enter().append("rect").attr("x",e=>e%i*u-u/2).attr("y",e=>Math.floor(e/i)*u-u/2).attr("width",u).attr("height",u).attr("fill","transparent").attr("cursor","pointer").on("click",(e,t)=>{const n=t%i,r=Math.floor(t/i);d({x:n,y:r})})},[e,t,n,o,c,d,u,p,x]),(0,a.jsx)("div",{className:"go-board-container",children:(0,a.jsx)("svg",{ref:h,width:e,height:e,className:"go-board"})})}var d=n(42948);const c=19,h={empty:function(){const e=[];for(let t=0;t<c;t++)for(let n=0;n<c;n++)e.push({x:n,y:t,prob:1/361});return e}(),corner:function(){const e=[],t=[[3,3],[3,15],[15,3],[15,15]],n=[[2,4],[4,2],[2,14],[4,16],[14,2],[16,4],[14,16],[16,14]];for(let r=0;r<c;r++)for(let s=0;s<c;s++){let a=.001;t.some(([e,t])=>e===s&&t===r)?a=.15:n.some(([e,t])=>e===s&&t===r)?a=.05:0!==s&&18!==s&&0!==r&&18!==r||(a=5e-4),e.push({x:s,y:r,prob:a})}return p(e)}(),move37:function(){const e=[],t={x:9,y:4},n=[[3,2],[15,2],[10,10],[8,6]];for(let r=0;r<c;r++)for(let s=0;s<c;s++){let a=.001;s===t.x&&r===t.y?a=.08:n.some(([e,t])=>e===s&&t===r)?a=.12:s>=5&&s<=13&&r>=5&&r<=13&&(a=.005+.01*Math.random()),e.push({x:s,y:r,prob:a})}return p(e)}()};function p(e){const t=e.reduce((e,t)=>e+t.prob,0);return e.map(e=>({...e,prob:e.prob/t}))}function x({initialPosition:e="corner",stones:t=[],highlightMoves:n=[],size:i=450,showTopN:l=5,interactive:o=!0}){const d=(0,r.useRef)(null),p=(0,r.useRef)(null),[x,u]=(0,r.useState)(h[e]||h.corner),[m,g]=(0,r.useState)(null),j=35,f=i-70,y=f/18;(0,r.useEffect)(()=>{if(!d.current)return;const e=s.Ltv(d.current);e.selectAll("*").remove();const n=e.append("g").attr("transform","translate(35, 35)");n.append("rect").attr("x",-y/2).attr("y",-y/2).attr("width",f+y).attr("height",f+y).attr("fill","#dcb35c").attr("rx",4);const r=Math.max(...x.map(e=>e.prob)),a=s.exT(s.oKI).domain([0,r]);n.append("g").attr("class","heatmap").selectAll("rect").data(x).enter().append("rect").attr("class","heatmap-cell").attr("x",e=>e.x*y-y/2).attr("y",e=>e.y*y-y/2).attr("width",y).attr("height",y).attr("fill",e=>a(e.prob)).attr("opacity",e=>.3+e.prob/r*.6).attr("cursor",o?"pointer":"default").on("mouseover",function(e,t){if(!o)return;s.Ltv(this).attr("stroke","#333").attr("stroke-width",2);s.Ltv(p.current).style("display","block").style("left",`${e.pageX+10}px`).style("top",e.pageY-10+"px").html(`\u4f4d\u7f6e: ${String.fromCharCode(65+t.x)}${19-t.y}<br>\u6a5f\u7387: ${(100*t.prob).toFixed(2)}%`)}).on("mouseout",function(){s.Ltv(this).attr("stroke","none"),s.Ltv(p.current).style("display","none")}).on("click",function(e,t){o&&g(t)});const i=n.append("g").attr("class","grid");for(let t=0;t<c;t++)i.append("line").attr("class","grid-line").attr("x1",0).attr("y1",t*y).attr("x2",18*y).attr("y2",t*y).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5),i.append("line").attr("class","grid-line").attr("x1",t*y).attr("y1",0).attr("x2",t*y).attr("y2",18*y).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5);const h=n.append("g").attr("class","stones");t.forEach(({x:e,y:t,color:n})=>{h.append("circle").attr("cx",e*y).attr("cy",t*y).attr("r",.45*y).attr("fill","black"===n?"#1a1a1a":"#f5f5f5").attr("stroke","black"===n?"#000":"#333").attr("stroke-width",1)});const u=[...x].sort((e,t)=>t.prob-e.prob).slice(0,l),m=n.append("g").attr("class","top-labels");u.forEach((e,n)=>{t.some(t=>t.x===e.x&&t.y===e.y)||(m.append("circle").attr("cx",e.x*y).attr("cy",e.y*y).attr("r",.3*y).attr("fill","rgba(255,255,255,0.8)").attr("stroke","#e74c3c").attr("stroke-width",2),m.append("text").attr("x",e.x*y).attr("y",e.y*y).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#e74c3c").attr("font-size",.4*y).attr("font-weight","bold").text(n+1))});const v=e.append("g").attr("class","coordinates");for(let t=0;t<c;t++)v.append("text").attr("x",j+t*y).attr("y",17.5).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text("ABCDEFGHJKLMNOPQRST"[t]),v.append("text").attr("x",17.5).attr("y",j+t*y).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(c-t)},[x,t,l,o,y,j,f]);const v=e=>{u(h[e]||h.corner)};return(0,a.jsxs)("div",{children:[o&&(0,a.jsxs)("div",{className:"d3-controls",children:[(0,a.jsx)("button",{className:"empty"===e?"active":"",onClick:()=>v("empty"),children:"\u5747\u52fb\u5206\u5e03"}),(0,a.jsx)("button",{className:"corner"===e?"active":"",onClick:()=>v("corner"),children:"\u958b\u5c40\u661f\u4f4d"}),(0,a.jsx)("button",{className:"move37"===e?"active":"",onClick:()=>v("move37"),children:"\u7b2c 37 \u624b"})]}),(0,a.jsx)("div",{className:"go-board-container",children:(0,a.jsx)("svg",{ref:d,width:i,height:i,className:"go-board"})}),(0,a.jsx)("div",{ref:p,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),m&&(0,a.jsx)("div",{className:"d3-legend",children:(0,a.jsxs)("div",{className:"d3-legend-item",children:["\u5df2\u9078\u64c7: ",String.fromCharCode(65+m.x),19-m.y,"\u2014 \u6a5f\u7387: ",(100*m.prob).toFixed(2),"%"]})}),(0,a.jsxs)("div",{className:"d3-legend",children:[(0,a.jsxs)("div",{className:"d3-legend-item",children:[(0,a.jsx)("div",{className:"d3-legend-color",style:{background:"#ffffb2"}}),"\u4f4e\u6a5f\u7387"]}),(0,a.jsxs)("div",{className:"d3-legend-item",children:[(0,a.jsx)("div",{className:"d3-legend-color",style:{background:"#fd8d3c"}}),"\u4e2d\u6a5f\u7387"]}),(0,a.jsxs)("div",{className:"d3-legend-item",children:[(0,a.jsx)("div",{className:"d3-legend-color",style:{background:"#bd0026"}}),"\u9ad8\u6a5f\u7387"]})]})]})}function u(e){return(0,a.jsx)(d.A,{fallback:(0,a.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,a.jsx)(x,{...e})})}const m={name:"Root",visits:1600,value:.55,prior:1,children:[{name:"D4",visits:800,value:.62,prior:.35,selected:!0,children:[{name:"Q16",visits:400,value:.58,prior:.3},{name:"R4",visits:300,value:.65,prior:.25,selected:!0},{name:"C16",visits:100,value:.55,prior:.2}]},{name:"Q4",visits:500,value:.52,prior:.3,children:[{name:"D16",visits:300,value:.5,prior:.28},{name:"Q16",visits:200,value:.54,prior:.22}]},{name:"D16",visits:200,value:.48,prior:.2},{name:"Q16",visits:100,value:.45,prior:.15}]};function g({data:e=m,width:t=700,height:n=450,showPUCT:i=!0,cPuct:l=1.5,interactive:o=!0}){const d=(0,r.useRef)(null),c=(0,r.useRef)(null),[h,p]=(0,r.useState)(null),[x,u]=(0,r.useState)(l),g=40,j=40,f=t-j-40,y=n-g-40;return(0,r.useEffect)(()=>{if(!d.current)return;const r=s.Ltv(d.current);r.selectAll("*").remove();const a=s.B22().size([f,y-50]),l=s.Sk5(e);a(l);const h=r.append("g").attr("transform",`translate(${j}, ${g})`);h.append("g").attr("class","links").selectAll("path").data(l.links()).enter().append("path").attr("class",e=>"link "+(e.target.data.selected?"selected":"")).attr("fill","none").attr("stroke",e=>e.target.data.selected?"#4a90d9":"#999").attr("stroke-width",e=>e.target.data.selected?3:1.5).attr("d",s.vu().x(e=>e.x).y(e=>e.y));const u=h.append("g").attr("class","nodes").selectAll("g").data(l.descendants()).enter().append("g").attr("class","node").attr("transform",e=>`translate(${e.x}, ${e.y})`).attr("cursor",o?"pointer":"default").on("mouseover",function(e,t){if(!o)return;s.Ltv(this).select("circle").transition().duration(200).attr("r",30);const n=t.parent?t.parent.data.visits:t.data.visits,r=((e,t)=>{if(!t)return 0;const n=e.value,r=e.prior,s=e.visits;return n+x*r*Math.sqrt(t)/(1+s)})(t.data,n);s.Ltv(c.current).style("display","block").style("left",`${e.pageX+15}px`).style("top",e.pageY-10+"px").html(`\n            <strong>${t.data.name}</strong><br>\n            \u8a2a\u554f\u6b21\u6578 (N): ${t.data.visits}<br>\n            \u5e73\u5747\u50f9\u503c (Q): ${t.data.value.toFixed(3)}<br>\n            \u5148\u9a57\u6a5f\u7387 (P): ${(100*t.data.prior).toFixed(1)}%<br>\n            ${i?`PUCT \u5206\u6578: ${r.toFixed(3)}`:""}\n          `)}).on("mouseout",function(){s.Ltv(this).select("circle").transition().duration(200).attr("r",25),s.Ltv(c.current).style("display","none")}).on("click",function(e,t){o&&p(t.data)});u.append("circle").attr("r",25).attr("fill",e=>e.data.selected?"#4a90d9":"#fff").attr("stroke",t=>{if(t.data.selected)return"#2c5282";const n=t.data.visits/e.visits;return s.dM(.3+.5*n)}).attr("stroke-width",e=>e.data.selected?3:2),u.append("text").attr("dy",-5).attr("text-anchor","middle").attr("fill",e=>e.data.selected?"#fff":"#333").attr("font-size",11).attr("font-weight","bold").text(e=>e.data.name),u.append("text").attr("dy",10).attr("text-anchor","middle").attr("fill",e=>e.data.selected?"#fff":"#666").attr("font-size",9).text(e=>`N=${e.data.visits}`),r.append("text").attr("x",t/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("MCTS \u641c\u7d22\u6a39"),i&&r.append("text").attr("x",t/2).attr("y",n-10).attr("text-anchor","middle").attr("font-size",11).attr("fill","#666").text("\u85cd\u8272\u8def\u5f91\uff1aPUCT \u9078\u64c7\u7684\u6700\u4f73\u8def\u5f91")},[e,t,n,i,x,o,f,y]),(0,a.jsxs)("div",{children:[i&&o&&(0,a.jsx)("div",{className:"d3-controls",children:(0,a.jsxs)("div",{className:"d3-slider",children:[(0,a.jsxs)("label",{children:["c_puct: ",x.toFixed(1)]}),(0,a.jsx)("input",{type:"range",min:"0.5",max:"3",step:"0.1",value:x,onChange:e=>u(parseFloat(e.target.value))})]})}),(0,a.jsx)("div",{className:"mcts-tree-container",children:(0,a.jsx)("svg",{ref:d,width:t,height:n,className:"mcts-tree"})}),(0,a.jsx)("div",{ref:c,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),h&&(0,a.jsxs)("div",{className:"d3-legend",style:{background:"#f5f5f5",padding:"1rem",borderRadius:"4px"},children:[(0,a.jsxs)("strong",{children:["\u5df2\u9078\u64c7\u7bc0\u9ede: ",h.name]}),(0,a.jsxs)("div",{children:["\u8a2a\u554f\u6b21\u6578: ",h.visits]}),(0,a.jsxs)("div",{children:["\u5e73\u5747\u50f9\u503c: ",h.value.toFixed(3)]}),(0,a.jsxs)("div",{children:["\u5148\u9a57\u6a5f\u7387: ",(100*h.prior).toFixed(1),"%"]})]}),(0,a.jsxs)("div",{className:"d3-legend",children:[(0,a.jsxs)("div",{className:"d3-legend-item",children:[(0,a.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"\u9078\u4e2d\u8def\u5f91"]}),(0,a.jsxs)("div",{className:"d3-legend-item",children:[(0,a.jsx)("div",{className:"d3-legend-color",style:{background:"#fff",border:"2px solid #999"}}),"\u5176\u4ed6\u7bc0\u9ede"]}),(0,a.jsx)("div",{className:"d3-legend-item",children:(0,a.jsx)("span",{style:{fontSize:"12px"},children:"\u7bc0\u9ede\u5927\u5c0f \u221d \u8a2a\u554f\u6b21\u6578"})})]})]})}function j(e){return(0,a.jsx)(d.A,{fallback:(0,a.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,a.jsx)(g,{...e})})}const f=[{hours:0,elo:0,label:"\u96a8\u6a5f"},{hours:3,elo:1e3,label:"\u767c\u73fe\u898f\u5247"},{hours:6,elo:2e3},{hours:12,elo:3e3,label:"\u767c\u73fe\u5b9a\u5f0f"},{hours:24,elo:4e3},{hours:36,elo:4500,label:"\u8d85\u8d8a Fan Hui"},{hours:48,elo:5e3},{hours:60,elo:5200,label:"\u8d85\u8d8a Lee Sedol"},{hours:72,elo:5400,label:"\u8d85\u8d8a\u539f\u7248 AlphaGo"}],y=[{elo:2700,label:"\u696d\u9918\u5f37\u8c6a"},{elo:3500,label:"Fan Hui (\u8077\u696d\u4e8c\u6bb5)"},{elo:4500,label:"Lee Sedol (\u4e16\u754c\u51a0\u8ecd)"},{elo:5e3,label:"\u539f\u7248 AlphaGo"}],v=[{epochs:0,elo:0},{epochs:10,elo:1500},{epochs:20,elo:2500},{epochs:30,elo:3e3},{epochs:40,elo:3200},{epochs:50,elo:3300}],b=[{games:0,elo:3300},{games:1e3,elo:3800},{games:5e3,elo:4200},{games:1e4,elo:4500},{games:5e4,elo:4800},{games:1e5,elo:5e3}];function w({mode:e="zero",width:t=600,height:n=400,animated:i=!0,showMilestones:l=!0}){const o=(0,r.useRef)(null),[d,c]=(0,r.useState)(e),h=40,p=70,x=t-p-100,u=n-h-60;return(0,r.useEffect)(()=>{if(!o.current)return;const e=s.Ltv(o.current);let n,r,a;e.selectAll("*").remove(),"zero"===d?(n=f,r="\u8a13\u7df4\u6642\u9593\uff08\u5c0f\u6642\uff09",a=[0,80]):"sl"===d?(n=v,r="\u8a13\u7df4\u8f2a\u6578\uff08Epochs\uff09",a=[0,60]):(n=b,r="\u81ea\u6211\u5c0d\u5f08\u5c40\u6578",a=[0,12e4]);const c="selfplay"===d?s.ZEH().domain([1,a[1]]).range([0,x]):s.m4Y().domain(a).range([0,x]),m=s.m4Y().domain([0,6e3]).range([u,0]),g=e.append("g").attr("transform",`translate(${p}, ${h})`);if(g.append("g").attr("class","grid").selectAll(".grid-line-y").data(m.ticks(6)).enter().append("line").attr("class","grid-line-y").attr("x1",0).attr("x2",x).attr("y1",e=>m(e)).attr("y2",e=>m(e)).attr("stroke","#ddd").attr("stroke-dasharray","3,3"),l&&"zero"===d){const e=g.append("g").attr("class","human-levels");y.forEach(t=>{e.append("line").attr("x1",0).attr("x2",x).attr("y1",m(t.elo)).attr("y2",m(t.elo)).attr("stroke","#e74c3c").attr("stroke-dasharray","5,5").attr("opacity",.6),e.append("text").attr("x",x+5).attr("y",m(t.elo)).attr("dy","0.35em").attr("fill","#e74c3c").attr("font-size",10).text(t.label)})}const j=s.n8j().x(e=>c("zero"===d?e.hours:"sl"===d?e.epochs:Math.max(1,e.games))).y(e=>m(e.elo)).curve(s.nVG),w=s.Wcw().x(e=>c("zero"===d?e.hours:"sl"===d?e.epochs:Math.max(1,e.games))).y0(u).y1(e=>m(e.elo)).curve(s.nVG);g.append("path").datum(n).attr("class","area").attr("fill","#4a90d9").attr("opacity",.1).attr("d",w);const k=g.append("path").datum(n).attr("class","line").attr("fill","none").attr("stroke","#4a90d9").attr("stroke-width",3).attr("d",j);if(i){const e=k.node().getTotalLength();k.attr("stroke-dasharray",`${e} ${e}`).attr("stroke-dashoffset",e).transition().duration(2e3).ease(s.yfw).attr("stroke-dashoffset",0)}if(l&&"zero"===d){const e=n.filter(e=>e.label),t=g.append("g").attr("class","milestones");t.selectAll("circle").data(e).enter().append("circle").attr("cx",e=>c(e.hours)).attr("cy",e=>m(e.elo)).attr("r",6).attr("fill","#e74c3c").attr("stroke","#fff").attr("stroke-width",2),t.selectAll("text").data(e).enter().append("text").attr("x",e=>c(e.hours)).attr("y",e=>m(e.elo)-15).attr("text-anchor","middle").attr("fill","#333").attr("font-size",10).text(e=>e.label)}const S="selfplay"===d?s.l78(c).ticks(5,"~s"):s.l78(c);g.append("g").attr("class","x-axis").attr("transform",`translate(0, ${u})`).call(S),g.append("text").attr("class","axis-label").attr("x",x/2).attr("y",u+45).attr("text-anchor","middle").attr("fill","#666").text(r),g.append("g").attr("class","y-axis").call(s.V4s(m).ticks(6)),g.append("text").attr("class","axis-label").attr("transform","rotate(-90)").attr("x",-u/2).attr("y",-50).attr("text-anchor","middle").attr("fill","#666").text("ELO \u8a55\u5206"),e.append("text").attr("x",t/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("zero"===d?"AlphaGo Zero \u8a13\u7df4\u66f2\u7dda":"sl"===d?"\u76e3\u7763\u5b78\u7fd2\u68cb\u529b\u6210\u9577":"\u81ea\u6211\u5c0d\u5f08\u68cb\u529b\u6210\u9577")},[d,t,n,i,l,x,u]),(0,a.jsxs)("div",{children:[(0,a.jsxs)("div",{className:"d3-controls",children:[(0,a.jsx)("button",{className:"zero"===d?"active":"",onClick:()=>c("zero"),children:"AlphaGo Zero"}),(0,a.jsx)("button",{className:"sl"===d?"active":"",onClick:()=>c("sl"),children:"\u76e3\u7763\u5b78\u7fd2"}),(0,a.jsx)("button",{className:"selfplay"===d?"active":"",onClick:()=>c("selfplay"),children:"\u81ea\u6211\u5c0d\u5f08"})]}),(0,a.jsx)("div",{className:"elo-chart-container",children:(0,a.jsx)("svg",{ref:o,width:t,height:n,className:"elo-chart"})}),(0,a.jsxs)("div",{className:"d3-legend",children:[(0,a.jsxs)("div",{className:"d3-legend-item",children:[(0,a.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"ELO \u8a55\u5206"]}),l&&"zero"===d&&(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)("div",{className:"d3-legend-item",children:[(0,a.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c"}}),"\u91cc\u7a0b\u7891"]}),(0,a.jsxs)("div",{className:"d3-legend-item",children:[(0,a.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c",opacity:.3}}),"\u4eba\u985e\u6c34\u5e73\u53c3\u8003\u7dda"]})]})]})]})}function k(e){return(0,a.jsx)(d.A,{fallback:(0,a.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,a.jsx)(w,{...e})})}},30416(e,t,n){n.d(t,{R:()=>i,x:()=>l});var r=n(59471);const s={},a=r.createContext(s);function i(e){const t=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(a.Provider,{value:t},e.children)}}}]);