"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[3869],{66668(e,t,n){n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"for-engineers/how-it-works/alphago-explained/policy-network","title":"Policy Network Deep Dive","description":"Deep understanding of AlphaGo\'s policy network architecture, training methods, and practical applications, from 13 convolutional layers to Softmax output","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/for-engineers/how-it-works/alphago-explained/07-policy-network.mdx","sourceDirName":"for-engineers/how-it-works/alphago-explained","slug":"/for-engineers/how-it-works/alphago-explained/policy-network","permalink":"/en/docs/for-engineers/how-it-works/alphago-explained/policy-network","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/07-policy-network.mdx","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"title":"Policy Network Deep Dive","description":"Deep understanding of AlphaGo\'s policy network architecture, training methods, and practical applications, from 13 convolutional layers to Softmax output"},"sidebar":"tutorialSidebar","previous":{"title":"Board State Representation","permalink":"/en/docs/for-engineers/how-it-works/alphago-explained/board-representation"},"next":{"title":"Value Network Explained","permalink":"/en/docs/for-engineers/how-it-works/alphago-explained/value-network"}}');var r=n(62615),s=n(30416),l=n(45695);const a={sidebar_position:8,title:"Policy Network Deep Dive",description:"Deep understanding of AlphaGo's policy network architecture, training methods, and practical applications, from 13 convolutional layers to Softmax output"},o="Policy Network Deep Dive",c={},d=[{value:"What is the Policy Network?",id:"what-is-the-policy-network",level:2},{value:"Core Function",id:"core-function",level:3},{value:"Intuitive Understanding",id:"intuitive-understanding",level:3},{value:"Why Do We Need a Policy Network?",id:"why-do-we-need-a-policy-network",level:3},{value:"Network Architecture",id:"network-architecture",level:2},{value:"Overall Structure",id:"overall-structure",level:3},{value:"Input Layer",id:"input-layer",level:3},{value:"Convolutional Layers",id:"convolutional-layers",level:3},{value:"Why 192 Filters?",id:"why-192-filters",level:4},{value:"Why 3\xd73 Kernels?",id:"why-33-kernels",level:4},{value:"Why 5\xd75 for the First Layer?",id:"why-55-for-the-first-layer",level:4},{value:"ReLU Activation Function",id:"relu-activation-function",level:3},{value:"Output Layer",id:"output-layer",level:3},{value:"1\xd71 Convolution",id:"11-convolution",level:4},{value:"Softmax Output",id:"softmax-output",level:4},{value:"Parameter Count",id:"parameter-count",level:3},{value:"Training Objective and Methods",id:"training-objective-and-methods",level:2},{value:"Training Data",id:"training-data",level:3},{value:"Cross-Entropy Loss Function",id:"cross-entropy-loss-function",level:3},{value:"Intuitive Understanding",id:"intuitive-understanding-1",level:4},{value:"Training Process",id:"training-process",level:3},{value:"Data Augmentation",id:"data-augmentation",level:3},{value:"Training Results",id:"training-results",level:2},{value:"57% Accuracy",id:"57-accuracy",level:3},{value:"Is This Accuracy High?",id:"is-this-accuracy-high",level:4},{value:"Playing Strength Improvement",id:"playing-strength-improvement",level:3},{value:"Why Only 57%?",id:"why-only-57",level:3},{value:"1. Multiple Good Moves",id:"1-multiple-good-moves",level:4},{value:"2. Style Differences",id:"2-style-differences",level:4},{value:"3. Humans Make Mistakes Too",id:"3-humans-make-mistakes-too",level:4},{value:"Role in MCTS",id:"role-in-mcts",level:2},{value:"1. Guiding Search Direction",id:"1-guiding-search-direction",level:3},{value:"2. Priors for Expanding Nodes",id:"2-priors-for-expanding-nodes",level:3},{value:"Lightweight vs Full Version",id:"lightweight-vs-full-version",level:2},{value:"Full Version (SL Policy Network)",id:"full-version-sl-policy-network",level:3},{value:"Lightweight Version (Rollout Policy Network)",id:"lightweight-version-rollout-policy-network",level:3},{value:"Why a Lightweight Version?",id:"why-a-lightweight-version",level:3},{value:"Lightweight Version Features",id:"lightweight-version-features",level:3},{value:"AlphaGo Zero&#39;s Improvement",id:"alphago-zeros-improvement",level:3},{value:"Reinforcement Learning Fine-Tuning (RL Policy Network)",id:"reinforcement-learning-fine-tuning-rl-policy-network",level:2},{value:"Limitations of Supervised Learning",id:"limitations-of-supervised-learning",level:3},{value:"Self-Play Reinforcement",id:"self-play-reinforcement",level:3},{value:"REINFORCE Algorithm",id:"reinforce-algorithm",level:3},{value:"Results",id:"results",level:3},{value:"From &quot;Imitation&quot; to &quot;Innovation&quot;",id:"from-imitation-to-innovation",level:3},{value:"Visual Analysis",id:"visual-analysis",level:2},{value:"Probability Distribution for Different Positions",id:"probability-distribution-for-different-positions",level:3},{value:"Opening (Fuseki Stage)",id:"opening-fuseki-stage",level:4},{value:"Fighting Position",id:"fighting-position",level:4},{value:"Endgame Stage",id:"endgame-stage",level:4},{value:"What Do Hidden Layers Learn?",id:"what-do-hidden-layers-learn",level:3},{value:"Implementation Notes",id:"implementation-notes",level:2},{value:"PyTorch Implementation",id:"pytorch-implementation",level:3},{value:"Training Loop",id:"training-loop",level:3},{value:"Notes for Inference",id:"notes-for-inference",level:3},{value:"Animation Mapping",id:"animation-mapping",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"References",id:"references",level:2}];function h(e){const t={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"policy-network-deep-dive",children:"Policy Network Deep Dive"})}),"\n",(0,r.jsx)(t.p,{children:"In any Go position, there are on average 250 legal moves. If a computer chose randomly, it would never play good moves."}),"\n",(0,r.jsx)(t.p,{children:'AlphaGo\'s breakthrough was this: it learned to "glance at the board and know which positions are worth considering."'}),"\n",(0,r.jsxs)(t.p,{children:["This ability comes from the ",(0,r.jsx)(t.strong,{children:"Policy Network"}),"."]}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"what-is-the-policy-network",children:"What is the Policy Network?"}),"\n",(0,r.jsx)(t.h3,{id:"core-function",children:"Core Function"}),"\n",(0,r.jsx)(t.p,{children:"The Policy Network is a deep convolutional neural network with the task of:"}),"\n",(0,r.jsxs)(t.blockquote,{children:["\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:"Given the current board state, output the probability of playing at each position"})}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"In mathematical terms:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"p = f_\u03b8(s)\n"})}),"\n",(0,r.jsx)(t.p,{children:"Where:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"s"}),": Current board state (19\xd719 board + other features)"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"f_\u03b8"}),": Policy Network (\u03b8 is the network parameters)"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"p"}),": Probability distribution over 361 positions (including pass)"]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"intuitive-understanding",children:"Intuitive Understanding"}),"\n",(0,r.jsx)(t.p,{children:'Imagine you\'re a professional player. When you see a position, your brain automatically "lights up" several important locations\u2014these are the points you intuitively consider worth examining.'}),"\n",(0,r.jsx)(t.p,{children:"The Policy Network simulates this process."}),"\n",(0,r.jsx)(l.dW,{initialPosition:"corner",size:400}),"\n",(0,r.jsx)(t.p,{children:"The heatmap above shows the Policy Network's output. Brighter positions are what the model considers more worth playing."}),"\n",(0,r.jsx)(t.h3,{id:"why-do-we-need-a-policy-network",children:"Why Do We Need a Policy Network?"}),"\n",(0,r.jsx)(t.p,{children:"Go's search space is too large. If we search all possible moves without filtering:"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Strategy"}),(0,r.jsx)(t.th,{children:"Moves considered per turn"}),(0,r.jsx)(t.th,{children:"Nodes for 10-move search"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Consider all"}),(0,r.jsx)(t.td,{children:"361"}),(0,r.jsx)(t.td,{children:"361^10 \u2248 10^25"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Policy Network filtering"}),(0,r.jsx)(t.td,{children:"~20"}),(0,r.jsx)(t.td,{children:"20^10 \u2248 10^13"})]})]})]}),"\n",(0,r.jsxs)(t.p,{children:["The Policy Network reduces the search space by ",(0,r.jsx)(t.strong,{children:"10^12 times"})," (one trillion times)."]}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"network-architecture",children:"Network Architecture"}),"\n",(0,r.jsx)(t.h3,{id:"overall-structure",children:"Overall Structure"}),"\n",(0,r.jsx)(t.p,{children:"AlphaGo's Policy Network uses a deep convolutional neural network (CNN) architecture:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"Input layer \u2192 Conv layers \xd712 \u2192 Output conv layer \u2192 Softmax\n     \u2193            \u2193                  \u2193                \u2193\n19\xd719\xd748      19\xd719\xd7192          19\xd719\xd71         362 probabilities\n"})}),"\n",(0,r.jsx)(t.h3,{id:"input-layer",children:"Input Layer"}),"\n",(0,r.jsxs)(t.p,{children:["Input is a ",(0,r.jsx)(t.strong,{children:"19\xd719\xd748"})," feature tensor:"]}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"19\xd719"}),": Board size"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"48"}),": 48 feature planes (see ",(0,r.jsx)(t.a,{href:"../input-features",children:"Input Features Design"}),")"]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"These 48 planes include:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"Black stone positions, white stone positions"}),"\n",(0,r.jsx)(t.li,{children:"History of the last 8 moves"}),"\n",(0,r.jsx)(t.li,{children:"Liberties, atari, ladder features"}),"\n",(0,r.jsx)(t.li,{children:"Legality (which positions can be played)"}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"convolutional-layers",children:"Convolutional Layers"}),"\n",(0,r.jsxs)(t.p,{children:["The network contains ",(0,r.jsx)(t.strong,{children:"12 convolutional layers"}),", each with this configuration:"]}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Parameter"}),(0,r.jsx)(t.th,{children:"Value"}),(0,r.jsx)(t.th,{children:"Description"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Number of filters"}),(0,r.jsx)(t.td,{children:"192"}),(0,r.jsx)(t.td,{children:"Each layer outputs 192 feature maps"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Kernel size"}),(0,r.jsx)(t.td,{children:"3\xd73 (5\xd75 for first layer)"}),(0,r.jsx)(t.td,{children:"Each convolution looks at a 3\xd73 region"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Padding"}),(0,r.jsx)(t.td,{children:"same"}),(0,r.jsx)(t.td,{children:"Maintains 19\xd719 size"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Activation"}),(0,r.jsx)(t.td,{children:"ReLU"}),(0,r.jsx)(t.td,{children:"max(0, x)"})]})]})]}),"\n",(0,r.jsx)(t.h4,{id:"why-192-filters",children:"Why 192 Filters?"}),"\n",(0,r.jsx)(t.p,{children:"This is an empirical value. Too few limits model capacity, too many increases computation and overfitting risk. The DeepMind team determined through experiments that 192 is a good balance point."}),"\n",(0,r.jsx)(t.h4,{id:"why-33-kernels",children:"Why 3\xd73 Kernels?"}),"\n",(0,r.jsx)(t.p,{children:"3\xd73 is the most common size in CNNs, because:"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Sufficient to capture local patterns"}),": Go patterns like eyes, connections, and cuts all fit within 3\xd73 regions"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Computationally efficient"}),": Compared to larger kernels, 3\xd73 has fewer parameters"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Stackable"}),": Multiple 3\xd73 convolutions can achieve a large receptive field"]}),"\n"]}),"\n",(0,r.jsx)(t.h4,{id:"why-55-for-the-first-layer",children:"Why 5\xd75 for the First Layer?"}),"\n",(0,r.jsx)(t.p,{children:"The first layer uses a larger 5\xd75 kernel to capture slightly larger patterns (like knight's moves, one-point jumps) at the input layer. This is a design choice; later, AlphaGo Zero unified to use 3\xd73 throughout."}),"\n",(0,r.jsx)(t.h3,{id:"relu-activation-function",children:"ReLU Activation Function"}),"\n",(0,r.jsx)(t.p,{children:"Each convolutional layer is followed by a ReLU (Rectified Linear Unit) activation function:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"ReLU(x) = max(0, x)\n"})}),"\n",(0,r.jsx)(t.p,{children:"Why use ReLU?"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Simple computation"}),": Just taking the maximum, much faster than sigmoid"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Mitigates vanishing gradient"}),": Gradient is always 1 in the positive region"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Sparse activation"}),": Negative values become zero, creating sparse representations"]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"output-layer",children:"Output Layer"}),"\n",(0,r.jsx)(t.p,{children:"The final layer is a special convolutional layer:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"19\xd719\xd7192 \u2192 Conv(1\xd71, 1 filter) \u2192 19\xd719\xd71 \u2192 Flatten \u2192 362-dim vector \u2192 Softmax\n"})}),"\n",(0,r.jsx)(t.h4,{id:"11-convolution",children:"1\xd71 Convolution"}),"\n",(0,r.jsx)(t.p,{children:"The output layer uses 1\xd71 convolution to compress 192 channels into 1. This is equivalent to a linear combination of the 192-dimensional features at each position."}),"\n",(0,r.jsx)(t.h4,{id:"softmax-output",children:"Softmax Output"}),"\n",(0,r.jsx)(t.p,{children:"The 362-dimensional vector (361 board positions + 1 pass) goes through the Softmax function:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"Softmax(z_i) = exp(z_i) / \u03a3_j exp(z_j)\n"})}),"\n",(0,r.jsx)(t.p,{children:"Softmax ensures the output is a valid probability distribution:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"All values are between 0 and 1"}),"\n",(0,r.jsx)(t.li,{children:"All values sum to 1"}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"parameter-count",children:"Parameter Count"}),"\n",(0,r.jsx)(t.p,{children:"Let's calculate the total number of parameters:"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Layer"}),(0,r.jsx)(t.th,{children:"Calculation"}),(0,r.jsx)(t.th,{children:"Parameters"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"First conv layer"}),(0,r.jsx)(t.td,{children:"5\xd75\xd748\xd7192 + 192"}),(0,r.jsx)(t.td,{children:"230,592"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Middle conv layers \xd711"}),(0,r.jsx)(t.td,{children:"(3\xd73\xd7192\xd7192 + 192) \xd7 11"}),(0,r.jsx)(t.td,{children:"3,633,792"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Output conv layer"}),(0,r.jsx)(t.td,{children:"1\xd71\xd7192\xd71 + 1"}),(0,r.jsx)(t.td,{children:"193"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"Total"})}),(0,r.jsx)(t.td,{}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"~3.9M"})})]})]})]}),"\n",(0,r.jsxs)(t.p,{children:["Approximately ",(0,r.jsx)(t.strong,{children:"3.9 million parameters"}),", which by today's standards is a small network."]}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"training-objective-and-methods",children:"Training Objective and Methods"}),"\n",(0,r.jsx)(t.h3,{id:"training-data",children:"Training Data"}),"\n",(0,r.jsxs)(t.p,{children:["The Policy Network uses ",(0,r.jsx)(t.strong,{children:"supervised learning"}),", learning from human game records."]}),"\n",(0,r.jsx)(t.p,{children:"Data sources:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"KGS Go Server"}),": Games from amateur and professional players"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"About 30 million positions"}),": Sampled from 160,000 games"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Labels"}),": The human's next move for each position"]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"cross-entropy-loss-function",children:"Cross-Entropy Loss Function"}),"\n",(0,r.jsx)(t.p,{children:"The training objective is to maximize the probability of predicting human moves. Using cross-entropy loss:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"L(\u03b8) = -\u03a3 log p_\u03b8(a | s)\n"})}),"\n",(0,r.jsx)(t.p,{children:"Where:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"s"}),": Board state"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"a"}),": Position where the human actually played"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"p_\u03b8(a | s)"}),": Model's predicted probability for that position"]}),"\n"]}),"\n",(0,r.jsx)(t.h4,{id:"intuitive-understanding-1",children:"Intuitive Understanding"}),"\n",(0,r.jsx)(t.p,{children:"Cross-entropy loss has a simple meaning:"}),"\n",(0,r.jsxs)(t.blockquote,{children:["\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:"When the model predicts higher probability for the correct position, the loss is lower"})}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"If a human plays at K10, and the model's probability for K10 is:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"0.9 \u2192 Loss = -log(0.9) \u2248 0.1 (very low, good)"}),"\n",(0,r.jsx)(t.li,{children:"0.1 \u2192 Loss = -log(0.1) \u2248 2.3 (high, bad)"}),"\n",(0,r.jsx)(t.li,{children:"0.01 \u2192 Loss = -log(0.01) \u2248 4.6 (very high, very bad)"}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"training-process",children:"Training Process"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:"# Pseudocode\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        states, actions = batch\n\n        # Forward pass\n        policy = network(states)  # 361-dimensional probability vector\n\n        # Calculate loss (cross-entropy)\n        loss = cross_entropy(policy, actions)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n"})}),"\n",(0,r.jsx)(t.p,{children:"Training details:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Optimizer"}),": SGD with momentum"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Learning rate"}),": Initial 0.003, gradually decayed"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Batch size"}),": 16"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Training time"}),": About 3 weeks (50 GPUs)"]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"data-augmentation",children:"Data Augmentation"}),"\n",(0,r.jsx)(t.p,{children:"The Go board has 8-fold symmetry (4 rotations \xd7 2 reflections). Each training sample can be transformed into 8 equivalent samples:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"Original \u2192 Rotate 90\xb0 \u2192 Rotate 180\xb0 \u2192 Rotate 270\xb0\n    \u2193          \u2193            \u2193            \u2193\nFlip horizontal \u2192 ...\n"})}),"\n",(0,r.jsx)(t.p,{children:"This increases effective training data by 8\xd7, and ensures the model learns patterns that don't depend on orientation."}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"training-results",children:"Training Results"}),"\n",(0,r.jsx)(t.h3,{id:"57-accuracy",children:"57% Accuracy"}),"\n",(0,r.jsxs)(t.p,{children:["After training, the Policy Network achieved ",(0,r.jsx)(t.strong,{children:"57% top-1 accuracy"}),"."]}),"\n",(0,r.jsx)(t.p,{children:"This means: Given any position, the model has a 57% chance of predicting the exact move the human expert played."}),"\n",(0,r.jsx)(t.h4,{id:"is-this-accuracy-high",children:"Is This Accuracy High?"}),"\n",(0,r.jsx)(t.p,{children:"Considering that each position has on average 250 legal moves, random guessing has only 0.4% accuracy."}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Method"}),(0,r.jsx)(t.th,{children:"Top-1 Accuracy"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Random guessing"}),(0,r.jsx)(t.td,{children:"0.4%"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Previous strongest computer Go"}),(0,r.jsx)(t.td,{children:"~44%"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"AlphaGo Policy Network"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"57%"})})]})]})]}),"\n",(0,r.jsx)(t.p,{children:"A 13 percentage point improvement may not seem like much, but it's highly significant."}),"\n",(0,r.jsx)(t.h3,{id:"playing-strength-improvement",children:"Playing Strength Improvement"}),"\n",(0,r.jsx)(t.p,{children:"What playing strength can be achieved using only the Policy Network (without search)?"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Configuration"}),(0,r.jsx)(t.th,{children:"Elo Rating"}),(0,r.jsx)(t.th,{children:"Approximate Level"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Previous strongest program (Pachi)"}),(0,r.jsx)(t.td,{children:"2,500"}),(0,r.jsx)(t.td,{children:"Amateur 4-5 dan"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Policy Network alone"}),(0,r.jsx)(t.td,{children:"2,800"}),(0,r.jsx)(t.td,{children:"Amateur 6-7 dan"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"+ MCTS 1600 simulations"}),(0,r.jsx)(t.td,{children:"3,200+"}),(0,r.jsx)(t.td,{children:"Professional level"})]})]})]}),"\n",(0,r.jsx)(t.p,{children:"The Policy Network alone is already strong amateur level, and with MCTS it jumps to professional level."}),"\n",(0,r.jsx)(t.h3,{id:"why-only-57",children:"Why Only 57%?"}),"\n",(0,r.jsx)(t.p,{children:"Human game records have the following characteristics that limit accuracy:"}),"\n",(0,r.jsx)(t.h4,{id:"1-multiple-good-moves",children:"1. Multiple Good Moves"}),"\n",(0,r.jsx)(t.p,{children:'Many positions have multiple good moves. For example, both "approach" and "defend corner" might be correct choices. If the model chooses a different good move, it\'s counted as "wrong."'}),"\n",(0,r.jsx)(t.h4,{id:"2-style-differences",children:"2. Style Differences"}),"\n",(0,r.jsx)(t.p,{children:'Different players have different styles. Aggressive players and steady players might play different moves in the same position. The model learns an "average" style.'}),"\n",(0,r.jsx)(t.h4,{id:"3-humans-make-mistakes-too",children:"3. Humans Make Mistakes Too"}),"\n",(0,r.jsx)(t.p,{children:'KGS data includes amateur player games, whose choices aren\'t always optimal. The model learning some "mistakes" is normal.'}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"role-in-mcts",children:"Role in MCTS"}),"\n",(0,r.jsx)(t.p,{children:"The Policy Network plays two key roles in AlphaGo's MCTS:"}),"\n",(0,r.jsx)(t.h3,{id:"1-guiding-search-direction",children:"1. Guiding Search Direction"}),"\n",(0,r.jsxs)(t.p,{children:["In the MCTS ",(0,r.jsx)(t.strong,{children:"Selection"})," phase, Policy Network output is used to calculate UCB (Upper Confidence Bound):"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"UCB(s, a) = Q(s, a) + c_puct \xd7 P(s, a) \xd7 \u221a(N(s)) / (1 + N(s, a))\n"})}),"\n",(0,r.jsxs)(t.p,{children:["Where ",(0,r.jsx)(t.code,{children:"P(s, a)"})," is the probability given by the Policy Network."]}),"\n",(0,r.jsx)(t.p,{children:"This means:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:(0,r.jsx)(t.strong,{children:"High-probability moves are explored first"})}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Low-probability moves also have a chance to be explored"})," (because of the exploration term)"]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"2-priors-for-expanding-nodes",children:"2. Priors for Expanding Nodes"}),"\n",(0,r.jsxs)(t.p,{children:["When MCTS expands a new node, the Policy Network provides ",(0,r.jsx)(t.strong,{children:"prior probabilities"})," for all child nodes."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"Expand node s:\n  for each action a:\n    child = Node()\n    child.prior = policy_network(s)[a]  # Prior probability\n    child.value = 0\n    child.visits = 0\n"})}),"\n",(0,r.jsx)(t.p,{children:'These prior probabilities let MCTS "know" which child nodes are more worth exploring, even if they haven\'t been visited yet.'}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"lightweight-vs-full-version",children:"Lightweight vs Full Version"}),"\n",(0,r.jsx)(t.p,{children:"AlphaGo actually has two Policy Networks:"}),"\n",(0,r.jsx)(t.h3,{id:"full-version-sl-policy-network",children:"Full Version (SL Policy Network)"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Architecture"}),": 13-layer CNN, 192 filters"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Accuracy"}),": 57%"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Inference time"}),": About 3 milliseconds/position"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Use"}),": Selection and Expansion in MCTS"]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"lightweight-version-rollout-policy-network",children:"Lightweight Version (Rollout Policy Network)"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Architecture"}),": Linear model + handcrafted features"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Accuracy"}),": 24%"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Inference time"}),": About 2 microseconds/position (1500\xd7 faster)"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Use"}),": Fast simulation (rollout)"]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"why-a-lightweight-version",children:"Why a Lightweight Version?"}),"\n",(0,r.jsxs)(t.p,{children:["In the MCTS ",(0,r.jsx)(t.strong,{children:"Simulation"})," phase, we need to play from the current node all the way to the end of the game, potentially playing 100+ moves. If every move used the full Policy Network, it would be too slow."]}),"\n",(0,r.jsx)(t.p,{children:"The lightweight version has only 24% accuracy, but is 1500\xd7 faster. In rollouts, speed matters more than precision."}),"\n",(0,r.jsx)(t.h3,{id:"lightweight-version-features",children:"Lightweight Version Features"}),"\n",(0,r.jsx)(t.p,{children:"The lightweight version uses handcrafted features, including:"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Feature Type"}),(0,r.jsx)(t.th,{children:"Examples"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Local patterns"}),(0,r.jsx)(t.td,{children:"Stone configurations in 3\xd73 regions"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Global features"}),(0,r.jsx)(t.td,{children:"Whether on edge/corner, big points"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Tactical features"}),(0,r.jsx)(t.td,{children:"Atari, ladder, connection"})]})]})]}),"\n",(0,r.jsx)(t.p,{children:"These features are input to a linear model (no hidden layers), making computation extremely fast."}),"\n",(0,r.jsx)(t.h3,{id:"alphago-zeros-improvement",children:"AlphaGo Zero's Improvement"}),"\n",(0,r.jsx)(t.p,{children:"Later, AlphaGo Zero completely abandoned the lightweight version and rollouts. It directly used the Value Network to evaluate leaf nodes, eliminating the need for fast simulation. This was a major simplification."}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"reinforcement-learning-fine-tuning-rl-policy-network",children:"Reinforcement Learning Fine-Tuning (RL Policy Network)"}),"\n",(0,r.jsx)(t.h3,{id:"limitations-of-supervised-learning",children:"Limitations of Supervised Learning"}),"\n",(0,r.jsx)(t.p,{children:"The supervised learning-trained Policy Network has a fundamental problem:"}),"\n",(0,r.jsxs)(t.blockquote,{children:["\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:'It learns to "imitate humans," not to "win games"'})}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"This means it will learn humans' bad habits and also perform poorly in positions humans have never encountered."}),"\n",(0,r.jsx)(t.h3,{id:"self-play-reinforcement",children:"Self-Play Reinforcement"}),"\n",(0,r.jsxs)(t.p,{children:["DeepMind's solution was to use ",(0,r.jsx)(t.strong,{children:"Policy Gradient"})," methods for reinforcement learning:"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"1. Have the Policy Network play against itself\n2. Record all moves in each game\n3. Adjust parameters based on outcome:\n   - Won \u2192 Increase probability of these moves\n   - Lost \u2192 Decrease probability of these moves\n"})}),"\n",(0,r.jsx)(t.h3,{id:"reinforce-algorithm",children:"REINFORCE Algorithm"}),"\n",(0,r.jsx)(t.p,{children:"Specifically, using the REINFORCE algorithm:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"\u2207J(\u03b8) = E[\u03a3_t \u2207log \u03c0_\u03b8(a_t | s_t) \xd7 z]\n"})}),"\n",(0,r.jsx)(t.p,{children:"Where:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"z"}),": Game outcome (+1 win, -1 loss)"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"\u03c0_\u03b8(a_t | s_t)"}),": Probability of choosing action ",(0,r.jsx)(t.code,{children:"a_t"})," in state ",(0,r.jsx)(t.code,{children:"s_t"})]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"results",children:"Results"}),"\n",(0,r.jsx)(t.p,{children:"After about 1 day of self-play training (1.28 million games), the RL Policy Network:"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Metric"}),(0,r.jsx)(t.th,{children:"SL Policy"}),(0,r.jsx)(t.th,{children:"RL Policy"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Win rate vs SL Policy"}),(0,r.jsx)(t.td,{children:"50%"}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.strong,{children:"80%"})})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Elo improvement"}),(0,r.jsx)(t.td,{children:"-"}),(0,r.jsx)(t.td,{children:"+100"})]})]})]}),"\n",(0,r.jsx)(t.p,{children:"Accuracy may drop slightly (since it no longer fully imitates humans), but actual game win rate significantly improved."}),"\n",(0,r.jsx)(t.h3,{id:"from-imitation-to-innovation",children:'From "Imitation" to "Innovation"'}),"\n",(0,r.jsx)(t.p,{children:"Reinforcement learning let the Policy Network learn some moves humans had never thought of. These moves never appeared in training data, but they're effective."}),"\n",(0,r.jsx)(t.p,{children:'This is why AlphaGo could play the "Divine Move"\u2014it\'s not limited by human experience.'}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"visual-analysis",children:"Visual Analysis"}),"\n",(0,r.jsx)(t.h3,{id:"probability-distribution-for-different-positions",children:"Probability Distribution for Different Positions"}),"\n",(0,r.jsx)(t.p,{children:"Let's look at the Policy Network's output in different positions:"}),"\n",(0,r.jsx)(t.h4,{id:"opening-fuseki-stage",children:"Opening (Fuseki Stage)"}),"\n",(0,r.jsx)(l.dW,{initialPosition:"opening",size:400}),"\n",(0,r.jsx)(t.p,{children:"During the opening, probability is mainly concentrated on:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"Corners (taking corners)"}),"\n",(0,r.jsx)(t.li,{children:"Edges (approaching, defending corners)"}),"\n",(0,r.jsx)(t.li,{children:'"Big point" positions'}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"This matches basic Go principles: corners are gold, edges are silver, center is grass."}),"\n",(0,r.jsx)(t.h4,{id:"fighting-position",children:"Fighting Position"}),"\n",(0,r.jsx)(l.dW,{initialPosition:"fighting",size:400}),"\n",(0,r.jsx)(t.p,{children:"During fighting, probability concentrates on:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"Key cutting points"}),"\n",(0,r.jsx)(t.li,{children:"Atari, connections"}),"\n",(0,r.jsx)(t.li,{children:"Making eyes, destroying eyes"}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"This shows the model learned local tactics."}),"\n",(0,r.jsx)(t.h4,{id:"endgame-stage",children:"Endgame Stage"}),"\n",(0,r.jsx)(l.dW,{initialPosition:"endgame",size:400}),"\n",(0,r.jsx)(t.p,{children:"During the endgame, probability is scattered across various endgame points, requiring precise point calculation."}),"\n",(0,r.jsx)(t.h3,{id:"what-do-hidden-layers-learn",children:"What Do Hidden Layers Learn?"}),"\n",(0,r.jsx)(t.p,{children:'By visualizing convolutional layer outputs, we can see the "features" the model learned:'}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Low layers"}),": Basic shapes (eyes, cutting points)"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Middle layers"}),": Tactical patterns (atari, ladders)"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"High layers"}),": Global concepts (influence, thickness)"]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"This closely resembles the hierarchical structure of how humans understand Go."}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"implementation-notes",children:"Implementation Notes"}),"\n",(0,r.jsx)(t.h3,{id:"pytorch-implementation",children:"PyTorch Implementation"}),"\n",(0,r.jsx)(t.p,{children:"Here's a simplified Policy Network implementation:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PolicyNetwork(nn.Module):\n    def __init__(self, input_channels=48, num_filters=192, num_layers=12):\n        super().__init__()\n\n        # First convolutional layer (5\xd75)\n        self.conv1 = nn.Conv2d(input_channels, num_filters,\n                               kernel_size=5, padding=2)\n\n        # Middle convolutional layers (3\xd73) \xd711\n        self.conv_layers = nn.ModuleList([\n            nn.Conv2d(num_filters, num_filters,\n                     kernel_size=3, padding=1)\n            for _ in range(num_layers - 1)\n        ])\n\n        # Output convolutional layer (1\xd71)\n        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)\n\n    def forward(self, x):\n        # x: (batch, 48, 19, 19)\n\n        # First layer\n        x = F.relu(self.conv1(x))\n\n        # Middle layers\n        for conv in self.conv_layers:\n            x = F.relu(conv(x))\n\n        # Output layer\n        x = self.conv_out(x)  # (batch, 1, 19, 19)\n\n        # Flatten + Softmax\n        x = x.view(x.size(0), -1)  # (batch, 361)\n        x = F.softmax(x, dim=1)\n\n        return x\n"})}),"\n",(0,r.jsx)(t.h3,{id:"training-loop",children:"Training Loop"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'def train_step(model, optimizer, states, actions):\n    """\n    states: (batch, 48, 19, 19) - Board features\n    actions: (batch,) - Positions where humans played (0-360)\n    """\n    # Forward pass\n    policy = model(states)  # (batch, 361)\n\n    # Cross-entropy loss\n    loss = F.cross_entropy(\n        torch.log(policy + 1e-8),  # Prevent log(0)\n        actions\n    )\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Calculate accuracy\n    predictions = policy.argmax(dim=1)\n    accuracy = (predictions == actions).float().mean()\n\n    return loss.item(), accuracy.item()\n'})}),"\n",(0,r.jsx)(t.h3,{id:"notes-for-inference",children:"Notes for Inference"}),"\n",(0,r.jsx)(t.p,{children:"When actually playing, note:"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Filter illegal moves"}),": Set probability of illegal positions to 0, then renormalize"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Temperature adjustment"}),': Use a temperature parameter to control the "sharpness" of the probability distribution']}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Batch inference"}),": In MCTS, multiple positions can be processed in batches"]}),"\n"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'def get_move_probabilities(model, state, legal_moves, temperature=1.0):\n    """Get probability distribution over legal moves"""\n    policy = model(state)  # (361,)\n\n    # Keep only legal moves\n    mask = torch.zeros(361)\n    mask[legal_moves] = 1\n    policy = policy * mask\n\n    # Temperature adjustment\n    if temperature != 1.0:\n        policy = policy ** (1 / temperature)\n\n    # Renormalize\n    policy = policy / policy.sum()\n\n    return policy\n'})}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"animation-mapping",children:"Animation Mapping"}),"\n",(0,r.jsx)(t.p,{children:"Core concepts covered in this article and their animation numbers:"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Number"}),(0,r.jsx)(t.th,{children:"Concept"}),(0,r.jsx)(t.th,{children:"Physics/Math Correspondence"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Animation E1"}),(0,r.jsx)(t.td,{children:"Policy Network"}),(0,r.jsx)(t.td,{children:"Probability field"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Animation D9"}),(0,r.jsx)(t.td,{children:"CNN feature extraction"}),(0,r.jsx)(t.td,{children:"Filter response"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Animation D3"}),(0,r.jsx)(t.td,{children:"Supervised learning"}),(0,r.jsx)(t.td,{children:"Maximum likelihood estimation"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Animation H4"}),(0,r.jsx)(t.td,{children:"Policy gradient"}),(0,r.jsx)(t.td,{children:"Stochastic optimization"})]})]})]}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Next article"}),": ",(0,r.jsx)(t.a,{href:"../value-network",children:"Value Network Deep Dive"})," \u2014 How AlphaGo evaluates positions"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Related topic"}),": ",(0,r.jsx)(t.a,{href:"../input-features",children:"Input Features Design"})," \u2014 Detailed explanation of 48 feature planes"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Deep dive"}),": ",(0,r.jsx)(t.a,{href:"../cnn-and-go",children:"CNN and Go"})," \u2014 Why CNNs are suitable for board games"]}),"\n"]}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Policy Network is a probability distribution generator"}),": Input board, output probabilities for 361 positions"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"13-layer CNN + Softmax"}),": Deep convolutions extract features, Softmax outputs probabilities"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"57% accuracy"}),": Far exceeding previous computer Go programs"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Two versions"}),": Full version for MCTS decisions, lightweight version for fast simulation"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Reinforcement learning fine-tuning"}),': Evolving from "imitating humans" to "pursuing victory"']}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:'The Policy Network is AlphaGo\'s "intuition"\u2014it allows the AI to quickly identify moves worth considering, just like a human.'}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:['Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." ',(0,r.jsx)(t.em,{children:"Nature"}),", 529, 484-489."]}),"\n",(0,r.jsxs)(t.li,{children:['Maddison, C. J., et al. (2014). "Move Evaluation in Go Using Deep Convolutional Neural Networks." ',(0,r.jsx)(t.em,{children:"arXiv:1412.6564"}),"."]}),"\n",(0,r.jsxs)(t.li,{children:["Sutton, R. S., & Barto, A. G. (2018). ",(0,r.jsx)(t.em,{children:"Reinforcement Learning: An Introduction"}),". MIT Press."]}),"\n",(0,r.jsxs)(t.li,{children:['LeCun, Y., Bengio, Y., & Hinton, G. (2015). "Deep learning." ',(0,r.jsx)(t.em,{children:"Nature"}),", 521, 436-444."]}),"\n"]})]})}function p(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},42948(e,t,n){n.d(t,{A:()=>s});n(59471);var i=n(61785),r=n(62615);function s({children:e,fallback:t}){return(0,i.A)()?(0,r.jsx)(r.Fragment,{children:e?.()}):t??null}},45695(e,t,n){n.d(t,{$W:()=>k,tO:()=>o,u8:()=>g,dW:()=>u});var i=n(59471),r=n(90989),s=n(62615);const l=19,a=[[3,3],[3,9],[3,15],[9,3],[9,9],[9,15],[15,3],[15,9],[15,15]];function o({size:e=400,stones:t=[],highlights:n=[],labels:o=[],onCellClick:c=null,showCoordinates:d=!0}){const h=(0,i.useRef)(null),p=d?30:15,x=e-2*p,u=x/18;return(0,i.useEffect)(()=>{if(!h.current)return;const e=r.Ltv(h.current);e.selectAll("*").remove();const i=e.append("g").attr("transform",`translate(${p}, ${p})`);i.append("rect").attr("x",-u/2).attr("y",-u/2).attr("width",x+u).attr("height",x+u).attr("fill","#dcb35c").attr("rx",4);const s=i.append("g").attr("class","grid");for(let t=0;t<l;t++)s.append("line").attr("class","grid-line").attr("x1",0).attr("y1",t*u).attr("x2",18*u).attr("y2",t*u);for(let t=0;t<l;t++)s.append("line").attr("class","grid-line").attr("x1",t*u).attr("y1",0).attr("x2",t*u).attr("y2",18*u);const j=i.append("g").attr("class","star-points");if(a.forEach(([e,t])=>{j.append("circle").attr("class","star-point").attr("cx",e*u).attr("cy",t*u).attr("r",u/8)}),n.length>0){const e=i.append("g").attr("class","highlights");n.forEach(({x:t,y:n,intensity:i})=>{e.append("rect").attr("class","heatmap-cell").attr("x",t*u-u/2).attr("y",n*u-u/2).attr("width",u).attr("height",u).attr("fill",r.Q3(i)).attr("opacity",.7*i)})}const m=i.append("g").attr("class","stones");if(t.forEach(({x:e,y:t,color:n})=>{const i="black"===n?"stone-black":"stone-white";m.append("circle").attr("cx",e*u+2).attr("cy",t*u+2).attr("r",.45*u).attr("fill","rgba(0,0,0,0.2)"),m.append("circle").attr("class",i).attr("cx",e*u).attr("cy",t*u).attr("r",.45*u)}),o.length>0){const e=i.append("g").attr("class","labels");o.forEach(({x:n,y:i,text:r})=>{const s=t.find(e=>e.x===n&&e.y===i),l="black"===s?.color?"#fff":"#000";e.append("text").attr("x",n*u).attr("y",i*u).attr("dy","0.35em").attr("text-anchor","middle").attr("fill",l).attr("font-size",.5*u).attr("font-weight","bold").text(r)})}if(d){const t=e.append("g").attr("class","coordinates"),n="ABCDEFGHJKLMNOPQRST";for(let e=0;e<l;e++)t.append("text").attr("x",p+e*u).attr("y",p/2).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(n[e]);for(let e=0;e<l;e++)t.append("text").attr("x",p/2).attr("y",p+e*u).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(l-e)}c&&i.append("g").attr("class","click-targets").selectAll("rect").data(r.y17(361)).enter().append("rect").attr("x",e=>e%l*u-u/2).attr("y",e=>Math.floor(e/l)*u-u/2).attr("width",u).attr("height",u).attr("fill","transparent").attr("cursor","pointer").on("click",(e,t)=>{const n=t%l,i=Math.floor(t/l);c({x:n,y:i})})},[e,t,n,o,d,c,u,p,x]),(0,s.jsx)("div",{className:"go-board-container",children:(0,s.jsx)("svg",{ref:h,width:e,height:e,className:"go-board"})})}var c=n(42948);const d=19,h={empty:function(){const e=[];for(let t=0;t<d;t++)for(let n=0;n<d;n++)e.push({x:n,y:t,prob:1/361});return e}(),corner:function(){const e=[],t=[[3,3],[3,15],[15,3],[15,15]],n=[[2,4],[4,2],[2,14],[4,16],[14,2],[16,4],[14,16],[16,14]];for(let i=0;i<d;i++)for(let r=0;r<d;r++){let s=.001;t.some(([e,t])=>e===r&&t===i)?s=.15:n.some(([e,t])=>e===r&&t===i)?s=.05:0!==r&&18!==r&&0!==i&&18!==i||(s=5e-4),e.push({x:r,y:i,prob:s})}return p(e)}(),move37:function(){const e=[],t={x:9,y:4},n=[[3,2],[15,2],[10,10],[8,6]];for(let i=0;i<d;i++)for(let r=0;r<d;r++){let s=.001;r===t.x&&i===t.y?s=.08:n.some(([e,t])=>e===r&&t===i)?s=.12:r>=5&&r<=13&&i>=5&&i<=13&&(s=.005+.01*Math.random()),e.push({x:r,y:i,prob:s})}return p(e)}()};function p(e){const t=e.reduce((e,t)=>e+t.prob,0);return e.map(e=>({...e,prob:e.prob/t}))}function x({initialPosition:e="corner",stones:t=[],highlightMoves:n=[],size:l=450,showTopN:a=5,interactive:o=!0}){const c=(0,i.useRef)(null),p=(0,i.useRef)(null),[x,u]=(0,i.useState)(h[e]||h.corner),[j,m]=(0,i.useState)(null),g=35,y=l-70,f=y/18;(0,i.useEffect)(()=>{if(!c.current)return;const e=r.Ltv(c.current);e.selectAll("*").remove();const n=e.append("g").attr("transform","translate(35, 35)");n.append("rect").attr("x",-f/2).attr("y",-f/2).attr("width",y+f).attr("height",y+f).attr("fill","#dcb35c").attr("rx",4);const i=Math.max(...x.map(e=>e.prob)),s=r.exT(r.oKI).domain([0,i]);n.append("g").attr("class","heatmap").selectAll("rect").data(x).enter().append("rect").attr("class","heatmap-cell").attr("x",e=>e.x*f-f/2).attr("y",e=>e.y*f-f/2).attr("width",f).attr("height",f).attr("fill",e=>s(e.prob)).attr("opacity",e=>.3+e.prob/i*.6).attr("cursor",o?"pointer":"default").on("mouseover",function(e,t){if(!o)return;r.Ltv(this).attr("stroke","#333").attr("stroke-width",2);r.Ltv(p.current).style("display","block").style("left",`${e.pageX+10}px`).style("top",e.pageY-10+"px").html(`\u4f4d\u7f6e: ${String.fromCharCode(65+t.x)}${19-t.y}<br>\u6a5f\u7387: ${(100*t.prob).toFixed(2)}%`)}).on("mouseout",function(){r.Ltv(this).attr("stroke","none"),r.Ltv(p.current).style("display","none")}).on("click",function(e,t){o&&m(t)});const l=n.append("g").attr("class","grid");for(let t=0;t<d;t++)l.append("line").attr("class","grid-line").attr("x1",0).attr("y1",t*f).attr("x2",18*f).attr("y2",t*f).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5),l.append("line").attr("class","grid-line").attr("x1",t*f).attr("y1",0).attr("x2",t*f).attr("y2",18*f).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5);const h=n.append("g").attr("class","stones");t.forEach(({x:e,y:t,color:n})=>{h.append("circle").attr("cx",e*f).attr("cy",t*f).attr("r",.45*f).attr("fill","black"===n?"#1a1a1a":"#f5f5f5").attr("stroke","black"===n?"#000":"#333").attr("stroke-width",1)});const u=[...x].sort((e,t)=>t.prob-e.prob).slice(0,a),j=n.append("g").attr("class","top-labels");u.forEach((e,n)=>{t.some(t=>t.x===e.x&&t.y===e.y)||(j.append("circle").attr("cx",e.x*f).attr("cy",e.y*f).attr("r",.3*f).attr("fill","rgba(255,255,255,0.8)").attr("stroke","#e74c3c").attr("stroke-width",2),j.append("text").attr("x",e.x*f).attr("y",e.y*f).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#e74c3c").attr("font-size",.4*f).attr("font-weight","bold").text(n+1))});const v=e.append("g").attr("class","coordinates");for(let t=0;t<d;t++)v.append("text").attr("x",g+t*f).attr("y",17.5).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text("ABCDEFGHJKLMNOPQRST"[t]),v.append("text").attr("x",17.5).attr("y",g+t*f).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(d-t)},[x,t,a,o,f,g,y]);const v=e=>{u(h[e]||h.corner)};return(0,s.jsxs)("div",{children:[o&&(0,s.jsxs)("div",{className:"d3-controls",children:[(0,s.jsx)("button",{className:"empty"===e?"active":"",onClick:()=>v("empty"),children:"\u5747\u52fb\u5206\u5e03"}),(0,s.jsx)("button",{className:"corner"===e?"active":"",onClick:()=>v("corner"),children:"\u958b\u5c40\u661f\u4f4d"}),(0,s.jsx)("button",{className:"move37"===e?"active":"",onClick:()=>v("move37"),children:"\u7b2c 37 \u624b"})]}),(0,s.jsx)("div",{className:"go-board-container",children:(0,s.jsx)("svg",{ref:c,width:l,height:l,className:"go-board"})}),(0,s.jsx)("div",{ref:p,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),j&&(0,s.jsx)("div",{className:"d3-legend",children:(0,s.jsxs)("div",{className:"d3-legend-item",children:["\u5df2\u9078\u64c7: ",String.fromCharCode(65+j.x),19-j.y,"\u2014 \u6a5f\u7387: ",(100*j.prob).toFixed(2),"%"]})}),(0,s.jsxs)("div",{className:"d3-legend",children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#ffffb2"}}),"\u4f4e\u6a5f\u7387"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#fd8d3c"}}),"\u4e2d\u6a5f\u7387"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#bd0026"}}),"\u9ad8\u6a5f\u7387"]})]})]})}function u(e){return(0,s.jsx)(c.A,{fallback:(0,s.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,s.jsx)(x,{...e})})}const j={name:"Root",visits:1600,value:.55,prior:1,children:[{name:"D4",visits:800,value:.62,prior:.35,selected:!0,children:[{name:"Q16",visits:400,value:.58,prior:.3},{name:"R4",visits:300,value:.65,prior:.25,selected:!0},{name:"C16",visits:100,value:.55,prior:.2}]},{name:"Q4",visits:500,value:.52,prior:.3,children:[{name:"D16",visits:300,value:.5,prior:.28},{name:"Q16",visits:200,value:.54,prior:.22}]},{name:"D16",visits:200,value:.48,prior:.2},{name:"Q16",visits:100,value:.45,prior:.15}]};function m({data:e=j,width:t=700,height:n=450,showPUCT:l=!0,cPuct:a=1.5,interactive:o=!0}){const c=(0,i.useRef)(null),d=(0,i.useRef)(null),[h,p]=(0,i.useState)(null),[x,u]=(0,i.useState)(a),m=40,g=40,y=t-g-40,f=n-m-40;return(0,i.useEffect)(()=>{if(!c.current)return;const i=r.Ltv(c.current);i.selectAll("*").remove();const s=r.B22().size([y,f-50]),a=r.Sk5(e);s(a);const h=i.append("g").attr("transform",`translate(${g}, ${m})`);h.append("g").attr("class","links").selectAll("path").data(a.links()).enter().append("path").attr("class",e=>"link "+(e.target.data.selected?"selected":"")).attr("fill","none").attr("stroke",e=>e.target.data.selected?"#4a90d9":"#999").attr("stroke-width",e=>e.target.data.selected?3:1.5).attr("d",r.vu().x(e=>e.x).y(e=>e.y));const u=h.append("g").attr("class","nodes").selectAll("g").data(a.descendants()).enter().append("g").attr("class","node").attr("transform",e=>`translate(${e.x}, ${e.y})`).attr("cursor",o?"pointer":"default").on("mouseover",function(e,t){if(!o)return;r.Ltv(this).select("circle").transition().duration(200).attr("r",30);const n=t.parent?t.parent.data.visits:t.data.visits,i=((e,t)=>{if(!t)return 0;const n=e.value,i=e.prior,r=e.visits;return n+x*i*Math.sqrt(t)/(1+r)})(t.data,n);r.Ltv(d.current).style("display","block").style("left",`${e.pageX+15}px`).style("top",e.pageY-10+"px").html(`\n            <strong>${t.data.name}</strong><br>\n            \u8a2a\u554f\u6b21\u6578 (N): ${t.data.visits}<br>\n            \u5e73\u5747\u50f9\u503c (Q): ${t.data.value.toFixed(3)}<br>\n            \u5148\u9a57\u6a5f\u7387 (P): ${(100*t.data.prior).toFixed(1)}%<br>\n            ${l?`PUCT \u5206\u6578: ${i.toFixed(3)}`:""}\n          `)}).on("mouseout",function(){r.Ltv(this).select("circle").transition().duration(200).attr("r",25),r.Ltv(d.current).style("display","none")}).on("click",function(e,t){o&&p(t.data)});u.append("circle").attr("r",25).attr("fill",e=>e.data.selected?"#4a90d9":"#fff").attr("stroke",t=>{if(t.data.selected)return"#2c5282";const n=t.data.visits/e.visits;return r.dM(.3+.5*n)}).attr("stroke-width",e=>e.data.selected?3:2),u.append("text").attr("dy",-5).attr("text-anchor","middle").attr("fill",e=>e.data.selected?"#fff":"#333").attr("font-size",11).attr("font-weight","bold").text(e=>e.data.name),u.append("text").attr("dy",10).attr("text-anchor","middle").attr("fill",e=>e.data.selected?"#fff":"#666").attr("font-size",9).text(e=>`N=${e.data.visits}`),i.append("text").attr("x",t/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("MCTS \u641c\u7d22\u6a39"),l&&i.append("text").attr("x",t/2).attr("y",n-10).attr("text-anchor","middle").attr("font-size",11).attr("fill","#666").text("\u85cd\u8272\u8def\u5f91\uff1aPUCT \u9078\u64c7\u7684\u6700\u4f73\u8def\u5f91")},[e,t,n,l,x,o,y,f]),(0,s.jsxs)("div",{children:[l&&o&&(0,s.jsx)("div",{className:"d3-controls",children:(0,s.jsxs)("div",{className:"d3-slider",children:[(0,s.jsxs)("label",{children:["c_puct: ",x.toFixed(1)]}),(0,s.jsx)("input",{type:"range",min:"0.5",max:"3",step:"0.1",value:x,onChange:e=>u(parseFloat(e.target.value))})]})}),(0,s.jsx)("div",{className:"mcts-tree-container",children:(0,s.jsx)("svg",{ref:c,width:t,height:n,className:"mcts-tree"})}),(0,s.jsx)("div",{ref:d,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),h&&(0,s.jsxs)("div",{className:"d3-legend",style:{background:"#f5f5f5",padding:"1rem",borderRadius:"4px"},children:[(0,s.jsxs)("strong",{children:["\u5df2\u9078\u64c7\u7bc0\u9ede: ",h.name]}),(0,s.jsxs)("div",{children:["\u8a2a\u554f\u6b21\u6578: ",h.visits]}),(0,s.jsxs)("div",{children:["\u5e73\u5747\u50f9\u503c: ",h.value.toFixed(3)]}),(0,s.jsxs)("div",{children:["\u5148\u9a57\u6a5f\u7387: ",(100*h.prior).toFixed(1),"%"]})]}),(0,s.jsxs)("div",{className:"d3-legend",children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"\u9078\u4e2d\u8def\u5f91"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#fff",border:"2px solid #999"}}),"\u5176\u4ed6\u7bc0\u9ede"]}),(0,s.jsx)("div",{className:"d3-legend-item",children:(0,s.jsx)("span",{style:{fontSize:"12px"},children:"\u7bc0\u9ede\u5927\u5c0f \u221d \u8a2a\u554f\u6b21\u6578"})})]})]})}function g(e){return(0,s.jsx)(c.A,{fallback:(0,s.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,s.jsx)(m,{...e})})}const y=[{hours:0,elo:0,label:"\u96a8\u6a5f"},{hours:3,elo:1e3,label:"\u767c\u73fe\u898f\u5247"},{hours:6,elo:2e3},{hours:12,elo:3e3,label:"\u767c\u73fe\u5b9a\u5f0f"},{hours:24,elo:4e3},{hours:36,elo:4500,label:"\u8d85\u8d8a Fan Hui"},{hours:48,elo:5e3},{hours:60,elo:5200,label:"\u8d85\u8d8a Lee Sedol"},{hours:72,elo:5400,label:"\u8d85\u8d8a\u539f\u7248 AlphaGo"}],f=[{elo:2700,label:"\u696d\u9918\u5f37\u8c6a"},{elo:3500,label:"Fan Hui (\u8077\u696d\u4e8c\u6bb5)"},{elo:4500,label:"Lee Sedol (\u4e16\u754c\u51a0\u8ecd)"},{elo:5e3,label:"\u539f\u7248 AlphaGo"}],v=[{epochs:0,elo:0},{epochs:10,elo:1500},{epochs:20,elo:2500},{epochs:30,elo:3e3},{epochs:40,elo:3200},{epochs:50,elo:3300}],b=[{games:0,elo:3300},{games:1e3,elo:3800},{games:5e3,elo:4200},{games:1e4,elo:4500},{games:5e4,elo:4800},{games:1e5,elo:5e3}];function w({mode:e="zero",width:t=600,height:n=400,animated:l=!0,showMilestones:a=!0}){const o=(0,i.useRef)(null),[c,d]=(0,i.useState)(e),h=40,p=70,x=t-p-100,u=n-h-60;return(0,i.useEffect)(()=>{if(!o.current)return;const e=r.Ltv(o.current);let n,i,s;e.selectAll("*").remove(),"zero"===c?(n=y,i="\u8a13\u7df4\u6642\u9593\uff08\u5c0f\u6642\uff09",s=[0,80]):"sl"===c?(n=v,i="\u8a13\u7df4\u8f2a\u6578\uff08Epochs\uff09",s=[0,60]):(n=b,i="\u81ea\u6211\u5c0d\u5f08\u5c40\u6578",s=[0,12e4]);const d="selfplay"===c?r.ZEH().domain([1,s[1]]).range([0,x]):r.m4Y().domain(s).range([0,x]),j=r.m4Y().domain([0,6e3]).range([u,0]),m=e.append("g").attr("transform",`translate(${p}, ${h})`);if(m.append("g").attr("class","grid").selectAll(".grid-line-y").data(j.ticks(6)).enter().append("line").attr("class","grid-line-y").attr("x1",0).attr("x2",x).attr("y1",e=>j(e)).attr("y2",e=>j(e)).attr("stroke","#ddd").attr("stroke-dasharray","3,3"),a&&"zero"===c){const e=m.append("g").attr("class","human-levels");f.forEach(t=>{e.append("line").attr("x1",0).attr("x2",x).attr("y1",j(t.elo)).attr("y2",j(t.elo)).attr("stroke","#e74c3c").attr("stroke-dasharray","5,5").attr("opacity",.6),e.append("text").attr("x",x+5).attr("y",j(t.elo)).attr("dy","0.35em").attr("fill","#e74c3c").attr("font-size",10).text(t.label)})}const g=r.n8j().x(e=>d("zero"===c?e.hours:"sl"===c?e.epochs:Math.max(1,e.games))).y(e=>j(e.elo)).curve(r.nVG),w=r.Wcw().x(e=>d("zero"===c?e.hours:"sl"===c?e.epochs:Math.max(1,e.games))).y0(u).y1(e=>j(e.elo)).curve(r.nVG);m.append("path").datum(n).attr("class","area").attr("fill","#4a90d9").attr("opacity",.1).attr("d",w);const k=m.append("path").datum(n).attr("class","line").attr("fill","none").attr("stroke","#4a90d9").attr("stroke-width",3).attr("d",g);if(l){const e=k.node().getTotalLength();k.attr("stroke-dasharray",`${e} ${e}`).attr("stroke-dashoffset",e).transition().duration(2e3).ease(r.yfw).attr("stroke-dashoffset",0)}if(a&&"zero"===c){const e=n.filter(e=>e.label),t=m.append("g").attr("class","milestones");t.selectAll("circle").data(e).enter().append("circle").attr("cx",e=>d(e.hours)).attr("cy",e=>j(e.elo)).attr("r",6).attr("fill","#e74c3c").attr("stroke","#fff").attr("stroke-width",2),t.selectAll("text").data(e).enter().append("text").attr("x",e=>d(e.hours)).attr("y",e=>j(e.elo)-15).attr("text-anchor","middle").attr("fill","#333").attr("font-size",10).text(e=>e.label)}const N="selfplay"===c?r.l78(d).ticks(5,"~s"):r.l78(d);m.append("g").attr("class","x-axis").attr("transform",`translate(0, ${u})`).call(N),m.append("text").attr("class","axis-label").attr("x",x/2).attr("y",u+45).attr("text-anchor","middle").attr("fill","#666").text(i),m.append("g").attr("class","y-axis").call(r.V4s(j).ticks(6)),m.append("text").attr("class","axis-label").attr("transform","rotate(-90)").attr("x",-u/2).attr("y",-50).attr("text-anchor","middle").attr("fill","#666").text("ELO \u8a55\u5206"),e.append("text").attr("x",t/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("zero"===c?"AlphaGo Zero \u8a13\u7df4\u66f2\u7dda":"sl"===c?"\u76e3\u7763\u5b78\u7fd2\u68cb\u529b\u6210\u9577":"\u81ea\u6211\u5c0d\u5f08\u68cb\u529b\u6210\u9577")},[c,t,n,l,a,x,u]),(0,s.jsxs)("div",{children:[(0,s.jsxs)("div",{className:"d3-controls",children:[(0,s.jsx)("button",{className:"zero"===c?"active":"",onClick:()=>d("zero"),children:"AlphaGo Zero"}),(0,s.jsx)("button",{className:"sl"===c?"active":"",onClick:()=>d("sl"),children:"\u76e3\u7763\u5b78\u7fd2"}),(0,s.jsx)("button",{className:"selfplay"===c?"active":"",onClick:()=>d("selfplay"),children:"\u81ea\u6211\u5c0d\u5f08"})]}),(0,s.jsx)("div",{className:"elo-chart-container",children:(0,s.jsx)("svg",{ref:o,width:t,height:n,className:"elo-chart"})}),(0,s.jsxs)("div",{className:"d3-legend",children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"ELO \u8a55\u5206"]}),a&&"zero"===c&&(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c"}}),"\u91cc\u7a0b\u7891"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c",opacity:.3}}),"\u4eba\u985e\u6c34\u5e73\u53c3\u8003\u7dda"]})]})]})]})}function k(e){return(0,s.jsx)(c.A,{fallback:(0,s.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,s.jsx)(w,{...e})})}},30416(e,t,n){n.d(t,{R:()=>l,x:()=>a});var i=n(59471);const r={},s=i.createContext(r);function l(e){const t=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(s.Provider,{value:t},e.children)}}}]);