"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[9793],{44851(e,n,i){i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"for-engineers/how-it-works/alphago-explained/dual-head-resnet","title":"Dual-Head Network and Residual Network","description":"Deep dive into AlphaGo Zero\'s neural network architecture - shared backbone, Policy Head, Value Head, and 40-layer ResNet","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/for-engineers/how-it-works/alphago-explained/17-dual-head-resnet.mdx","sourceDirName":"for-engineers/how-it-works/alphago-explained","slug":"/for-engineers/how-it-works/alphago-explained/dual-head-resnet","permalink":"/en/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/17-dual-head-resnet.mdx","tags":[],"version":"current","sidebarPosition":18,"frontMatter":{"sidebar_position":18,"title":"Dual-Head Network and Residual Network","description":"Deep dive into AlphaGo Zero\'s neural network architecture - shared backbone, Policy Head, Value Head, and 40-layer ResNet","keywords":["dual-head network","residual network","ResNet","Policy Head","Value Head","deep learning","neural network architecture"]},"sidebar":"tutorialSidebar","previous":{"title":"AlphaGo Zero Overview","permalink":"/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero"},"next":{"title":"Training from Scratch","permalink":"/en/docs/for-engineers/how-it-works/alphago-explained/training-from-scratch"}}');var s=i(62615),t=i(30416);const l={sidebar_position:18,title:"Dual-Head Network and Residual Network",description:"Deep dive into AlphaGo Zero's neural network architecture - shared backbone, Policy Head, Value Head, and 40-layer ResNet",keywords:["dual-head network","residual network","ResNet","Policy Head","Value Head","deep learning","neural network architecture"]},a="Dual-Head Network and Residual Network",d={},o=[{value:"Dual-Head Network Design",id:"dual-head-network-design",level:2},{value:"Overall Architecture",id:"overall-architecture",level:3},{value:"Shared Backbone",id:"shared-backbone",level:3},{value:"Architecture Details",id:"architecture-details",level:4},{value:"Mathematical Representation",id:"mathematical-representation",level:4},{value:"Policy Head",id:"policy-head",level:3},{value:"Architecture Details",id:"architecture-details-1",level:4},{value:"Mathematical Representation",id:"mathematical-representation-1",level:4},{value:"Value Head",id:"value-head",level:3},{value:"Architecture Details",id:"architecture-details-2",level:4},{value:"Mathematical Representation",id:"mathematical-representation-2",level:4},{value:"Why Share the Backbone?",id:"why-share-the-backbone",level:2},{value:"Intuitive Understanding",id:"intuitive-understanding",level:3},{value:"Multi-task Learning Perspective",id:"multi-task-learning-perspective",level:3},{value:"1. Regularization Effect",id:"1-regularization-effect",level:4},{value:"2. Data Efficiency",id:"2-data-efficiency",level:4},{value:"3. Rich Gradient Signal",id:"3-rich-gradient-signal",level:4},{value:"Experimental Evidence",id:"experimental-evidence",level:3},{value:"Residual Network Principles",id:"residual-network-principles",level:2},{value:"The Deep Network Dilemma",id:"the-deep-network-dilemma",level:3},{value:"Residual Block Design",id:"residual-block-design",level:3},{value:"Mathematical Representation",id:"mathematical-representation-3",level:4},{value:"Why Skip Connections Work?",id:"why-skip-connections-work",level:3},{value:"1. Gradient Highway",id:"1-gradient-highway",level:4},{value:"2. Identity Mapping is Easier to Learn",id:"2-identity-mapping-is-easier-to-learn",level:4},{value:"3. Ensemble Effect",id:"3-ensemble-effect",level:4},{value:"ResNet&#39;s Breakthrough on ImageNet",id:"resnets-breakthrough-on-imagenet",level:3},{value:"AlphaGo Zero&#39;s 40-Layer ResNet",id:"alphago-zeros-40-layer-resnet",level:2},{value:"Why Choose 40 Layers?",id:"why-choose-40-layers",level:3},{value:"Specific Configuration",id:"specific-configuration",level:3},{value:"Parameter Count Estimate",id:"parameter-count-estimate",level:4},{value:"Role of Batch Normalization",id:"role-of-batch-normalization",level:3},{value:"1. Normalize Activations",id:"1-normalize-activations",level:4},{value:"2. Mitigate Internal Covariate Shift",id:"2-mitigate-internal-covariate-shift",level:4},{value:"3. Regularization Effect",id:"3-regularization-effect",level:4},{value:"Comparison with Other Architectures",id:"comparison-with-other-architectures",level:2},{value:"vs. Original AlphaGo&#39;s CNN",id:"vs-original-alphagos-cnn",level:3},{value:"vs. VGG-style Networks",id:"vs-vgg-style-networks",level:3},{value:"vs. Inception / GoogLeNet",id:"vs-inception--googlenet",level:3},{value:"vs. Transformer",id:"vs-transformer",level:3},{value:"Deep Analysis of Design Choices",id:"deep-analysis-of-design-choices",level:2},{value:"Why Use 3\xd73 Convolutions?",id:"why-use-33-convolutions",level:3},{value:"Why Use 256 Channels?",id:"why-use-256-channels",level:3},{value:"Why Does Policy Head Use Softmax, Value Head Use Tanh?",id:"why-does-policy-head-use-softmax-value-head-use-tanh",level:3},{value:"Policy Head: Softmax",id:"policy-head-softmax",level:4},{value:"Value Head: Tanh",id:"value-head-tanh",level:4},{value:"Training Details",id:"training-details",level:2},{value:"Loss Function",id:"loss-function",level:3},{value:"Policy Loss",id:"policy-loss",level:4},{value:"Value Loss",id:"value-loss",level:4},{value:"Regularization Loss",id:"regularization-loss",level:4},{value:"Optimizer Configuration",id:"optimizer-configuration",level:3},{value:"Data Augmentation",id:"data-augmentation",level:3},{value:"Implementation Considerations",id:"implementation-considerations",level:2},{value:"Memory Optimization",id:"memory-optimization",level:3},{value:"Inference Optimization",id:"inference-optimization",level:3},{value:"Quantization and Compression",id:"quantization-and-compression",level:3},{value:"Animation Reference",id:"animation-reference",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"References",id:"references",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"dual-head-network-and-residual-network",children:"Dual-Head Network and Residual Network"})}),"\n",(0,s.jsxs)(n.p,{children:["One of AlphaGo Zero's most important architectural innovations is using a ",(0,s.jsx)(n.strong,{children:"Dual-Head Network"})," to replace the original AlphaGo's dual-network design. This seemingly simple change brought significant performance improvements and a more elegant learning process."]}),"\n",(0,s.jsx)(n.p,{children:"This article will deeply analyze the design principles, mathematical foundations, and why this architecture is so effective."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"dual-head-network-design",children:"Dual-Head Network Design"}),"\n",(0,s.jsx)(n.h3,{id:"overall-architecture",children:"Overall Architecture"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero's neural network can be divided into three parts:"}),"\n",(0,s.jsx)(n.mermaid,{value:'flowchart TB\n    Input["Input (17 x 19 x 19)"]\n    Backbone["Shared Backbone (ResNet)<br/>40 residual blocks, 256 channels"]\n    PolicyHead["Policy Head"]\n    ValueHead["Value Head"]\n    PolicyOut["19x19 probability distribution<br/>+ 1 Pass probability"]\n    ValueOut["Win rate [-1, 1]"]\n\n    Input --\x3e Backbone\n    Backbone --\x3e PolicyHead\n    Backbone --\x3e ValueHead\n    PolicyHead --\x3e PolicyOut\n    ValueHead --\x3e ValueOut'}),"\n",(0,s.jsx)(n.p,{children:"Let's analyze each part in detail."}),"\n",(0,s.jsx)(n.h3,{id:"shared-backbone",children:"Shared Backbone"}),"\n",(0,s.jsxs)(n.p,{children:["The shared backbone is a deep ",(0,s.jsx)(n.strong,{children:"Residual Network (ResNet)"})," responsible for extracting features from the board state."]}),"\n",(0,s.jsx)(n.h4,{id:"architecture-details",children:"Architecture Details"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"Specification"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Input layer"}),(0,s.jsx)(n.td,{children:"3\xd73 convolution, 256 channels"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Residual blocks"}),(0,s.jsx)(n.td,{children:"40 (or 20 for compact version)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Each residual block"}),(0,s.jsx)(n.td,{children:"2 layers of 3\xd73 convolution, 256 channels"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Activation function"}),(0,s.jsx)(n.td,{children:"ReLU"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Normalization"}),(0,s.jsx)(n.td,{children:"Batch Normalization"})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"mathematical-representation",children:"Mathematical Representation"}),"\n",(0,s.jsx)(n.p,{children:"Let input be x (dimension 17 \xd7 19 \xd7 19), the shared backbone output is:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"f(x) = ResNet_40(Conv_3x3(x))\n"})}),"\n",(0,s.jsx)(n.p,{children:"where f(x) (dimension 256 \xd7 19 \xd7 19) is a high-dimensional feature representation."}),"\n",(0,s.jsx)(n.h3,{id:"policy-head",children:"Policy Head"}),"\n",(0,s.jsx)(n.p,{children:"The Policy Head is responsible for predicting the move probability for each position."}),"\n",(0,s.jsx)(n.h4,{id:"architecture-details-1",children:"Architecture Details"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Shared backbone output (256 \xd7 19 \xd7 19)\n       \u2193\n1\xd71 convolution (2 channels)\n       \u2193\nBatch Normalization\n       \u2193\nReLU\n       \u2193\nFlatten (2 \xd7 19 \xd7 19 = 722)\n       \u2193\nFully connected layer (362)\n       \u2193\nSoftmax\n       \u2193\nOutput: 362 probabilities (361 positions + Pass)\n"})}),"\n",(0,s.jsx)(n.h4,{id:"mathematical-representation-1",children:"Mathematical Representation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u03c0 = Softmax(FC(Flatten(ReLU(BN(Conv_1x1(f(x)))))))\n"})}),"\n",(0,s.jsx)(n.p,{children:"Output \u03c0 is a 362-dimensional vector where all elements are non-negative and sum to 1."}),"\n",(0,s.jsx)(n.h3,{id:"value-head",children:"Value Head"}),"\n",(0,s.jsx)(n.p,{children:"The Value Head is responsible for predicting the win rate of the current position."}),"\n",(0,s.jsx)(n.h4,{id:"architecture-details-2",children:"Architecture Details"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Shared backbone output (256 \xd7 19 \xd7 19)\n       \u2193\n1\xd71 convolution (1 channel)\n       \u2193\nBatch Normalization\n       \u2193\nReLU\n       \u2193\nFlatten (1 \xd7 19 \xd7 19 = 361)\n       \u2193\nFully connected layer (256)\n       \u2193\nReLU\n       \u2193\nFully connected layer (1)\n       \u2193\nTanh\n       \u2193\nOutput: Win rate [-1, 1]\n"})}),"\n",(0,s.jsx)(n.h4,{id:"mathematical-representation-2",children:"Mathematical Representation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"v = Tanh(FC_1(ReLU(FC_2(Flatten(ReLU(BN(Conv_1x1(f(x)))))))))\n"})}),"\n",(0,s.jsx)(n.p,{children:"Output v ranges from [-1, 1]:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"v = 1: Current side will definitely win"}),"\n",(0,s.jsx)(n.li,{children:"v = -1: Current side will definitely lose"}),"\n",(0,s.jsx)(n.li,{children:"v = 0: Even game"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"why-share-the-backbone",children:"Why Share the Backbone?"}),"\n",(0,s.jsx)(n.h3,{id:"intuitive-understanding",children:"Intuitive Understanding"}),"\n",(0,s.jsx)(n.p,{children:'The two questions "where should the next move be" (Policy) and "who will win" (Value) actually require understanding the same board patterns:'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Shapes"}),": Which shapes are good, which are bad"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Influence"}),": Which side is bigger, where there's still space"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Life and death"}),": Which groups are alive, which are still in ko"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fighting"}),": Where are the attacks, what's the local outcome"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"If using two independent networks, these features need to be learned twice. A shared backbone lets these underlying features be learned once and used by both tasks."}),"\n",(0,s.jsx)(n.h3,{id:"multi-task-learning-perspective",children:"Multi-task Learning Perspective"}),"\n",(0,s.jsxs)(n.p,{children:["From a machine learning perspective, this is a form of ",(0,s.jsx)(n.strong,{children:"Multi-task Learning"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"L = L_policy + L_value\n"})}),"\n",(0,s.jsx)(n.p,{children:"Two tasks sharing the underlying representation brings several benefits:"}),"\n",(0,s.jsx)(n.h4,{id:"1-regularization-effect",children:"1. Regularization Effect"}),"\n",(0,s.jsx)(n.p,{children:"Shared parameters act as implicit regularization. If a feature is only useful for Policy but not Value (or vice versa), it's harder to be overly amplified."}),"\n",(0,s.jsx)(n.p,{children:"The effective parameter count is smaller than two independent networks."}),"\n",(0,s.jsx)(n.h4,{id:"2-data-efficiency",children:"2. Data Efficiency"}),"\n",(0,s.jsx)(n.p,{children:"Each game simultaneously produces Policy labels (MCTS search probabilities) and Value labels (final outcome). The shared backbone uses both labels to train shared features, improving data utilization efficiency."}),"\n",(0,s.jsx)(n.h4,{id:"3-rich-gradient-signal",children:"3. Rich Gradient Signal"}),"\n",(0,s.jsx)(n.p,{children:"Gradients from both tasks flow to the shared backbone:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u2202L/\u2202\u03b8_shared = \u2202L_policy/\u2202\u03b8_shared + \u2202L_value/\u2202\u03b8_shared\n"})}),"\n",(0,s.jsx)(n.p,{children:"This provides richer supervision signal, making shared features more robust."}),"\n",(0,s.jsx)(n.h3,{id:"experimental-evidence",children:"Experimental Evidence"}),"\n",(0,s.jsx)(n.p,{children:"DeepMind's ablation experiments showed that dual-head networks significantly outperform separate dual networks:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Configuration"}),(0,s.jsx)(n.th,{children:"ELO Rating"}),(0,s.jsx)(n.th,{children:"Relative Gap"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Separate Policy + Value networks"}),(0,s.jsx)(n.td,{children:"Baseline"}),(0,s.jsx)(n.td,{children:"-"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Dual-head network (shared backbone)"}),(0,s.jsx)(n.td,{children:"+300 ELO"}),(0,s.jsx)(n.td,{children:"~65% win rate difference"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"A 300 ELO gap means the dual-head network has about 65% win rate against separated networks. This is a significant improvement."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"residual-network-principles",children:"Residual Network Principles"}),"\n",(0,s.jsx)(n.h3,{id:"the-deep-network-dilemma",children:"The Deep Network Dilemma"}),"\n",(0,s.jsx)(n.p,{children:"Before ResNet was invented, deep neural networks faced a paradox:"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"In theory, deeper networks should be at least as good as shallow networks (in the worst case, extra layers can learn identity mapping). But in practice, deeper networks often performed worse."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["This is the ",(0,s.jsx)(n.strong,{children:"Degradation Problem"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Training error increases with depth (not overfitting, but optimization difficulty)"}),"\n",(0,s.jsx)(n.li,{children:"Gradients vanish during backpropagation (Vanishing Gradient)"}),"\n",(0,s.jsx)(n.li,{children:"Parameters in deep layers can hardly be effectively updated"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"residual-block-design",children:"Residual Block Design"}),"\n",(0,s.jsxs)(n.p,{children:["In 2015, Kaiming He and colleagues proposed a simple and elegant solution: ",(0,s.jsx)(n.strong,{children:"Skip Connection"}),"."]}),"\n",(0,s.jsx)(n.mermaid,{value:'flowchart TB\n    X["Input x"]\n    Conv["Conv layer<br/>BN + ReLU<br/>Conv layer<br/>BN"]\n    Add["+"]\n    ReLU["ReLU"]\n    Out["Output x + F(x)"]\n\n    X --\x3e Conv\n    Conv --\x3e|"F(x)"| Add\n    X --\x3e|"skip connection"| Add\n    Add --\x3e ReLU --\x3e Out'}),"\n",(0,s.jsx)(n.h4,{id:"mathematical-representation-3",children:"Mathematical Representation"}),"\n",(0,s.jsx)(n.p,{children:"Traditional network: Learn target mapping H(x)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"y = H(x)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Residual network: Learn ",(0,s.jsx)(n.strong,{children:"residual mapping"})," F(x) = H(x) - x"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"y = F(x) + x\n"})}),"\n",(0,s.jsx)(n.h3,{id:"why-skip-connections-work",children:"Why Skip Connections Work?"}),"\n",(0,s.jsx)(n.h4,{id:"1-gradient-highway",children:"1. Gradient Highway"}),"\n",(0,s.jsx)(n.p,{children:"Consider the backpropagation gradient:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u2202L/\u2202x = \u2202L/\u2202y \xd7 \u2202y/\u2202x = \u2202L/\u2202y \xd7 (1 + \u2202F(x)/\u2202x)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The key is that ",(0,s.jsx)(n.strong,{children:"+1"}),". Even if \u2202F(x)/\u2202x is very small or zero, gradients can still flow directly back through +1."]}),"\n",(0,s.jsx)(n.p,{children:'It\'s like building a "gradient highway" that lets gradients flow smoothly from output layer back to input layer.'}),"\n",(0,s.jsx)(n.h4,{id:"2-identity-mapping-is-easier-to-learn",children:"2. Identity Mapping is Easier to Learn"}),"\n",(0,s.jsx)(n.p,{children:"If the optimal solution is close to identity mapping (H(x) \u2248 x), then:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Traditional network: Needs to learn H(x) = x, which can be difficult"}),"\n",(0,s.jsx)(n.li,{children:"Residual network: Just needs to learn F(x) \u2248 0, relatively easy"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Initializing weights to zero or near zero, residual blocks naturally tend toward identity mapping."}),"\n",(0,s.jsx)(n.h4,{id:"3-ensemble-effect",children:"3. Ensemble Effect"}),"\n",(0,s.jsxs)(n.p,{children:["Deep ResNet can be viewed as an ",(0,s.jsx)(n.strong,{children:"implicit ensemble"})," of many shallow networks. With n residual blocks, information can flow through 2^n different paths."]}),"\n",(0,s.jsx)(n.p,{children:"This ensemble effect increases model robustness."}),"\n",(0,s.jsx)(n.h3,{id:"resnets-breakthrough-on-imagenet",children:"ResNet's Breakthrough on ImageNet"}),"\n",(0,s.jsx)(n.p,{children:"ResNet achieved stunning results in the 2015 ImageNet competition:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Depth"}),(0,s.jsx)(n.th,{children:"Top-5 Error Rate"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"VGG-19 (no residual)"}),(0,s.jsx)(n.td,{children:"7.3%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"ResNet-34"}),(0,s.jsx)(n.td,{children:"5.7%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"ResNet-152"}),(0,s.jsx)(n.td,{children:"4.5%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Human level"}),(0,s.jsx)(n.td,{children:"~5.1%"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"152-layer"})," ResNet not only trains successfully but also performs much better than 19-layer VGG. This proves skip connections truly solve the deep network training problem."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"alphago-zeros-40-layer-resnet",children:"AlphaGo Zero's 40-Layer ResNet"}),"\n",(0,s.jsx)(n.h3,{id:"why-choose-40-layers",children:"Why Choose 40 Layers?"}),"\n",(0,s.jsx)(n.p,{children:"DeepMind tested ResNets of different depths:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Residual Blocks"}),(0,s.jsx)(n.th,{children:"Total Layers"}),(0,s.jsx)(n.th,{children:"ELO Rating"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"5"}),(0,s.jsx)(n.td,{children:"11"}),(0,s.jsx)(n.td,{children:"Baseline"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"10"}),(0,s.jsx)(n.td,{children:"21"}),(0,s.jsx)(n.td,{children:"+200"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"20"}),(0,s.jsx)(n.td,{children:"41"}),(0,s.jsx)(n.td,{children:"+400"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"40"}),(0,s.jsx)(n.td,{children:"81"}),(0,s.jsx)(n.td,{children:"+500"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"Deeper networks are indeed stronger, but with diminishing returns. AlphaGo Zero uses 20 or 40 residual blocks:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AlphaGo Zero (paper version)"}),": 40 residual blocks, 256 channels"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compact version"}),": 20 residual blocks, 256 channels"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The 40-layer configuration achieves a good balance between playing strength and training cost."}),"\n",(0,s.jsx)(n.h3,{id:"specific-configuration",children:"Specific Configuration"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero's ResNet configuration:"}),"\n",(0,s.jsx)(n.mermaid,{value:'flowchart TB\n    Input["Input: 17 x 19 x 19"]\n    Conv1["Conv layer: 3x3, 256 channels, BN, ReLU"]\n    ResBlock["Residual block x40:<br/>Conv 3x3, 256 ch, BN, ReLU<br/>Conv 3x3, 256 ch, BN<br/>Skip connection + ReLU"]\n    Heads["Policy Head / Value Head"]\n\n    Input --\x3e Conv1 --\x3e ResBlock --\x3e Heads'}),"\n",(0,s.jsx)(n.h4,{id:"parameter-count-estimate",children:"Parameter Count Estimate"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"Parameters (approx.)"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Input convolution"}),(0,s.jsx)(n.td,{children:"17 \xd7 3 \xd7 3 \xd7 256 \u2248 39K"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Each residual block"}),(0,s.jsx)(n.td,{children:"2 \xd7 256 \xd7 3 \xd7 3 \xd7 256 \u2248 1.2M"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"40 residual blocks"}),(0,s.jsx)(n.td,{children:"40 \xd7 1.2M \u2248 47M"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Policy Head"}),(0,s.jsx)(n.td,{children:"~1M"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Value Head"}),(0,s.jsx)(n.td,{children:"~0.2M"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Total"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"~48M"})})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"About 48 million parameters, a medium-sized neural network by modern standards."}),"\n",(0,s.jsx)(n.h3,{id:"role-of-batch-normalization",children:"Role of Batch Normalization"}),"\n",(0,s.jsxs)(n.p,{children:["Every convolutional layer is followed by ",(0,s.jsx)(n.strong,{children:"Batch Normalization (BN)"}),", which is crucial for training stability:"]}),"\n",(0,s.jsx)(n.h4,{id:"1-normalize-activations",children:"1. Normalize Activations"}),"\n",(0,s.jsx)(n.p,{children:"BN normalizes each layer's activations to mean 0 and variance 1:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"x_hat = (x - \u03bc_B) / sqrt(\u03c3_B\xb2 + \u03b5)\ny = \u03b3 \xd7 x_hat + \u03b2\n"})}),"\n",(0,s.jsx)(n.p,{children:"where \u03b3 and \u03b2 are learnable parameters."}),"\n",(0,s.jsx)(n.h4,{id:"2-mitigate-internal-covariate-shift",children:"2. Mitigate Internal Covariate Shift"}),"\n",(0,s.jsx)(n.p,{children:"In deep networks, each layer's input distribution changes as parameters in previous layers update. BN keeps each layer's input distribution stable, accelerating training convergence."}),"\n",(0,s.jsx)(n.h4,{id:"3-regularization-effect",children:"3. Regularization Effect"}),"\n",(0,s.jsx)(n.p,{children:"BN uses mini-batch statistics during training, introducing randomness and providing a mild regularization effect."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"comparison-with-other-architectures",children:"Comparison with Other Architectures"}),"\n",(0,s.jsx)(n.h3,{id:"vs-original-alphagos-cnn",children:"vs. Original AlphaGo's CNN"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"Original AlphaGo"}),(0,s.jsx)(n.th,{children:"AlphaGo Zero"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Architecture type"}),(0,s.jsx)(n.td,{children:"Standard CNN"}),(0,s.jsx)(n.td,{children:"ResNet"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Depth"}),(0,s.jsx)(n.td,{children:"13 layers"}),(0,s.jsx)(n.td,{children:"41-81 layers"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Skip connections"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"Yes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Number of networks"}),(0,s.jsx)(n.td,{children:"2 (separate)"}),(0,s.jsx)(n.td,{children:"1 (shared)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BN"}),(0,s.jsx)(n.td,{children:"No"}),(0,s.jsx)(n.td,{children:"Yes"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"vs-vgg-style-networks",children:"vs. VGG-style Networks"}),"\n",(0,s.jsx)(n.p,{children:"VGG was the runner-up architecture in 2014 ImageNet, using stacked 3\xd73 convolutions:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"VGG"}),(0,s.jsx)(n.th,{children:"ResNet"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Maximum trainable depth"}),(0,s.jsx)(n.td,{children:"~19 layers"}),(0,s.jsx)(n.td,{children:"152+ layers"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Gradient flow"}),(0,s.jsx)(n.td,{children:"Decreases layer by layer"}),(0,s.jsx)(n.td,{children:"Has highway"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Training difficulty"}),(0,s.jsx)(n.td,{children:"Deep is difficult"}),(0,s.jsx)(n.td,{children:"Deep is trainable"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"vs-inception--googlenet",children:"vs. Inception / GoogLeNet"}),"\n",(0,s.jsx)(n.p,{children:"Inception uses multi-scale convolutions in parallel:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"Inception"}),(0,s.jsx)(n.th,{children:"ResNet"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Characteristic"}),(0,s.jsx)(n.td,{children:"Multi-scale features"}),(0,s.jsx)(n.td,{children:"Deep stacking"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Complexity"}),(0,s.jsx)(n.td,{children:"Higher"}),(0,s.jsx)(n.td,{children:"Simple"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Go suitability"}),(0,s.jsx)(n.td,{children:"Average"}),(0,s.jsx)(n.td,{children:"Excellent"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"ResNet's simple design is more suitable for tasks like Go that require deep reasoning."}),"\n",(0,s.jsx)(n.h3,{id:"vs-transformer",children:"vs. Transformer"}),"\n",(0,s.jsx)(n.p,{children:"The Transformer architecture proposed in 2017 achieved great success in NLP. Some have attempted to apply Transformers to Go:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"ResNet"}),(0,s.jsx)(n.th,{children:"Transformer"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Inductive bias"}),(0,s.jsx)(n.td,{children:"Locality (convolution)"}),(0,s.jsx)(n.td,{children:"Global attention"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Position encoding"}),(0,s.jsx)(n.td,{children:"Implicit (convolution)"}),(0,s.jsx)(n.td,{children:"Explicit"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Go performance"}),(0,s.jsx)(n.td,{children:"Excellent"}),(0,s.jsx)(n.td,{children:"Feasible but not better than ResNet"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Computational efficiency"}),(0,s.jsx)(n.td,{children:"Higher"}),(0,s.jsx)(n.td,{children:"Lower (O(n\xb2))"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"For problems with obvious spatial structure like Go, CNN/ResNet's inductive bias is more appropriate."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"deep-analysis-of-design-choices",children:"Deep Analysis of Design Choices"}),"\n",(0,s.jsx)(n.h3,{id:"why-use-33-convolutions",children:"Why Use 3\xd73 Convolutions?"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero uses 3\xd73 convolutions throughout, rather than larger kernels:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parameter efficiency"}),": Two 3\xd73 convolutions have the same receptive field as one 5\xd75, but fewer parameters (18 vs 25)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deeper networks"}),": Same parameter count allows stacking more layers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"More nonlinearity"}),": ReLU between each layer increases expressiveness"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"why-use-256-channels",children:"Why Use 256 Channels?"}),"\n",(0,s.jsx)(n.p,{children:"256 channels is an empirical choice:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Too few"})," (like 64): Insufficient expressiveness, can't capture complex patterns"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Too many"})," (like 512): Parameter count doubles, training cost increases greatly, but strength improvement is limited"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Later KataGo experiments showed channel count can be adjusted based on training resources:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Low resources: 128 channels, 20 blocks"}),"\n",(0,s.jsx)(n.li,{children:"High resources: 256 channels, 40 blocks"}),"\n",(0,s.jsx)(n.li,{children:"Higher resources: 384 channels, 60 blocks"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"why-does-policy-head-use-softmax-value-head-use-tanh",children:"Why Does Policy Head Use Softmax, Value Head Use Tanh?"}),"\n",(0,s.jsx)(n.h4,{id:"policy-head-softmax",children:"Policy Head: Softmax"}),"\n",(0,s.jsxs)(n.p,{children:["Move selection is a ",(0,s.jsx)(n.strong,{children:"classification problem"})," - choosing one from 361 positions (plus Pass). Softmax output satisfies:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"All probabilities non-negative: \u03c0_i >= 0"}),"\n",(0,s.jsx)(n.li,{children:"Probabilities sum to 1: \u03a3\u03c0_i = 1"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This matches the definition of a probability distribution."}),"\n",(0,s.jsx)(n.h4,{id:"value-head-tanh",children:"Value Head: Tanh"}),"\n",(0,s.jsxs)(n.p,{children:["Win rate is a ",(0,s.jsx)(n.strong,{children:"regression problem"})," - predicting a continuous value. Tanh output range is [-1, 1]:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Bounded: Won't produce extreme values"}),"\n",(0,s.jsx)(n.li,{children:"Symmetric: Treats wins and losses symmetrically"}),"\n",(0,s.jsx)(n.li,{children:"Differentiable: Convenient for gradient computation"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Using Tanh instead of unbounded output (like a linear layer) prevents training instability."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"training-details",children:"Training Details"}),"\n",(0,s.jsx)(n.h3,{id:"loss-function",children:"Loss Function"}),"\n",(0,s.jsx)(n.p,{children:"AlphaGo Zero's total loss is the sum of three terms:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"L = L_policy + L_value + L_reg\n"})}),"\n",(0,s.jsx)(n.h4,{id:"policy-loss",children:"Policy Loss"}),"\n",(0,s.jsxs)(n.p,{children:["Uses ",(0,s.jsx)(n.strong,{children:"cross-entropy loss"})," to make network output approach MCTS search probabilities:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"L_policy = -\u03a3 \u03c0_MCTS(a) \xd7 log(\u03c0_net(a))\n"})}),"\n",(0,s.jsx)(n.p,{children:"where:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u03c0_MCTS(a) is MCTS search probability for action a"}),"\n",(0,s.jsx)(n.li,{children:"\u03c0_net(a) is network output probability"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"value-loss",children:"Value Loss"}),"\n",(0,s.jsxs)(n.p,{children:["Uses ",(0,s.jsx)(n.strong,{children:"Mean Squared Error (MSE)"})," to make network output approach actual game outcome:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"L_value = (v_net - z)\xb2\n"})}),"\n",(0,s.jsx)(n.p,{children:"where:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"v_net is network predicted win rate"}),"\n",(0,s.jsx)(n.li,{children:"z is actual game result (+1 or -1)"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"regularization-loss",children:"Regularization Loss"}),"\n",(0,s.jsxs)(n.p,{children:["Uses ",(0,s.jsx)(n.strong,{children:"L2 regularization"})," to prevent overfitting:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"L_reg = c \xd7 ||\u03b8||\xb2\n"})}),"\n",(0,s.jsx)(n.p,{children:"where c is regularization coefficient and \u03b8 is network parameters."}),"\n",(0,s.jsx)(n.h3,{id:"optimizer-configuration",children:"Optimizer Configuration"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Value"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Optimizer"}),(0,s.jsx)(n.td,{children:"SGD + Momentum"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Momentum"}),(0,s.jsx)(n.td,{children:"0.9"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Initial learning rate"}),(0,s.jsx)(n.td,{children:"0.01"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Learning rate decay"}),(0,s.jsx)(n.td,{children:"Halve every X steps"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Batch Size"}),(0,s.jsx)(n.td,{children:"32 \xd7 2048 = 64K (distributed)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"L2 regularization coefficient"}),(0,s.jsx)(n.td,{children:"1e-4"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"data-augmentation",children:"Data Augmentation"}),"\n",(0,s.jsx)(n.p,{children:"The Go board has 8-fold symmetry (4 rotations \xd7 2 flips). During training, each position can produce 8 equivalent training samples."}),"\n",(0,s.jsx)(n.p,{children:"This increases effective training data 8-fold without additional self-play."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"implementation-considerations",children:"Implementation Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"memory-optimization",children:"Memory Optimization"}),"\n",(0,s.jsx)(n.p,{children:"Training a 40-layer ResNet requires substantial memory:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Forward pass"}),": Need to store activations from each layer (for backpropagation)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Backward pass"}),": Need to store gradients"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Optimization strategies:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gradient Checkpointing"}),": Only store some activations, recompute when needed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mixed precision training"}),": Use FP16 to reduce memory footprint"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distributed training"}),": Distribute batch across multiple GPUs/TPUs"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"inference-optimization",children:"Inference Optimization"}),"\n",(0,s.jsx)(n.p,{children:"During inference, BN doesn't need mini-batch statistics; it can use moving averages accumulated during training:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"x_hat = (x - \u03bc_moving) / sqrt(\u03c3_moving\xb2 + \u03b5)\n"})}),"\n",(0,s.jsx)(n.p,{children:"This makes inference faster and deterministic."}),"\n",(0,s.jsx)(n.h3,{id:"quantization-and-compression",children:"Quantization and Compression"}),"\n",(0,s.jsx)(n.p,{children:"Networks can be further compressed for deployment:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Weight quantization"}),": FP32 \u2192 INT8, 4\xd7 memory reduction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pruning"}),": Remove small weight connections"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Knowledge distillation"}),": Train small network using large network"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"animation-reference",children:"Animation Reference"}),"\n",(0,s.jsx)(n.p,{children:"Core concepts covered in this article with animation numbers:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Number"}),(0,s.jsx)(n.th,{children:"Concept"}),(0,s.jsx)(n.th,{children:"Physics/Math Correspondence"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Animation E3"}),(0,s.jsx)(n.td,{children:"Dual-head network"}),(0,s.jsx)(n.td,{children:"Multi-task learning"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Animation D12"}),(0,s.jsx)(n.td,{children:"Skip connections"}),(0,s.jsx)(n.td,{children:"Gradient highway"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Animation D8"}),(0,s.jsx)(n.td,{children:"CNN"}),(0,s.jsx)(n.td,{children:"Local receptive field"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Animation D10"}),(0,s.jsx)(n.td,{children:"Batch Normalization"}),(0,s.jsx)(n.td,{children:"Distribution normalization"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Previous"}),": ",(0,s.jsx)(n.a,{href:"../alphago-zero",children:"AlphaGo Zero Overview"})," - Why human game records aren't needed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Next"}),": ",(0,s.jsx)(n.a,{href:"../training-from-scratch",children:"Training from Scratch"})," - Detailed Day 0-3 evolution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Technical Deep Dive"}),": ",(0,s.jsx)(n.a,{href:"../cnn-and-go",children:"CNN and Go"})," - Why CNN suits the board"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:['Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." ',(0,s.jsx)(n.em,{children:"Nature"}),", 550, 354-359."]}),"\n",(0,s.jsxs)(n.li,{children:['He, K., et al. (2016). "Deep Residual Learning for Image Recognition." ',(0,s.jsx)(n.em,{children:"CVPR 2016"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:['Ioffe, S., & Szegedy, C. (2015). "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift." ',(0,s.jsx)(n.em,{children:"ICML 2015"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:['Caruana, R. (1997). "Multitask Learning." ',(0,s.jsx)(n.em,{children:"Machine Learning"}),", 28(1), 41-75."]}),"\n",(0,s.jsxs)(n.li,{children:['Veit, A., et al. (2016). "Residual Networks Behave Like Ensembles of Relatively Shallow Networks." ',(0,s.jsx)(n.em,{children:"NeurIPS 2016"}),"."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},30416(e,n,i){i.d(n,{R:()=>l,x:()=>a});var r=i(59471);const s={},t=r.createContext(s);function l(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);