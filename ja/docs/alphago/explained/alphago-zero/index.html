<!doctype html>
<html lang="ja" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-alphago/explained/alphago-zero" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">AlphaGo Zero 概要 | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/ja/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/ja/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/ja/docs/alphago/explained/alphago-zero/"><meta data-rh="true" property="og:locale" content="ja"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="ja"><meta data-rh="true" name="docsearch:language" content="ja"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="AlphaGo Zero 概要 | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="ゼロから始まり、完全に自己学習。AlphaGo Zero は人間の棋譜なしでどのようにして全ての前バージョンを超えたのか"><meta data-rh="true" property="og:description" content="ゼロから始まり、完全に自己学習。AlphaGo Zero は人間の棋譜なしでどのようにして全ての前バージョンを超えたのか"><meta data-rh="true" name="keywords" content="AlphaGo Zero,自己対戦,強化学習,深層学習,囲碁AI,教師なし学習"><link data-rh="true" rel="icon" href="/ja/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/ja/docs/alphago/explained/alphago-zero/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/explained/alphago-zero/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/alphago/explained/alphago-zero/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/alphago/explained/alphago-zero/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/alphago/explained/alphago-zero/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/alphago/explained/alphago-zero/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/alphago/explained/alphago-zero/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/alphago/explained/alphago-zero/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/alphago/explained/alphago-zero/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/alphago/explained/alphago-zero/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/alphago/explained/alphago-zero/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/alphago/explained/alphago-zero/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/explained/alphago-zero/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AlphaGo","item":"https://www.weiqi.kids/ja/docs/alphago/"},{"@type":"ListItem","position":2,"name":"完整解析","item":"https://www.weiqi.kids/ja/docs/alphago/explained/"},{"@type":"ListItem","position":3,"name":"AlphaGo Zero 概要","item":"https://www.weiqi.kids/ja/docs/alphago/explained/alphago-zero"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/ja/assets/css/styles.f23bf74b.css">
<script src="/ja/assets/js/runtime~main.a1068d2c.js" defer="defer"></script>
<script src="/ja/assets/js/main.d9aeac32.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ja/img/logo.svg"><div role="region" aria-label="メインコンテンツまでスキップ"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">メインコンテンツまでスキップ</a></div><nav aria-label="ナビゲーション" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="ナビゲーションバーを開く" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ja/"><div class="navbar__logo"><img src="/ja/img/logo.svg" alt="好棋宝宝協会ロゴ" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/ja/img/logo.svg" alt="好棋宝宝協会ロゴ" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">囲碁キッズ</b></a><a class="navbar__item navbar__link" href="/ja/docs/learn/">囲碁を学ぶ</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ja/docs/alphago/">AlphaGo</a><a class="navbar__item navbar__link" href="/ja/docs/animations/">アニメ教室</a><a class="navbar__item navbar__link" href="/ja/docs/tech/">技術ドキュメント</a><a class="navbar__item navbar__link" href="/ja/docs/about/">私たちについて</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>日本語</a><ul class="dropdown__menu"><li><a href="/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="ja">日本語</a></li><li><a href="/ko/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="検索" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="先頭へ戻る" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="ドキュメントのサイドバー" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ja/docs/intro/"><span title="ご利用ガイド" class="linkLabel_REp1">ご利用ガイド</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/ja/docs/learn/"><span title="學圍棋" class="categoryLinkLabel_ezQx">學圍棋</span></a><button aria-label="&#x27;學圍棋&#x27;の目次を開く" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/ja/docs/alphago/"><span title="AlphaGo" class="categoryLinkLabel_ezQx">AlphaGo</span></a><button aria-label="&#x27;AlphaGo&#x27;の目次を隠す" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ja/docs/alphago/explained/"><span title="完整解析" class="categoryLinkLabel_ezQx">完整解析</span></a><button aria-label="&#x27;完整解析&#x27;の目次を隠す" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/birth-of-alphago/"><span title="AlphaGo の誕生" class="linkLabel_REp1">AlphaGo の誕生</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/key-matches/"><span title="重要対局の回顧" class="linkLabel_REp1">重要対局の回顧</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/move-37/"><span title="「神の一手」徹底分析" class="linkLabel_REp1">「神の一手」徹底分析</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/why-go-is-hard/"><span title="囲碁はなぜ難しいのか？" class="linkLabel_REp1">囲碁はなぜ難しいのか？</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/traditional-limits/"><span title="従来手法の限界" class="linkLabel_REp1">従来手法の限界</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/board-representation/"><span title="盤面状態の表現" class="linkLabel_REp1">盤面状態の表現</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/policy-network/"><span title="Policy Network 詳解" class="linkLabel_REp1">Policy Network 詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/value-network/"><span title="Value Network 詳解" class="linkLabel_REp1">Value Network 詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/input-features/"><span title="入力特徴量の設計" class="linkLabel_REp1">入力特徴量の設計</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/cnn-and-go/"><span title="CNNと囲碁の融合" class="linkLabel_REp1">CNNと囲碁の融合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/supervised-learning/"><span title="教師あり学習フェーズ" class="linkLabel_REp1">教師あり学習フェーズ</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/reinforcement-intro/"><span title="強化学習入門" class="linkLabel_REp1">強化学習入門</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/self-play/"><span title="自己対局" class="linkLabel_REp1">自己対局</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/mcts-neural-combo/"><span title="MCTS とニューラルネットワークの融合" class="linkLabel_REp1">MCTS とニューラルネットワークの融合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/puct-formula/"><span title="PUCT 公式詳解" class="linkLabel_REp1">PUCT 公式詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ja/docs/alphago/explained/alphago-zero/"><span title="AlphaGo Zero 概要" class="linkLabel_REp1">AlphaGo Zero 概要</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/dual-head-resnet/"><span title="デュアルヘッドネットワークと残差ネットワーク" class="linkLabel_REp1">デュアルヘッドネットワークと残差ネットワーク</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/training-from-scratch/"><span title="ゼロからの学習過程" class="linkLabel_REp1">ゼロからの学習過程</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/distributed-systems/"><span title="分散システムと TPU" class="linkLabel_REp1">分散システムと TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ja/docs/alphago/explained/legacy-and-impact/"><span title="AlphaGo の遺産" class="linkLabel_REp1">AlphaGo の遺産</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ja/docs/animations/"><span title="動畫教室" class="linkLabel_REp1">動畫教室</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/ja/docs/tech/"><span title="技術文件" class="categoryLinkLabel_ezQx">技術文件</span></a><button aria-label="&#x27;技術文件&#x27;の目次を開く" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/ja/docs/about/"><span title="關於我們" class="categoryLinkLabel_ezQx">關於我們</span></a><button aria-label="&#x27;關於我們&#x27;の目次を開く" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="パンくずリストのナビゲーション"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="ホームページ" class="breadcrumbs__link" href="/ja/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ja/docs/alphago/"><span>AlphaGo</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ja/docs/alphago/explained/"><span>完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">AlphaGo Zero 概要</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">このページの見出し</button></div><div class="theme-doc-markdown markdown"><header><h1>AlphaGo Zero 概要</h1></header>
<p>2017年10月、DeepMind は AI 界を震撼させる成果を発表しました。<strong>AlphaGo Zero</strong> は人間の棋譜を一切使用せず、完全にランダムな状態から訓練を開始し、わずか3日でイ・セドル九段を破った元祖 AlphaGo を超え、<strong>100:0</strong> のスコアで完勝しました。</p>
<p>これは単なる数字上の進歩ではありません。これは全く新しいパラダイムを意味します：<strong>AI は人間の知識なしで、ゼロから全てを発見できる</strong>。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="なぜ人間の棋譜は不要なのか">なぜ人間の棋譜は不要なのか？<a href="#なぜ人間の棋譜は不要なのか" class="hash-link" aria-label="なぜ人間の棋譜は不要なのか？ への直接リンク" title="なぜ人間の棋譜は不要なのか？ への直接リンク" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="人間の棋譜の限界">人間の棋譜の限界<a href="#人間の棋譜の限界" class="hash-link" aria-label="人間の棋譜の限界 への直接リンク" title="人間の棋譜の限界 への直接リンク" translate="no">​</a></h3>
<p>元祖 AlphaGo の訓練プロセスは2つの段階に分かれていました：</p>
<ol>
<li class=""><strong>教師あり学習</strong>：3000万局の人間の棋譜で Policy Network を訓練</li>
<li class=""><strong>強化学習</strong>：自己対戦を通じてさらに向上</li>
</ol>
<p>この方法にはいくつかの根本的な問題があります：</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-人間の棋譜には上限がある">1. 人間の棋譜には上限がある<a href="#1-人間の棋譜には上限がある" class="hash-link" aria-label="1. 人間の棋譜には上限がある への直接リンク" title="1. 人間の棋譜には上限がある への直接リンク" translate="no">​</a></h4>
<p>人間の棋士の棋力には限界があり、棋譜に含まれるのは人間の理解であると同時に、人間の誤りや偏見も含まれます。AI が人間の棋譜から学習する時、学ぶのは：</p>
<ul>
<li class="">人間が良いと考える手法（しかし最適とは限らない）</li>
<li class="">人間の思考パターン（しかしイノベーションを制限する可能性がある）</li>
<li class="">人間の誤り（正しいサンプルとして学習されてしまう）</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-教師あり学習のボトルネック">2. 教師あり学習のボトルネック<a href="#2-教師あり学習のボトルネック" class="hash-link" aria-label="2. 教師あり学習のボトルネック への直接リンク" title="2. 教師あり学習のボトルネック への直接リンク" translate="no">​</a></h4>
<p>教師あり学習の目標は「人間を模倣する」こと——人間の棋士がどこに打つかを予測することです。これは AI の能力上限が人間の棋士の能力によって制限されることを意味します。</p>
<p>師匠を模倣するだけの弟子が、決して師匠を超えられないのと同じです。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-データ収集コスト">3. データ収集コスト<a href="#3-データ収集コスト" class="hash-link" aria-label="3. データ収集コスト への直接リンク" title="3. データ収集コスト への直接リンク" translate="no">​</a></h4>
<p>高品質な人間の棋譜は何年もの蓄積が必要であり、囲碁のような長い歴史を持つゲームにのみ存在します。AI を新しい分野（例えばタンパク質構造予測）に応用したい場合、「人間の専門家の棋譜」はそもそも存在しません。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="zero-の突破">Zero の突破<a href="#zero-の突破" class="hash-link" aria-label="Zero の突破 への直接リンク" title="Zero の突破 への直接リンク" translate="no">​</a></h3>
<p>AlphaGo Zero は教師あり学習の段階を完全にスキップし、<strong>ランダム初期化</strong>から直接自己対戦を開始しました。これは上記の全ての問題を解決します：</p>
<table><thead><tr><th>問題</th><th>元祖 AlphaGo</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>人間の知識の上限</td><td>棋譜の品質に制限される</td><td>この制限なし</td></tr><tr><td>学習目標</td><td>人間を模倣</td><td>勝率を最大化</td></tr><tr><td>データ要件</td><td>3000万局の棋譜</td><td>0</td></tr><tr><td>汎用性</td><td>囲碁のみ</td><td>他の分野に拡張可能</td></tr></tbody></table>
<p>これは根本的なパラダイムシフトです：「人間の知識を学習する」から「第一原理から知識を発見する」へ。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="元祖-alphago-との対比1000">元祖 AlphaGo との対比：100:0<a href="#元祖-alphago-との対比1000" class="hash-link" aria-label="元祖 AlphaGo との対比：100:0 への直接リンク" title="元祖 AlphaGo との対比：100:0 への直接リンク" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="圧倒的な勝利">圧倒的な勝利<a href="#圧倒的な勝利" class="hash-link" aria-label="圧倒的な勝利 への直接リンク" title="圧倒的な勝利 への直接リンク" translate="no">​</a></h3>
<p>DeepMind は訓練完了した AlphaGo Zero を各バージョンの AlphaGo と対戦させました：</p>
<table><thead><tr><th>対戦相手</th><th>AlphaGo Zero の戦績</th></tr></thead><tbody><tr><td>AlphaGo Fan（ファン・フイに勝利したバージョン）</td><td>100:0</td></tr><tr><td>AlphaGo Lee（イ・セドルに勝利したバージョン）</td><td>100:0</td></tr><tr><td>AlphaGo Master（60連勝バージョン）</td><td>89:11</td></tr></tbody></table>
<p><strong>100:0</strong>——これは100局の対戦で、元祖 AlphaGo が1局も勝てなかったことを意味します。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="より少ないリソースでより強い棋力">より少ないリソースで、より強い棋力<a href="#より少ないリソースでより強い棋力" class="hash-link" aria-label="より少ないリソースで、より強い棋力 への直接リンク" title="より少ないリソースで、より強い棋力 への直接リンク" translate="no">​</a></h3>
<p>勝つだけでなく、AlphaGo Zero はより少ないリソースでより強い棋力を達成しました：</p>
<table><thead><tr><th>指標</th><th>AlphaGo Lee</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>訓練時間</td><td>数ヶ月</td><td>40日（3日で AlphaGo Lee を超越）</td></tr><tr><td>訓練局数</td><td>3000万人間棋譜 + 自己対戦</td><td>490万局の自己対戦</td></tr><tr><td>TPU数（訓練）</td><td>50+</td><td>4</td></tr><tr><td>TPU数（推論）</td><td>48</td><td>4</td></tr><tr><td>入力特徴</td><td>48プレーン</td><td>17プレーン</td></tr><tr><td>ニューラルネットワーク</td><td>SL + RL デュアルネットワーク</td><td>単一デュアルヘッドネットワーク</td></tr></tbody></table>
<p>これは驚くべき効率向上です：<strong>リソースは10倍以上削減されたのに、棋力は大幅に向上</strong>。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="なぜ-zero-はより強いのか">なぜ Zero はより強いのか？<a href="#なぜ-zero-はより強いのか" class="hash-link" aria-label="なぜ Zero はより強いのか？ への直接リンク" title="なぜ Zero はより強いのか？ への直接リンク" translate="no">​</a></h3>
<p>AlphaGo Zero がより強い理由はいくつかの角度から理解できます：</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-バイアスのない学習">1. バイアスのない学習<a href="#1-バイアスのない学習" class="hash-link" aria-label="1. バイアスのない学習 への直接リンク" title="1. バイアスのない学習 への直接リンク" translate="no">​</a></h4>
<p>元祖 AlphaGo は人間の棋譜から学習し、人間のバイアスを引き継ぎました。例えば、人間の棋士は特定の定石を過度に重視したり、特定の局面に対して誤った評価をしていることがあります。</p>
<p>AlphaGo Zero にはこれらの荷物がありません。白紙の状態から、勝敗の結果だけを通じて何が良い手かを学習します。これにより、人間が考えたこともない手法を発見できます。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-一貫した学習目標">2. 一貫した学習目標<a href="#2-一貫した学習目標" class="hash-link" aria-label="2. 一貫した学習目標 への直接リンク" title="2. 一貫した学習目標 への直接リンク" translate="no">​</a></h4>
<p>元祖 AlphaGo の訓練には2つの異なる目標がありました：</p>
<ul>
<li class="">教師あり学習：人間の着手予測精度を最大化</li>
<li class="">強化学習：勝率を最大化</li>
</ul>
<p>この2つの目標は互いに矛盾する可能性があります。AlphaGo Zero は1つの目標のみ：<strong>勝率の最大化</strong>。これにより学習プロセスがより一貫し効果的になります。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-よりシンプルなアーキテクチャ">3. よりシンプルなアーキテクチャ<a href="#3-よりシンプルなアーキテクチャ" class="hash-link" aria-label="3. よりシンプルなアーキテクチャ への直接リンク" title="3. よりシンプルなアーキテクチャ への直接リンク" translate="no">​</a></h4>
<p>元祖 AlphaGo は分離した Policy Network と Value Network を使用していました。AlphaGo Zero は単一のデュアルヘッドネットワークを使用し（詳細は<a class="" href="/ja/docs/alphago/explained/dual-head-resnet/">次の記事：デュアルヘッドネットワークと残差ネットワーク</a>を参照）、特徴表現を共有できるようにし、学習効率を向上させました。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="簡素化された入力特徴48から17へ">簡素化された入力特徴：48から17へ<a href="#簡素化された入力特徴48から17へ" class="hash-link" aria-label="簡素化された入力特徴：48から17へ への直接リンク" title="簡素化された入力特徴：48から17へ への直接リンク" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="元祖-alphago-の48特徴プレーン">元祖 AlphaGo の48特徴プレーン<a href="#元祖-alphago-の48特徴プレーン" class="hash-link" aria-label="元祖 AlphaGo の48特徴プレーン への直接リンク" title="元祖 AlphaGo の48特徴プレーン への直接リンク" translate="no">​</a></h3>
<p>元祖 AlphaGo のニューラルネットワーク入力には、48個の19x19特徴プレーンが含まれ、多くの人間が設計した特徴がエンコードされていました：</p>
<table><thead><tr><th>カテゴリ</th><th>特徴数</th><th>内容</th></tr></thead><tbody><tr><td>石の位置</td><td>3</td><td>黒石、白石、空点</td></tr><tr><td>呼吸点</td><td>8</td><td>1-8呼吸点の連</td></tr><tr><td>取り</td><td>8</td><td>1-8個取れる</td></tr><tr><td>コウ</td><td>1</td><td>コウの位置</td></tr><tr><td>辺までの距離</td><td>4</td><td>一線から四線</td></tr><tr><td>着手の合法性</td><td>1</td><td>どの位置に打てるか</td></tr><tr><td>履歴状態</td><td>8</td><td>過去8手の位置</td></tr><tr><td>手番</td><td>1</td><td>黒番または白番</td></tr><tr><td>その他</td><td>14</td><td>シチョウ、眼形など</td></tr></tbody></table>
<p>これら48の特徴は囲碁の専門家が慎重に設計したもので、多くのドメイン知識を含んでいます。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero-の17特徴プレーン">AlphaGo Zero の17特徴プレーン<a href="#alphago-zero-の17特徴プレーン" class="hash-link" aria-label="AlphaGo Zero の17特徴プレーン への直接リンク" title="AlphaGo Zero の17特徴プレーン への直接リンク" translate="no">​</a></h3>
<p>AlphaGo Zero は入力を大幅に簡素化し、17個の特徴プレーンのみを使用します：</p>
<table><thead><tr><th>プレーン番号</th><th>内容</th><th>数量</th></tr></thead><tbody><tr><td>1-8</td><td>黒石の位置（直近8手）</td><td>8</td></tr><tr><td>9-16</td><td>白石の位置（直近8手）</td><td>8</td></tr><tr><td>17</td><td>現在の手番（全て1または全て0）</td><td>1</td></tr></tbody></table>
<p>これら17の特徴には以下のみが含まれます：</p>
<ul>
<li class=""><strong>現在の盤面状態</strong>：各位置に黒石、白石、または空</li>
<li class=""><strong>履歴情報</strong>：過去8手の盤面状態</li>
<li class=""><strong>手番情報</strong>：どちらの番か</li>
</ul>
<p>呼吸点も、シチョウ判定も、辺までの距離もありません——これら全ての「囲碁知識」はニューラルネットワーク自身に学習させます。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="なぜ簡素化が良いのか">なぜ簡素化が良いのか？<a href="#なぜ簡素化が良いのか" class="hash-link" aria-label="なぜ簡素化が良いのか？ への直接リンク" title="なぜ簡素化が良いのか？ への直接リンク" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-ネットワーク自身に特徴を発見させる">1. ネットワーク自身に特徴を発見させる<a href="#1-ネットワーク自身に特徴を発見させる" class="hash-link" aria-label="1. ネットワーク自身に特徴を発見させる への直接リンク" title="1. ネットワーク自身に特徴を発見させる への直接リンク" translate="no">​</a></h4>
<p>複雑な手作業の特徴は重要な情報を見落としたり、誤った仮定をエンコードする可能性があります。ニューラルネットワークに生データから学習させることで、より良い特徴表現を発見できる可能性があります。</p>
<p>実際、AlphaGo Zero は人間が設計した全ての特徴（呼吸点、シチョウなど）を学習し、さらに人間が明確に意識していなかったパターンも学習しました。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-より良い汎用性">2. より良い汎用性<a href="#2-より良い汎用性" class="hash-link" aria-label="2. より良い汎用性 への直接リンク" title="2. より良い汎用性 への直接リンク" translate="no">​</a></h4>
<p>48の特徴の多くは囲碁専用のもの（シチョウ、辺までの距離など）でした。17の簡素化された特徴は汎用的で——どのボードゲームでも同様の方法でエンコードできます。</p>
<p>これは後の <strong>AlphaZero</strong>（汎用ゲームAI）の基礎を築きました。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-人為的エラーの削減">3. 人為的エラーの削減<a href="#3-人為的エラーの削減" class="hash-link" aria-label="3. 人為的エラーの削減 への直接リンク" title="3. 人為的エラーの削減 への直接リンク" translate="no">​</a></h4>
<p>手作業で設計された特徴には、誤りや不完全な定義が含まれる可能性があります。入力の簡素化により、このような問題の可能性が排除されます。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="単一ネットワークアーキテクチャ">単一ネットワークアーキテクチャ<a href="#単一ネットワークアーキテクチャ" class="hash-link" aria-label="単一ネットワークアーキテクチャ への直接リンク" title="単一ネットワークアーキテクチャ への直接リンク" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="元祖のデュアルネットワーク設計">元祖のデュアルネットワーク設計<a href="#元祖のデュアルネットワーク設計" class="hash-link" aria-label="元祖のデュアルネットワーク設計 への直接リンク" title="元祖のデュアルネットワーク設計 への直接リンク" translate="no">​</a></h3>
<p>元祖 AlphaGo は2つの独立したニューラルネットワークを使用していました：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Policy Network:  入力 → CNN → 19x19 着手確率</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Value Network:   入力 → CNN → 勝率評価（-1から1）</span><br></span></code></pre></div></div>
<p>これら2つのネットワーク：</p>
<ul>
<li class="">異なるアーキテクチャを持つ（層数、チャネル数がやや異なる）</li>
<li class="">独立して訓練（まず Policy を訓練し、次に Value を訓練）</li>
<li class="">パラメータを一切共有しない</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="zero-のデュアルヘッドネットワーク">Zero のデュアルヘッドネットワーク<a href="#zero-のデュアルヘッドネットワーク" class="hash-link" aria-label="Zero のデュアルヘッドネットワーク への直接リンク" title="Zero のデュアルヘッドネットワーク への直接リンク" translate="no">​</a></h3>
<p>AlphaGo Zero は単一のネットワークを使用しますが、2つの出力ヘッド（heads）があります：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">入力 → ResNet 共有バックボーン → Policy Head → 19x19 着手確率</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                              → Value Head  → 勝率評価</span><br></span></code></pre></div></div>
<p>2つの Head は同じ ResNet バックボーンを共有し（詳細は<a class="" href="/ja/docs/alphago/explained/dual-head-resnet/">次の記事：デュアルヘッドネットワークと残差ネットワーク</a>を参照）、これにはいくつかの利点があります：</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-パラメータ効率">1. パラメータ効率<a href="#1-パラメータ効率" class="hash-link" aria-label="1. パラメータ効率 への直接リンク" title="1. パラメータ効率 への直接リンク" translate="no">​</a></h4>
<p>バックボーンの共有は、大部分のパラメータが2つのタスクで共用されることを意味します。これにより総パラメータ量が削減され、過学習のリスクが低下します。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-特徴共有">2. 特徴共有<a href="#2-特徴共有" class="hash-link" aria-label="2. 特徴共有 への直接リンク" title="2. 特徴共有 への直接リンク" translate="no">​</a></h4>
<p>「どこに打つべきか」（Policy）と「誰が勝つか」（Value）は、類似した盤面パターンを理解する必要があります。バックボーンの共有により、これらの特徴を2つのタスクが同時に学習し利用できます。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-訓練の安定性">3. 訓練の安定性<a href="#3-訓練の安定性" class="hash-link" aria-label="3. 訓練の安定性 への直接リンク" title="3. 訓練の安定性 への直接リンク" translate="no">​</a></h4>
<p>共同訓練により、勾配信号が2つのソースから来るため、より豊富な監督信号が提供され、訓練がより安定します。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="残差ネットワークの威力">残差ネットワークの威力<a href="#残差ネットワークの威力" class="hash-link" aria-label="残差ネットワークの威力 への直接リンク" title="残差ネットワークの威力 への直接リンク" translate="no">​</a></h3>
<p>AlphaGo Zero のバックボーンは <strong>40層の残差ネットワーク（ResNet）</strong> を使用しており、元祖 AlphaGo の13層 CNN よりもはるかに深いです。</p>
<p>残差接続（skip connections）により、深層ネットワークの効果的な訓練が可能になり、勾配消失問題を回避できます。これは2015年の ImageNet コンペティションでの画期的な技術であり、AlphaGo Zero で囲碁分野に成功裏に適用されました。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="訓練効率の向上">訓練効率の向上<a href="#訓練効率の向上" class="hash-link" aria-label="訓練効率の向上 への直接リンク" title="訓練効率の向上 への直接リンク" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="自己対戦の指数関数的成長">自己対戦の指数関数的成長<a href="#自己対戦の指数関数的成長" class="hash-link" aria-label="自己対戦の指数関数的成長 への直接リンク" title="自己対戦の指数関数的成長 への直接リンク" translate="no">​</a></h3>
<p>AlphaGo Zero の訓練プロセスは驚くべき効率を示しました：</p>
<table><thead><tr><th>訓練時間</th><th>ELO レーティング</th><th>相当するレベル</th></tr></thead><tbody><tr><td>0時間</td><td>0</td><td>ランダムな手</td></tr><tr><td>3時間</td><td>~1000</td><td>基本ルールを発見</td></tr><tr><td>12時間</td><td>~3000</td><td>定石を発見</td></tr><tr><td>36時間</td><td>~4500</td><td>ファン・フイ版を超越</td></tr><tr><td>60時間</td><td>~5200</td><td>イ・セドル版を超越</td></tr><tr><td>72時間</td><td>~5400</td><td>元祖 AlphaGo を超越</td></tr><tr><td>40日</td><td>~5600</td><td>最強バージョン</td></tr></tbody></table>
<p><strong>3日で人間を超え、3日で数ヶ月かけて訓練した以前の AI を超越</strong>——これは指数関数的な効率向上です。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="なぜこれほど速いのか">なぜこれほど速いのか？<a href="#なぜこれほど速いのか" class="hash-link" aria-label="なぜこれほど速いのか？ への直接リンク" title="なぜこれほど速いのか？ への直接リンク" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-より強力な探索ガイダンス">1. より強力な探索ガイダンス<a href="#1-より強力な探索ガイダンス" class="hash-link" aria-label="1. より強力な探索ガイダンス への直接リンク" title="1. より強力な探索ガイダンス への直接リンク" translate="no">​</a></h4>
<p>AlphaGo Zero の MCTS は完全にニューラルネットワークによってガイドされ、高速プレイアウト戦略（rollout）をもはや使用しません。これにより探索がより効率的かつ正確になります。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-より速い自己対戦">2. より速い自己対戦<a href="#2-より速い自己対戦" class="hash-link" aria-label="2. より速い自己対戦 への直接リンク" title="2. より速い自己対戦 への直接リンク" translate="no">​</a></h4>
<p>ネットワークが1つだけで済む（2つではなく）ため、各自己対戦の計算コストが削減されます。これは同じ時間でより多くの訓練データを生成できることを意味します。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-より効果的な学習">3. より効果的な学習<a href="#3-より効果的な学習" class="hash-link" aria-label="3. より効果的な学習 への直接リンク" title="3. より効果的な学習 への直接リンク" translate="no">​</a></h4>
<p>デュアルヘッドネットワークの共同訓練により、各対局の情報がより効果的に利用されます。Policy と Value の勾配が相互に強化し合い、収束を加速します。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="人間の学習との比較">人間の学習との比較<a href="#人間の学習との比較" class="hash-link" aria-label="人間の学習との比較 への直接リンク" title="人間の学習との比較 への直接リンク" translate="no">​</a></h3>
<p>人間の棋士は異なるレベルに達するまでにどれくらいの時間が必要でしょうか？</p>
<table><thead><tr><th>レベル</th><th>人間の所要時間</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>入門</td><td>数週間</td><td>数分</td></tr><tr><td>アマチュア初段</td><td>数年</td><td>数時間</td></tr><tr><td>プロレベル</td><td>10-20年</td><td>1-2日</td></tr><tr><td>世界チャンピオン</td><td>20年以上のフルタイム投入</td><td>3日</td></tr><tr><td>人間を超越</td><td>不可能</td><td>3日</td></tr></tbody></table>
<p>この比較は人間の棋士を貶めるためではありません——彼らは生物学的ニューロンを使用しており、AlphaGo Zero は専用設計の TPU と数キロワットの電力を使用しています。しかし、正しい学習方法がいかに効率的であり得るかを確実に示しています。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="汎用性チェス将棋">汎用性：チェス、将棋<a href="#汎用性チェス将棋" class="hash-link" aria-label="汎用性：チェス、将棋 への直接リンク" title="汎用性：チェス、将棋 への直接リンク" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphazero-の誕生">AlphaZero の誕生<a href="#alphazero-の誕生" class="hash-link" aria-label="AlphaZero の誕生 への直接リンク" title="AlphaZero の誕生 への直接リンク" translate="no">​</a></h3>
<p>2017年12月、DeepMind は <strong>AlphaZero</strong> を発表しました——AlphaGo Zero の汎用版です。同じアルゴリズムで、ゲームルールを変更するだけで、3つのボードゲームで世界トップレベルに到達できます：</p>
<table><thead><tr><th>ゲーム</th><th>訓練時間</th><th>対戦相手</th><th>戦績</th></tr></thead><tbody><tr><td>囲碁</td><td>8時間</td><td>AlphaGo Zero</td><td>60:40</td></tr><tr><td>チェス</td><td>4時間</td><td>Stockfish 8</td><td>28勝72引き分け0敗</td></tr><tr><td>将棋</td><td>2時間</td><td>Elmo</td><td>90:8:2</td></tr></tbody></table>
<p>ここでの対戦相手に注目してください：</p>
<ul>
<li class=""><strong>Stockfish</strong> は当時最強のチェスエンジンで、数十年の人間の知識と最適化を使用</li>
<li class=""><strong>Elmo</strong> は当時最強の将棋 AI</li>
</ul>
<p>AlphaZero は数時間の訓練で、何年もかけて開発されたこれらの専用システムを超越しました。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="汎用性の意義">汎用性の意義<a href="#汎用性の意義" class="hash-link" aria-label="汎用性の意義 への直接リンク" title="汎用性の意義 への直接リンク" translate="no">​</a></h3>
<p>AlphaGo Zero / AlphaZero は重要なことを証明しました：</p>
<blockquote>
<p><strong>同じ学習アルゴリズムが、異なる分野で超人的レベルに到達できる。</strong></p>
</blockquote>
<p>これは3つの異なる AI ではなく、1つの汎用的な学習フレームワークです：</p>
<ol>
<li class=""><strong>自己対戦</strong>で経験を生成</li>
<li class=""><strong>モンテカルロ木探索</strong>で可能性を探索</li>
<li class=""><strong>ニューラルネットワーク</strong>で戦略と価値関数を学習</li>
<li class=""><strong>強化学習</strong>で目的関数を最適化</li>
</ol>
<p>このフレームワークはドメイン固有の知識に依存せず、AI の汎用化に向けた重要な一歩を踏み出しました。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="従来の-ai-への衝撃">従来の AI への衝撃<a href="#従来の-ai-への衝撃" class="hash-link" aria-label="従来の AI への衝撃 への直接リンク" title="従来の AI への衝撃 への直接リンク" translate="no">​</a></h3>
<p>AlphaZero 以前、チェスと将棋の最強 AI は全て「エキスパートシステム」スタイルでした：</p>
<ul>
<li class=""><strong>大量の人間の知識</strong>：オープニングブック、エンドゲームデータベース、評価関数</li>
<li class=""><strong>数十年の最適化</strong>：無数の棋士とエンジニアの心血</li>
<li class=""><strong>極度の専門化</strong>：Stockfish は囲碁を打てず、Elmo はチェスを打てない</li>
</ul>
<p>AlphaZero は1つの汎用アルゴリズムで数時間でこれら全てを超越しました。これにより多くの AI 研究者が再考しました：</p>
<blockquote>
<p>「汎用学習アルゴリズム」にもっと努力を注ぐべきか、それとも「専門家の知識のエンコード」にか？</p>
</blockquote>
<p>答えはますます明らかになっているようです：機械に自ら学習させることは、知識を教えるよりも効果的です。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero-の棋風">AlphaGo Zero の棋風<a href="#alphago-zero-の棋風" class="hash-link" aria-label="AlphaGo Zero の棋風 への直接リンク" title="AlphaGo Zero の棋風 への直接リンク" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="人間を超えた美学">人間を超えた美学<a href="#人間を超えた美学" class="hash-link" aria-label="人間を超えた美学 への直接リンク" title="人間を超えた美学 への直接リンク" translate="no">​</a></h3>
<p>囲碁界は AlphaGo Zero の着手について共通の評価を持っています：<strong>より美しい</strong>。</p>
<p>AlphaGo Lee の着手は時として「奇妙」に見えました——第37手のような着手は、人間が事後分析してようやくその妙味を理解できました。しかし AlphaGo Zero の着手は、しばしば事後に「一目で良い手だとわかる」と評価されます。</p>
<p>これはおそらく以下の理由によります：</p>
<ol>
<li class=""><strong>より強い棋力</strong>：Zero はより深く読めるため、着手がより余裕がある</li>
<li class=""><strong>人間のバイアスがない</strong>：従来の定石に縛られない</li>
<li class=""><strong>一貫した目標</strong>：勝率のみを追求し、人間を模倣しない</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="人間の棋理の再発見">人間の棋理の再発見<a href="#人間の棋理の再発見" class="hash-link" aria-label="人間の棋理の再発見 への直接リンク" title="人間の棋理の再発見 への直接リンク" translate="no">​</a></h3>
<p>興味深いことに、AlphaGo Zero は訓練プロセスで人間が数千年かけて蓄積した囲碁の知識を「再発見」しました：</p>
<ul>
<li class=""><strong>定石</strong>：Zero は多くの一般的な定石を自ら発見しました。なぜならこれらは確かに双方の最適解だからです</li>
<li class=""><strong>布石の原則</strong>：隅、辺、中央の重要性の順序</li>
<li class=""><strong>石形の知識</strong>：愚形と好形の違い</li>
</ul>
<p>これは人間の棋理の妥当性を検証しました——これらの知識は偶然ではなく、囲碁の本質の反映です。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="人間を超えたイノベーション">人間を超えたイノベーション<a href="#人間を超えたイノベーション" class="hash-link" aria-label="人間を超えたイノベーション への直接リンク" title="人間を超えたイノベーション への直接リンク" translate="no">​</a></h3>
<p>しかし Zero は人間が考えたこともない着手も発見しました：</p>
<ul>
<li class=""><strong>非伝統的な序盤</strong>：従来の序盤を基礎とした変化</li>
<li class=""><strong>積極的な捨て石</strong>：人間よりも局部を放棄して全局の優勢を得ることを厭わない</li>
<li class=""><strong>反直感的な形</strong>：表面上の「悪形」が実は最適解</li>
</ul>
<p>これらのイノベーションは人間の囲碁理解を変えつつあります。多くのプロ棋士は、AlphaGo Zero の棋譜を研究することで、囲碁に対する全く新しい認識を得たと述べています。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="技術詳細まとめ">技術詳細まとめ<a href="#技術詳細まとめ" class="hash-link" aria-label="技術詳細まとめ への直接リンク" title="技術詳細まとめ への直接リンク" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="元祖-alphago-との完全比較">元祖 AlphaGo との完全比較<a href="#元祖-alphago-との完全比較" class="hash-link" aria-label="元祖 AlphaGo との完全比較 への直接リンク" title="元祖 AlphaGo との完全比較 への直接リンク" translate="no">​</a></h3>
<table><thead><tr><th>側面</th><th>AlphaGo（元祖）</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td><strong>訓練データ</strong></td><td>人間の棋譜 + 自己対戦</td><td>純粋な自己対戦</td></tr><tr><td><strong>学習方法</strong></td><td>教師あり学習 + 強化学習</td><td>純粋な強化学習</td></tr><tr><td><strong>入力特徴</strong></td><td>48プレーン</td><td>17プレーン</td></tr><tr><td><strong>ネットワークアーキテクチャ</strong></td><td>分離した Policy/Value</td><td>デュアルヘッド ResNet</td></tr><tr><td><strong>ネットワーク深度</strong></td><td>13層</td><td>40層（またはそれ以上）</td></tr><tr><td><strong>MCTS 評価</strong></td><td>ニューラルネットワーク + Rollout</td><td>純粋なニューラルネットワーク</td></tr><tr><td><strong>探索回数</strong></td><td>1手あたり約100,000</td><td>1手あたり約1,600</td></tr><tr><td><strong>訓練 TPU</strong></td><td>50+</td><td>4</td></tr><tr><td><strong>推論 TPU</strong></td><td>48</td><td>4（スケーラブル）</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="コアアルゴリズム">コアアルゴリズム<a href="#コアアルゴリズム" class="hash-link" aria-label="コアアルゴリズム への直接リンク" title="コアアルゴリズム への直接リンク" translate="no">​</a></h3>
<p>AlphaGo Zero の訓練ループは非常にシンプルです：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. 自己対戦</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 現在のネットワークで MCTS を実行</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - MCTS 探索確率に従って着手を選択</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 各手の（局面, MCTS確率, 勝敗結果）を記録</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. ネットワーク訓練</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 経験プールからサンプリング</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Policy Head：MCTS 確率との交差エントロピーを最小化</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Value Head：実際の勝敗との平均二乗誤差を最小化</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 2つの目標を共同最適化</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. ネットワーク更新</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 新しいネットワークで古いネットワークを置き換え（対戦で新ネットワークがより強いことを検証）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - ステップ1に戻る</span><br></span></code></pre></div></div>
<p>このループは継続的に実行され、ネットワークは絶えず強くなります。人間のデータも、人間の知識もなく、ゲームルールと勝敗目標のみ。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="ai-研究への示唆">AI 研究への示唆<a href="#ai-研究への示唆" class="hash-link" aria-label="AI 研究への示唆 への直接リンク" title="AI 研究への示唆 への直接リンク" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="第一原理からの学習">第一原理からの学習<a href="#第一原理からの学習" class="hash-link" aria-label="第一原理からの学習 への直接リンク" title="第一原理からの学習 への直接リンク" translate="no">​</a></h3>
<p>AlphaGo Zero は「第一原理」からの学習方法を示しました：</p>
<blockquote>
<p>AI に方法を教えるのではなく、目標が何かを伝えて、方法を自ら発見させる。</p>
</blockquote>
<p>これは従来のエキスパートシステムアプローチと鮮明なコントラストを成します。エキスパートシステムは人間の知識を AI にエンコードしようとしましたが、AlphaGo Zero は AI に自ら知識を発見させます。</p>
<p>結果として：AI が発見した知識は人間の知識よりも完全で正確である可能性があります。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="自己対戦の威力">自己対戦の威力<a href="#自己対戦の威力" class="hash-link" aria-label="自己対戦の威力 への直接リンク" title="自己対戦の威力 への直接リンク" translate="no">​</a></h3>
<p>AlphaGo Zero は、自己対戦が無限の訓練データを生成でき、そのデータの品質はネットワークの向上とともに向上することを証明しました。</p>
<p>これは「正のサイクル」です：</p>
<ul>
<li class="">より強いネットワーク → より良い自己対戦データ</li>
<li class="">より良いデータ → より強いネットワーク</li>
</ul>
<p>このサイクルはゲームの理論的上限（存在する場合）に達するまで継続的に実行できます。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="簡素化の重要性">簡素化の重要性<a href="#簡素化の重要性" class="hash-link" aria-label="簡素化の重要性 への直接リンク" title="簡素化の重要性 への直接リンク" translate="no">​</a></h3>
<p>AlphaGo Zero の成功は「簡素化」の重要性を証明しました：</p>
<ul>
<li class="">入力の簡素化（48 → 17）</li>
<li class="">アーキテクチャの簡素化（デュアルネットワーク → 単一ネットワーク）</li>
<li class="">訓練の簡素化（教師あり + 強化 → 純粋な強化）</li>
</ul>
<p>各簡素化がシステムをより強力にしました。これは私たちに教えています：複雑さは良さではない、最もシンプルな解決策がしばしば最良です。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="アニメーション対応">アニメーション対応<a href="#アニメーション対応" class="hash-link" aria-label="アニメーション対応 への直接リンク" title="アニメーション対応 への直接リンク" translate="no">​</a></h2>
<p>本記事に関連するコア概念とアニメーション番号：</p>
<table><thead><tr><th>番号</th><th>概念</th><th>物理/数学対応</th></tr></thead><tbody><tr><td>🎬 E7</td><td>ゼロからの訓練</td><td>自己組織化現象</td></tr><tr><td>🎬 E5</td><td>自己対戦</td><td>不動点収束</td></tr><tr><td>🎬 E12</td><td>棋力成長曲線</td><td>S字型成長</td></tr><tr><td>🎬 D12</td><td>残差ネットワーク</td><td>勾配ハイウェイ</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="関連記事">関連記事<a href="#関連記事" class="hash-link" aria-label="関連記事 への直接リンク" title="関連記事 への直接リンク" translate="no">​</a></h2>
<ul>
<li class=""><strong>次の記事</strong>：<a class="" href="/ja/docs/alphago/explained/dual-head-resnet/">デュアルヘッドネットワークと残差ネットワーク</a> — AlphaGo Zero のニューラルネットワークアーキテクチャの詳細解説</li>
<li class=""><strong>関連記事</strong>：<a class="" href="/ja/docs/alphago/explained/self-play/">自己対戦</a> — なぜ自己対戦が超人的レベルを生み出せるのか</li>
<li class=""><strong>技術深掘り</strong>：<a class="" href="/ja/docs/alphago/explained/training-from-scratch/">ゼロからの訓練プロセス</a> — Day 0-3 の詳細な進化</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="参考文献">参考文献<a href="#参考文献" class="hash-link" aria-label="参考文献 への直接リンク" title="参考文献 への直接リンク" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">Silver, D., et al. (2018). &quot;A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.&quot; <em>Science</em>, 362(6419), 1140-1144.</li>
<li class="">DeepMind. (2017). &quot;AlphaGo Zero: Starting from scratch.&quot; <em>DeepMind Blog</em>.</li>
<li class="">Schrittwieser, J., et al. (2020). &quot;Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.&quot; <em>Nature</em>, 588, 604-609.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/explained/16-alphago-zero.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>このページを編集</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="ドキュメントページ"><a class="pagination-nav__link pagination-nav__link--prev" href="/ja/docs/alphago/explained/puct-formula/"><div class="pagination-nav__sublabel">前へ</div><div class="pagination-nav__label">PUCT 公式詳解</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ja/docs/alphago/explained/dual-head-resnet/"><div class="pagination-nav__sublabel">次へ</div><div class="pagination-nav__label">デュアルヘッドネットワークと残差ネットワーク</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#なぜ人間の棋譜は不要なのか" class="table-of-contents__link toc-highlight">なぜ人間の棋譜は不要なのか？</a><ul><li><a href="#人間の棋譜の限界" class="table-of-contents__link toc-highlight">人間の棋譜の限界</a></li><li><a href="#zero-の突破" class="table-of-contents__link toc-highlight">Zero の突破</a></li></ul></li><li><a href="#元祖-alphago-との対比1000" class="table-of-contents__link toc-highlight">元祖 AlphaGo との対比：100:0</a><ul><li><a href="#圧倒的な勝利" class="table-of-contents__link toc-highlight">圧倒的な勝利</a></li><li><a href="#より少ないリソースでより強い棋力" class="table-of-contents__link toc-highlight">より少ないリソースで、より強い棋力</a></li><li><a href="#なぜ-zero-はより強いのか" class="table-of-contents__link toc-highlight">なぜ Zero はより強いのか？</a></li></ul></li><li><a href="#簡素化された入力特徴48から17へ" class="table-of-contents__link toc-highlight">簡素化された入力特徴：48から17へ</a><ul><li><a href="#元祖-alphago-の48特徴プレーン" class="table-of-contents__link toc-highlight">元祖 AlphaGo の48特徴プレーン</a></li><li><a href="#alphago-zero-の17特徴プレーン" class="table-of-contents__link toc-highlight">AlphaGo Zero の17特徴プレーン</a></li><li><a href="#なぜ簡素化が良いのか" class="table-of-contents__link toc-highlight">なぜ簡素化が良いのか？</a></li></ul></li><li><a href="#単一ネットワークアーキテクチャ" class="table-of-contents__link toc-highlight">単一ネットワークアーキテクチャ</a><ul><li><a href="#元祖のデュアルネットワーク設計" class="table-of-contents__link toc-highlight">元祖のデュアルネットワーク設計</a></li><li><a href="#zero-のデュアルヘッドネットワーク" class="table-of-contents__link toc-highlight">Zero のデュアルヘッドネットワーク</a></li><li><a href="#残差ネットワークの威力" class="table-of-contents__link toc-highlight">残差ネットワークの威力</a></li></ul></li><li><a href="#訓練効率の向上" class="table-of-contents__link toc-highlight">訓練効率の向上</a><ul><li><a href="#自己対戦の指数関数的成長" class="table-of-contents__link toc-highlight">自己対戦の指数関数的成長</a></li><li><a href="#なぜこれほど速いのか" class="table-of-contents__link toc-highlight">なぜこれほど速いのか？</a></li><li><a href="#人間の学習との比較" class="table-of-contents__link toc-highlight">人間の学習との比較</a></li></ul></li><li><a href="#汎用性チェス将棋" class="table-of-contents__link toc-highlight">汎用性：チェス、将棋</a><ul><li><a href="#alphazero-の誕生" class="table-of-contents__link toc-highlight">AlphaZero の誕生</a></li><li><a href="#汎用性の意義" class="table-of-contents__link toc-highlight">汎用性の意義</a></li><li><a href="#従来の-ai-への衝撃" class="table-of-contents__link toc-highlight">従来の AI への衝撃</a></li></ul></li><li><a href="#alphago-zero-の棋風" class="table-of-contents__link toc-highlight">AlphaGo Zero の棋風</a><ul><li><a href="#人間を超えた美学" class="table-of-contents__link toc-highlight">人間を超えた美学</a></li><li><a href="#人間の棋理の再発見" class="table-of-contents__link toc-highlight">人間の棋理の再発見</a></li><li><a href="#人間を超えたイノベーション" class="table-of-contents__link toc-highlight">人間を超えたイノベーション</a></li></ul></li><li><a href="#技術詳細まとめ" class="table-of-contents__link toc-highlight">技術詳細まとめ</a><ul><li><a href="#元祖-alphago-との完全比較" class="table-of-contents__link toc-highlight">元祖 AlphaGo との完全比較</a></li><li><a href="#コアアルゴリズム" class="table-of-contents__link toc-highlight">コアアルゴリズム</a></li></ul></li><li><a href="#ai-研究への示唆" class="table-of-contents__link toc-highlight">AI 研究への示唆</a><ul><li><a href="#第一原理からの学習" class="table-of-contents__link toc-highlight">第一原理からの学習</a></li><li><a href="#自己対戦の威力" class="table-of-contents__link toc-highlight">自己対戦の威力</a></li><li><a href="#簡素化の重要性" class="table-of-contents__link toc-highlight">簡素化の重要性</a></li></ul></li><li><a href="#アニメーション対応" class="table-of-contents__link toc-highlight">アニメーション対応</a></li><li><a href="#関連記事" class="table-of-contents__link toc-highlight">関連記事</a></li><li><a href="#参考文献" class="table-of-contents__link toc-highlight">参考文献</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>