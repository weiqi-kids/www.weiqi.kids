<!doctype html>
<html lang="es" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-alphago/explained/alphago-zero" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Visión General de AlphaGo Zero | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/es/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/es/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/es/docs/alphago/explained/alphago-zero/"><meta data-rh="true" property="og:locale" content="es"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="es"><meta data-rh="true" name="docsearch:language" content="es"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Visión General de AlphaGo Zero | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="Desde cero, completamente autodidacta, cómo AlphaGo Zero superó todas las versiones anteriores sin usar partidas humanas"><meta data-rh="true" property="og:description" content="Desde cero, completamente autodidacta, cómo AlphaGo Zero superó todas las versiones anteriores sin usar partidas humanas"><meta data-rh="true" name="keywords" content="AlphaGo Zero,auto-juego,aprendizaje por refuerzo,deep learning,Go AI,aprendizaje no supervisado"><link data-rh="true" rel="icon" href="/es/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/es/docs/alphago/explained/alphago-zero/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/explained/alphago-zero/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/alphago/explained/alphago-zero/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/alphago/explained/alphago-zero/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/alphago/explained/alphago-zero/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/alphago/explained/alphago-zero/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/alphago/explained/alphago-zero/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/alphago/explained/alphago-zero/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/alphago/explained/alphago-zero/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/alphago/explained/alphago-zero/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/alphago/explained/alphago-zero/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/alphago/explained/alphago-zero/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/explained/alphago-zero/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AlphaGo","item":"https://www.weiqi.kids/es/docs/alphago/"},{"@type":"ListItem","position":2,"name":"完整解析","item":"https://www.weiqi.kids/es/docs/alphago/explained/"},{"@type":"ListItem","position":3,"name":"Visión General de AlphaGo Zero","item":"https://www.weiqi.kids/es/docs/alphago/explained/alphago-zero"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/es/assets/css/styles.f23bf74b.css">
<script src="/es/assets/js/runtime~main.95f0457a.js" defer="defer"></script>
<script src="/es/assets/js/main.998fa461.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/es/img/logo.svg"><div role="region" aria-label="Saltar al contenido principal"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">Saltar al contenido principal</a></div><nav aria-label="Principal" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Alternar barra lateral" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/es/"><div class="navbar__logo"><img src="/es/img/logo.svg" alt="Logo de la Asociación Weiqi Kids" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/es/img/logo.svg" alt="Logo de la Asociación Weiqi Kids" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">Weiqi.Kids</b></a><a class="navbar__item navbar__link" href="/es/docs/learn/">Aprender Go</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/es/docs/alphago/">AlphaGo</a><a class="navbar__item navbar__link" href="/es/docs/animations/">Estudio de Animación</a><a class="navbar__item navbar__link" href="/es/docs/tech/">Documentación Técnica</a><a class="navbar__item navbar__link" href="/es/docs/about/">Sobre Nosotros</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>Español</a><ul class="dropdown__menu"><li><a href="/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="es">Español</a></li><li><a href="/pt/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="Volver al principio" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="Barra lateral de Documentos" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/es/docs/intro/"><span title="Guia de uso" class="linkLabel_REp1">Guia de uso</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/es/docs/learn/"><span title="學圍棋" class="categoryLinkLabel_ezQx">學圍棋</span></a><button aria-label="Ampliar la categoría &#x27;學圍棋&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/es/docs/alphago/"><span title="AlphaGo" class="categoryLinkLabel_ezQx">AlphaGo</span></a><button aria-label="Colapsar categoría &#x27;AlphaGo&#x27; de la barra lateral" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/es/docs/alphago/explained/"><span title="完整解析" class="categoryLinkLabel_ezQx">完整解析</span></a><button aria-label="Colapsar categoría &#x27;完整解析&#x27; de la barra lateral" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/birth-of-alphago/"><span title="El Nacimiento de AlphaGo" class="linkLabel_REp1">El Nacimiento de AlphaGo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/key-matches/"><span title="Revisiones de partidas clave" class="linkLabel_REp1">Revisiones de partidas clave</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/move-37/"><span title="Análisis Profundo del &quot;Movimiento Divino&quot;" class="linkLabel_REp1">Análisis Profundo del &quot;Movimiento Divino&quot;</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/why-go-is-hard/"><span title="¿Por Qué es Difícil el Go?" class="linkLabel_REp1">¿Por Qué es Difícil el Go?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/traditional-limits/"><span title="Los Limites de los Metodos Tradicionales" class="linkLabel_REp1">Los Limites de los Metodos Tradicionales</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/board-representation/"><span title="Representacion del Estado del Tablero" class="linkLabel_REp1">Representacion del Estado del Tablero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/policy-network/"><span title="Policy Network en detalle" class="linkLabel_REp1">Policy Network en detalle</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/value-network/"><span title="Value Network en detalle" class="linkLabel_REp1">Value Network en detalle</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/input-features/"><span title="Diseno de Caracteristicas de Entrada" class="linkLabel_REp1">Diseno de Caracteristicas de Entrada</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/cnn-and-go/"><span title="CNN y Go" class="linkLabel_REp1">CNN y Go</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/supervised-learning/"><span title="Fase de aprendizaje supervisado" class="linkLabel_REp1">Fase de aprendizaje supervisado</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/reinforcement-intro/"><span title="Introducción al aprendizaje por refuerzo" class="linkLabel_REp1">Introducción al aprendizaje por refuerzo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/self-play/"><span title="Auto-juego" class="linkLabel_REp1">Auto-juego</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/mcts-neural-combo/"><span title="La Combinación de MCTS y Redes Neuronales" class="linkLabel_REp1">La Combinación de MCTS y Redes Neuronales</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/puct-formula/"><span title="Explicación Detallada de la Fórmula PUCT" class="linkLabel_REp1">Explicación Detallada de la Fórmula PUCT</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/es/docs/alphago/explained/alphago-zero/"><span title="Visión General de AlphaGo Zero" class="linkLabel_REp1">Visión General de AlphaGo Zero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/dual-head-resnet/"><span title="Red de Doble Cabeza y Redes Residuales" class="linkLabel_REp1">Red de Doble Cabeza y Redes Residuales</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/training-from-scratch/"><span title="El proceso de entrenamiento desde cero" class="linkLabel_REp1">El proceso de entrenamiento desde cero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/distributed-systems/"><span title="Sistemas distribuidos y TPU" class="linkLabel_REp1">Sistemas distribuidos y TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/explained/legacy-and-impact/"><span title="El legado de AlphaGo" class="linkLabel_REp1">El legado de AlphaGo</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/es/docs/animations/"><span title="動畫教室" class="linkLabel_REp1">動畫教室</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/es/docs/tech/"><span title="技術文件" class="categoryLinkLabel_ezQx">技術文件</span></a><button aria-label="Ampliar la categoría &#x27;技術文件&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/es/docs/about/"><span title="關於我們" class="categoryLinkLabel_ezQx">關於我們</span></a><button aria-label="Ampliar la categoría &#x27;關於我們&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="Rastro de navegación"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Página de Inicio" class="breadcrumbs__link" href="/es/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/es/docs/alphago/"><span>AlphaGo</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/es/docs/alphago/explained/"><span>完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Visión General de AlphaGo Zero</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">En esta página</button></div><div class="theme-doc-markdown markdown"><header><h1>Visión General de AlphaGo Zero</h1></header>
<p>En octubre de 2017, DeepMind publicó un resultado que conmocionó al mundo de la IA: <strong>AlphaGo Zero</strong>, entrenado desde un estado completamente aleatorio sin usar ninguna partida humana, superó al AlphaGo original que derrotó a Lee Sedol en solo tres días, ganando por un marcador de <strong>100:0</strong>.</p>
<p>Esto no es solo un avance numérico. Representa un paradigma completamente nuevo: <strong>la IA no necesita conocimiento humano, puede descubrir todo desde cero</strong>.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="por-qué-no-se-necesitan-partidas-humanas">¿Por Qué No Se Necesitan Partidas Humanas?<a href="#por-qué-no-se-necesitan-partidas-humanas" class="hash-link" aria-label="Enlace directo al ¿Por Qué No Se Necesitan Partidas Humanas?" title="Enlace directo al ¿Por Qué No Se Necesitan Partidas Humanas?" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="limitaciones-de-las-partidas-humanas">Limitaciones de las Partidas Humanas<a href="#limitaciones-de-las-partidas-humanas" class="hash-link" aria-label="Enlace directo al Limitaciones de las Partidas Humanas" title="Enlace directo al Limitaciones de las Partidas Humanas" translate="no">​</a></h3>
<p>El proceso de entrenamiento del AlphaGo original se dividió en dos etapas:</p>
<ol>
<li class=""><strong>Aprendizaje supervisado</strong>: Entrenar la Policy Network con 30 millones de partidas humanas</li>
<li class=""><strong>Aprendizaje por refuerzo</strong>: Mejorar aún más a través del auto-juego</li>
</ol>
<p>Este método tiene varios problemas fundamentales:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-las-partidas-humanas-tienen-un-límite-superior">1. Las Partidas Humanas Tienen un Límite Superior<a href="#1-las-partidas-humanas-tienen-un-límite-superior" class="hash-link" aria-label="Enlace directo al 1. Las Partidas Humanas Tienen un Límite Superior" title="Enlace directo al 1. Las Partidas Humanas Tienen un Límite Superior" translate="no">​</a></h4>
<p>La fuerza de los jugadores humanos tiene límites, y las partidas contienen la comprensión humana, incluyendo errores y sesgos humanos. Cuando la IA aprende de partidas humanas, aprende:</p>
<ul>
<li class="">Lo que los humanos creen que son buenos movimientos (pero no necesariamente óptimos)</li>
<li class="">Patrones de pensamiento humano (pero pueden limitar la innovación)</li>
<li class="">Errores humanos (que se aprenden como muestras correctas)</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-el-cuello-de-botella-del-aprendizaje-supervisado">2. El Cuello de Botella del Aprendizaje Supervisado<a href="#2-el-cuello-de-botella-del-aprendizaje-supervisado" class="hash-link" aria-label="Enlace directo al 2. El Cuello de Botella del Aprendizaje Supervisado" title="Enlace directo al 2. El Cuello de Botella del Aprendizaje Supervisado" translate="no">​</a></h4>
<p>El objetivo del aprendizaje supervisado es &quot;imitar a humanos&quot; -- predecir qué movimiento jugará un jugador humano. Esto significa que el límite superior de la capacidad de la IA está limitado por la capacidad de los jugadores humanos.</p>
<p>Es como un aprendiz que solo puede imitar al maestro, nunca puede superar al maestro.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-costo-de-recolección-de-datos">3. Costo de Recolección de Datos<a href="#3-costo-de-recolección-de-datos" class="hash-link" aria-label="Enlace directo al 3. Costo de Recolección de Datos" title="Enlace directo al 3. Costo de Recolección de Datos" translate="no">​</a></h4>
<p>Las partidas humanas de alta calidad necesitan años para acumularse, y solo existen para juegos con larga historia como Go. Si quisieras aplicar IA a nuevos dominios (como predicción de estructuras de proteínas), simplemente no hay &quot;partidas de expertos humanos&quot; disponibles.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="el-avance-de-zero">El Avance de Zero<a href="#el-avance-de-zero" class="hash-link" aria-label="Enlace directo al El Avance de Zero" title="Enlace directo al El Avance de Zero" translate="no">​</a></h3>
<p>AlphaGo Zero omitió completamente la etapa de aprendizaje supervisado, comenzando directamente desde <strong>inicialización aleatoria</strong> con auto-juego. Esto resolvió todos los problemas mencionados:</p>
<table><thead><tr><th>Problema</th><th>AlphaGo Original</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Límite del conocimiento humano</td><td>Limitado por calidad de partidas</td><td>Sin esta limitación</td></tr><tr><td>Objetivo de aprendizaje</td><td>Imitar humanos</td><td>Maximizar tasa de victoria</td></tr><tr><td>Requisitos de datos</td><td>30 millones de partidas</td><td>0</td></tr><tr><td>Generalización</td><td>Solo Go</td><td>Generalizable a otros dominios</td></tr></tbody></table>
<p>Este es un cambio de paradigma fundamental: de &quot;aprender conocimiento humano&quot; a &quot;descubrir conocimiento desde primeros principios&quot;.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="comparación-con-alphago-original-1000">Comparación con AlphaGo Original: 100:0<a href="#comparación-con-alphago-original-1000" class="hash-link" aria-label="Enlace directo al Comparación con AlphaGo Original: 100:0" title="Enlace directo al Comparación con AlphaGo Original: 100:0" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="victoria-aplastante">Victoria Aplastante<a href="#victoria-aplastante" class="hash-link" aria-label="Enlace directo al Victoria Aplastante" title="Enlace directo al Victoria Aplastante" translate="no">​</a></h3>
<p>DeepMind hizo que AlphaGo Zero entrenado jugara contra varias versiones de AlphaGo:</p>
<table><thead><tr><th>Oponente</th><th>Récord de AlphaGo Zero</th></tr></thead><tbody><tr><td>AlphaGo Fan (versión que derrotó a Fan Hui)</td><td>100:0</td></tr><tr><td>AlphaGo Lee (versión que derrotó a Lee Sedol)</td><td>100:0</td></tr><tr><td>AlphaGo Master (versión 60 victorias consecutivas)</td><td>89:11</td></tr></tbody></table>
<p><strong>100:0</strong> -- esto significa que en 100 partidas, el AlphaGo original no pudo ganar ni una sola.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="menos-recursos-mayor-fuerza">Menos Recursos, Mayor Fuerza<a href="#menos-recursos-mayor-fuerza" class="hash-link" aria-label="Enlace directo al Menos Recursos, Mayor Fuerza" title="Enlace directo al Menos Recursos, Mayor Fuerza" translate="no">​</a></h3>
<p>No solo ganó, AlphaGo Zero logró mayor fuerza con menos recursos:</p>
<table><thead><tr><th>Métrica</th><th>AlphaGo Lee</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Tiempo de entrenamiento</td><td>Varios meses</td><td>40 días (3 días para superar a AlphaGo Lee)</td></tr><tr><td>Partidas de entrenamiento</td><td>30 millones humanas + auto-juego</td><td>4.9 millones de auto-juego</td></tr><tr><td>TPUs (entrenamiento)</td><td>50+</td><td>4</td></tr><tr><td>TPUs (inferencia)</td><td>48</td><td>4</td></tr><tr><td>Características de entrada</td><td>48 planos</td><td>17 planos</td></tr><tr><td>Red neuronal</td><td>Redes SL + RL separadas</td><td>Red única de doble cabeza</td></tr></tbody></table>
<p>Esta es una mejora de eficiencia asombrosa: <strong>más de 10 veces menos recursos, pero fuerza significativamente mayor</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="por-qué-zero-es-más-fuerte">¿Por Qué Zero Es Más Fuerte?<a href="#por-qué-zero-es-más-fuerte" class="hash-link" aria-label="Enlace directo al ¿Por Qué Zero Es Más Fuerte?" title="Enlace directo al ¿Por Qué Zero Es Más Fuerte?" translate="no">​</a></h3>
<p>Las razones por las que AlphaGo Zero es más fuerte se pueden entender desde varios ángulos:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-aprendizaje-sin-sesgos">1. Aprendizaje Sin Sesgos<a href="#1-aprendizaje-sin-sesgos" class="hash-link" aria-label="Enlace directo al 1. Aprendizaje Sin Sesgos" title="Enlace directo al 1. Aprendizaje Sin Sesgos" translate="no">​</a></h4>
<p>El AlphaGo original aprendió de partidas humanas, heredando sesgos humanos. Por ejemplo, los jugadores humanos pueden sobrevalorar ciertas joseki, o tener evaluaciones incorrectas de ciertas posiciones.</p>
<p>AlphaGo Zero no tiene esta carga. Comenzó desde una hoja en blanco, aprendiendo solo a través de resultados de victoria/derrota qué es un buen movimiento. Esto le permitió descubrir movimientos que los humanos nunca habían pensado.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-objetivo-de-aprendizaje-consistente">2. Objetivo de Aprendizaje Consistente<a href="#2-objetivo-de-aprendizaje-consistente" class="hash-link" aria-label="Enlace directo al 2. Objetivo de Aprendizaje Consistente" title="Enlace directo al 2. Objetivo de Aprendizaje Consistente" translate="no">​</a></h4>
<p>El entrenamiento del AlphaGo original tenía dos objetivos diferentes:</p>
<ul>
<li class="">Aprendizaje supervisado: Maximizar precisión de predicción de movimientos humanos</li>
<li class="">Aprendizaje por refuerzo: Maximizar tasa de victoria</li>
</ul>
<p>Estos dos objetivos pueden entrar en conflicto. AlphaGo Zero tiene solo un objetivo: <strong>maximización de tasa de victoria</strong>. Esto hace que el proceso de aprendizaje sea más consistente y efectivo.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-arquitectura-más-simple">3. Arquitectura Más Simple<a href="#3-arquitectura-más-simple" class="hash-link" aria-label="Enlace directo al 3. Arquitectura Más Simple" title="Enlace directo al 3. Arquitectura Más Simple" translate="no">​</a></h4>
<p>El AlphaGo original usaba Policy Network y Value Network separadas. AlphaGo Zero usa una red única de doble cabeza (ver siguiente artículo), permitiendo compartir representaciones de características, mejorando la eficiencia del aprendizaje.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="características-de-entrada-simplificadas-de-48-a-17">Características de Entrada Simplificadas: De 48 a 17<a href="#características-de-entrada-simplificadas-de-48-a-17" class="hash-link" aria-label="Enlace directo al Características de Entrada Simplificadas: De 48 a 17" title="Enlace directo al Características de Entrada Simplificadas: De 48 a 17" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="los-48-planos-de-características-del-alphago-original">Los 48 Planos de Características del AlphaGo Original<a href="#los-48-planos-de-características-del-alphago-original" class="hash-link" aria-label="Enlace directo al Los 48 Planos de Características del AlphaGo Original" title="Enlace directo al Los 48 Planos de Características del AlphaGo Original" translate="no">​</a></h3>
<p>La entrada de la red neuronal del AlphaGo original incluía 48 planos de 19x19, codificando muchas características diseñadas por humanos:</p>
<table><thead><tr><th>Categoría</th><th>Número de características</th><th>Contenido</th></tr></thead><tbody><tr><td>Posición de piedras</td><td>3</td><td>Negras, blancas, vacías</td></tr><tr><td>Libertades</td><td>8</td><td>Grupos con 1-8 libertades</td></tr><tr><td>Capturas</td><td>8</td><td>Puede capturar 1-8 piedras</td></tr><tr><td>Ko</td><td>1</td><td>Posición de ko</td></tr><tr><td>Distancia al borde</td><td>4</td><td>Primera a cuarta línea</td></tr><tr><td>Legalidad de jugada</td><td>1</td><td>Qué posiciones pueden jugarse</td></tr><tr><td>Estado histórico</td><td>8</td><td>Posiciones de últimos 8 movimientos</td></tr><tr><td>Turno</td><td>1</td><td>Negro o blanco</td></tr><tr><td>Otros</td><td>14</td><td>Escalera, ojos, etc.</td></tr></tbody></table>
<p>Estas 48 características fueron cuidadosamente diseñadas por expertos en Go, conteniendo mucho conocimiento del dominio.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="los-17-planos-de-características-de-alphago-zero">Los 17 Planos de Características de AlphaGo Zero<a href="#los-17-planos-de-características-de-alphago-zero" class="hash-link" aria-label="Enlace directo al Los 17 Planos de Características de AlphaGo Zero" title="Enlace directo al Los 17 Planos de Características de AlphaGo Zero" translate="no">​</a></h3>
<p>AlphaGo Zero simplificó dramáticamente la entrada, usando solo 17 planos de características:</p>
<table><thead><tr><th>Número de plano</th><th>Contenido</th><th>Cantidad</th></tr></thead><tbody><tr><td>1-8</td><td>Posición de negras (últimos 8 movimientos)</td><td>8</td></tr><tr><td>9-16</td><td>Posición de blancas (últimos 8 movimientos)</td><td>8</td></tr><tr><td>17</td><td>Turno actual (todo 1 o todo 0)</td><td>1</td></tr></tbody></table>
<p>Estos 17 planos solo incluyen:</p>
<ul>
<li class=""><strong>Estado actual del tablero</strong>: Cada posición tiene piedra negra, blanca o vacía</li>
<li class=""><strong>Información histórica</strong>: Estados del tablero de los últimos 8 movimientos</li>
<li class=""><strong>Información de turno</strong>: Quién juega</li>
</ul>
<p>Sin libertades, sin juicio de escalera, sin distancia al borde -- todo este &quot;conocimiento de Go&quot; lo aprende la red neuronal por sí misma.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="por-qué-la-simplificación-es-buena">¿Por Qué la Simplificación Es Buena?<a href="#por-qué-la-simplificación-es-buena" class="hash-link" aria-label="Enlace directo al ¿Por Qué la Simplificación Es Buena?" title="Enlace directo al ¿Por Qué la Simplificación Es Buena?" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-dejar-que-la-red-descubra-características">1. Dejar que la Red Descubra Características<a href="#1-dejar-que-la-red-descubra-características" class="hash-link" aria-label="Enlace directo al 1. Dejar que la Red Descubra Características" title="Enlace directo al 1. Dejar que la Red Descubra Características" translate="no">​</a></h4>
<p>Características manuales complejas pueden perder información importante, o codificar suposiciones erróneas. Dejar que la red neuronal aprenda de datos crudos puede descubrir mejores representaciones de características.</p>
<p>De hecho, AlphaGo Zero aprendió todas las características diseñadas por humanos (libertades, escalera, etc.), y también aprendió algunos patrones que los humanos no habían identificado conscientemente.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-mejor-generalización">2. Mejor Generalización<a href="#2-mejor-generalización" class="hash-link" aria-label="Enlace directo al 2. Mejor Generalización" title="Enlace directo al 2. Mejor Generalización" translate="no">​</a></h4>
<p>Muchas de las 48 características eran específicas de Go (como escalera, distancia al borde). Los 17 planos simplificados son universales -- cualquier juego de tablero puede codificarse de manera similar.</p>
<p>Esto sentó las bases para el posterior <strong>AlphaZero</strong> (IA de juegos general).</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-reducir-errores-humanos">3. Reducir Errores Humanos<a href="#3-reducir-errores-humanos" class="hash-link" aria-label="Enlace directo al 3. Reducir Errores Humanos" title="Enlace directo al 3. Reducir Errores Humanos" translate="no">​</a></h4>
<p>Las características diseñadas manualmente pueden contener definiciones erróneas o incompletas. La entrada simplificada elimina la posibilidad de tales problemas.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="arquitectura-de-red-única">Arquitectura de Red Única<a href="#arquitectura-de-red-única" class="hash-link" aria-label="Enlace directo al Arquitectura de Red Única" title="Enlace directo al Arquitectura de Red Única" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="diseño-de-doble-red-original">Diseño de Doble Red Original<a href="#diseño-de-doble-red-original" class="hash-link" aria-label="Enlace directo al Diseño de Doble Red Original" title="Enlace directo al Diseño de Doble Red Original" translate="no">​</a></h3>
<p>El AlphaGo original usaba dos redes neuronales independientes:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Policy Network:  Entrada → CNN → Probabilidades de jugada 19x19</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Value Network:   Entrada → CNN → Evaluación de tasa de victoria (-1 a 1)</span><br></span></code></pre></div></div>
<p>Estas dos redes:</p>
<ul>
<li class="">Tenían arquitecturas diferentes (número de capas, canales ligeramente diferentes)</li>
<li class="">Se entrenaban independientemente (primero Policy, luego Value)</li>
<li class="">No compartían ningún parámetro</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="red-de-doble-cabeza-de-zero">Red de Doble Cabeza de Zero<a href="#red-de-doble-cabeza-de-zero" class="hash-link" aria-label="Enlace directo al Red de Doble Cabeza de Zero" title="Enlace directo al Red de Doble Cabeza de Zero" translate="no">​</a></h3>
<p>AlphaGo Zero usa una red única, pero con dos cabezas de salida (heads):</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Entrada → ResNet Backbone Compartido → Policy Head → Probabilidades de jugada 19x19</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                    → Value Head  → Evaluación de tasa de victoria</span><br></span></code></pre></div></div>
<p>Las dos Heads comparten el mismo backbone ResNet (ver <a class="" href="/es/docs/alphago/explained/dual-head-resnet/">siguiente artículo: Red de Doble Cabeza y Redes Residuales</a>), lo que trae varios beneficios:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-eficiencia-de-parámetros">1. Eficiencia de Parámetros<a href="#1-eficiencia-de-parámetros" class="hash-link" aria-label="Enlace directo al 1. Eficiencia de Parámetros" title="Enlace directo al 1. Eficiencia de Parámetros" translate="no">​</a></h4>
<p>Compartir el backbone significa que la mayoría de parámetros son usados por ambas tareas. Esto reduce la cantidad total de parámetros, disminuyendo el riesgo de sobreajuste.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-compartición-de-características">2. Compartición de Características<a href="#2-compartición-de-características" class="hash-link" aria-label="Enlace directo al 2. Compartición de Características" title="Enlace directo al 2. Compartición de Características" translate="no">​</a></h4>
<p>&quot;Dónde debería jugar&quot; (Policy) y &quot;Quién ganará&quot; (Value) necesitan entender patrones de tablero similares. El backbone compartido permite que estas características sean aprendidas y utilizadas simultáneamente por ambas tareas.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-estabilidad-de-entrenamiento">3. Estabilidad de Entrenamiento<a href="#3-estabilidad-de-entrenamiento" class="hash-link" aria-label="Enlace directo al 3. Estabilidad de Entrenamiento" title="Enlace directo al 3. Estabilidad de Entrenamiento" translate="no">​</a></h4>
<p>El entrenamiento conjunto hace que las señales de gradiente vengan de dos fuentes, proporcionando señales de supervisión más ricas, haciendo el entrenamiento más estable.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="el-poder-de-las-redes-residuales">El Poder de las Redes Residuales<a href="#el-poder-de-las-redes-residuales" class="hash-link" aria-label="Enlace directo al El Poder de las Redes Residuales" title="Enlace directo al El Poder de las Redes Residuales" translate="no">​</a></h3>
<p>El backbone de AlphaGo Zero usa una <strong>Red Residual de 40 capas (ResNet)</strong>, mucho más profunda que la CNN de 13 capas del AlphaGo original.</p>
<p>Las conexiones residuales (skip connections) permiten entrenar efectivamente redes profundas, evitando el problema de desvanecimiento de gradientes. Esta fue la tecnología revolucionaria de la competencia ImageNet 2015, aplicada exitosamente por AlphaGo Zero al dominio del Go.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="mejora-en-la-eficiencia-del-entrenamiento">Mejora en la Eficiencia del Entrenamiento<a href="#mejora-en-la-eficiencia-del-entrenamiento" class="hash-link" aria-label="Enlace directo al Mejora en la Eficiencia del Entrenamiento" title="Enlace directo al Mejora en la Eficiencia del Entrenamiento" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="crecimiento-exponencial-del-auto-juego">Crecimiento Exponencial del Auto-juego<a href="#crecimiento-exponencial-del-auto-juego" class="hash-link" aria-label="Enlace directo al Crecimiento Exponencial del Auto-juego" title="Enlace directo al Crecimiento Exponencial del Auto-juego" translate="no">​</a></h3>
<p>El proceso de entrenamiento de AlphaGo Zero mostró una eficiencia asombrosa:</p>
<table><thead><tr><th>Tiempo de entrenamiento</th><th>Puntuación ELO</th><th>Equivalente a</th></tr></thead><tbody><tr><td>0 horas</td><td>0</td><td>Jugadas aleatorias</td></tr><tr><td>3 horas</td><td>~1000</td><td>Descubriendo reglas básicas</td></tr><tr><td>12 horas</td><td>~3000</td><td>Descubriendo joseki</td></tr><tr><td>36 horas</td><td>~4500</td><td>Superando versión Fan Hui</td></tr><tr><td>60 horas</td><td>~5200</td><td>Superando versión Lee Sedol</td></tr><tr><td>72 horas</td><td>~5400</td><td>Superando AlphaGo original</td></tr><tr><td>40 días</td><td>~5600</td><td>Versión más fuerte</td></tr></tbody></table>
<p><strong>Tres días para superar humanos, tres días para superar IA que tomó meses entrenar</strong> -- esta es una mejora de eficiencia exponencial.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="por-qué-tan-rápido">¿Por Qué Tan Rápido?<a href="#por-qué-tan-rápido" class="hash-link" aria-label="Enlace directo al ¿Por Qué Tan Rápido?" title="Enlace directo al ¿Por Qué Tan Rápido?" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-guía-de-búsqueda-más-fuerte">1. Guía de Búsqueda Más Fuerte<a href="#1-guía-de-búsqueda-más-fuerte" class="hash-link" aria-label="Enlace directo al 1. Guía de Búsqueda Más Fuerte" title="Enlace directo al 1. Guía de Búsqueda Más Fuerte" translate="no">​</a></h4>
<p>El MCTS de AlphaGo Zero está completamente guiado por la red neuronal, sin usar más la política de rollout rápido. Esto hace la búsqueda más eficiente y precisa.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-auto-juego-más-rápido">2. Auto-juego Más Rápido<a href="#2-auto-juego-más-rápido" class="hash-link" aria-label="Enlace directo al 2. Auto-juego Más Rápido" title="Enlace directo al 2. Auto-juego Más Rápido" translate="no">​</a></h4>
<p>Ya que solo se necesita una red (en lugar de dos), el costo computacional de cada partida de auto-juego se reduce. Esto significa que se pueden generar más datos de entrenamiento en el mismo tiempo.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-aprendizaje-más-efectivo">3. Aprendizaje Más Efectivo<a href="#3-aprendizaje-más-efectivo" class="hash-link" aria-label="Enlace directo al 3. Aprendizaje Más Efectivo" title="Enlace directo al 3. Aprendizaje Más Efectivo" translate="no">​</a></h4>
<p>El entrenamiento conjunto de la red de doble cabeza hace que la información de cada partida sea utilizada más efectivamente. Los gradientes de Policy y Value se refuerzan mutuamente, acelerando la convergencia.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="comparación-con-aprendizaje-humano">Comparación con Aprendizaje Humano<a href="#comparación-con-aprendizaje-humano" class="hash-link" aria-label="Enlace directo al Comparación con Aprendizaje Humano" title="Enlace directo al Comparación con Aprendizaje Humano" translate="no">​</a></h3>
<p>¿Cuánto tiempo necesitan los jugadores humanos para alcanzar diferentes niveles?</p>
<table><thead><tr><th>Nivel</th><th>Tiempo requerido por humanos</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Principiante</td><td>Semanas</td><td>Minutos</td></tr><tr><td>1 dan amateur</td><td>Años</td><td>Horas</td></tr><tr><td>Nivel profesional</td><td>10-20 años</td><td>1-2 días</td></tr><tr><td>Campeón mundial</td><td>20+ años de dedicación a tiempo completo</td><td>3 días</td></tr><tr><td>Superar humanos</td><td>Imposible</td><td>3 días</td></tr></tbody></table>
<p>Esta comparación no pretende menospreciar a los jugadores humanos -- ellos usan neuronas biológicas, mientras AlphaGo Zero usa TPUs especialmente diseñados y miles de vatios de electricidad. Pero sí demuestra cuán eficiente puede ser el método de aprendizaje correcto.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="generalidad-ajedrez-shogi">Generalidad: Ajedrez, Shogi<a href="#generalidad-ajedrez-shogi" class="hash-link" aria-label="Enlace directo al Generalidad: Ajedrez, Shogi" title="Enlace directo al Generalidad: Ajedrez, Shogi" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="el-nacimiento-de-alphazero">El Nacimiento de AlphaZero<a href="#el-nacimiento-de-alphazero" class="hash-link" aria-label="Enlace directo al El Nacimiento de AlphaZero" title="Enlace directo al El Nacimiento de AlphaZero" translate="no">​</a></h3>
<p>En diciembre de 2017, DeepMind publicó <strong>AlphaZero</strong> -- la versión general de AlphaGo Zero. El mismo algoritmo, solo cambiando las reglas del juego, alcanzó nivel mundial en tres juegos de tablero:</p>
<table><thead><tr><th>Juego</th><th>Tiempo de entrenamiento</th><th>Oponente</th><th>Récord</th></tr></thead><tbody><tr><td>Go</td><td>8 horas</td><td>AlphaGo Zero</td><td>60:40</td></tr><tr><td>Ajedrez</td><td>4 horas</td><td>Stockfish 8</td><td>28 victorias 72 empates 0 derrotas</td></tr><tr><td>Shogi</td><td>2 horas</td><td>Elmo</td><td>90:8:2</td></tr></tbody></table>
<p>Nota los oponentes aquí:</p>
<ul>
<li class=""><strong>Stockfish</strong> era el motor de ajedrez más fuerte entonces, usando décadas de conocimiento humano y optimización</li>
<li class=""><strong>Elmo</strong> era la IA de Shogi más fuerte entonces</li>
</ul>
<p>AlphaZero con unas pocas horas de entrenamiento superó estos sistemas especializados desarrollados durante años.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="el-significado-de-la-generalidad">El Significado de la Generalidad<a href="#el-significado-de-la-generalidad" class="hash-link" aria-label="Enlace directo al El Significado de la Generalidad" title="Enlace directo al El Significado de la Generalidad" translate="no">​</a></h3>
<p>AlphaGo Zero / AlphaZero probó algo importante:</p>
<blockquote>
<p><strong>El mismo algoritmo de aprendizaje puede alcanzar nivel sobrehumano en diferentes dominios.</strong></p>
</blockquote>
<p>Estos no son tres IAs diferentes, sino un marco de aprendizaje general:</p>
<ol>
<li class=""><strong>Auto-juego</strong> genera experiencia</li>
<li class=""><strong>Búsqueda de Árbol Monte Carlo</strong> explora posibilidades</li>
<li class=""><strong>Redes neuronales</strong> aprenden funciones de política y valor</li>
<li class=""><strong>Aprendizaje por refuerzo</strong> optimiza la función objetivo</li>
</ol>
<p>Este marco no depende de conocimiento específico del dominio, dando un paso importante hacia la generalización de la IA.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="impacto-en-la-ia-tradicional">Impacto en la IA Tradicional<a href="#impacto-en-la-ia-tradicional" class="hash-link" aria-label="Enlace directo al Impacto en la IA Tradicional" title="Enlace directo al Impacto en la IA Tradicional" translate="no">​</a></h3>
<p>Antes de AlphaZero, las IAs más fuertes de ajedrez y shogi eran estilo &quot;sistema experto&quot;:</p>
<ul>
<li class=""><strong>Mucho conocimiento humano</strong>: Libros de apertura, tablas de finales, funciones de evaluación</li>
<li class=""><strong>Décadas de optimización</strong>: Esfuerzo de incontables jugadores e ingenieros</li>
<li class=""><strong>Extremadamente especializadas</strong>: Stockfish no puede jugar Go, Elmo no puede jugar ajedrez</li>
</ul>
<p>AlphaZero superó todo esto con un algoritmo general en unas pocas horas. Esto hizo que muchos investigadores de IA reconsideraran:</p>
<blockquote>
<p>¿Deberíamos invertir más esfuerzo en &quot;algoritmos de aprendizaje general&quot; o en &quot;codificación de conocimiento experto&quot;?</p>
</blockquote>
<p>La respuesta parece cada vez más clara: dejar que la máquina aprenda por sí misma es más efectivo que enseñarle conocimiento.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="el-estilo-de-juego-de-alphago-zero">El Estilo de Juego de AlphaGo Zero<a href="#el-estilo-de-juego-de-alphago-zero" class="hash-link" aria-label="Enlace directo al El Estilo de Juego de AlphaGo Zero" title="Enlace directo al El Estilo de Juego de AlphaGo Zero" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="superando-la-estética-humana">Superando la Estética Humana<a href="#superando-la-estética-humana" class="hash-link" aria-label="Enlace directo al Superando la Estética Humana" title="Enlace directo al Superando la Estética Humana" translate="no">​</a></h3>
<p>La comunidad del Go tiene una evaluación universal del estilo de juego de AlphaGo Zero: <strong>más elegante</strong>.</p>
<p>Los movimientos de AlphaGo Lee a veces parecían &quot;extraños&quot; -- como el movimiento 37, los humanos necesitaron análisis posterior para entender su brillantez. Pero los movimientos de AlphaGo Zero a menudo se evaluaban después como &quot;obviamente buenos a primera vista&quot;.</p>
<p>Esto puede ser porque:</p>
<ol>
<li class=""><strong>Mayor fuerza de juego</strong>: Zero puede ver más profundo, jugando más compuesto</li>
<li class=""><strong>Sin sesgos humanos</strong>: No restringido por joseki tradicional</li>
<li class=""><strong>Objetivo consistente</strong>: Solo persigue tasa de victoria, no imita humanos</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="redescubriendo-la-teoría-del-go-humana">Redescubriendo la Teoría del Go Humana<a href="#redescubriendo-la-teoría-del-go-humana" class="hash-link" aria-label="Enlace directo al Redescubriendo la Teoría del Go Humana" title="Enlace directo al Redescubriendo la Teoría del Go Humana" translate="no">​</a></h3>
<p>Interesantemente, AlphaGo Zero &quot;redescubrió&quot; durante el entrenamiento el conocimiento del Go acumulado por humanos durante miles de años:</p>
<ul>
<li class=""><strong>Joseki</strong>: Zero descubrió muchas joseki comunes, porque estas son realmente las soluciones óptimas para ambos lados</li>
<li class=""><strong>Principios de apertura</strong>: Importancia de esquinas, lados, centro en ese orden</li>
<li class=""><strong>Conocimiento de forma</strong>: Diferencia entre mala forma y buena forma</li>
</ul>
<p>Esto validó la racionalidad de la teoría del Go humana -- este conocimiento no es coincidencia, sino un reflejo de la naturaleza del Go.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="innovaciones-que-superan-a-humanos">Innovaciones Que Superan a Humanos<a href="#innovaciones-que-superan-a-humanos" class="hash-link" aria-label="Enlace directo al Innovaciones Que Superan a Humanos" title="Enlace directo al Innovaciones Que Superan a Humanos" translate="no">​</a></h3>
<p>Pero Zero también descubrió movimientos que los humanos nunca habían pensado:</p>
<ul>
<li class=""><strong>Aperturas no convencionales</strong>: Variaciones sobre aperturas tradicionales</li>
<li class=""><strong>Sacrificios agresivos</strong>: Más dispuesto que humanos a abandonar ventaja local por ventaja global</li>
<li class=""><strong>Formas contra-intuitivas</strong>: &quot;Mala forma&quot; superficial que en realidad es óptima</li>
</ul>
<p>Estas innovaciones están cambiando la comprensión humana del Go. Muchos jugadores profesionales dicen que estudiar las partidas de AlphaGo Zero les dio una comprensión completamente nueva del Go.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="resumen-de-detalles-técnicos">Resumen de Detalles Técnicos<a href="#resumen-de-detalles-técnicos" class="hash-link" aria-label="Enlace directo al Resumen de Detalles Técnicos" title="Enlace directo al Resumen de Detalles Técnicos" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="comparación-completa-con-alphago-original">Comparación Completa con AlphaGo Original<a href="#comparación-completa-con-alphago-original" class="hash-link" aria-label="Enlace directo al Comparación Completa con AlphaGo Original" title="Enlace directo al Comparación Completa con AlphaGo Original" translate="no">​</a></h3>
<table><thead><tr><th>Aspecto</th><th>AlphaGo (original)</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td><strong>Datos de entrenamiento</strong></td><td>Partidas humanas + auto-juego</td><td>Auto-juego puro</td></tr><tr><td><strong>Método de aprendizaje</strong></td><td>Supervisado + refuerzo</td><td>Refuerzo puro</td></tr><tr><td><strong>Características de entrada</strong></td><td>48 planos</td><td>17 planos</td></tr><tr><td><strong>Arquitectura de red</strong></td><td>Policy/Value separadas</td><td>ResNet de doble cabeza</td></tr><tr><td><strong>Profundidad de red</strong></td><td>13 capas</td><td>40 capas (o más)</td></tr><tr><td><strong>Evaluación MCTS</strong></td><td>Red neuronal + Rollout</td><td>Red neuronal pura</td></tr><tr><td><strong>Búsquedas por movimiento</strong></td><td>~100,000</td><td>~1,600</td></tr><tr><td><strong>TPUs de entrenamiento</strong></td><td>50+</td><td>4</td></tr><tr><td><strong>TPUs de inferencia</strong></td><td>48</td><td>4 (escalable)</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="algoritmo-central">Algoritmo Central<a href="#algoritmo-central" class="hash-link" aria-label="Enlace directo al Algoritmo Central" title="Enlace directo al Algoritmo Central" translate="no">​</a></h3>
<p>El bucle de entrenamiento de AlphaGo Zero es muy conciso:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. Auto-juego</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Usar red actual para MCTS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Seleccionar movimientos según probabilidades de búsqueda MCTS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Registrar cada movimiento (posición, probabilidades MCTS, resultado)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Entrenar red</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Muestrear del pool de experiencia</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Policy Head: Minimizar entropía cruzada con probabilidades MCTS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Value Head: Minimizar error cuadrático medio con resultado real</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Optimizar conjuntamente ambos objetivos</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Actualizar red</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Reemplazar red vieja con nueva (verificar que nueva red sea más fuerte jugando)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Volver al paso 1</span><br></span></code></pre></div></div>
<p>Este bucle se ejecuta continuamente, la red se vuelve más fuerte constantemente. Sin datos humanos, sin conocimiento humano, solo reglas del juego y objetivo de victoria.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="implicaciones-para-la-investigación-en-ia">Implicaciones para la Investigación en IA<a href="#implicaciones-para-la-investigación-en-ia" class="hash-link" aria-label="Enlace directo al Implicaciones para la Investigación en IA" title="Enlace directo al Implicaciones para la Investigación en IA" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="aprendizaje-desde-primeros-principios">Aprendizaje desde Primeros Principios<a href="#aprendizaje-desde-primeros-principios" class="hash-link" aria-label="Enlace directo al Aprendizaje desde Primeros Principios" title="Enlace directo al Aprendizaje desde Primeros Principios" translate="no">​</a></h3>
<p>AlphaGo Zero demostró un método de aprendizaje de &quot;primeros principios&quot;:</p>
<blockquote>
<p>No digas a la IA cómo hacerlo, solo dile cuál es el objetivo, deja que descubra el método por sí misma.</p>
</blockquote>
<p>Esto contrasta fuertemente con el enfoque tradicional de sistemas expertos. Los sistemas expertos intentan codificar conocimiento humano en la IA, mientras AlphaGo Zero deja que la IA descubra el conocimiento por sí misma.</p>
<p>El resultado es: el conocimiento que descubre la IA puede ser más completo y preciso que el conocimiento humano.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="el-poder-del-auto-juego">El Poder del Auto-juego<a href="#el-poder-del-auto-juego" class="hash-link" aria-label="Enlace directo al El Poder del Auto-juego" title="Enlace directo al El Poder del Auto-juego" translate="no">​</a></h3>
<p>AlphaGo Zero probó que el auto-juego puede generar datos de entrenamiento infinitos, y la calidad de estos datos mejora a medida que la red mejora.</p>
<p>Este es un &quot;ciclo positivo&quot;:</p>
<ul>
<li class="">Red más fuerte → Mejores datos de auto-juego</li>
<li class="">Mejores datos → Red más fuerte</li>
</ul>
<p>Este ciclo puede continuar ejecutándose hasta alcanzar el límite teórico del juego (si existe).</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="la-importancia-de-la-simplificación">La Importancia de la Simplificación<a href="#la-importancia-de-la-simplificación" class="hash-link" aria-label="Enlace directo al La Importancia de la Simplificación" title="Enlace directo al La Importancia de la Simplificación" translate="no">​</a></h3>
<p>El éxito de AlphaGo Zero probó la importancia de la &quot;simplificación&quot;:</p>
<ul>
<li class="">Simplificar entrada (48 → 17)</li>
<li class="">Simplificar arquitectura (doble red → red única)</li>
<li class="">Simplificar entrenamiento (supervisado + refuerzo → refuerzo puro)</li>
</ul>
<p>Cada simplificación hizo el sistema más poderoso. Esto nos dice: complejidad no es igual a bueno, la solución más simple a menudo es la mejor.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="correspondencia-con-animaciones">Correspondencia con Animaciones<a href="#correspondencia-con-animaciones" class="hash-link" aria-label="Enlace directo al Correspondencia con Animaciones" title="Enlace directo al Correspondencia con Animaciones" translate="no">​</a></h2>
<p>Conceptos centrales cubiertos en este artículo y sus números de animación:</p>
<table><thead><tr><th>Número</th><th>Concepto</th><th>Correspondencia Física/Matemática</th></tr></thead><tbody><tr><td>E7</td><td>Entrenamiento desde cero</td><td>Fenómeno de auto-organización</td></tr><tr><td>E5</td><td>Auto-juego</td><td>Convergencia de punto fijo</td></tr><tr><td>E12</td><td>Curva de crecimiento de fuerza</td><td>Crecimiento en S</td></tr><tr><td>D12</td><td>Red residual</td><td>Autopista de gradientes</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="lecturas-adicionales">Lecturas Adicionales<a href="#lecturas-adicionales" class="hash-link" aria-label="Enlace directo al Lecturas Adicionales" title="Enlace directo al Lecturas Adicionales" translate="no">​</a></h2>
<ul>
<li class=""><strong>Siguiente artículo</strong>: <a class="" href="/es/docs/alphago/explained/dual-head-resnet/">Red de Doble Cabeza y Redes Residuales</a> — Arquitectura de red neuronal de AlphaGo Zero en detalle</li>
<li class=""><strong>Artículo relacionado</strong>: <a class="" href="/es/docs/alphago/explained/self-play/">Auto-juego</a> — Por qué el auto-juego puede producir nivel sobrehumano</li>
<li class=""><strong>Profundización técnica</strong>: <a class="" href="/es/docs/alphago/explained/training-from-scratch/">Proceso de Entrenamiento desde Cero</a> — Evolución detallada del Día 0-3</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="referencias">Referencias<a href="#referencias" class="hash-link" aria-label="Enlace directo al Referencias" title="Enlace directo al Referencias" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">Silver, D., et al. (2018). &quot;A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.&quot; <em>Science</em>, 362(6419), 1140-1144.</li>
<li class="">DeepMind. (2017). &quot;AlphaGo Zero: Starting from scratch.&quot; <em>DeepMind Blog</em>.</li>
<li class="">Schrittwieser, J., et al. (2020). &quot;Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.&quot; <em>Nature</em>, 588, 604-609.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/explained/16-alphago-zero.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Editar esta página</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Página del documento"><a class="pagination-nav__link pagination-nav__link--prev" href="/es/docs/alphago/explained/puct-formula/"><div class="pagination-nav__sublabel">Anterior</div><div class="pagination-nav__label">Explicación Detallada de la Fórmula PUCT</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/es/docs/alphago/explained/dual-head-resnet/"><div class="pagination-nav__sublabel">Siguiente</div><div class="pagination-nav__label">Red de Doble Cabeza y Redes Residuales</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#por-qué-no-se-necesitan-partidas-humanas" class="table-of-contents__link toc-highlight">¿Por Qué No Se Necesitan Partidas Humanas?</a><ul><li><a href="#limitaciones-de-las-partidas-humanas" class="table-of-contents__link toc-highlight">Limitaciones de las Partidas Humanas</a></li><li><a href="#el-avance-de-zero" class="table-of-contents__link toc-highlight">El Avance de Zero</a></li></ul></li><li><a href="#comparación-con-alphago-original-1000" class="table-of-contents__link toc-highlight">Comparación con AlphaGo Original: 100:0</a><ul><li><a href="#victoria-aplastante" class="table-of-contents__link toc-highlight">Victoria Aplastante</a></li><li><a href="#menos-recursos-mayor-fuerza" class="table-of-contents__link toc-highlight">Menos Recursos, Mayor Fuerza</a></li><li><a href="#por-qué-zero-es-más-fuerte" class="table-of-contents__link toc-highlight">¿Por Qué Zero Es Más Fuerte?</a></li></ul></li><li><a href="#características-de-entrada-simplificadas-de-48-a-17" class="table-of-contents__link toc-highlight">Características de Entrada Simplificadas: De 48 a 17</a><ul><li><a href="#los-48-planos-de-características-del-alphago-original" class="table-of-contents__link toc-highlight">Los 48 Planos de Características del AlphaGo Original</a></li><li><a href="#los-17-planos-de-características-de-alphago-zero" class="table-of-contents__link toc-highlight">Los 17 Planos de Características de AlphaGo Zero</a></li><li><a href="#por-qué-la-simplificación-es-buena" class="table-of-contents__link toc-highlight">¿Por Qué la Simplificación Es Buena?</a></li></ul></li><li><a href="#arquitectura-de-red-única" class="table-of-contents__link toc-highlight">Arquitectura de Red Única</a><ul><li><a href="#diseño-de-doble-red-original" class="table-of-contents__link toc-highlight">Diseño de Doble Red Original</a></li><li><a href="#red-de-doble-cabeza-de-zero" class="table-of-contents__link toc-highlight">Red de Doble Cabeza de Zero</a></li><li><a href="#el-poder-de-las-redes-residuales" class="table-of-contents__link toc-highlight">El Poder de las Redes Residuales</a></li></ul></li><li><a href="#mejora-en-la-eficiencia-del-entrenamiento" class="table-of-contents__link toc-highlight">Mejora en la Eficiencia del Entrenamiento</a><ul><li><a href="#crecimiento-exponencial-del-auto-juego" class="table-of-contents__link toc-highlight">Crecimiento Exponencial del Auto-juego</a></li><li><a href="#por-qué-tan-rápido" class="table-of-contents__link toc-highlight">¿Por Qué Tan Rápido?</a></li><li><a href="#comparación-con-aprendizaje-humano" class="table-of-contents__link toc-highlight">Comparación con Aprendizaje Humano</a></li></ul></li><li><a href="#generalidad-ajedrez-shogi" class="table-of-contents__link toc-highlight">Generalidad: Ajedrez, Shogi</a><ul><li><a href="#el-nacimiento-de-alphazero" class="table-of-contents__link toc-highlight">El Nacimiento de AlphaZero</a></li><li><a href="#el-significado-de-la-generalidad" class="table-of-contents__link toc-highlight">El Significado de la Generalidad</a></li><li><a href="#impacto-en-la-ia-tradicional" class="table-of-contents__link toc-highlight">Impacto en la IA Tradicional</a></li></ul></li><li><a href="#el-estilo-de-juego-de-alphago-zero" class="table-of-contents__link toc-highlight">El Estilo de Juego de AlphaGo Zero</a><ul><li><a href="#superando-la-estética-humana" class="table-of-contents__link toc-highlight">Superando la Estética Humana</a></li><li><a href="#redescubriendo-la-teoría-del-go-humana" class="table-of-contents__link toc-highlight">Redescubriendo la Teoría del Go Humana</a></li><li><a href="#innovaciones-que-superan-a-humanos" class="table-of-contents__link toc-highlight">Innovaciones Que Superan a Humanos</a></li></ul></li><li><a href="#resumen-de-detalles-técnicos" class="table-of-contents__link toc-highlight">Resumen de Detalles Técnicos</a><ul><li><a href="#comparación-completa-con-alphago-original" class="table-of-contents__link toc-highlight">Comparación Completa con AlphaGo Original</a></li><li><a href="#algoritmo-central" class="table-of-contents__link toc-highlight">Algoritmo Central</a></li></ul></li><li><a href="#implicaciones-para-la-investigación-en-ia" class="table-of-contents__link toc-highlight">Implicaciones para la Investigación en IA</a><ul><li><a href="#aprendizaje-desde-primeros-principios" class="table-of-contents__link toc-highlight">Aprendizaje desde Primeros Principios</a></li><li><a href="#el-poder-del-auto-juego" class="table-of-contents__link toc-highlight">El Poder del Auto-juego</a></li><li><a href="#la-importancia-de-la-simplificación" class="table-of-contents__link toc-highlight">La Importancia de la Simplificación</a></li></ul></li><li><a href="#correspondencia-con-animaciones" class="table-of-contents__link toc-highlight">Correspondencia con Animaciones</a></li><li><a href="#lecturas-adicionales" class="table-of-contents__link toc-highlight">Lecturas Adicionales</a></li><li><a href="#referencias" class="table-of-contents__link toc-highlight">Referencias</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>