<!doctype html>
<html lang="es" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-alphago/policy-network" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Policy Network en detalle | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/es/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/es/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/es/docs/alphago/policy-network/"><meta data-rh="true" property="og:locale" content="es"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="es"><meta data-rh="true" name="docsearch:language" content="es"><meta data-rh="true" name="keywords" content="圍棋, Go, 好棋寶寶, AI, KataGo, AlphaGo, 圍棋教學, 圍棋入門"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Policy Network en detalle | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="Comprensión profunda de la arquitectura de la red de políticas de AlphaGo, métodos de entrenamiento y aplicaciones prácticas, desde 13 capas convolucionales hasta la salida Softmax"><meta data-rh="true" property="og:description" content="Comprensión profunda de la arquitectura de la red de políticas de AlphaGo, métodos de entrenamiento y aplicaciones prácticas, desde 13 capas convolucionales hasta la salida Softmax"><link data-rh="true" rel="icon" href="/es/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/es/docs/alphago/policy-network/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/policy-network/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/alphago/policy-network/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/alphago/policy-network/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/alphago/policy-network/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/alphago/policy-network/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/alphago/policy-network/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/alphago/policy-network/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/alphago/policy-network/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/alphago/policy-network/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/alphago/policy-network/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/alphago/policy-network/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/policy-network/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AlphaGo","item":"https://www.weiqi.kids/es/docs/alphago/"},{"@type":"ListItem","position":2,"name":"Policy Network en detalle","item":"https://www.weiqi.kids/es/docs/alphago/policy-network"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/es/assets/css/styles.f23bf74b.css">
<script src="/es/assets/js/runtime~main.83035c8e.js" defer="defer"></script>
<script src="/es/assets/js/main.ea58415e.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/es/img/logo.svg"><div role="region" aria-label="Saltar al contenido principal"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">Saltar al contenido principal</a></div><nav aria-label="Principal" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Alternar barra lateral" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/es/"><div class="navbar__logo"><img src="/es/img/logo.svg" alt="Logo de la Asociación Weiqi Kids" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/es/img/logo.svg" alt="Logo de la Asociación Weiqi Kids" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">Weiqi.Kids</b></a><a class="navbar__item navbar__link" href="/es/docs/learn/">Aprender Go</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/es/docs/alphago/">AlphaGo</a><a class="navbar__item navbar__link" href="/es/docs/animations/">Estudio de Animación</a><a class="navbar__item navbar__link" href="/es/docs/tech/">Documentación Técnica</a><a class="navbar__item navbar__link" href="/es/docs/about/">Sobre Nosotros</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>Español</a><ul class="dropdown__menu"><li><a href="/docs/alphago/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/alphago/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/alphago/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/alphago/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/alphago/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/alphago/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/alphago/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="es">Español</a></li><li><a href="/pt/docs/alphago/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/alphago/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/alphago/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/alphago/policy-network/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="Volver al principio" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="Barra lateral de Documentos" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/es/docs/intro/"><span title="Guia de uso" class="linkLabel_REp1">Guia de uso</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/es/docs/alphago/"><span title="AlphaGo" class="categoryLinkLabel_ezQx">AlphaGo</span></a><button aria-label="Colapsar categoría &#x27;AlphaGo&#x27; de la barra lateral" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/birth-of-alphago/"><span title="El Nacimiento de AlphaGo" class="linkLabel_REp1">El Nacimiento de AlphaGo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/key-matches/"><span title="Revisiones de partidas clave" class="linkLabel_REp1">Revisiones de partidas clave</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/move-37/"><span title="Análisis Profundo del &quot;Movimiento Divino&quot;" class="linkLabel_REp1">Análisis Profundo del &quot;Movimiento Divino&quot;</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/why-go-is-hard/"><span title="¿Por Qué es Difícil el Go?" class="linkLabel_REp1">¿Por Qué es Difícil el Go?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/traditional-limits/"><span title="Los Limites de los Metodos Tradicionales" class="linkLabel_REp1">Los Limites de los Metodos Tradicionales</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/board-representation/"><span title="Representacion del Estado del Tablero" class="linkLabel_REp1">Representacion del Estado del Tablero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/es/docs/alphago/policy-network/"><span title="Policy Network en detalle" class="linkLabel_REp1">Policy Network en detalle</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/value-network/"><span title="Value Network en detalle" class="linkLabel_REp1">Value Network en detalle</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/input-features/"><span title="Diseno de Caracteristicas de Entrada" class="linkLabel_REp1">Diseno de Caracteristicas de Entrada</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/cnn-and-go/"><span title="CNN y Go" class="linkLabel_REp1">CNN y Go</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/supervised-learning/"><span title="Fase de aprendizaje supervisado" class="linkLabel_REp1">Fase de aprendizaje supervisado</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/reinforcement-intro/"><span title="Introducción al aprendizaje por refuerzo" class="linkLabel_REp1">Introducción al aprendizaje por refuerzo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/self-play/"><span title="Auto-juego" class="linkLabel_REp1">Auto-juego</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/mcts-neural-combo/"><span title="La Combinación de MCTS y Redes Neuronales" class="linkLabel_REp1">La Combinación de MCTS y Redes Neuronales</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/puct-formula/"><span title="Explicación Detallada de la Fórmula PUCT" class="linkLabel_REp1">Explicación Detallada de la Fórmula PUCT</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/alphago-zero/"><span title="Visión General de AlphaGo Zero" class="linkLabel_REp1">Visión General de AlphaGo Zero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/dual-head-resnet/"><span title="Red de Doble Cabeza y Redes Residuales" class="linkLabel_REp1">Red de Doble Cabeza y Redes Residuales</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/training-from-scratch/"><span title="El proceso de entrenamiento desde cero" class="linkLabel_REp1">El proceso de entrenamiento desde cero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/distributed-systems/"><span title="Sistemas distribuidos y TPU" class="linkLabel_REp1">Sistemas distribuidos y TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/alphago/legacy-and-impact/"><span title="El legado de AlphaGo" class="linkLabel_REp1">El legado de AlphaGo</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/es/docs/learn/"><span title="學圍棋" class="categoryLinkLabel_ezQx">學圍棋</span></a><button aria-label="Ampliar la categoría &#x27;學圍棋&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/es/docs/animations/"><span title="動畫教室" class="linkLabel_REp1">動畫教室</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/es/docs/tech/"><span title="技術文件" class="categoryLinkLabel_ezQx">技術文件</span></a><button aria-label="Ampliar la categoría &#x27;技術文件&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/es/docs/about/"><span title="關於我們" class="categoryLinkLabel_ezQx">關於我們</span></a><button aria-label="Ampliar la categoría &#x27;關於我們&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="Rastro de navegación"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Página de Inicio" class="breadcrumbs__link" href="/es/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/es/docs/alphago/"><span>AlphaGo</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Policy Network en detalle</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">En esta página</button></div><div class="theme-doc-markdown markdown"><header><h1>Policy Network en detalle</h1></header>
<p>En cualquier posición de Go, hay un promedio de 250 movimientos legales. Si la computadora elige al azar, nunca podrá jugar bien.</p>
<p>El avance de AlphaGo fue: aprendió a &quot;mirar el tablero y saber qué posiciones vale la pena considerar&quot;.</p>
<p>Esta capacidad proviene de la <strong>Policy Network (red de políticas)</strong>.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="qué-es-la-policy-network">¿Qué es la Policy Network?<a href="#qué-es-la-policy-network" class="hash-link" aria-label="Enlace directo al ¿Qué es la Policy Network?" title="Enlace directo al ¿Qué es la Policy Network?" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="función-principal">Función principal<a href="#función-principal" class="hash-link" aria-label="Enlace directo al Función principal" title="Enlace directo al Función principal" translate="no">​</a></h3>
<p>La Policy Network es una red neuronal convolucional profunda cuya tarea es:</p>
<blockquote>
<p><strong>Dado el estado actual del tablero, generar la probabilidad de jugar en cada posición</strong></p>
</blockquote>
<p>Expresado matemáticamente:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">p = f_θ(s)</span><br></span></code></pre></div></div>
<p>Donde:</p>
<ul>
<li class=""><code>s</code>: estado actual del tablero (tablero de 19×19 + otras características)</li>
<li class=""><code>f_θ</code>: Policy Network (θ son los parámetros de la red)</li>
<li class=""><code>p</code>: distribución de probabilidad sobre 361 posiciones (incluyendo pasar)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="comprensión-intuitiva">Comprensión intuitiva<a href="#comprensión-intuitiva" class="hash-link" aria-label="Enlace directo al Comprensión intuitiva" title="Enlace directo al Comprensión intuitiva" translate="no">​</a></h3>
<p>Imagina que eres un jugador profesional. Cuando ves una posición, tu cerebro automáticamente &quot;ilumina&quot; varias posiciones importantes — estas son las que intuitivamente consideras que vale la pena analizar.</p>
<p>La Policy Network simula este proceso.</p>
<div>載入中...</div>
<p>El mapa de calor anterior muestra la salida de la Policy Network. Cuanto más brillante es el color de una posición, más valiosa la considera el modelo.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="por-qué-se-necesita-la-policy-network">¿Por qué se necesita la Policy Network?<a href="#por-qué-se-necesita-la-policy-network" class="hash-link" aria-label="Enlace directo al ¿Por qué se necesita la Policy Network?" title="Enlace directo al ¿Por qué se necesita la Policy Network?" translate="no">​</a></h3>
<p>El espacio de búsqueda en Go es demasiado grande. Si se buscan todos los movimientos posibles sin filtrar:</p>
<table><thead><tr><th>Estrategia</th><th>Movimientos por turno</th><th>Nodos al buscar 10 movimientos</th></tr></thead><tbody><tr><td>Considerar todos</td><td>361</td><td>361^10 ≈ 10^25</td></tr><tr><td>Filtrado por Policy Network</td><td>~20</td><td>20^10 ≈ 10^13</td></tr></tbody></table>
<p>La Policy Network reduce el espacio de búsqueda en <strong>10^12 veces</strong> (un billón de veces).</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="arquitectura-de-la-red">Arquitectura de la red<a href="#arquitectura-de-la-red" class="hash-link" aria-label="Enlace directo al Arquitectura de la red" title="Enlace directo al Arquitectura de la red" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="estructura-general">Estructura general<a href="#estructura-general" class="hash-link" aria-label="Enlace directo al Estructura general" title="Enlace directo al Estructura general" translate="no">​</a></h3>
<p>La Policy Network de AlphaGo utiliza una arquitectura de red neuronal convolucional profunda (CNN):</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Capa de entrada → Capas convolucionales ×12 → Capa convolucional de salida → Softmax</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       ↓                 ↓                            ↓                    ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   19×19×48          19×19×192                     19×19×1            362 probabilidades</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="capa-de-entrada">Capa de entrada<a href="#capa-de-entrada" class="hash-link" aria-label="Enlace directo al Capa de entrada" title="Enlace directo al Capa de entrada" translate="no">​</a></h3>
<p>La entrada es un tensor de características de <strong>19×19×48</strong>:</p>
<ul>
<li class=""><strong>19×19</strong>: tamaño del tablero</li>
<li class=""><strong>48</strong>: 48 planos de características (ver <a class="" href="/es/docs/alphago/input-features/">Diseño de características de entrada</a>)</li>
</ul>
<p>Estos 48 planos incluyen:</p>
<ul>
<li class="">Posiciones de piedras negras y blancas</li>
<li class="">Historial de los últimos 8 movimientos</li>
<li class="">Libertades, atari, escalera y otras características</li>
<li class="">Legalidad (qué posiciones son jugables)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="capas-convolucionales">Capas convolucionales<a href="#capas-convolucionales" class="hash-link" aria-label="Enlace directo al Capas convolucionales" title="Enlace directo al Capas convolucionales" translate="no">​</a></h3>
<p>La red contiene <strong>12 capas convolucionales</strong>, cada una con la siguiente configuración:</p>
<table><thead><tr><th>Parámetro</th><th>Valor</th><th>Descripción</th></tr></thead><tbody><tr><td>Número de filtros</td><td>192</td><td>Cada capa genera 192 mapas de características</td></tr><tr><td>Tamaño del kernel</td><td>3×3 (primera capa 5×5)</td><td>Cada vez observa un área de 3×3</td></tr><tr><td>Tipo de padding</td><td>same</td><td>Mantiene el tamaño 19×19</td></tr><tr><td>Función de activación</td><td>ReLU</td><td>max(0, x)</td></tr></tbody></table>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="por-qué-192-filtros">¿Por qué 192 filtros?<a href="#por-qué-192-filtros" class="hash-link" aria-label="Enlace directo al ¿Por qué 192 filtros?" title="Enlace directo al ¿Por qué 192 filtros?" translate="no">​</a></h4>
<p>Es un valor empírico. Muy pocos limitarían la capacidad del modelo, demasiados aumentarían el cálculo y el riesgo de sobreajuste. El equipo de DeepMind determinó mediante experimentos que 192 es un buen equilibrio.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="por-qué-kernels-de-33">¿Por qué kernels de 3×3?<a href="#por-qué-kernels-de-33" class="hash-link" aria-label="Enlace directo al ¿Por qué kernels de 3×3?" title="Enlace directo al ¿Por qué kernels de 3×3?" translate="no">​</a></h4>
<p>3×3 es el tamaño más común en redes neuronales convolucionales por estas razones:</p>
<ol>
<li class=""><strong>Suficiente para capturar patrones locales</strong>: ojos, conexiones, cortes en Go están dentro del rango 3×3</li>
<li class=""><strong>Alta eficiencia computacional</strong>: menos parámetros que kernels grandes</li>
<li class=""><strong>Apilables</strong>: múltiples convoluciones 3×3 pueden lograr un campo receptivo grande</li>
</ol>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="por-qué-la-primera-capa-usa-55">¿Por qué la primera capa usa 5×5?<a href="#por-qué-la-primera-capa-usa-55" class="hash-link" aria-label="Enlace directo al ¿Por qué la primera capa usa 5×5?" title="Enlace directo al ¿Por qué la primera capa usa 5×5?" translate="no">​</a></h4>
<p>La primera capa usa un kernel de 5×5 más grande para capturar patrones de mayor alcance desde la entrada (como saltos pequeños). Esta es una decisión de diseño; el posterior AlphaGo Zero usa uniformemente 3×3.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="función-de-activación-relu">Función de activación ReLU<a href="#función-de-activación-relu" class="hash-link" aria-label="Enlace directo al Función de activación ReLU" title="Enlace directo al Función de activación ReLU" translate="no">​</a></h3>
<p>Cada capa convolucional va seguida de la función de activación ReLU (Rectified Linear Unit):</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">ReLU(x) = max(0, x)</span><br></span></code></pre></div></div>
<p>¿Por qué usar ReLU?</p>
<ol>
<li class=""><strong>Cálculo simple</strong>: solo tomar el máximo, mucho más rápido que sigmoid</li>
<li class=""><strong>Mitiga el desvanecimiento del gradiente</strong>: gradiente constante de 1 en la región positiva</li>
<li class=""><strong>Activación dispersa</strong>: los valores negativos se reducen a cero, produciendo representaciones dispersas</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="capa-de-salida">Capa de salida<a href="#capa-de-salida" class="hash-link" aria-label="Enlace directo al Capa de salida" title="Enlace directo al Capa de salida" translate="no">​</a></h3>
<p>La última capa es una capa convolucional especial:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">19×19×192 → Convolución(1×1, 1 filtro) → 19×19×1 → Aplanar → Vector 362-dim → Softmax</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="convolución-11">Convolución 1×1<a href="#convolución-11" class="hash-link" aria-label="Enlace directo al Convolución 1×1" title="Enlace directo al Convolución 1×1" translate="no">​</a></h4>
<p>La capa de salida usa convolución 1×1, comprimiendo 192 canales a 1. Esto equivale a hacer una combinación lineal de las 192 características dimensionales para cada posición.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="salida-softmax">Salida Softmax<a href="#salida-softmax" class="hash-link" aria-label="Enlace directo al Salida Softmax" title="Enlace directo al Salida Softmax" translate="no">​</a></h4>
<p>El vector de 362 dimensiones (361 posiciones del tablero + 1 para pasar) pasa por la función Softmax:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Softmax(z_i) = exp(z_i) / Σ_j exp(z_j)</span><br></span></code></pre></div></div>
<p>Softmax asegura que la salida sea una distribución de probabilidad válida:</p>
<ul>
<li class="">Todos los valores están entre 0 y 1</li>
<li class="">La suma de todos los valores es 1</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="cantidad-de-parámetros">Cantidad de parámetros<a href="#cantidad-de-parámetros" class="hash-link" aria-label="Enlace directo al Cantidad de parámetros" title="Enlace directo al Cantidad de parámetros" translate="no">​</a></h3>
<p>Calculemos la cantidad total de parámetros de la red:</p>
<table><thead><tr><th>Capa</th><th>Cálculo</th><th>Cantidad de parámetros</th></tr></thead><tbody><tr><td>Primera capa conv</td><td>5×5×48×192 + 192</td><td>230,592</td></tr><tr><td>Capas conv intermedias ×11</td><td>(3×3×192×192 + 192) × 11</td><td>3,633,792</td></tr><tr><td>Capa conv de salida</td><td>1×1×192×1 + 1</td><td>193</td></tr><tr><td><strong>Total</strong></td><td></td><td><strong>~3.9M</strong></td></tr></tbody></table>
<p>Aproximadamente <strong>3.9 millones de parámetros</strong>, una red pequeña según los estándares actuales.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="objetivo-y-métodos-de-entrenamiento">Objetivo y métodos de entrenamiento<a href="#objetivo-y-métodos-de-entrenamiento" class="hash-link" aria-label="Enlace directo al Objetivo y métodos de entrenamiento" title="Enlace directo al Objetivo y métodos de entrenamiento" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="datos-de-entrenamiento">Datos de entrenamiento<a href="#datos-de-entrenamiento" class="hash-link" aria-label="Enlace directo al Datos de entrenamiento" title="Enlace directo al Datos de entrenamiento" translate="no">​</a></h3>
<p>La Policy Network utiliza <strong>aprendizaje supervisado</strong>, aprendiendo de partidas humanas.</p>
<p>Fuentes de datos:</p>
<ul>
<li class=""><strong>KGS Go Server</strong>: partidas de jugadores aficionados y profesionales</li>
<li class=""><strong>Aproximadamente 30 millones de posiciones</strong>: muestreadas de 160,000 partidas</li>
<li class=""><strong>Etiquetas</strong>: el siguiente movimiento humano correspondiente a cada posición</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="función-de-pérdida-de-entropía-cruzada">Función de pérdida de entropía cruzada<a href="#función-de-pérdida-de-entropía-cruzada" class="hash-link" aria-label="Enlace directo al Función de pérdida de entropía cruzada" title="Enlace directo al Función de pérdida de entropía cruzada" translate="no">​</a></h3>
<p>El objetivo de entrenamiento es maximizar la probabilidad de predecir los movimientos humanos. Usando la función de pérdida de entropía cruzada:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L(θ) = -Σ log p_θ(a | s)</span><br></span></code></pre></div></div>
<p>Donde:</p>
<ul>
<li class=""><code>s</code>: estado del tablero</li>
<li class=""><code>a</code>: posición donde jugó el humano</li>
<li class=""><code>p_θ(a | s)</code>: probabilidad que el modelo predice para esa posición</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="comprensión-intuitiva-1">Comprensión intuitiva<a href="#comprensión-intuitiva-1" class="hash-link" aria-label="Enlace directo al Comprensión intuitiva" title="Enlace directo al Comprensión intuitiva" translate="no">​</a></h4>
<p>La pérdida de entropía cruzada tiene un significado simple:</p>
<blockquote>
<p><strong>Cuanto mayor sea la probabilidad que el modelo predice para la posición correcta, menor será la pérdida</strong></p>
</blockquote>
<p>Si el humano jugó en K10, y el modelo da a K10 una probabilidad de:</p>
<ul>
<li class="">0.9 → pérdida = -log(0.9) ≈ 0.1 (muy baja, bien)</li>
<li class="">0.1 → pérdida = -log(0.1) ≈ 2.3 (alta, mal)</li>
<li class="">0.01 → pérdida = -log(0.01) ≈ 4.6 (muy alta, muy mal)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="proceso-de-entrenamiento">Proceso de entrenamiento<a href="#proceso-de-entrenamiento" class="hash-link" aria-label="Enlace directo al Proceso de entrenamiento" title="Enlace directo al Proceso de entrenamiento" translate="no">​</a></h3>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Pseudocódigo</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> epoch </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_epochs</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> batch </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> dataloader</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        states</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> actions </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> batch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Propagación hacia adelante</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> network</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">states</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># vector de probabilidad de 361 dim</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Calcular pérdida (entropía cruzada)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cross_entropy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">policy</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> actions</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Retropropagación</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        loss</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">backward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p>Detalles de entrenamiento:</p>
<ul>
<li class=""><strong>Optimizador</strong>: SGD con momentum</li>
<li class=""><strong>Tasa de aprendizaje</strong>: inicial 0.003, decreciente gradualmente</li>
<li class=""><strong>Tamaño de lote</strong>: 16</li>
<li class=""><strong>Tiempo de entrenamiento</strong>: aproximadamente 3 semanas (50 GPUs)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="aumento-de-datos">Aumento de datos<a href="#aumento-de-datos" class="hash-link" aria-label="Enlace directo al Aumento de datos" title="Enlace directo al Aumento de datos" translate="no">​</a></h3>
<p>El tablero de Go tiene 8 simetrías (4 rotaciones × 2 reflexiones). Cada muestra de entrenamiento puede transformarse en 8 muestras equivalentes:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Original → Rotar 90° → Rotar 180° → Rotar 270°</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   ↓           ↓           ↓            ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Volteo horizontal → ...</span><br></span></code></pre></div></div>
<p>Esto aumenta los datos de entrenamiento efectivos 8 veces y asegura que los patrones aprendidos no dependan de la orientación.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="resultados-del-entrenamiento">Resultados del entrenamiento<a href="#resultados-del-entrenamiento" class="hash-link" aria-label="Enlace directo al Resultados del entrenamiento" title="Enlace directo al Resultados del entrenamiento" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="57-de-precisión">57% de precisión<a href="#57-de-precisión" class="hash-link" aria-label="Enlace directo al 57% de precisión" title="Enlace directo al 57% de precisión" translate="no">​</a></h3>
<p>Después del entrenamiento, la Policy Network alcanzó una <strong>precisión top-1 del 57%</strong>.</p>
<p>Esto significa: dada cualquier posición, el modelo tiene un 57% de probabilidad de predecir exactamente el movimiento que jugó el experto humano.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="es-alta-esta-precisión">¿Es alta esta precisión?<a href="#es-alta-esta-precisión" class="hash-link" aria-label="Enlace directo al ¿Es alta esta precisión?" title="Enlace directo al ¿Es alta esta precisión?" translate="no">​</a></h4>
<p>Considerando que cada posición tiene en promedio 250 movimientos legales, adivinar al azar tendría solo 0.4% de precisión.</p>
<table><thead><tr><th>Método</th><th>Precisión Top-1</th></tr></thead><tbody><tr><td>Adivinanza aleatoria</td><td>0.4%</td></tr><tr><td>Mejor programa de Go anterior</td><td>~44%</td></tr><tr><td>Policy Network de AlphaGo</td><td><strong>57%</strong></td></tr></tbody></table>
<p>Un aumento de 13 puntos porcentuales puede parecer poco, pero es muy significativo.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="mejora-en-la-fuerza-de-juego">Mejora en la fuerza de juego<a href="#mejora-en-la-fuerza-de-juego" class="hash-link" aria-label="Enlace directo al Mejora en la fuerza de juego" title="Enlace directo al Mejora en la fuerza de juego" translate="no">​</a></h3>
<p>¿Qué nivel de juego se puede alcanzar usando solo la Policy Network (sin búsqueda)?</p>
<table><thead><tr><th>Configuración</th><th>Puntuación Elo</th><th>Nivel aproximado</th></tr></thead><tbody><tr><td>Mejor programa anterior (Pachi)</td><td>2,500</td><td>4-5 dan amateur</td></tr><tr><td>Solo Policy Network</td><td>2,800</td><td>6-7 dan amateur</td></tr><tr><td>+ MCTS 1600 simulaciones</td><td>3,200+</td><td>Nivel profesional</td></tr></tbody></table>
<p>La Policy Network sola ya alcanza nivel alto amateur, y con MCTS salta a nivel profesional.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="por-qué-solo-57">¿Por qué solo 57%?<a href="#por-qué-solo-57" class="hash-link" aria-label="Enlace directo al ¿Por qué solo 57%?" title="Enlace directo al ¿Por qué solo 57%?" translate="no">​</a></h3>
<p>Las partidas humanas tienen las siguientes características que limitan la precisión:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-múltiples-buenos-movimientos">1. Múltiples buenos movimientos<a href="#1-múltiples-buenos-movimientos" class="hash-link" aria-label="Enlace directo al 1. Múltiples buenos movimientos" title="Enlace directo al 1. Múltiples buenos movimientos" translate="no">​</a></h4>
<p>Muchas posiciones tienen varios buenos movimientos. Por ejemplo, &quot;acercarse a la esquina&quot; y &quot;defender la esquina&quot; pueden ser ambas opciones correctas. Si el modelo elige otro buen movimiento, se cuenta como &quot;error&quot;.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-diferencias-de-estilo">2. Diferencias de estilo<a href="#2-diferencias-de-estilo" class="hash-link" aria-label="Enlace directo al 2. Diferencias de estilo" title="Enlace directo al 2. Diferencias de estilo" translate="no">​</a></h4>
<p>Diferentes jugadores tienen diferentes estilos. Jugadores agresivos y jugadores sólidos pueden jugar diferente en la misma posición. El modelo aprende un estilo &quot;promedio&quot;.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-los-humanos-también-cometen-errores">3. Los humanos también cometen errores<a href="#3-los-humanos-también-cometen-errores" class="hash-link" aria-label="Enlace directo al 3. Los humanos también cometen errores" title="Enlace directo al 3. Los humanos también cometen errores" translate="no">​</a></h4>
<p>Los datos de KGS incluyen partidas de jugadores amateur, cuyas elecciones no siempre son óptimas. Es normal que el modelo aprenda algunos &quot;errores&quot;.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="rol-en-mcts">Rol en MCTS<a href="#rol-en-mcts" class="hash-link" aria-label="Enlace directo al Rol en MCTS" title="Enlace directo al Rol en MCTS" translate="no">​</a></h2>
<p>La Policy Network desempeña dos roles clave en el MCTS de AlphaGo:</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="1-guiar-la-dirección-de-búsqueda">1. Guiar la dirección de búsqueda<a href="#1-guiar-la-dirección-de-búsqueda" class="hash-link" aria-label="Enlace directo al 1. Guiar la dirección de búsqueda" title="Enlace directo al 1. Guiar la dirección de búsqueda" translate="no">​</a></h3>
<p>En la fase de <strong>Selección</strong> de MCTS, la salida de la Policy Network se usa para calcular UCB (Upper Confidence Bound):</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">UCB(s, a) = Q(s, a) + c_puct × P(s, a) × √(N(s)) / (1 + N(s, a))</span><br></span></code></pre></div></div>
<p>Donde <code>P(s, a)</code> es la probabilidad dada por la Policy Network.</p>
<p>Esto significa:</p>
<ul>
<li class=""><strong>Los movimientos de alta probabilidad se exploran primero</strong></li>
<li class=""><strong>Los movimientos de baja probabilidad también tienen oportunidad de ser explorados</strong> (debido al término de exploración)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="2-prior-para-expandir-nodos">2. Prior para expandir nodos<a href="#2-prior-para-expandir-nodos" class="hash-link" aria-label="Enlace directo al 2. Prior para expandir nodos" title="Enlace directo al 2. Prior para expandir nodos" translate="no">​</a></h3>
<p>Cuando MCTS expande un nuevo nodo, la Policy Network proporciona las <strong>probabilidades a priori</strong> de todos los nodos hijos.</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Expandir nodo s:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  for each action a:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    child = Node()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    child.prior = policy_network(s)[a]  # probabilidad a priori</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    child.value = 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    child.visits = 0</span><br></span></code></pre></div></div>
<p>Estas probabilidades a priori permiten que MCTS &quot;sepa&quot; qué nodos hijos vale la pena explorar, incluso antes de visitarlos.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="versión-ligera-vs-versión-completa">Versión ligera vs. versión completa<a href="#versión-ligera-vs-versión-completa" class="hash-link" aria-label="Enlace directo al Versión ligera vs. versión completa" title="Enlace directo al Versión ligera vs. versión completa" translate="no">​</a></h2>
<p>AlphaGo en realidad tiene dos Policy Networks:</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="versión-completa-sl-policy-network">Versión completa (SL Policy Network)<a href="#versión-completa-sl-policy-network" class="hash-link" aria-label="Enlace directo al Versión completa (SL Policy Network)" title="Enlace directo al Versión completa (SL Policy Network)" translate="no">​</a></h3>
<ul>
<li class=""><strong>Arquitectura</strong>: CNN de 13 capas, 192 filtros</li>
<li class=""><strong>Precisión</strong>: 57%</li>
<li class=""><strong>Tiempo de inferencia</strong>: aproximadamente 3 ms/posición</li>
<li class=""><strong>Uso</strong>: Selección y Expansión en MCTS</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="versión-ligera-rollout-policy-network">Versión ligera (Rollout Policy Network)<a href="#versión-ligera-rollout-policy-network" class="hash-link" aria-label="Enlace directo al Versión ligera (Rollout Policy Network)" title="Enlace directo al Versión ligera (Rollout Policy Network)" translate="no">​</a></h3>
<ul>
<li class=""><strong>Arquitectura</strong>: modelo lineal + características manuales</li>
<li class=""><strong>Precisión</strong>: 24%</li>
<li class=""><strong>Tiempo de inferencia</strong>: aproximadamente 2 μs/posición (1500 veces más rápido)</li>
<li class=""><strong>Uso</strong>: simulaciones rápidas (rollout)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="por-qué-se-necesita-la-versión-ligera">¿Por qué se necesita la versión ligera?<a href="#por-qué-se-necesita-la-versión-ligera" class="hash-link" aria-label="Enlace directo al ¿Por qué se necesita la versión ligera?" title="Enlace directo al ¿Por qué se necesita la versión ligera?" translate="no">​</a></h3>
<p>En la fase de <strong>Simulación</strong> de MCTS, se necesita jugar desde el nodo actual hasta el final del juego, posiblemente 100+ movimientos. Si cada movimiento usa la Policy Network completa, es demasiado lento.</p>
<p>La versión ligera tiene solo 24% de precisión, pero es 1500 veces más rápida. En rollouts, la velocidad importa más que la precisión.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="características-de-la-versión-ligera">Características de la versión ligera<a href="#características-de-la-versión-ligera" class="hash-link" aria-label="Enlace directo al Características de la versión ligera" title="Enlace directo al Características de la versión ligera" translate="no">​</a></h3>
<p>La versión ligera usa características diseñadas manualmente, incluyendo:</p>
<table><thead><tr><th>Tipo de característica</th><th>Ejemplo</th></tr></thead><tbody><tr><td>Patrones locales</td><td>Configuración de piedras en área 3×3</td></tr><tr><td>Características globales</td><td>Si está en esquina/borde, puntos grandes</td></tr><tr><td>Características tácticas</td><td>Atari, escalera, conexión</td></tr></tbody></table>
<p>Estas características se introducen en un modelo lineal (sin capas ocultas), con cálculo extremadamente rápido.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="mejoras-de-alphago-zero">Mejoras de AlphaGo Zero<a href="#mejoras-de-alphago-zero" class="hash-link" aria-label="Enlace directo al Mejoras de AlphaGo Zero" title="Enlace directo al Mejoras de AlphaGo Zero" translate="no">​</a></h3>
<p>El posterior AlphaGo Zero abandonó completamente la versión ligera y los rollouts. Evalúa directamente los nodos hoja con la Value Network, sin necesidad de simulaciones rápidas. Esta fue una simplificación importante.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="ajuste-fino-con-aprendizaje-por-refuerzo-rl-policy-network">Ajuste fino con aprendizaje por refuerzo (RL Policy Network)<a href="#ajuste-fino-con-aprendizaje-por-refuerzo-rl-policy-network" class="hash-link" aria-label="Enlace directo al Ajuste fino con aprendizaje por refuerzo (RL Policy Network)" title="Enlace directo al Ajuste fino con aprendizaje por refuerzo (RL Policy Network)" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="limitaciones-del-aprendizaje-supervisado">Limitaciones del aprendizaje supervisado<a href="#limitaciones-del-aprendizaje-supervisado" class="hash-link" aria-label="Enlace directo al Limitaciones del aprendizaje supervisado" title="Enlace directo al Limitaciones del aprendizaje supervisado" translate="no">​</a></h3>
<p>La Policy Network entrenada con aprendizaje supervisado tiene un problema fundamental:</p>
<blockquote>
<p><strong>Aprende a &quot;imitar humanos&quot;, no a &quot;ganar&quot;</strong></p>
</blockquote>
<p>Esto significa que aprende malos hábitos de los humanos y también tiene mal desempeño en posiciones que los humanos nunca encontraron.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="auto-juego-con-refuerzo">Auto-juego con refuerzo<a href="#auto-juego-con-refuerzo" class="hash-link" aria-label="Enlace directo al Auto-juego con refuerzo" title="Enlace directo al Auto-juego con refuerzo" translate="no">​</a></h3>
<p>La solución de DeepMind fue usar el método de <strong>gradiente de políticas</strong> (Policy Gradient) para aprendizaje por refuerzo:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. Hacer que Policy Network juegue contra sí misma</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Registrar todos los movimientos de cada partida</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Ajustar parámetros según victoria/derrota:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Victoria → aumentar probabilidad de estos movimientos</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Derrota → disminuir probabilidad de estos movimientos</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="algoritmo-reinforce">Algoritmo REINFORCE<a href="#algoritmo-reinforce" class="hash-link" aria-label="Enlace directo al Algoritmo REINFORCE" title="Enlace directo al Algoritmo REINFORCE" translate="no">​</a></h3>
<p>Se usa específicamente el algoritmo REINFORCE:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">∇J(θ) = E[Σ_t ∇log π_θ(a_t | s_t) × z]</span><br></span></code></pre></div></div>
<p>Donde:</p>
<ul>
<li class=""><code>z</code>: resultado de la partida (+1 victoria, -1 derrota)</li>
<li class=""><code>π_θ(a_t | s_t)</code>: probabilidad de elegir la acción <code>a_t</code> en el estado <code>s_t</code></li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="resultados">Resultados<a href="#resultados" class="hash-link" aria-label="Enlace directo al Resultados" title="Enlace directo al Resultados" translate="no">​</a></h3>
<p>Después de aproximadamente 1 día de entrenamiento de auto-juego (1.28 millones de partidas), la RL Policy Network:</p>
<table><thead><tr><th>Métrica</th><th>SL Policy</th><th>RL Policy</th></tr></thead><tbody><tr><td>Contra SL Policy</td><td>50%</td><td><strong>80%</strong></td></tr><tr><td>Mejora Elo</td><td>-</td><td>+100</td></tr></tbody></table>
<p>La precisión puede disminuir ligeramente (porque ya no imita completamente a los humanos), pero la tasa de victoria real mejora significativamente.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="de-imitar-a-innovar">De &quot;imitar&quot; a &quot;innovar&quot;<a href="#de-imitar-a-innovar" class="hash-link" aria-label="Enlace directo al De &quot;imitar&quot; a &quot;innovar&quot;" title="Enlace directo al De &quot;imitar&quot; a &quot;innovar&quot;" translate="no">​</a></h3>
<p>El aprendizaje por refuerzo permitió que la Policy Network aprendiera algunos movimientos que los humanos nunca habían pensado. Estos movimientos nunca aparecieron en los datos de entrenamiento, pero son efectivos.</p>
<p>Por eso AlphaGo puede hacer &quot;movimientos divinos&quot; — no está limitado por la experiencia humana.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="análisis-visual">Análisis visual<a href="#análisis-visual" class="hash-link" aria-label="Enlace directo al Análisis visual" title="Enlace directo al Análisis visual" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="distribución-de-probabilidad-en-diferentes-posiciones">Distribución de probabilidad en diferentes posiciones<a href="#distribución-de-probabilidad-en-diferentes-posiciones" class="hash-link" aria-label="Enlace directo al Distribución de probabilidad en diferentes posiciones" title="Enlace directo al Distribución de probabilidad en diferentes posiciones" translate="no">​</a></h3>
<p>Veamos la salida de la Policy Network en diferentes posiciones:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="apertura-fase-de-fuseki">Apertura (fase de fuseki)<a href="#apertura-fase-de-fuseki" class="hash-link" aria-label="Enlace directo al Apertura (fase de fuseki)" title="Enlace directo al Apertura (fase de fuseki)" translate="no">​</a></h4>
<div>載入中...</div>
<p>En la apertura, la probabilidad se concentra principalmente en:</p>
<ul>
<li class="">Esquinas (ocupar esquinas)</li>
<li class="">Bordes (acercamiento, defensa de esquina)</li>
<li class="">Posiciones de &quot;puntos grandes&quot;</li>
</ul>
<p>Esto coincide con los principios básicos de Go: esquina de oro, borde de plata, centro de hierba.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="posición-de-combate">Posición de combate<a href="#posición-de-combate" class="hash-link" aria-label="Enlace directo al Posición de combate" title="Enlace directo al Posición de combate" translate="no">​</a></h4>
<div>載入中...</div>
<p>Durante el combate, la probabilidad se concentra en:</p>
<ul>
<li class="">Puntos de corte críticos</li>
<li class="">Atari, conexiones</li>
<li class="">Hacer ojos, destruir ojos</li>
</ul>
<p>Esto muestra que el modelo aprendió tácticas locales.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="fase-final-yose">Fase final (yose)<a href="#fase-final-yose" class="hash-link" aria-label="Enlace directo al Fase final (yose)" title="Enlace directo al Fase final (yose)" translate="no">​</a></h4>
<div>載入中...</div>
<p>En la fase final, la probabilidad se dispersa en varios puntos de yose, requiriendo cálculo preciso de territorio.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="qué-aprenden-las-capas-ocultas">¿Qué aprenden las capas ocultas?<a href="#qué-aprenden-las-capas-ocultas" class="hash-link" aria-label="Enlace directo al ¿Qué aprenden las capas ocultas?" title="Enlace directo al ¿Qué aprenden las capas ocultas?" translate="no">​</a></h3>
<p>Visualizando la salida de las capas convolucionales, podemos ver las &quot;características&quot; que el modelo aprendió:</p>
<ul>
<li class=""><strong>Capas bajas</strong>: formas básicas (ojos, puntos de corte)</li>
<li class=""><strong>Capas medias</strong>: patrones tácticos (atari, escalera)</li>
<li class=""><strong>Capas altas</strong>: conceptos globales (influencia, grosor)</li>
</ul>
<p>Esto es muy similar a la estructura jerárquica de cómo los humanos entienden el Go.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="puntos-clave-de-implementación">Puntos clave de implementación<a href="#puntos-clave-de-implementación" class="hash-link" aria-label="Enlace directo al Puntos clave de implementación" title="Enlace directo al Puntos clave de implementación" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="implementación-en-pytorch">Implementación en PyTorch<a href="#implementación-en-pytorch" class="hash-link" aria-label="Enlace directo al Implementación en PyTorch" title="Enlace directo al Implementación en PyTorch" translate="no">​</a></h3>
<p>Esta es una implementación simplificada de la Policy Network:</p>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> nn</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">functional </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> F</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">PolicyNetwork</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Module</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> input_channels</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">48</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_filters</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">192</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_layers</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">12</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Primera capa convolucional (5×5)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv1 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Conv2d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">input_channels</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_filters</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                               kernel_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> padding</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Capas convolucionales intermedias (3×3) ×11</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv_layers </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ModuleList</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Conv2d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_filters</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_filters</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                     kernel_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> padding</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> _ </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_layers </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Capa convolucional de salida (1×1)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv_out </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Conv2d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_filters</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> kernel_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">forward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># x: (batch, 48, 19, 19)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Primera capa</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> F</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">relu</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv1</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Capas intermedias</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> conv </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv_layers</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> F</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">relu</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">conv</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Capa de salida</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv_out</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># (batch, 1, 19, 19)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Aplanar + Softmax</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">view</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">size</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># (batch, 361)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> F</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">softmax</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dim</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> x</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="ciclo-de-entrenamiento">Ciclo de entrenamiento<a href="#ciclo-de-entrenamiento" class="hash-link" aria-label="Enlace directo al Ciclo de entrenamiento" title="Enlace directo al Ciclo de entrenamiento" translate="no">​</a></h3>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">train_step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> optimizer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> states</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> actions</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    states: (batch, 48, 19, 19) - características del tablero</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    actions: (batch,) - posición donde jugó el humano (0-360)</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Propagación hacia adelante</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">states</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># (batch, 361)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Pérdida de entropía cruzada</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> F</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cross_entropy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">log</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">policy </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1e-8</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># prevenir log(0)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        actions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Retropropagación</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zero_grad</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    loss</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">backward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Calcular precisión</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    predictions </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> policy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">argmax</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">dim</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    accuracy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">predictions </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> actions</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">float</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">mean</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> loss</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">item</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> accuracy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">item</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="consideraciones-para-inferencia">Consideraciones para inferencia<a href="#consideraciones-para-inferencia" class="hash-link" aria-label="Enlace directo al Consideraciones para inferencia" title="Enlace directo al Consideraciones para inferencia" translate="no">​</a></h3>
<p>Durante el juego real, hay que tener en cuenta:</p>
<ol>
<li class=""><strong>Filtrar movimientos ilegales</strong>: establecer la probabilidad de posiciones ilegales a 0, luego renormalizar</li>
<li class=""><strong>Ajuste de temperatura</strong>: se puede usar un parámetro de temperatura para controlar la &quot;nitidez&quot; de la distribución de probabilidad</li>
<li class=""><strong>Inferencia por lotes</strong>: en MCTS se pueden procesar múltiples posiciones en lote</li>
</ol>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">get_move_probabilities</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> state</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> legal_moves</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> temperature</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1.0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;Obtener distribución de probabilidad de movimientos legales&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">state</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># (361,)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Solo mantener movimientos legales</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mask </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zeros</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">361</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mask</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">legal_moves</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> policy </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> mask</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Ajuste de temperatura</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> temperature </span><span class="token operator" style="color:#393A34">!=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.0</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> policy </span><span class="token operator" style="color:#393A34">**</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> temperature</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Renormalizar</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> policy </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> policy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">sum</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> policy</span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="correspondencia-de-animaciones">Correspondencia de animaciones<a href="#correspondencia-de-animaciones" class="hash-link" aria-label="Enlace directo al Correspondencia de animaciones" title="Enlace directo al Correspondencia de animaciones" translate="no">​</a></h2>
<p>Los conceptos principales de este artículo y los números de animación correspondientes:</p>
<table><thead><tr><th>Número</th><th>Concepto</th><th>Correspondencia física/matemática</th></tr></thead><tbody><tr><td>E1</td><td>Policy Network</td><td>Campo de probabilidad</td></tr><tr><td>D9</td><td>Extracción de características CNN</td><td>Respuesta de filtro</td></tr><tr><td>D3</td><td>Aprendizaje supervisado</td><td>Estimación de máxima verosimilitud</td></tr><tr><td>H4</td><td>Gradiente de políticas</td><td>Optimización estocástica</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="lecturas-adicionales">Lecturas adicionales<a href="#lecturas-adicionales" class="hash-link" aria-label="Enlace directo al Lecturas adicionales" title="Enlace directo al Lecturas adicionales" translate="no">​</a></h2>
<ul>
<li class=""><strong>Siguiente artículo</strong>: <a class="" href="/es/docs/alphago/value-network/">Value Network en detalle</a> — Cómo AlphaGo evalúa posiciones</li>
<li class=""><strong>Tema relacionado</strong>: <a class="" href="/es/docs/alphago/input-features/">Diseño de características de entrada</a> — Detalles de los 48 planos de características</li>
<li class=""><strong>Principios profundos</strong>: <a class="" href="/es/docs/alphago/cnn-and-go/">CNN y Go</a> — Por qué las redes neuronales convolucionales son adecuadas para tableros</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="puntos-clave">Puntos clave<a href="#puntos-clave" class="hash-link" aria-label="Enlace directo al Puntos clave" title="Enlace directo al Puntos clave" translate="no">​</a></h2>
<ol>
<li class=""><strong>Policy Network es un generador de distribución de probabilidad</strong>: entrada tablero, salida probabilidades para 361 posiciones</li>
<li class=""><strong>13 capas CNN + Softmax</strong>: convolución profunda para extraer características, Softmax para salida de probabilidad</li>
<li class=""><strong>57% de precisión</strong>: muy superior a programas de Go anteriores</li>
<li class=""><strong>Dos versiones</strong>: versión completa para decisiones MCTS, versión ligera para simulaciones rápidas</li>
<li class=""><strong>Ajuste fino con RL</strong>: evoluciona de &quot;imitar humanos&quot; a &quot;buscar la victoria&quot;</li>
</ol>
<p>La Policy Network es la &quot;intuición&quot; de AlphaGo — permite que la IA identifique rápidamente los movimientos que vale la pena considerar, como lo hacen los humanos.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="referencias">Referencias<a href="#referencias" class="hash-link" aria-label="Enlace directo al Referencias" title="Enlace directo al Referencias" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2016). &quot;Mastering the game of Go with deep neural networks and tree search.&quot; <em>Nature</em>, 529, 484-489.</li>
<li class="">Maddison, C. J., et al. (2014). &quot;Move Evaluation in Go Using Deep Convolutional Neural Networks.&quot; <em>arXiv:1412.6564</em>.</li>
<li class="">Sutton, R. S., &amp; Barto, A. G. (2018). <em>Reinforcement Learning: An Introduction</em>. MIT Press.</li>
<li class="">LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). &quot;Deep learning.&quot; <em>Nature</em>, 521, 436-444.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/07-policy-network.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Editar esta página</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Página del documento"><a class="pagination-nav__link pagination-nav__link--prev" href="/es/docs/alphago/board-representation/"><div class="pagination-nav__sublabel">Anterior</div><div class="pagination-nav__label">Representacion del Estado del Tablero</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/es/docs/alphago/value-network/"><div class="pagination-nav__sublabel">Siguiente</div><div class="pagination-nav__label">Value Network en detalle</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#qué-es-la-policy-network" class="table-of-contents__link toc-highlight">¿Qué es la Policy Network?</a><ul><li><a href="#función-principal" class="table-of-contents__link toc-highlight">Función principal</a></li><li><a href="#comprensión-intuitiva" class="table-of-contents__link toc-highlight">Comprensión intuitiva</a></li><li><a href="#por-qué-se-necesita-la-policy-network" class="table-of-contents__link toc-highlight">¿Por qué se necesita la Policy Network?</a></li></ul></li><li><a href="#arquitectura-de-la-red" class="table-of-contents__link toc-highlight">Arquitectura de la red</a><ul><li><a href="#estructura-general" class="table-of-contents__link toc-highlight">Estructura general</a></li><li><a href="#capa-de-entrada" class="table-of-contents__link toc-highlight">Capa de entrada</a></li><li><a href="#capas-convolucionales" class="table-of-contents__link toc-highlight">Capas convolucionales</a></li><li><a href="#función-de-activación-relu" class="table-of-contents__link toc-highlight">Función de activación ReLU</a></li><li><a href="#capa-de-salida" class="table-of-contents__link toc-highlight">Capa de salida</a></li><li><a href="#cantidad-de-parámetros" class="table-of-contents__link toc-highlight">Cantidad de parámetros</a></li></ul></li><li><a href="#objetivo-y-métodos-de-entrenamiento" class="table-of-contents__link toc-highlight">Objetivo y métodos de entrenamiento</a><ul><li><a href="#datos-de-entrenamiento" class="table-of-contents__link toc-highlight">Datos de entrenamiento</a></li><li><a href="#función-de-pérdida-de-entropía-cruzada" class="table-of-contents__link toc-highlight">Función de pérdida de entropía cruzada</a></li><li><a href="#proceso-de-entrenamiento" class="table-of-contents__link toc-highlight">Proceso de entrenamiento</a></li><li><a href="#aumento-de-datos" class="table-of-contents__link toc-highlight">Aumento de datos</a></li></ul></li><li><a href="#resultados-del-entrenamiento" class="table-of-contents__link toc-highlight">Resultados del entrenamiento</a><ul><li><a href="#57-de-precisión" class="table-of-contents__link toc-highlight">57% de precisión</a></li><li><a href="#mejora-en-la-fuerza-de-juego" class="table-of-contents__link toc-highlight">Mejora en la fuerza de juego</a></li><li><a href="#por-qué-solo-57" class="table-of-contents__link toc-highlight">¿Por qué solo 57%?</a></li></ul></li><li><a href="#rol-en-mcts" class="table-of-contents__link toc-highlight">Rol en MCTS</a><ul><li><a href="#1-guiar-la-dirección-de-búsqueda" class="table-of-contents__link toc-highlight">1. Guiar la dirección de búsqueda</a></li><li><a href="#2-prior-para-expandir-nodos" class="table-of-contents__link toc-highlight">2. Prior para expandir nodos</a></li></ul></li><li><a href="#versión-ligera-vs-versión-completa" class="table-of-contents__link toc-highlight">Versión ligera vs. versión completa</a><ul><li><a href="#versión-completa-sl-policy-network" class="table-of-contents__link toc-highlight">Versión completa (SL Policy Network)</a></li><li><a href="#versión-ligera-rollout-policy-network" class="table-of-contents__link toc-highlight">Versión ligera (Rollout Policy Network)</a></li><li><a href="#por-qué-se-necesita-la-versión-ligera" class="table-of-contents__link toc-highlight">¿Por qué se necesita la versión ligera?</a></li><li><a href="#características-de-la-versión-ligera" class="table-of-contents__link toc-highlight">Características de la versión ligera</a></li><li><a href="#mejoras-de-alphago-zero" class="table-of-contents__link toc-highlight">Mejoras de AlphaGo Zero</a></li></ul></li><li><a href="#ajuste-fino-con-aprendizaje-por-refuerzo-rl-policy-network" class="table-of-contents__link toc-highlight">Ajuste fino con aprendizaje por refuerzo (RL Policy Network)</a><ul><li><a href="#limitaciones-del-aprendizaje-supervisado" class="table-of-contents__link toc-highlight">Limitaciones del aprendizaje supervisado</a></li><li><a href="#auto-juego-con-refuerzo" class="table-of-contents__link toc-highlight">Auto-juego con refuerzo</a></li><li><a href="#algoritmo-reinforce" class="table-of-contents__link toc-highlight">Algoritmo REINFORCE</a></li><li><a href="#resultados" class="table-of-contents__link toc-highlight">Resultados</a></li><li><a href="#de-imitar-a-innovar" class="table-of-contents__link toc-highlight">De &quot;imitar&quot; a &quot;innovar&quot;</a></li></ul></li><li><a href="#análisis-visual" class="table-of-contents__link toc-highlight">Análisis visual</a><ul><li><a href="#distribución-de-probabilidad-en-diferentes-posiciones" class="table-of-contents__link toc-highlight">Distribución de probabilidad en diferentes posiciones</a></li><li><a href="#qué-aprenden-las-capas-ocultas" class="table-of-contents__link toc-highlight">¿Qué aprenden las capas ocultas?</a></li></ul></li><li><a href="#puntos-clave-de-implementación" class="table-of-contents__link toc-highlight">Puntos clave de implementación</a><ul><li><a href="#implementación-en-pytorch" class="table-of-contents__link toc-highlight">Implementación en PyTorch</a></li><li><a href="#ciclo-de-entrenamiento" class="table-of-contents__link toc-highlight">Ciclo de entrenamiento</a></li><li><a href="#consideraciones-para-inferencia" class="table-of-contents__link toc-highlight">Consideraciones para inferencia</a></li></ul></li><li><a href="#correspondencia-de-animaciones" class="table-of-contents__link toc-highlight">Correspondencia de animaciones</a></li><li><a href="#lecturas-adicionales" class="table-of-contents__link toc-highlight">Lecturas adicionales</a></li><li><a href="#puntos-clave" class="table-of-contents__link toc-highlight">Puntos clave</a></li><li><a href="#referencias" class="table-of-contents__link toc-highlight">Referencias</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>