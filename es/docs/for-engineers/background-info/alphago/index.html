<!doctype html>
<html lang="es" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-for-engineers/background-info/alphago" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Analisis del paper de AlphaGo | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/es/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/es/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/es/docs/for-engineers/background-info/alphago/"><meta data-rh="true" property="og:locale" content="es"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="es"><meta data-rh="true" name="docsearch:language" content="es"><meta data-rh="true" name="keywords" content="圍棋, Go, 好棋寶寶, AI, KataGo, AlphaGo, 圍棋教學, 圍棋入門"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Analisis del paper de AlphaGo | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="Este articulo analiza en profundidad el paper clasico publicado en Nature por DeepMind &quot;Mastering the game of Go with deep neural networks and tree search&quot;, asi como los papers posteriores de AlphaGo Zero y AlphaZero."><meta data-rh="true" property="og:description" content="Este articulo analiza en profundidad el paper clasico publicado en Nature por DeepMind &quot;Mastering the game of Go with deep neural networks and tree search&quot;, asi como los papers posteriores de AlphaGo Zero y AlphaZero."><link data-rh="true" rel="icon" href="/es/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/es/docs/for-engineers/background-info/alphago/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/background-info/alphago/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/for-engineers/background-info/alphago/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/for-engineers/background-info/alphago/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/for-engineers/background-info/alphago/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/for-engineers/background-info/alphago/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/for-engineers/background-info/alphago/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/for-engineers/background-info/alphago/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/for-engineers/background-info/alphago/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/for-engineers/background-info/alphago/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/for-engineers/background-info/alphago/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/for-engineers/background-info/alphago/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/background-info/alphago/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Guia de IA de Go para ingenieros","item":"https://www.weiqi.kids/es/docs/for-engineers/"},{"@type":"ListItem","position":2,"name":"Conocimientos previos","item":"https://www.weiqi.kids/es/docs/for-engineers/background-info/"},{"@type":"ListItem","position":3,"name":"Analisis del paper de AlphaGo","item":"https://www.weiqi.kids/es/docs/for-engineers/background-info/alphago"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會"><link rel="stylesheet" href="/es/assets/css/styles.5b0c3226.css">
<script src="/es/assets/js/runtime~main.7fff394b.js" defer="defer"></script>
<script src="/es/assets/js/main.e8950639.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/es/img/logo.svg"><div role="region" aria-label="Saltar al contenido principal"><a class="skipToContent_bFSG" href="#__docusaurus_skipToContent_fallback">Saltar al contenido principal</a></div><nav aria-label="Principal" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Alternar barra lateral" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/es/"><div class="navbar__logo"><img src="/es/img/logo.svg" alt="My Site Logo" class="themedComponent_Wuuq themedComponent--light_w2On"><img src="/es/img/logo.svg" alt="My Site Logo" class="themedComponent_Wuuq themedComponent--dark_zC0W"></div><b class="navbar__title text--truncate">Weiqi.Kids</b></a><a class="navbar__item navbar__link" href="/es/docs/for-players/">圍棋棋友</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/es/docs/for-engineers/">AI工程師</a><a class="navbar__item navbar__link" href="/es/docs/evolution/">圍棋 AI 演進整理</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/es/docs/aboutus/">協會介紹</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_Zh_g"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>Español</a><ul class="dropdown__menu"><li><a href="/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="es">Español</a></li><li><a href="/pt/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_I1mA"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_D_Tj colorModeToggle_nf8C"><button class="clean-btn toggleButton_m_sb toggleButtonDisabled_nndi" type="button" disabled="" title="system mode" aria-label="Cambiar entre modo oscuro y claro (actualmente system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_rMtU lightToggleIcon_PeQM"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_rMtU darkToggleIcon_PkGC"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_rMtU systemToggleIcon_n9Iu"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_pmRL"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_WlJ3"><div class="docsWrapper_ramL"><button aria-label="Volver al principio" class="clean-btn theme-back-to-top-button backToTopButton_MqxJ" type="button"></button><div class="docRoot_UZ1h"><aside class="theme-doc-sidebar-container docSidebarContainer_y0QC"><div class="sidebarViewport_jAkm"><div class="sidebar_HjkB"><nav aria-label="Barra lateral de Documentos" class="menu thin-scrollbar menu_jqYH"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/es/docs/intro/">Guia de uso</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/es/docs/for-players/">Para jugadores de Go</a><button aria-label="Ampliar la categoría &#x27;Para jugadores de Go&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/es/docs/for-engineers/">Guia de IA de Go para ingenieros</a><button aria-label="Colapsar categoría &#x27;Guia de IA de Go para ingenieros&#x27; de la barra lateral" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/es/docs/for-engineers/background-info/">Conocimientos previos</a><button aria-label="Colapsar categoría &#x27;Conocimientos previos&#x27; de la barra lateral" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/es/docs/for-engineers/background-info/alphago/">Analisis del paper de AlphaGo</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/background-info/katago-paper/">Analisis del paper de KataGo</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/background-info/zen/">Introduccion a otras IAs de Go</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/es/docs/for-engineers/katago-source/">Guia de inicio practico de KataGo</a><button aria-label="Ampliar la categoría &#x27;Guia de inicio practico de KataGo&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/es/docs/evolution/">Evolucion del Go con IA</a><button aria-label="Ampliar la categoría &#x27;Evolucion del Go con IA&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/es/docs/aboutus/">Acerca de nosotros</a></li></ul></nav></div></div></aside><main class="docMainContainer_qMQS"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_Wlt9"><div class="docItemContainer_oLHG"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_M4ek" aria-label="Rastro de navegación"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Página de Inicio" class="breadcrumbs__link" href="/es/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_pL7f"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/es/docs/for-engineers/"><span>Guia de IA de Go para ingenieros</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/es/docs/for-engineers/background-info/"><span>Conocimientos previos</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Analisis del paper de AlphaGo</span></li></ul></nav><div class="tocCollapsible_K0FP theme-doc-toc-mobile tocMobile_PzQu"><button type="button" class="clean-btn tocCollapsibleButton_E0oR">En esta página</button></div><div class="theme-doc-markdown markdown"><header><h1>Analisis del paper de AlphaGo</h1></header>
<p>Este articulo analiza en profundidad el paper clasico publicado en Nature por DeepMind &quot;Mastering the game of Go with deep neural networks and tree search&quot;, asi como los papers posteriores de AlphaGo Zero y AlphaZero.</p>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="significado-historico-de-alphago">Significado historico de AlphaGo<a href="#significado-historico-de-alphago" class="hash-link" aria-label="Enlace directo al Significado historico de AlphaGo" title="Enlace directo al Significado historico de AlphaGo">​</a></h2>
<p>El Go fue considerado durante mucho tiempo el desafio &quot;santo grial&quot; de la inteligencia artificial. A diferencia del ajedrez, el espacio de busqueda del Go es extremadamente enorme:</p>
<table><thead><tr><th>Juego</th><th>Factor de ramificacion promedio</th><th>Longitud promedio del juego</th><th>Espacio de estados</th></tr></thead><tbody><tr><td>Ajedrez</td><td>~35</td><td>~80</td><td>~10^47</td></tr><tr><td>Go</td><td>~250</td><td>~150</td><td>~10^170</td></tr></tbody></table>
<p>Los metodos tradicionales de busqueda por fuerza bruta son completamente inviables en Go. En 2016, AlphaGo derroto a Lee Sedol, demostrando el poderoso poder de la combinacion del aprendizaje profundo y el aprendizaje por refuerzo.</p>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="eventos-hito">Eventos hito<a href="#eventos-hito" class="hash-link" aria-label="Enlace directo al Eventos hito" title="Enlace directo al Eventos hito">​</a></h3>
<ul>
<li><strong>Octubre 2015</strong>: AlphaGo Fan derrota al campeon europeo Fan Hui (profesional 2-dan) 5:0</li>
<li><strong>Marzo 2016</strong>: AlphaGo Lee derrota al campeon mundial Lee Sedol (profesional 9-dan) 4:1</li>
<li><strong>Mayo 2017</strong>: AlphaGo Master derrota a Ke Jie, el numero uno del mundo, 3:0</li>
<li><strong>Octubre 2017</strong>: Publicacion de AlphaGo Zero, self-play puro, supera todas las versiones anteriores</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="arquitectura-tecnica-central">Arquitectura tecnica central<a href="#arquitectura-tecnica-central" class="hash-link" aria-label="Enlace directo al Arquitectura tecnica central" title="Enlace directo al Arquitectura tecnica central">​</a></h2>
<p>La innovacion central de AlphaGo esta en la combinacion de tres tecnologias clave:</p>
<!-- -->
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="policy-network-red-de-estrategia">Policy Network (Red de estrategia)<a href="#policy-network-red-de-estrategia" class="hash-link" aria-label="Enlace directo al Policy Network (Red de estrategia)" title="Enlace directo al Policy Network (Red de estrategia)">​</a></h3>
<p>La Policy Network es responsable de predecir la probabilidad de jugar en cada posicion, usada para guiar la direccion de busqueda.</p>
<h4 class="anchor anchorWithStickyNavbar_BVHq" id="arquitectura-de-red">Arquitectura de red<a href="#arquitectura-de-red" class="hash-link" aria-label="Enlace directo al Arquitectura de red" title="Enlace directo al Arquitectura de red">​</a></h4>
<!-- -->
<h4 class="anchor anchorWithStickyNavbar_BVHq" id="caracteristicas-de-entrada">Caracteristicas de entrada<a href="#caracteristicas-de-entrada" class="hash-link" aria-label="Enlace directo al Caracteristicas de entrada" title="Enlace directo al Caracteristicas de entrada">​</a></h4>
<p>AlphaGo usa 48 planos de caracteristicas como entrada:</p>
<table><thead><tr><th>Caracteristica</th><th>Numero de planos</th><th>Descripcion</th></tr></thead><tbody><tr><td>Color de piedra</td><td>3</td><td>Piedra negra, piedra blanca, punto vacio</td></tr><tr><td>Numero de libertades</td><td>8</td><td>1 libertad, 2 libertades, ..., 8+ libertades</td></tr><tr><td>Libertades despues de captura</td><td>8</td><td>Cuantas libertades tendria despues de capturar</td></tr><tr><td>Numero de capturas</td><td>8</td><td>Cuantas piedras se pueden capturar en esa posicion</td></tr><tr><td>Ko</td><td>1</td><td>Si es posicion de ko</td></tr><tr><td>Legalidad de jugada</td><td>1</td><td>Si se puede jugar en esa posicion</td></tr><tr><td>Posiciones de los ultimos 1-8 movimientos</td><td>8</td><td>Posiciones de jugadas anteriores</td></tr><tr><td>Turno de jugar</td><td>1</td><td>Es turno de negro o blanco</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_BVHq" id="metodo-de-entrenamiento">Metodo de entrenamiento<a href="#metodo-de-entrenamiento" class="hash-link" aria-label="Enlace directo al Metodo de entrenamiento" title="Enlace directo al Metodo de entrenamiento">​</a></h4>
<p>El entrenamiento de Policy Network se divide en dos etapas:</p>
<p><strong>Primera etapa: Aprendizaje supervisado (SL Policy Network)</strong></p>
<ul>
<li>Usa 30 millones de partidas del servidor de Go KGS</li>
<li>Objetivo: Predecir la siguiente jugada de jugadores humanos</li>
<li>Alcanza 57% de precision de prediccion</li>
</ul>
<p><strong>Segunda etapa: Aprendizaje por refuerzo (RL Policy Network)</strong></p>
<ul>
<li>Comienza desde SL Policy Network</li>
<li>Juega contra versiones anteriores de si mismo</li>
<li>Usa el algoritmo REINFORCE para optimizar</li>
</ul>
<div class="language-python codeBlockContainer_WV0T theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_GRu9"><pre tabindex="0" class="prism-code language-python codeBlock_md1K thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_L4YE"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Actualizacion simplificada de Policy Gradient</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># reward: +1 victoria, -1 derrota</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">log</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">policy</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">action</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> reward</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="value-network-red-de-valor">Value Network (Red de valor)<a href="#value-network-red-de-valor" class="hash-link" aria-label="Enlace directo al Value Network (Red de valor)" title="Enlace directo al Value Network (Red de valor)">​</a></h3>
<p>La Value Network evalua la tasa de victoria de la posicion actual, usada para reducir la profundidad de busqueda.</p>
<h4 class="anchor anchorWithStickyNavbar_BVHq" id="arquitectura-de-red-1">Arquitectura de red<a href="#arquitectura-de-red-1" class="hash-link" aria-label="Enlace directo al Arquitectura de red" title="Enlace directo al Arquitectura de red">​</a></h4>
<!-- -->
<h4 class="anchor anchorWithStickyNavbar_BVHq" id="metodo-de-entrenamiento-1">Metodo de entrenamiento<a href="#metodo-de-entrenamiento-1" class="hash-link" aria-label="Enlace directo al Metodo de entrenamiento" title="Enlace directo al Metodo de entrenamiento">​</a></h4>
<p>Value Network se entrena con 30 millones de posiciones generadas por self-play de RL Policy Network:</p>
<ul>
<li>Se muestrea aleatoriamente una posicion de cada partida</li>
<li>Se usa el resultado final de victoria/derrota como etiqueta</li>
<li>Se usa funcion de perdida MSE</li>
</ul>
<div class="language-python codeBlockContainer_WV0T theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_GRu9"><pre tabindex="0" class="prism-code language-python codeBlock_md1K thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_L4YE"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Entrenamiento de Value Network</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">value_prediction </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> value_network</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">position</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">value_prediction </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> game_outcome</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">**</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><br></span></code></pre></div></div>
<p><strong>Por que solo se toma una muestra de cada partida?</strong></p>
<p>Si se toman multiples muestras, las posiciones adyacentes de la misma partida estaran altamente correlacionadas, causando sobreajuste. El muestreo aleatorio asegura la diversidad de los datos de entrenamiento.</p>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="busqueda-de-arbol-monte-carlo-mcts">Busqueda de Arbol Monte Carlo (MCTS)<a href="#busqueda-de-arbol-monte-carlo-mcts" class="hash-link" aria-label="Enlace directo al Busqueda de Arbol Monte Carlo (MCTS)" title="Enlace directo al Busqueda de Arbol Monte Carlo (MCTS)">​</a></h2>
<p>MCTS es el nucleo de decision de AlphaGo, combinando redes neuronales para buscar eficientemente la mejor jugada.</p>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="cuatro-pasos-de-mcts">Cuatro pasos de MCTS<a href="#cuatro-pasos-de-mcts" class="hash-link" aria-label="Enlace directo al Cuatro pasos de MCTS" title="Enlace directo al Cuatro pasos de MCTS">​</a></h3>
<!-- -->
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="formula-de-seleccion-puct">Formula de seleccion (PUCT)<a href="#formula-de-seleccion-puct" class="hash-link" aria-label="Enlace directo al Formula de seleccion (PUCT)" title="Enlace directo al Formula de seleccion (PUCT)">​</a></h3>
<p>AlphaGo usa la formula PUCT (Predictor + UCT) para seleccionar que rama explorar:</p>
<div class="language-text codeBlockContainer_WV0T theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_GRu9"><pre tabindex="0" class="prism-code language-text codeBlock_md1K thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_L4YE"><span class="token-line" style="color:#393A34"><span class="token plain">a = argmax[Q(s,a) + u(s,a)]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">u(s,a) = c_puct * P(s,a) * sqrt(N(s)) / (1 + N(s,a))</span><br></span></code></pre></div></div>
<p>Donde:</p>
<ul>
<li><strong>Q(s,a)</strong>: Valor promedio de la accion a (explotacion)</li>
<li><strong>P(s,a)</strong>: Probabilidad a priori predicha por Policy Network</li>
<li><strong>N(s)</strong>: Numero de visitas al nodo padre</li>
<li><strong>N(s,a)</strong>: Numero de visitas a esa accion</li>
<li><strong>c_puct</strong>: Constante de exploracion, equilibra exploracion y explotacion</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="detalle-del-proceso-de-busqueda">Detalle del proceso de busqueda<a href="#detalle-del-proceso-de-busqueda" class="hash-link" aria-label="Enlace directo al Detalle del proceso de busqueda" title="Enlace directo al Detalle del proceso de busqueda">​</a></h3>
<ol>
<li><strong>Seleccion</strong>: Desde el nodo raiz, usar formula PUCT para seleccionar acciones hasta llegar al nodo hoja</li>
<li><strong>Expansion</strong>: Expandir nuevos nodos hijos en el nodo hoja, usar Policy Network para inicializar probabilidades a priori</li>
<li><strong>Evaluacion</strong>: Combinar evaluacion de Value Network y simulacion rapida (Rollout) para evaluar el valor</li>
<li><strong>Retropropagacion</strong>: Propagar el valor de evaluacion a lo largo del camino hacia atras, actualizar valores Q y N</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="rollout-simulacion-rapida">Rollout (Simulacion rapida)<a href="#rollout-simulacion-rapida" class="hash-link" aria-label="Enlace directo al Rollout (Simulacion rapida)" title="Enlace directo al Rollout (Simulacion rapida)">​</a></h3>
<p>AlphaGo (version no Zero) tambien usa una pequena red de politica rapida para simulacion:</p>
<div class="language-text codeBlockContainer_WV0T theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_GRu9"><pre tabindex="0" class="prism-code language-text codeBlock_md1K thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_L4YE"><span class="token-line" style="color:#393A34"><span class="token plain">Nodo hoja -&gt; Jugadas rapidas aleatorias hasta el final -&gt; Calcular resultado</span><br></span></code></pre></div></div>
<p>El valor de evaluacion final combina Value Network y Rollout:</p>
<div class="language-text codeBlockContainer_WV0T theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_GRu9"><pre tabindex="0" class="prism-code language-text codeBlock_md1K thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_L4YE"><span class="token-line" style="color:#393A34"><span class="token plain">V = lambda * v_network + (1-lambda) * v_rollout</span><br></span></code></pre></div></div>
<p>AlphaGo usa lambda = 0.5, dando igual peso a ambos.</p>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="metodo-de-entrenamiento-self-play">Metodo de entrenamiento Self-play<a href="#metodo-de-entrenamiento-self-play" class="hash-link" aria-label="Enlace directo al Metodo de entrenamiento Self-play" title="Enlace directo al Metodo de entrenamiento Self-play">​</a></h2>
<p>Self-play es la estrategia de entrenamiento central de AlphaGo, permitiendo que la IA mejore continuamente jugando contra si misma.</p>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="ciclo-de-entrenamiento">Ciclo de entrenamiento<a href="#ciclo-de-entrenamiento" class="hash-link" aria-label="Enlace directo al Ciclo de entrenamiento" title="Enlace directo al Ciclo de entrenamiento">​</a></h3>
<!-- -->
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="por-que-self-play-es-efectivo">Por que Self-play es efectivo?<a href="#por-que-self-play-es-efectivo" class="hash-link" aria-label="Enlace directo al Por que Self-play es efectivo?" title="Enlace directo al Por que Self-play es efectivo?">​</a></h3>
<ol>
<li><strong>Datos infinitos</strong>: No esta limitado por la cantidad de partidas humanas</li>
<li><strong>Dificultad adaptativa</strong>: La fuerza del oponente mejora sincronizadamente con la tuya</li>
<li><strong>Exploracion de innovaciones</strong>: No esta limitado por patrones de pensamiento humanos</li>
<li><strong>Objetivo claro</strong>: Optimiza directamente la tasa de victoria, no imita a humanos</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="mejoras-de-alphago-zero">Mejoras de AlphaGo Zero<a href="#mejoras-de-alphago-zero" class="hash-link" aria-label="Enlace directo al Mejoras de AlphaGo Zero" title="Enlace directo al Mejoras de AlphaGo Zero">​</a></h2>
<p>AlphaGo Zero publicado en 2017 trajo mejoras revolucionarias:</p>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="diferencias-principales">Diferencias principales<a href="#diferencias-principales" class="hash-link" aria-label="Enlace directo al Diferencias principales" title="Enlace directo al Diferencias principales">​</a></h3>
<table><thead><tr><th>Caracteristica</th><th>AlphaGo</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Entrenamiento inicial</td><td>Aprendizaje supervisado con partidas humanas</td><td>Completamente desde cero</td></tr><tr><td>Arquitectura de red</td><td>Policy/Value separados</td><td>Red unica de dos cabezas</td></tr><tr><td>Estructura de red</td><td>CNN regular</td><td>ResNet</td></tr><tr><td>Ingenieria de caracteristicas</td><td>48 caracteristicas manuales</td><td>17 caracteristicas simples</td></tr><tr><td>Rollout</td><td>Necesario</td><td>No necesario</td></tr><tr><td>Tiempo de entrenamiento</td><td>Varios meses</td><td>3 dias para superar humanos</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="simplificacion-de-arquitectura">Simplificacion de arquitectura<a href="#simplificacion-de-arquitectura" class="hash-link" aria-label="Enlace directo al Simplificacion de arquitectura" title="Enlace directo al Simplificacion de arquitectura">​</a></h3>
<!-- -->
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="caracteristicas-de-entrada-simplificadas">Caracteristicas de entrada simplificadas<a href="#caracteristicas-de-entrada-simplificadas" class="hash-link" aria-label="Enlace directo al Caracteristicas de entrada simplificadas" title="Enlace directo al Caracteristicas de entrada simplificadas">​</a></h3>
<p>AlphaGo Zero solo usa 17 planos de caracteristicas:</p>
<ul>
<li>8 planos: Posiciones de tus piedras de los ultimos 8 movimientos</li>
<li>8 planos: Posiciones de piedras del oponente de los ultimos 8 movimientos</li>
<li>1 plano: Turno actual (todo 0 o todo 1)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="mejoras-de-entrenamiento">Mejoras de entrenamiento<a href="#mejoras-de-entrenamiento" class="hash-link" aria-label="Enlace directo al Mejoras de entrenamiento" title="Enlace directo al Mejoras de entrenamiento">​</a></h3>
<ol>
<li><strong>Self-play puro</strong>: No usa ningun dato humano</li>
<li><strong>Usa directamente probabilidades MCTS como objetivo de entrenamiento</strong>: En lugar de victoria/derrota binaria</li>
<li><strong>Sin Rollout</strong>: Depende completamente de Value Network</li>
<li><strong>Entrenamiento de red unica</strong>: Policy y Value comparten parametros, se refuerzan mutuamente</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="generalizacion-de-alphazero">Generalizacion de AlphaZero<a href="#generalizacion-de-alphazero" class="hash-link" aria-label="Enlace directo al Generalizacion de AlphaZero" title="Enlace directo al Generalizacion de AlphaZero">​</a></h2>
<p>AlphaZero publicado a finales de 2017 aplico la misma arquitectura a Go, ajedrez y shogi:</p>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="caracteristicas-clave">Caracteristicas clave<a href="#caracteristicas-clave" class="hash-link" aria-label="Enlace directo al Caracteristicas clave" title="Enlace directo al Caracteristicas clave">​</a></h3>
<ul>
<li><strong>Cero conocimiento de dominio</strong>: No usa ningun conocimiento especifico del dominio excepto las reglas del juego</li>
<li><strong>Arquitectura unificada</strong>: El mismo algoritmo aplica a diferentes juegos de tablero</li>
<li><strong>Entrenamiento mas rapido</strong>:<!-- -->
<ul>
<li>Go: 8 horas para superar AlphaGo Lee</li>
<li>Ajedrez: 4 horas para superar Stockfish</li>
<li>Shogi: 2 horas para superar Elmo</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="diferencias-con-alphago-zero">Diferencias con AlphaGo Zero<a href="#diferencias-con-alphago-zero" class="hash-link" aria-label="Enlace directo al Diferencias con AlphaGo Zero" title="Enlace directo al Diferencias con AlphaGo Zero">​</a></h3>
<table><thead><tr><th>Caracteristica</th><th>AlphaGo Zero</th><th>AlphaZero</th></tr></thead><tbody><tr><td>Juego objetivo</td><td>Solo Go</td><td>Go, ajedrez, shogi</td></tr><tr><td>Uso de simetria</td><td>Usa simetria 8-fold del Go</td><td>No asume simetria</td></tr><tr><td>Ajuste de hiperparametros</td><td>Optimizado para Go</td><td>Configuracion general</td></tr><tr><td>Modo de entrenamiento</td><td>Self-play con mejor modelo</td><td>Self-play con modelo mas reciente</td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="puntos-clave-de-implementacion">Puntos clave de implementacion<a href="#puntos-clave-de-implementacion" class="hash-link" aria-label="Enlace directo al Puntos clave de implementacion" title="Enlace directo al Puntos clave de implementacion">​</a></h2>
<p>Si quieres implementar un sistema similar, estos son los puntos clave a considerar:</p>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="recursos-de-computo">Recursos de computo<a href="#recursos-de-computo" class="hash-link" aria-label="Enlace directo al Recursos de computo" title="Enlace directo al Recursos de computo">​</a></h3>
<p>El entrenamiento de AlphaGo requiere enormes recursos de computo:</p>
<ul>
<li><strong>AlphaGo Lee</strong>: 176 GPU + 48 TPU</li>
<li><strong>AlphaGo Zero</strong>: 4 TPU (entrenamiento) + 1 TPU (self-play)</li>
<li><strong>AlphaZero</strong>: 5000 TPU (entrenamiento)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="hiperparametros-clave">Hiperparametros clave<a href="#hiperparametros-clave" class="hash-link" aria-label="Enlace directo al Hiperparametros clave" title="Enlace directo al Hiperparametros clave">​</a></h3>
<div class="language-python codeBlockContainer_WV0T theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_GRu9"><pre tabindex="0" class="prism-code language-python codeBlock_md1K thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_L4YE"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Relacionados con MCTS</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">num_simulations </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">800</span><span class="token plain">     </span><span class="token comment" style="color:#999988;font-style:italic"># Numero de simulaciones de busqueda por jugada</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">c_puct </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.5</span><span class="token plain">              </span><span class="token comment" style="color:#999988;font-style:italic"># Constante de exploracion</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">temperature </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.0</span><span class="token plain">         </span><span class="token comment" style="color:#999988;font-style:italic"># Parametro de temperatura para seleccion de accion</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Relacionados con entrenamiento</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">batch_size </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2048</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">learning_rate </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.01</span><span class="token plain">      </span><span class="token comment" style="color:#999988;font-style:italic"># Con decay</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">l2_regularization </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1e-4</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="problemas-comunes">Problemas comunes<a href="#problemas-comunes" class="hash-link" aria-label="Enlace directo al Problemas comunes" title="Enlace directo al Problemas comunes">​</a></h3>
<ol>
<li><strong>Entrenamiento inestable</strong>: Usar learning rate mas pequeno, aumentar batch size</li>
<li><strong>Sobreajuste</strong>: Asegurar diversidad de datos de entrenamiento, usar regularizacion</li>
<li><strong>Eficiencia de busqueda</strong>: Optimizar inferencia batch GPU, paralelizar MCTS</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="lectura-adicional">Lectura adicional<a href="#lectura-adicional" class="hash-link" aria-label="Enlace directo al Lectura adicional" title="Enlace directo al Lectura adicional">​</a></h2>
<ul>
<li><a href="https://www.nature.com/articles/nature16961" target="_blank" rel="noopener noreferrer">Paper original: Mastering the game of Go with deep neural networks and tree search</a></li>
<li><a href="https://www.nature.com/articles/nature24270" target="_blank" rel="noopener noreferrer">Paper AlphaGo Zero: Mastering the game of Go without human knowledge</a></li>
<li><a href="https://www.science.org/doi/10.1126/science.aar6404" target="_blank" rel="noopener noreferrer">Paper AlphaZero: A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</a></li>
</ul>
<p>Despues de entender la tecnologia de AlphaGo, veamos como <a href="/es/docs/for-engineers/background-info/katago-paper/">KataGo hizo mejoras sobre esta base</a>.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/background-info/alphago.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_O1mQ" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Editar esta página</a></div><div class="col lastUpdated_hyfO"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Página del documento"><a class="pagination-nav__link pagination-nav__link--prev" href="/es/docs/for-engineers/background-info/"><div class="pagination-nav__sublabel">Anterior</div><div class="pagination-nav__label">Conocimientos previos</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/es/docs/for-engineers/background-info/katago-paper/"><div class="pagination-nav__sublabel">Siguiente</div><div class="pagination-nav__label">Analisis del paper de KataGo</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_tqik thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#significado-historico-de-alphago" class="table-of-contents__link toc-highlight">Significado historico de AlphaGo</a><ul><li><a href="#eventos-hito" class="table-of-contents__link toc-highlight">Eventos hito</a></li></ul></li><li><a href="#arquitectura-tecnica-central" class="table-of-contents__link toc-highlight">Arquitectura tecnica central</a><ul><li><a href="#policy-network-red-de-estrategia" class="table-of-contents__link toc-highlight">Policy Network (Red de estrategia)</a></li><li><a href="#value-network-red-de-valor" class="table-of-contents__link toc-highlight">Value Network (Red de valor)</a></li></ul></li><li><a href="#busqueda-de-arbol-monte-carlo-mcts" class="table-of-contents__link toc-highlight">Busqueda de Arbol Monte Carlo (MCTS)</a><ul><li><a href="#cuatro-pasos-de-mcts" class="table-of-contents__link toc-highlight">Cuatro pasos de MCTS</a></li><li><a href="#formula-de-seleccion-puct" class="table-of-contents__link toc-highlight">Formula de seleccion (PUCT)</a></li><li><a href="#detalle-del-proceso-de-busqueda" class="table-of-contents__link toc-highlight">Detalle del proceso de busqueda</a></li><li><a href="#rollout-simulacion-rapida" class="table-of-contents__link toc-highlight">Rollout (Simulacion rapida)</a></li></ul></li><li><a href="#metodo-de-entrenamiento-self-play" class="table-of-contents__link toc-highlight">Metodo de entrenamiento Self-play</a><ul><li><a href="#ciclo-de-entrenamiento" class="table-of-contents__link toc-highlight">Ciclo de entrenamiento</a></li><li><a href="#por-que-self-play-es-efectivo" class="table-of-contents__link toc-highlight">Por que Self-play es efectivo?</a></li></ul></li><li><a href="#mejoras-de-alphago-zero" class="table-of-contents__link toc-highlight">Mejoras de AlphaGo Zero</a><ul><li><a href="#diferencias-principales" class="table-of-contents__link toc-highlight">Diferencias principales</a></li><li><a href="#simplificacion-de-arquitectura" class="table-of-contents__link toc-highlight">Simplificacion de arquitectura</a></li><li><a href="#caracteristicas-de-entrada-simplificadas" class="table-of-contents__link toc-highlight">Caracteristicas de entrada simplificadas</a></li><li><a href="#mejoras-de-entrenamiento" class="table-of-contents__link toc-highlight">Mejoras de entrenamiento</a></li></ul></li><li><a href="#generalizacion-de-alphazero" class="table-of-contents__link toc-highlight">Generalizacion de AlphaZero</a><ul><li><a href="#caracteristicas-clave" class="table-of-contents__link toc-highlight">Caracteristicas clave</a></li><li><a href="#diferencias-con-alphago-zero" class="table-of-contents__link toc-highlight">Diferencias con AlphaGo Zero</a></li></ul></li><li><a href="#puntos-clave-de-implementacion" class="table-of-contents__link toc-highlight">Puntos clave de implementacion</a><ul><li><a href="#recursos-de-computo" class="table-of-contents__link toc-highlight">Recursos de computo</a></li><li><a href="#hiperparametros-clave" class="table-of-contents__link toc-highlight">Hiperparametros clave</a></li><li><a href="#problemas-comunes" class="table-of-contents__link toc-highlight">Problemas comunes</a></li></ul></li><li><a href="#lectura-adicional" class="table-of-contents__link toc-highlight">Lectura adicional</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>