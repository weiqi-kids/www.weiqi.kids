<!doctype html>
<html lang="es" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-for-engineers/how-it-works/alphago-explained/self-play" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Auto-juego | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/es/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/es/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/es/docs/for-engineers/how-it-works/alphago-explained/self-play/"><meta data-rh="true" property="og:locale" content="es"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="es"><meta data-rh="true" name="docsearch:language" content="es"><meta data-rh="true" name="keywords" content="圍棋, Go, 好棋寶寶, AI, KataGo, AlphaGo, 圍棋教學, 圍棋入門"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Auto-juego | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="Comprensión profunda de cómo AlphaGo superó los límites del nivel humano a través del auto-juego"><meta data-rh="true" property="og:description" content="Comprensión profunda de cómo AlphaGo superó los límites del nivel humano a través del auto-juego"><link data-rh="true" rel="icon" href="/es/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/es/docs/for-engineers/how-it-works/alphago-explained/self-play/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"給工程師的圍棋 AI 指南","item":"https://www.weiqi.kids/es/docs/for-engineers/"},{"@type":"ListItem","position":2,"name":"一篇文章搞懂圍棋 AI","item":"https://www.weiqi.kids/es/docs/for-engineers/how-it-works/"},{"@type":"ListItem","position":3,"name":"AlphaGo 完整解析","item":"https://www.weiqi.kids/es/docs/for-engineers/how-it-works/alphago-explained/"},{"@type":"ListItem","position":4,"name":"Auto-juego","item":"https://www.weiqi.kids/es/docs/for-engineers/how-it-works/alphago-explained/self-play"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/es/assets/css/styles.f23bf74b.css">
<script src="/es/assets/js/runtime~main.491e58eb.js" defer="defer"></script>
<script src="/es/assets/js/main.c1693d88.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/es/img/logo.svg"><div role="region" aria-label="Saltar al contenido principal"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">Saltar al contenido principal</a></div><nav aria-label="Principal" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Alternar barra lateral" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/es/"><div class="navbar__logo"><img src="/es/img/logo.svg" alt="Logo de la Asociación Weiqi Kids" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/es/img/logo.svg" alt="Logo de la Asociación Weiqi Kids" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">Weiqi.Kids</b></a><a class="navbar__item navbar__link" href="/es/docs/for-players/">Para Jugadores</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/es/docs/for-engineers/">Para Ingenieros</a><a class="navbar__item navbar__link" href="/es/docs/about/">Sobre Nosotros</a><a class="navbar__item navbar__link" href="/es/docs/activities/">Actividades</a><a class="navbar__item navbar__link" href="/es/docs/references/">Referencias</a><a class="navbar__item navbar__link" href="/es/docs/sop/">Procedimientos</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>Español</a><ul class="dropdown__menu"><li><a href="/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="es">Español</a></li><li><a href="/pt/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="Volver al principio" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="Barra lateral de Documentos" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/es/docs/intro/"><span title="Guia de uso" class="linkLabel_REp1">Guia de uso</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/es/docs/about/"><span title="關於協會" class="categoryLinkLabel_ezQx">關於協會</span></a><button aria-label="Ampliar la categoría &#x27;關於協會&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/es/docs/activities/"><span title="活動實績" class="categoryLinkLabel_ezQx">活動實績</span></a><button aria-label="Ampliar la categoría &#x27;活動實績&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/es/docs/for-players/"><span title="Para jugadores de Go" class="categoryLinkLabel_ezQx">Para jugadores de Go</span></a><button aria-label="Ampliar la categoría &#x27;Para jugadores de Go&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/es/docs/references/"><span title="參考資料" class="categoryLinkLabel_ezQx">參考資料</span></a><button aria-label="Ampliar la categoría &#x27;參考資料&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/es/docs/sop/"><span title="標準作業流程" class="categoryLinkLabel_ezQx">標準作業流程</span></a><button aria-label="Ampliar la categoría &#x27;標準作業流程&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/es/docs/for-engineers/"><span title="給工程師的圍棋 AI 指南" class="categoryLinkLabel_ezQx">給工程師的圍棋 AI 指南</span></a><button aria-label="Colapsar categoría &#x27;給工程師的圍棋 AI 指南&#x27; de la barra lateral" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/es/docs/for-engineers/deep-dive/"><span title="Para quienes desean profundizar" class="categoryLinkLabel_ezQx">Para quienes desean profundizar</span></a><button aria-label="Ampliar la categoría &#x27;Para quienes desean profundizar&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/es/docs/for-engineers/hands-on/"><span title="30 分鐘跑起第一個圍棋 AI" class="categoryLinkLabel_ezQx">30 分鐘跑起第一個圍棋 AI</span></a><button aria-label="Ampliar la categoría &#x27;30 分鐘跑起第一個圍棋 AI&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/es/docs/for-engineers/how-it-works/"><span title="一篇文章搞懂圍棋 AI" class="categoryLinkLabel_ezQx">一篇文章搞懂圍棋 AI</span></a><button aria-label="Colapsar categoría &#x27;一篇文章搞懂圍棋 AI&#x27; de la barra lateral" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/"><span title="AlphaGo 完整解析" class="categoryLinkLabel_ezQx">AlphaGo 完整解析</span></a><button aria-label="Colapsar categoría &#x27;AlphaGo 完整解析&#x27; de la barra lateral" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/birth-of-alphago/"><span title="El Nacimiento de AlphaGo" class="linkLabel_REp1">El Nacimiento de AlphaGo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/key-matches/"><span title="Revisiones de partidas clave" class="linkLabel_REp1">Revisiones de partidas clave</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/move-37/"><span title="Análisis Profundo del &quot;Movimiento Divino&quot;" class="linkLabel_REp1">Análisis Profundo del &quot;Movimiento Divino&quot;</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/why-go-is-hard/"><span title="¿Por Qué es Difícil el Go?" class="linkLabel_REp1">¿Por Qué es Difícil el Go?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/traditional-limits/"><span title="Los Limites de los Metodos Tradicionales" class="linkLabel_REp1">Los Limites de los Metodos Tradicionales</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/board-representation/"><span title="Representacion del Estado del Tablero" class="linkLabel_REp1">Representacion del Estado del Tablero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/policy-network/"><span title="Policy Network en detalle" class="linkLabel_REp1">Policy Network en detalle</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/value-network/"><span title="Value Network en detalle" class="linkLabel_REp1">Value Network en detalle</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/input-features/"><span title="Diseno de Caracteristicas de Entrada" class="linkLabel_REp1">Diseno de Caracteristicas de Entrada</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/cnn-and-go/"><span title="CNN y Go" class="linkLabel_REp1">CNN y Go</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/supervised-learning/"><span title="Fase de aprendizaje supervisado" class="linkLabel_REp1">Fase de aprendizaje supervisado</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/"><span title="Introducción al aprendizaje por refuerzo" class="linkLabel_REp1">Introducción al aprendizaje por refuerzo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/self-play/"><span title="Auto-juego" class="linkLabel_REp1">Auto-juego</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/"><span title="La Combinación de MCTS y Redes Neuronales" class="linkLabel_REp1">La Combinación de MCTS y Redes Neuronales</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/puct-formula/"><span title="Explicación Detallada de la Fórmula PUCT" class="linkLabel_REp1">Explicación Detallada de la Fórmula PUCT</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><span title="Visión General de AlphaGo Zero" class="linkLabel_REp1">Visión General de AlphaGo Zero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/"><span title="Red de Doble Cabeza y Redes Residuales" class="linkLabel_REp1">Red de Doble Cabeza y Redes Residuales</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/training-from-scratch/"><span title="El proceso de entrenamiento desde cero" class="linkLabel_REp1">El proceso de entrenamiento desde cero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/distributed-systems/"><span title="Sistemas distribuidos y TPU" class="linkLabel_REp1">Sistemas distribuidos y TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/alphago-explained/legacy-and-impact/"><span title="El legado de AlphaGo" class="linkLabel_REp1">El legado de AlphaGo</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/katago-innovations/"><span title="KataGo 的關鍵創新" class="linkLabel_REp1">KataGo 的關鍵創新</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/es/docs/for-engineers/how-it-works/concepts/"><span title="概念速查表" class="linkLabel_REp1">概念速查表</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/es/docs/for-engineers/industry/"><span title="圍棋 AI 產業現況" class="categoryLinkLabel_ezQx">圍棋 AI 產業現況</span></a><button aria-label="Ampliar la categoría &#x27;圍棋 AI 產業現況&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/es/docs/for-engineers/overview/"><span title="圍棋 AI 能做什麼？" class="categoryLinkLabel_ezQx">圍棋 AI 能做什麼？</span></a><button aria-label="Ampliar la categoría &#x27;圍棋 AI 能做什麼？&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="Rastro de navegación"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Página de Inicio" class="breadcrumbs__link" href="/es/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/es/docs/for-engineers/"><span>給工程師的圍棋 AI 指南</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/es/docs/for-engineers/how-it-works/"><span>一篇文章搞懂圍棋 AI</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/es/docs/for-engineers/how-it-works/alphago-explained/"><span>AlphaGo 完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Auto-juego</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">En esta página</button></div><div class="theme-doc-markdown markdown"><header><h1>Auto-juego</h1></header>
<p>En el artículo anterior, presentamos los conceptos básicos del aprendizaje por refuerzo. Ahora, exploremos una de las claves del éxito de AlphaGo: el <strong>auto-juego (Self-Play)</strong>.</p>
<p>Este es un concepto aparentemente contradictorio: <strong>¿Cómo puede una IA volverse más fuerte jugando contra sí misma?</strong></p>
<p>La respuesta es profunda y elegante, involucrando teoría de juegos, dinámica evolutiva y la naturaleza del aprendizaje.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="por-qué-funciona-el-auto-juego">¿Por qué funciona el auto-juego?<a href="#por-qué-funciona-el-auto-juego" class="hash-link" aria-label="Enlace directo al ¿Por qué funciona el auto-juego?" title="Enlace directo al ¿Por qué funciona el auto-juego?" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="explicación-intuitiva">Explicación intuitiva<a href="#explicación-intuitiva" class="hash-link" aria-label="Enlace directo al Explicación intuitiva" title="Enlace directo al Explicación intuitiva" translate="no">​</a></h3>
<p>Imagina que eres un principiante de Go, practicando solo en una isla desierta:</p>
<ol>
<li class="">Juegas una partida, jugando simultáneamente como negro y blanco</li>
<li class="">Después de la partida, analizas qué movimientos fueron buenos y cuáles malos</li>
<li class="">En la siguiente partida, intentas evitar los errores anteriores</li>
<li class="">Repites este proceso millones de veces</li>
</ol>
<p>Intuitivamente, esto parece tener problemas:</p>
<ul>
<li class="">Si tu nivel es muy bajo, ambos lados juegan mal, ¿qué se puede aprender?</li>
<li class="">¿No caerás en un &quot;equilibrio de errores&quot; donde ambos lados juegan mal pero se cancelan mutuamente?</li>
</ul>
<p>Pero en realidad, el auto-juego puede producir progreso continuo. Aquí está el porqué:</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="descubrimiento-progresivo-de-debilidades">Descubrimiento progresivo de debilidades<a href="#descubrimiento-progresivo-de-debilidades" class="hash-link" aria-label="Enlace directo al Descubrimiento progresivo de debilidades" title="Enlace directo al Descubrimiento progresivo de debilidades" translate="no">​</a></h3>
<p>La idea clave es: <strong>incluso si ambos lados son la misma IA, el resultado de cada partida todavía contiene información</strong>.</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Posición A: la IA eligió el movimiento X, finalmente ganó</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Posición A: la IA eligió el movimiento Y, finalmente perdió</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">→ Conclusión: en la posición A, X es mejor que Y</span><br></span></code></pre></div></div>
<p>A través de estadísticas de muchas partidas, la IA puede aprender qué elecciones son mejores en cada posición. Esta es la esencia del <strong>gradiente de política</strong>: las buenas elecciones se refuerzan, las malas se suprimen.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="aprendizaje-adversarial">Aprendizaje adversarial<a href="#aprendizaje-adversarial" class="hash-link" aria-label="Enlace directo al Aprendizaje adversarial" title="Enlace directo al Aprendizaje adversarial" translate="no">​</a></h3>
<p>El auto-juego tiene una propiedad especial: <strong>el oponente de entrenamiento se adapta automáticamente a tu nivel</strong>.</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Ciclo de entrenamiento 1: la IA descubre una táctica efectiva T</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Ciclo de entrenamiento 2: la IA como oponente aprende a defender contra T</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Ciclo de entrenamiento 3: la IA original se ve forzada a buscar una mejor táctica T&#x27;</span><br></span></code></pre></div></div>
<p>Esto forma una <strong>carrera armamentista (Arms Race)</strong>, donde ambos lados continuamente descubren y superan las debilidades del otro.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="comparación-con-partidas-humanas">Comparación con partidas humanas<a href="#comparación-con-partidas-humanas" class="hash-link" aria-label="Enlace directo al Comparación con partidas humanas" title="Enlace directo al Comparación con partidas humanas" translate="no">​</a></h3>
<table><thead><tr><th>Método de entrenamiento</th><th>Ventajas</th><th>Desventajas</th></tr></thead><tbody><tr><td><strong>Partidas humanas</strong></td><td>Aprende la cristalización de la sabiduría humana</td><td>Limitado al nivel humano</td></tr><tr><td><strong>Auto-juego</strong></td><td>Potencial de mejora ilimitado</td><td>Puede caer en óptimos locales</td></tr><tr><td><strong>Ambos combinados</strong></td><td>Inicio rápido + mejora continua</td><td>Mejor estrategia</td></tr></tbody></table>
<p>La versión original de AlphaGo primero usó aprendizaje supervisado con partidas humanas, luego aprendizaje por refuerzo con auto-juego. AlphaGo Zero demostró que solo con auto-juego también puede alcanzar nivel sobrehumano.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="perspectiva-de-teoría-de-juegos">Perspectiva de teoría de juegos<a href="#perspectiva-de-teoría-de-juegos" class="hash-link" aria-label="Enlace directo al Perspectiva de teoría de juegos" title="Enlace directo al Perspectiva de teoría de juegos" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="equilibrio-de-nash">Equilibrio de Nash<a href="#equilibrio-de-nash" class="hash-link" aria-label="Enlace directo al Equilibrio de Nash" title="Enlace directo al Equilibrio de Nash" translate="no">​</a></h3>
<p>En teoría de juegos, el <strong>equilibrio de Nash</strong> es un estado estable: en este estado, ningún jugador tiene incentivo para cambiar unilateralmente su estrategia.</p>
<p>Para <strong>juegos de suma cero con información perfecta</strong> como Go, el equilibrio de Nash tiene un significado especial:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>π</mi><mo>∗</mo></msup><mo>=</mo><mi>arg</mi><mo>⁡</mo><msub><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>π</mi></msub><msub><mrow><mi>min</mi><mo>⁡</mo></mrow><msup><mi>π</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mi>V</mi><mo stretchy="false">(</mo><mi>π</mi><mo separator="true">,</mo><msup><mi>π</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi^* = \arg\max_\pi \min_{\pi&#x27;} V(\pi, \pi&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mop">ar<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop">min</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em"><span style="top:-2.786em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<p>Donde <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><mi>π</mi><mo separator="true">,</mo><msup><mi>π</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(\pi, \pi&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> es el valor esperado cuando la estrategia <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">π</span></span></span></span> juega contra la estrategia <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>π</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">\pi&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>.</p>
<p>Este es el famoso <strong>principio Minimax</strong>: la mejor estrategia es aquella que tiene el mejor rendimiento en el peor caso.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="auto-juego-y-equilibrio-de-nash">Auto-juego y equilibrio de Nash<a href="#auto-juego-y-equilibrio-de-nash" class="hash-link" aria-label="Enlace directo al Auto-juego y equilibrio de Nash" title="Enlace directo al Auto-juego y equilibrio de Nash" translate="no">​</a></h3>
<p>Teóricamente, si el auto-juego puede converger, debería converger al equilibrio de Nash. Para juegos deterministas como Go, el equilibrio de Nash es el <strong>juego perfecto</strong>.</p>
<p>Pero el espacio de estados de Go es demasiado grande (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>10</mn><mn>170</mn></msup></mrow><annotation encoding="application/x-tex">10^{170}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">170</span></span></span></span></span></span></span></span></span></span></span></span>), no podemos encontrar el verdadero equilibrio de Nash. El auto-juego en realidad está <strong>aproximando</strong> este equilibrio.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="fictitious-play">Fictitious Play<a href="#fictitious-play" class="hash-link" aria-label="Enlace directo al Fictitious Play" title="Enlace directo al Fictitious Play" translate="no">​</a></h3>
<p>El auto-juego está relacionado con el concepto de <strong>fictitious play</strong> en teoría de juegos:</p>
<ol>
<li class="">Cada jugador observa el historial de estrategias del oponente</li>
<li class="">Calcula la distribución promedio de las estrategias del oponente</li>
<li class="">Elige la mejor respuesta contra esta distribución promedio</li>
</ol>
<p>Bajo ciertas condiciones, se puede probar que fictitious play converge al equilibrio de Nash.</p>
<p>El auto-juego de AlphaGo puede verse como una implementación de red neuronal de este concepto.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="mecanismo-del-auto-juego">Mecanismo del auto-juego<a href="#mecanismo-del-auto-juego" class="hash-link" aria-label="Enlace directo al Mecanismo del auto-juego" title="Enlace directo al Mecanismo del auto-juego" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="flujo-básico">Flujo básico<a href="#flujo-básico" class="hash-link" aria-label="Enlace directo al Flujo básico" title="Enlace directo al Flujo básico" translate="no">​</a></h3>
<p>El flujo de auto-juego de AlphaGo:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Algoritmo: Self-Play Training</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Inicialización: Policy Network π_θ (puede comenzar desde aprendizaje supervisado o inicialización aleatoria)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Repetir los siguientes pasos hasta convergencia:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1. Generar datos de partidas</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Para i = 1 hasta N (en paralelo):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     a. Realizar una partida de auto-juego con la política actual π_θ</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     b. Recopilar trayectoria: τ_i = (s_0, a_0, r_1, s_1, a_1, ...)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     c. Registrar resultado final z_i ∈ {-1, +1}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Actualizar política</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   a. Calcular gradiente de política:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ∇J = (1/N) Σ_i Σ_t ∇_θ log π_θ(a_t|s_t) · z_i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   b. Actualizar parámetros: θ ← θ + α · ∇J</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Actualizar red de valor</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   a. Entrenar Value Network con pares (s, z)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   b. Minimizar: L = E[(V_φ(s) - z)²]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">4. Opcional: Evaluar y guardar checkpoint</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   a. Hacer que la nueva política juegue contra versiones anteriores</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   b. Si tasa de victoria &gt; 55%, actualizar el pool de oponentes</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="generación-de-datos-de-entrenamiento">Generación de datos de entrenamiento<a href="#generación-de-datos-de-entrenamiento" class="hash-link" aria-label="Enlace directo al Generación de datos de entrenamiento" title="Enlace directo al Generación de datos de entrenamiento" translate="no">​</a></h3>
<p>Cada partida de auto-juego produce una <strong>trayectoria</strong>:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>a</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>s</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>a</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>s</mi><mi>T</mi></msub><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose">)</span></span></span></span></p>
<p>Donde:</p>
<ul>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">s_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>: estado del tablero en el paso de tiempo <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></li>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>: acción elegida en el paso de tiempo <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></li>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span>: resultado final (+1 victoria, -1 derrota)</li>
</ul>
<p>Una partida de 200 movimientos produce 200 muestras de entrenamiento. Con cientos de miles de partidas de auto-juego por día, la cantidad de datos de entrenamiento es asombrosa.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="actualización-de-política">Actualización de política<a href="#actualización-de-política" class="hash-link" aria-label="Enlace directo al Actualización de política" title="Enlace directo al Actualización de política" translate="no">​</a></h3>
<p>Usando gradiente de política para actualizar la Policy Network:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>+</mo><mi>α</mi><mo>⋅</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi mathvariant="double-struck">E</mi><mrow><mo fence="true">[</mo><msub><mo>∑</mo><mi>t</mi></msub><mi>log</mi><mo>⁡</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>⋅</mo><mi>z</mi><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\theta \leftarrow \theta + \alpha \cdot \nabla_\theta \mathbb{E}\left[\sum_t \log \pi_\theta(a_t|s_t) \cdot z\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathbb">E</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">[</span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1308em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose delimcenter" style="top:0em">]</span></span></span></span></span></p>
<p>El efecto de esta actualización:</p>
<ul>
<li class="">Si finalmente gana (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">z = +1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">+</span><span class="mord">1</span></span></span></span>), aumentar la probabilidad de todos los movimientos</li>
<li class="">Si finalmente pierde (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">z = -1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">−</span><span class="mord">1</span></span></span></span>), disminuir la probabilidad de todos los movimientos</li>
</ul>
<p>Esto parece tosco: al ganar también puede haber malos movimientos, al perder también puede haber buenos movimientos. Pero a través de estadísticas de muchas partidas, este &quot;ruido&quot; se promedia, y los verdaderos buenos movimientos son identificados.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="entrenamiento-de-la-red-de-valor">Entrenamiento de la red de valor<a href="#entrenamiento-de-la-red-de-valor" class="hash-link" aria-label="Enlace directo al Entrenamiento de la red de valor" title="Enlace directo al Entrenamiento de la red de valor" translate="no">​</a></h3>
<p>La Value Network usa <strong>regresión</strong> para entrenar:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mo>←</mo><mi>ϕ</mi><mo>−</mo><mi>β</mi><mo>⋅</mo><msub><mi mathvariant="normal">∇</mi><mi>ϕ</mi></msub><mi mathvariant="double-struck">E</mi><mrow><mo fence="true">[</mo><mo stretchy="false">(</mo><msub><mi>V</mi><mi>ϕ</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>−</mo><mi>z</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\phi \leftarrow \phi - \beta \cdot \nabla_\phi \mathbb{E}\left[(V_\phi(s) - z)^2\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">ϕ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">ϕ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord mathbb">E</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size1">[</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size1">]</span></span></span></span></span></span></p>
<p>Esto hace que la Value Network aprenda a predecir: ¿cuál es la probabilidad de ganar desde la posición actual?</p>
<p>El papel de la Value Network es:</p>
<ol>
<li class="">Proporcionar evaluación de nodos hoja en MCTS</li>
<li class="">Servir como línea base para el gradiente de política</li>
<li class="">Usarse directamente para evaluación de posición</li>
</ol>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="importancia-de-la-aleatorización">Importancia de la aleatorización<a href="#importancia-de-la-aleatorización" class="hash-link" aria-label="Enlace directo al Importancia de la aleatorización" title="Enlace directo al Importancia de la aleatorización" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="evitar-ciclos-deterministas">Evitar ciclos deterministas<a href="#evitar-ciclos-deterministas" class="hash-link" aria-label="Enlace directo al Evitar ciclos deterministas" title="Enlace directo al Evitar ciclos deterministas" translate="no">​</a></h3>
<p>Si el auto-juego es completamente determinista, puede caer en ciclos:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">La política A siempre juega la misma apertura</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">La política A vs la política A siempre produce la misma partida</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Solo una partida se aprende repetidamente</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">La IA no puede explorar otras posibilidades</span><br></span></code></pre></div></div>
<p>Por eso la <strong>aleatorización</strong> es crucial en el auto-juego.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="fuentes-de-aleatorización">Fuentes de aleatorización<a href="#fuentes-de-aleatorización" class="hash-link" aria-label="Enlace directo al Fuentes de aleatorización" title="Enlace directo al Fuentes de aleatorización" translate="no">​</a></h3>
<p>Formas en que AlphaGo introduce aleatorización en el auto-juego:</p>
<p><strong>1. La red de política en sí es estocástica</strong></p>
<p>La Policy Network produce una distribución de probabilidad, no una elección determinista:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a \sim \pi_\theta(a|s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span></p>
<p>La misma posición puede elegir diferentes movimientos cada vez.</p>
<p><strong>2. Parámetro de temperatura</strong></p>
<p>Usar temperatura más alta durante el entrenamiento para aumentar diversidad:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>τ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><msup><mo stretchy="false">)</mo><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>τ</mi></mrow></msup></mrow><mrow><msub><mo>∑</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mi mathvariant="normal">∣</mi><mi>s</mi><msup><mo stretchy="false">)</mo><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>τ</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\pi_\tau(a|s) = \frac{\pi_\theta(a|s)^{1/\tau}}{\sum_{a&#x27;} \pi_\theta(a&#x27;|s)^{1/\tau}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.1132em">τ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.7721em;vertical-align:-0.6104em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1617em"><span style="top:-2.6146em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2854em"><span style="top:-2.2854em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.6068em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8496em"><span style="top:-2.8496em;margin-right:0.1em"><span class="pstrut" style="height:2.5556em"></span><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3214em"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em"></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em"><span style="top:-2.786em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.822em"><span style="top:-2.822em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1/</span><span class="mord mathnormal mtight" style="margin-right:0.1132em">τ</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.485em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">a</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9667em"><span style="top:-2.9667em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1/</span><span class="mord mathnormal mtight" style="margin-right:0.1132em">τ</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6104em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<ul>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau &gt; 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>: más aleatorio, más exploración</li>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau &lt; 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>: más determinista, más explotación</li>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>: distribución original</li>
</ul>
<p><strong>3. Ruido de Dirichlet (Dirichlet Noise)</strong></p>
<p>AlphaGo Zero añade ruido de Dirichlet a la probabilidad a priori del nodo raíz durante el auto-juego:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>ε</mi><mo stretchy="false">)</mo><mo>⋅</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo><mo>+</mo><mi>ε</mi><mo>⋅</mo><msub><mi>η</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">P(s, a) = (1 - \varepsilon) \cdot \pi_\theta(a|s) + \varepsilon \cdot \eta_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">ε</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal">ε</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></p>
<p>Donde <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>∼</mo><mtext>Dir</mtext><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\eta \sim \text{Dir}(\alpha)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">Dir</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mclose">)</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ε</mi><mo>=</mo><mn>0.25</mn></mrow><annotation encoding="application/x-tex">\varepsilon = 0.25</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">ε</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.25</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.03</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.03</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.03</span></span></span></span> (para las 361 acciones de Go).</p>
<p>Esto asegura que incluso movimientos de muy baja probabilidad tengan oportunidad de ser explorados.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="método-de-pool-de-juego-population">Método de pool de juego (Population)<a href="#método-de-pool-de-juego-population" class="hash-link" aria-label="Enlace directo al Método de pool de juego (Population)" title="Enlace directo al Método de pool de juego (Population)" translate="no">​</a></h3>
<p>Otra forma de aumentar diversidad es mantener un <strong>pool de juego</strong>:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Pool de juego = [π_1, π_2, π_3, ..., π_k] (diferentes versiones de políticas)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Cada partida:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1. Elegir aleatoriamente un oponente del pool</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Jugar contra ese oponente</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Usar el resultado para actualizar la política actual</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">4. Periódicamente añadir políticas mejoradas al pool</span><br></span></code></pre></div></div>
<p>Beneficios de este método:</p>
<ul>
<li class=""><strong>Diversidad</strong>: oponentes de diferentes estilos</li>
<li class=""><strong>Estabilidad</strong>: evitar sobreajuste a un oponente específico</li>
<li class=""><strong>Robustez</strong>: aprender a manejar varias estrategias</li>
</ul>
<p>Tanto AlphaGo original como AlphaGo Zero usaron técnicas similares.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="curva-de-crecimiento-de-fuerza">Curva de crecimiento de fuerza<a href="#curva-de-crecimiento-de-fuerza" class="hash-link" aria-label="Enlace directo al Curva de crecimiento de fuerza" title="Enlace directo al Curva de crecimiento de fuerza" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="sistema-de-rating-elo">Sistema de rating Elo<a href="#sistema-de-rating-elo" class="hash-link" aria-label="Enlace directo al Sistema de rating Elo" title="Enlace directo al Sistema de rating Elo" translate="no">​</a></h3>
<p>Para rastrear los cambios en la fuerza de la IA, AlphaGo usa el <strong>sistema de rating Elo</strong>.</p>
<p>Principio básico del sistema Elo:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>A gana</mtext><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mn>10</mn><mrow><mo stretchy="false">(</mo><msub><mi>R</mi><mi>B</mi></msub><mo>−</mo><msub><mi>R</mi><mi>A</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mn>400</mn></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(\text{A gana}) = \frac{1}{1 + 10^{(R_B - R_A)/400}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">A gana</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.3331em;vertical-align:-0.488em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.5703em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mord mtight"><span class="mord mtight">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8853em"><span style="top:-2.8853em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3448em;margin-left:-0.0077em;margin-right:0.1em"><span class="pstrut" style="height:2.6833em"></span><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3385em"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3448em;margin-left:-0.0077em;margin-right:0.1em"><span class="pstrut" style="height:2.6833em"></span><span class="mord mathnormal mtight">A</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3385em"><span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mord mtight">/400</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.488em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<p>Donde <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">R_A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> y <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">R_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> son los puntajes Elo de ambos jugadores.</p>
<ul>
<li class="">Diferencia de 200: el más fuerte espera ganar 75%</li>
<li class="">Diferencia de 400: el más fuerte espera ganar 90%</li>
<li class="">Diferencia de 800: el más fuerte espera ganar 99%</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="crecimiento-de-fuerza-de-alphago">Crecimiento de fuerza de AlphaGo<a href="#crecimiento-de-fuerza-de-alphago" class="hash-link" aria-label="Enlace directo al Crecimiento de fuerza de AlphaGo" title="Enlace directo al Crecimiento de fuerza de AlphaGo" translate="no">​</a></h3>
<p>Visualicemos el crecimiento de fuerza de las diferentes versiones de AlphaGo:</p>
<div>載入中...</div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="análisis-de-velocidad-de-crecimiento">Análisis de velocidad de crecimiento<a href="#análisis-de-velocidad-de-crecimiento" class="hash-link" aria-label="Enlace directo al Análisis de velocidad de crecimiento" title="Enlace directo al Análisis de velocidad de crecimiento" translate="no">​</a></h3>
<p>De la curva podemos observar varios fenómenos interesantes:</p>
<p><strong>1. Crecimiento rápido inicial</strong></p>
<p>En las primeras horas de entrenamiento, la IA aprende reglas básicas y tácticas simples. Esta es la fase de <strong>fruta fácil</strong>: hay demasiados errores obvios por corregir.</p>
<p><strong>2. Crecimiento estable en la fase media</strong></p>
<p>A medida que se eliminan los errores básicos, la IA comienza a aprender tácticas más sutiles y joseki. La velocidad de crecimiento disminuye, pero sigue siendo estable.</p>
<p><strong>3. Crecimiento desacelerado en la fase tardía</strong></p>
<p>Cuando la IA ya es muy fuerte, la mejora adicional se vuelve difícil. Puede necesitar descubrir estrategias completamente nuevas, no solo corregir errores.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="el-momento-de-superar-a-los-humanos">El momento de superar a los humanos<a href="#el-momento-de-superar-a-los-humanos" class="hash-link" aria-label="Enlace directo al El momento de superar a los humanos" title="Enlace directo al El momento de superar a los humanos" translate="no">​</a></h3>
<p>Hitos clave en la curva de entrenamiento de AlphaGo:</p>
<table><thead><tr><th>Hito</th><th>Equivalente a</th><th>Tiempo alcanzado</th></tr></thead><tbody><tr><td>Superar aficionados fuertes</td><td>Elo ~2700</td><td>Aproximadamente 3 horas</td></tr><tr><td>Superar a Fan Hui</td><td>Elo ~3500</td><td>Aproximadamente 36 horas</td></tr><tr><td>Superar a Lee Sedol</td><td>Elo ~4500</td><td>Aproximadamente 60 horas</td></tr><tr><td>Superar AlphaGo original</td><td>Elo ~5000</td><td>Aproximadamente 72 horas</td></tr></tbody></table>
<p>Estos números (de AlphaGo Zero) son asombrosos: <strong>la IA superó miles de años de sabiduría humana del Go en 3 días partiendo desde cero</strong>.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="análisis-de-convergencia">Análisis de convergencia<a href="#análisis-de-convergencia" class="hash-link" aria-label="Enlace directo al Análisis de convergencia" title="Enlace directo al Análisis de convergencia" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="converge-el-auto-juego">¿Converge el auto-juego?<a href="#converge-el-auto-juego" class="hash-link" aria-label="Enlace directo al ¿Converge el auto-juego?" title="Enlace directo al ¿Converge el auto-juego?" translate="no">​</a></h3>
<p>Esta es una pregunta teórica importante. La respuesta corta es: <strong>bajo ciertas condiciones sí, pero Go es demasiado complejo para probarlo rigurosamente</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="garantías-teóricas">Garantías teóricas<a href="#garantías-teóricas" class="hash-link" aria-label="Enlace directo al Garantías teóricas" title="Enlace directo al Garantías teóricas" translate="no">​</a></h3>
<p>Para juegos más simples (como tres en raya), se puede probar:</p>
<ol>
<li class=""><strong>Existencia</strong>: existe equilibrio de Nash (teorema Minimax)</li>
<li class=""><strong>Convergencia</strong>: ciertos algoritmos (como fictitious play) convergen al equilibrio de Nash</li>
</ol>
<p>Para Go, no tenemos garantías rigurosas de convergencia, pero la evidencia experimental muestra:</p>
<ul>
<li class="">La fuerza continúa mejorando</li>
<li class="">No aparecen oscilaciones o degradación obvias</li>
<li class="">La fuerza final supera a todos los humanos conocidos</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="posibles-modos-de-fallo">Posibles modos de fallo<a href="#posibles-modos-de-fallo" class="hash-link" aria-label="Enlace directo al Posibles modos de fallo" title="Enlace directo al Posibles modos de fallo" translate="no">​</a></h3>
<p>Problemas que puede encontrar el auto-juego:</p>
<p><strong>1. Ciclo de estrategias (Strategy Cycling)</strong></p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Estrategia A vence a estrategia B</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Estrategia B vence a estrategia C</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Estrategia C vence a estrategia A</span><br></span></code></pre></div></div>
<p>Esto realmente ocurre en algunos juegos (como piedra-papel-tijera). Pero Go tiene suficiente complejidad, este tipo de ciclo puro parece no ocurrir.</p>
<p><strong>2. Sobreajuste a sí mismo</strong></p>
<p>La IA puede aprender estrategias que solo funcionan contra su propio estilo, y no puede manejar oponentes de otros estilos. Por eso AlphaGo juega contra diferentes versiones de sí mismo, y finalmente prueba contra jugadores humanos.</p>
<p><strong>3. Óptimo local</strong></p>
<p>La IA puede caer en un óptimo local: una estrategia &quot;bastante buena pero no la mejor&quot;. La aleatorización y muchas partidas ayudan a evitar este problema.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="observaciones-prácticas">Observaciones prácticas<a href="#observaciones-prácticas" class="hash-link" aria-label="Enlace directo al Observaciones prácticas" title="Enlace directo al Observaciones prácticas" translate="no">​</a></h3>
<p>De las observaciones del proceso de entrenamiento de AlphaGo:</p>
<ol>
<li class=""><strong>Progreso continuo</strong>: el puntaje Elo continúa subiendo con el entrenamiento</li>
<li class=""><strong>Sin degradación</strong>: no aparece una caída repentina de fuerza</li>
<li class=""><strong>Evolución de estilo</strong>: el estilo de juego de la IA cambia gradualmente con el entrenamiento</li>
<li class=""><strong>Descubrimiento de nuevos joseki</strong>: la IA descubre aperturas y tácticas que los humanos nunca han usado</li>
</ol>
<p>Estas observaciones indican que, aunque no tenemos garantías teóricas, el auto-juego realmente funciona en la práctica.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="detalles-de-implementación">Detalles de implementación<a href="#detalles-de-implementación" class="hash-link" aria-label="Enlace directo al Detalles de implementación" title="Enlace directo al Detalles de implementación" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="auto-juego-paralelo">Auto-juego paralelo<a href="#auto-juego-paralelo" class="hash-link" aria-label="Enlace directo al Auto-juego paralelo" title="Enlace directo al Auto-juego paralelo" translate="no">​</a></h3>
<p>Para acelerar el entrenamiento, AlphaGo usa auto-juego paralelo a gran escala:</p>
<!-- -->
<p><strong>Decisiones de diseño clave</strong>:</p>
<ul>
<li class=""><strong>Síncrono vs asíncrono</strong>: AlphaGo usa actualizaciones asíncronas, los Workers no necesitan esperarse</li>
<li class=""><strong>Frecuencia de actualización</strong>: actualiza parámetros cada N partidas completadas</li>
<li class=""><strong>Selección de oponente</strong>: elige aleatoriamente una de las versiones recientes como oponente</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="estrategia-de-checkpoint">Estrategia de checkpoint<a href="#estrategia-de-checkpoint" class="hash-link" aria-label="Enlace directo al Estrategia de checkpoint" title="Enlace directo al Estrategia de checkpoint" translate="no">​</a></h3>
<p>Guardar checkpoints del modelo periódicamente, para:</p>
<ol>
<li class=""><strong>Pool de juego</strong>: mantener oponentes de diferentes versiones</li>
<li class=""><strong>Evaluación</strong>: rastrear cambios de fuerza</li>
<li class=""><strong>Recuperación de fallos</strong>: puede recuperar si el entrenamiento se interrumpe</li>
</ol>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Pseudocódigo</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">training_loop</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> iteration </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_iterations</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Generar datos de partidas</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        trajectories </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> parallel_self_play</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">current_policy</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_games</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1000</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Actualizar política</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        update_policy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">trajectories</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Evaluar y guardar periódicamente</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> iteration </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">100</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            elo </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> evaluate_against_pool</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">current_policy</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            save_checkpoint</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">current_policy</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> elo</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> elo </span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> best_elo</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                add_to_pool</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">current_policy</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                best_elo </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> elo</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="requisitos-de-recursos-de-entrenamiento">Requisitos de recursos de entrenamiento<a href="#requisitos-de-recursos-de-entrenamiento" class="hash-link" aria-label="Enlace directo al Requisitos de recursos de entrenamiento" title="Enlace directo al Requisitos de recursos de entrenamiento" translate="no">​</a></h3>
<p>La escala de entrenamiento de AlphaGo es impresionante:</p>
<table><thead><tr><th>Versión</th><th>Hardware</th><th>Tiempo de entrenamiento</th><th>Partidas de auto-juego</th></tr></thead><tbody><tr><td>AlphaGo Fan</td><td>176 GPU</td><td>Varios meses</td><td>~30M</td></tr><tr><td>AlphaGo Lee</td><td>48 TPU</td><td>Varias semanas</td><td>~30M</td></tr><tr><td>AlphaGo Zero</td><td>4 TPU</td><td>3 días</td><td>~5M</td></tr><tr><td>AlphaGo Zero (versión 40 días)</td><td>4 TPU</td><td>40 días</td><td>~30M</td></tr></tbody></table>
<p>Nota que AlphaGo Zero logró fuerza mayor con menos hardware y menos tiempo: esto es mejora en eficiencia del algoritmo.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="configuración-de-hiperparámetros">Configuración de hiperparámetros<a href="#configuración-de-hiperparámetros" class="hash-link" aria-label="Enlace directo al Configuración de hiperparámetros" title="Enlace directo al Configuración de hiperparámetros" translate="no">​</a></h3>
<p>Algunos hiperparámetros clave:</p>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Configuración de auto-juego</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_PARALLEL_GAMES </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">5000</span><span class="token plain">      </span><span class="token comment" style="color:#999988;font-style:italic"># Partidas simultáneas</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">GAMES_PER_ITERATION </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">25000</span><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Partidas por iteración</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">MCTS_SIMULATIONS </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1600</span><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Simulaciones MCTS por movimiento</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Configuración de entrenamiento</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">BATCH_SIZE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2048</span><span class="token plain">              </span><span class="token comment" style="color:#999988;font-style:italic"># Tamaño de lote de entrenamiento</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">LEARNING_RATE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.01</span><span class="token plain">           </span><span class="token comment" style="color:#999988;font-style:italic"># Tasa de aprendizaje inicial</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">L2_REGULARIZATION </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1e-4</span><span class="token plain">       </span><span class="token comment" style="color:#999988;font-style:italic"># Decaimiento de pesos</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Configuración de exploración</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">TEMPERATURE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.0</span><span class="token plain">              </span><span class="token comment" style="color:#999988;font-style:italic"># Temperatura para primeros 30 movimientos</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">DIRICHLET_ALPHA </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.03</span><span class="token plain">         </span><span class="token comment" style="color:#999988;font-style:italic"># Parámetro de ruido Dirichlet</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">EXPLORATION_FRACTION </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.25</span><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Proporción de ruido</span><br></span></code></pre></div></div>
<p>Estos hiperparámetros fueron ajustados a través de muchos experimentos, con impacto significativo en los resultados del entrenamiento.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="variantes-del-auto-juego">Variantes del auto-juego<a href="#variantes-del-auto-juego" class="hash-link" aria-label="Enlace directo al Variantes del auto-juego" title="Enlace directo al Variantes del auto-juego" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-original">AlphaGo original<a href="#alphago-original" class="hash-link" aria-label="Enlace directo al AlphaGo original" title="Enlace directo al AlphaGo original" translate="no">​</a></h3>
<p>Flujo de entrenamiento de AlphaGo original:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. Aprendizaje supervisado (SL): aprender de partidas humanas</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   → Produce SL Policy Network (π_SL)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Aprendizaje por refuerzo (RL): auto-juego</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Inicializar π_RL = π_SL</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Pool de oponentes = [π_SL]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Repetir:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     a. π_RL juega contra políticas del pool</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     b. Actualizar π_RL con gradiente de política</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     c. Si π_RL se vuelve más fuerte, añadir al pool</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   → Produce RL Policy Network (π_RL)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Entrenamiento de red de valor:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Usar π_RL para auto-juego y generar posiciones</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Entrenar V(s) para predecir tasa de victoria</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero">AlphaGo Zero<a href="#alphago-zero" class="hash-link" aria-label="Enlace directo al AlphaGo Zero" title="Enlace directo al AlphaGo Zero" translate="no">​</a></h3>
<p>AlphaGo Zero simplificó este flujo:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. Auto-juego puro (sin datos humanos)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Inicializar red aleatoria f_θ</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Repetir:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     a. Usar MCTS + f_θ para auto-juego</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     b. Entrenar cabeza de política y cabeza de valor simultáneamente</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     c. Actualizar f_θ</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   → Una sola red produce política y valor</span><br></span></code></pre></div></div>
<p>Mejoras clave:</p>
<ul>
<li class=""><strong>Sin datos humanos necesarios</strong>: desde cero</li>
<li class=""><strong>Red única</strong>: política y valor comparten características</li>
<li class=""><strong>Entrenamiento más simple</strong>: aprendizaje de extremo a extremo</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphazero">AlphaZero<a href="#alphazero" class="hash-link" aria-label="Enlace directo al AlphaZero" title="Enlace directo al AlphaZero" translate="no">​</a></h3>
<p>AlphaZero generalizó aún más:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Mismo algoritmo, diferentes juegos:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Go: alcanza nivel más allá de AlphaGo Zero</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Ajedrez: supera a Stockfish</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Shogi: supera a Elmo</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Única parte específica del juego: codificación de reglas</span><br></span></code></pre></div></div>
<p>Esto demuestra que el auto-juego es un <strong>paradigma de aprendizaje general</strong>, no limitado a Go.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="qué-aprendieron-los-humanos-de-esto">¿Qué aprendieron los humanos de esto?<a href="#qué-aprendieron-los-humanos-de-esto" class="hash-link" aria-label="Enlace directo al ¿Qué aprendieron los humanos de esto?" title="Enlace directo al ¿Qué aprendieron los humanos de esto?" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="nuevos-joseki-descubiertos-por-la-ia">Nuevos joseki descubiertos por la IA<a href="#nuevos-joseki-descubiertos-por-la-ia" class="hash-link" aria-label="Enlace directo al Nuevos joseki descubiertos por la IA" title="Enlace directo al Nuevos joseki descubiertos por la IA" translate="no">​</a></h3>
<p>El auto-juego produjo muchos movimientos que los humanos nunca habían usado:</p>
<p><strong>1. Innovaciones de apertura</strong></p>
<p>Algunas aperturas preferidas por AlphaGo:</p>
<ul>
<li class="">Invasión 3-3: invadir la esquina temprano</li>
<li class="">Movimientos altos: tradicionalmente considerados &quot;inestables&quot;</li>
<li class="">Variación de gran avalancha: los humanos consideran compleja de calcular</li>
</ul>
<p><strong>2. Nueva evaluación posicional</strong></p>
<p>La evaluación de la IA de algunas posiciones difiere significativamente de los humanos:</p>
<ul>
<li class="">Algunas formas aparentemente &quot;delgadas&quot; son en realidad sólidas</li>
<li class="">El valor de algunas &quot;paredes gruesas&quot; está sobreestimado</li>
<li class="">Reevaluación de &quot;sente&quot; y &quot;gote&quot;</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="impacto-en-el-go-humano">Impacto en el Go humano<a href="#impacto-en-el-go-humano" class="hash-link" aria-label="Enlace directo al Impacto en el Go humano" title="Enlace directo al Impacto en el Go humano" translate="no">​</a></h3>
<p>Después de AlphaGo, el Go profesional cambió significativamente:</p>
<ol>
<li class=""><strong>Diversificación de aperturas</strong>: los profesionales comenzaron a usar nuevas aperturas descubiertas por la IA</li>
<li class=""><strong>Cambio en métodos de entrenamiento</strong>: la IA se convirtió en la principal herramienta de entrenamiento para profesionales</li>
<li class=""><strong>Reconsideración de la teoría</strong>: muchos &quot;principios&quot; tradicionales fueron cuestionados y corregidos</li>
<li class=""><strong>Nueva estética</strong>: comenzar a apreciar el estilo de juego de la IA</li>
</ol>
<p>Ke Jie dijo después de perder contra AlphaGo:</p>
<blockquote>
<p>&quot;AlphaGo me hizo redescubrir el Go. Antes pensaba que los humanos entendíamos el Go, ahora sé que solo habíamos tocado la superficie.&quot;</p>
</blockquote>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="reflexiones-filosóficas">Reflexiones filosóficas<a href="#reflexiones-filosóficas" class="hash-link" aria-label="Enlace directo al Reflexiones filosóficas" title="Enlace directo al Reflexiones filosóficas" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="la-naturaleza-del-aprendizaje">La naturaleza del aprendizaje<a href="#la-naturaleza-del-aprendizaje" class="hash-link" aria-label="Enlace directo al La naturaleza del aprendizaje" title="Enlace directo al La naturaleza del aprendizaje" translate="no">​</a></h3>
<p>El auto-juego plantea preguntas profundas sobre el aprendizaje:</p>
<p><strong>¿De dónde viene el conocimiento?</strong></p>
<ul>
<li class="">El aprendizaje humano depende de información externa (maestros, libros, experiencia)</li>
<li class="">La IA de auto-juego solo tiene reglas, no conocimiento externo</li>
<li class="">Pero aún puede &quot;descubrir&quot; conocimiento: ¿de dónde viene?</li>
</ul>
<p>La respuesta puede ser: <strong>el conocimiento está implícito en las reglas y estructura del juego</strong>. Las reglas de Go definen qué es un buen movimiento y qué es malo, el auto-juego simplemente revela estas estructuras implícitas.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="creatividad-y-descubrimiento">Creatividad y descubrimiento<a href="#creatividad-y-descubrimiento" class="hash-link" aria-label="Enlace directo al Creatividad y descubrimiento" title="Enlace directo al Creatividad y descubrimiento" translate="no">​</a></h3>
<p>Cuando la IA juega el &quot;Movimiento 37&quot; (Move 37), ¿es creación o descubrimiento?</p>
<p>Una perspectiva es: ese movimiento siempre &quot;existió&quot; en las reglas de Go, la IA solo lo &quot;descubrió&quot;.
Otra perspectiva es: la IA &quot;creó&quot; ese movimiento, porque nadie (incluyendo la IA misma) lo conocía de antemano.</p>
<p>Esta pregunta no tiene respuesta estándar, pero desafía nuestra comprensión tradicional de la creatividad.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="el-lugar-de-la-inteligencia-humana">El lugar de la inteligencia humana<a href="#el-lugar-de-la-inteligencia-humana" class="hash-link" aria-label="Enlace directo al El lugar de la inteligencia humana" title="Enlace directo al El lugar de la inteligencia humana" translate="no">​</a></h3>
<p>Si la IA puede partir de cero, a través del auto-juego superar miles de años de sabiduría humana, ¿qué significa esto para los humanos?</p>
<p>Visión optimista:</p>
<ul>
<li class="">La IA es una herramienta creada por humanos</li>
<li class="">Los descubrimientos de la IA pueden mejorar la comprensión humana</li>
<li class="">Los humanos pueden colaborar con la IA para alcanzar niveles más altos</li>
</ul>
<p>Visión cautelosa:</p>
<ul>
<li class="">En ciertos dominios, la computación pura puede superar la intuición humana</li>
<li class="">Necesita reconsiderarse el valor de las &quot;habilidades profesionales&quot;</li>
<li class="">Los métodos de educación y entrenamiento pueden necesitar cambiar</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="correspondencia-con-animaciones">Correspondencia con animaciones<a href="#correspondencia-con-animaciones" class="hash-link" aria-label="Enlace directo al Correspondencia con animaciones" title="Enlace directo al Correspondencia con animaciones" translate="no">​</a></h2>
<p>Conceptos centrales de este artículo y números de animación:</p>
<table><thead><tr><th>Número</th><th>Concepto</th><th>Correspondencia física/matemática</th></tr></thead><tbody><tr><td>E5</td><td>Ciclo de auto-juego</td><td>Iteración de punto fijo</td></tr><tr><td>E6</td><td>Evolución de estrategia</td><td>Dinámica evolutiva</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="resumen">Resumen<a href="#resumen" class="hash-link" aria-label="Enlace directo al Resumen" title="Enlace directo al Resumen" translate="no">​</a></h2>
<p>El auto-juego es una de las tecnologías clave del éxito de AlphaGo. Aprendimos:</p>
<ol>
<li class=""><strong>Por qué funciona</strong>: aprendizaje adversarial, descubrimiento progresivo de debilidades</li>
<li class=""><strong>Mecanismo</strong>: recopilación de trayectorias, gradiente de política, entrenamiento de red de valor</li>
<li class=""><strong>Aleatorización</strong>: parámetro de temperatura, ruido de Dirichlet, pool de juego</li>
<li class=""><strong>Crecimiento de fuerza</strong>: sistema Elo, análisis de curva de crecimiento</li>
<li class=""><strong>Convergencia</strong>: garantías teóricas y observaciones prácticas</li>
<li class=""><strong>Detalles de implementación</strong>: entrenamiento paralelo, estrategia de checkpoint, hiperparámetros</li>
</ol>
<p>En el próximo artículo, exploraremos cómo AlphaGo combina redes neuronales con MCTS, aprovechando las fortalezas de ambos.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="lecturas-adicionales">Lecturas adicionales<a href="#lecturas-adicionales" class="hash-link" aria-label="Enlace directo al Lecturas adicionales" title="Enlace directo al Lecturas adicionales" translate="no">​</a></h2>
<ul>
<li class=""><strong>Siguiente artículo</strong>: <a class="" href="/es/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/">Combinación de MCTS y redes neuronales</a> — La perfecta combinación de intuición y razonamiento</li>
<li class=""><strong>Artículo anterior</strong>: <a class="" href="/es/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/">Introducción al aprendizaje por refuerzo</a> — Conceptos básicos del aprendizaje por refuerzo</li>
<li class=""><strong>Relacionado</strong>: <a class="" href="/es/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/">Descripción general de AlphaGo Zero</a> — El avance desde cero</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="referencias">Referencias<a href="#referencias" class="hash-link" aria-label="Enlace directo al Referencias" title="Enlace directo al Referencias" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2016). &quot;Mastering the game of Go with deep neural networks and tree search.&quot; <em>Nature</em>, 529, 484-489.</li>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">Silver, D., et al. (2018). &quot;A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.&quot; <em>Science</em>, 362(6419), 1140-1144.</li>
<li class="">Heinrich, J., &amp; Silver, D. (2016). &quot;Deep Reinforcement Learning from Self-Play in Imperfect-Information Games.&quot; <em>arXiv preprint</em>.</li>
<li class="">Lanctot, M., et al. (2017). &quot;A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.&quot; <em>NeurIPS</em>.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/13-self-play.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Editar esta página</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Página del documento"><a class="pagination-nav__link pagination-nav__link--prev" href="/es/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/"><div class="pagination-nav__sublabel">Anterior</div><div class="pagination-nav__label">Introducción al aprendizaje por refuerzo</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/es/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/"><div class="pagination-nav__sublabel">Siguiente</div><div class="pagination-nav__label">La Combinación de MCTS y Redes Neuronales</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#por-qué-funciona-el-auto-juego" class="table-of-contents__link toc-highlight">¿Por qué funciona el auto-juego?</a><ul><li><a href="#explicación-intuitiva" class="table-of-contents__link toc-highlight">Explicación intuitiva</a></li><li><a href="#descubrimiento-progresivo-de-debilidades" class="table-of-contents__link toc-highlight">Descubrimiento progresivo de debilidades</a></li><li><a href="#aprendizaje-adversarial" class="table-of-contents__link toc-highlight">Aprendizaje adversarial</a></li><li><a href="#comparación-con-partidas-humanas" class="table-of-contents__link toc-highlight">Comparación con partidas humanas</a></li></ul></li><li><a href="#perspectiva-de-teoría-de-juegos" class="table-of-contents__link toc-highlight">Perspectiva de teoría de juegos</a><ul><li><a href="#equilibrio-de-nash" class="table-of-contents__link toc-highlight">Equilibrio de Nash</a></li><li><a href="#auto-juego-y-equilibrio-de-nash" class="table-of-contents__link toc-highlight">Auto-juego y equilibrio de Nash</a></li><li><a href="#fictitious-play" class="table-of-contents__link toc-highlight">Fictitious Play</a></li></ul></li><li><a href="#mecanismo-del-auto-juego" class="table-of-contents__link toc-highlight">Mecanismo del auto-juego</a><ul><li><a href="#flujo-básico" class="table-of-contents__link toc-highlight">Flujo básico</a></li><li><a href="#generación-de-datos-de-entrenamiento" class="table-of-contents__link toc-highlight">Generación de datos de entrenamiento</a></li><li><a href="#actualización-de-política" class="table-of-contents__link toc-highlight">Actualización de política</a></li><li><a href="#entrenamiento-de-la-red-de-valor" class="table-of-contents__link toc-highlight">Entrenamiento de la red de valor</a></li></ul></li><li><a href="#importancia-de-la-aleatorización" class="table-of-contents__link toc-highlight">Importancia de la aleatorización</a><ul><li><a href="#evitar-ciclos-deterministas" class="table-of-contents__link toc-highlight">Evitar ciclos deterministas</a></li><li><a href="#fuentes-de-aleatorización" class="table-of-contents__link toc-highlight">Fuentes de aleatorización</a></li><li><a href="#método-de-pool-de-juego-population" class="table-of-contents__link toc-highlight">Método de pool de juego (Population)</a></li></ul></li><li><a href="#curva-de-crecimiento-de-fuerza" class="table-of-contents__link toc-highlight">Curva de crecimiento de fuerza</a><ul><li><a href="#sistema-de-rating-elo" class="table-of-contents__link toc-highlight">Sistema de rating Elo</a></li><li><a href="#crecimiento-de-fuerza-de-alphago" class="table-of-contents__link toc-highlight">Crecimiento de fuerza de AlphaGo</a></li><li><a href="#análisis-de-velocidad-de-crecimiento" class="table-of-contents__link toc-highlight">Análisis de velocidad de crecimiento</a></li><li><a href="#el-momento-de-superar-a-los-humanos" class="table-of-contents__link toc-highlight">El momento de superar a los humanos</a></li></ul></li><li><a href="#análisis-de-convergencia" class="table-of-contents__link toc-highlight">Análisis de convergencia</a><ul><li><a href="#converge-el-auto-juego" class="table-of-contents__link toc-highlight">¿Converge el auto-juego?</a></li><li><a href="#garantías-teóricas" class="table-of-contents__link toc-highlight">Garantías teóricas</a></li><li><a href="#posibles-modos-de-fallo" class="table-of-contents__link toc-highlight">Posibles modos de fallo</a></li><li><a href="#observaciones-prácticas" class="table-of-contents__link toc-highlight">Observaciones prácticas</a></li></ul></li><li><a href="#detalles-de-implementación" class="table-of-contents__link toc-highlight">Detalles de implementación</a><ul><li><a href="#auto-juego-paralelo" class="table-of-contents__link toc-highlight">Auto-juego paralelo</a></li><li><a href="#estrategia-de-checkpoint" class="table-of-contents__link toc-highlight">Estrategia de checkpoint</a></li><li><a href="#requisitos-de-recursos-de-entrenamiento" class="table-of-contents__link toc-highlight">Requisitos de recursos de entrenamiento</a></li><li><a href="#configuración-de-hiperparámetros" class="table-of-contents__link toc-highlight">Configuración de hiperparámetros</a></li></ul></li><li><a href="#variantes-del-auto-juego" class="table-of-contents__link toc-highlight">Variantes del auto-juego</a><ul><li><a href="#alphago-original" class="table-of-contents__link toc-highlight">AlphaGo original</a></li><li><a href="#alphago-zero" class="table-of-contents__link toc-highlight">AlphaGo Zero</a></li><li><a href="#alphazero" class="table-of-contents__link toc-highlight">AlphaZero</a></li></ul></li><li><a href="#qué-aprendieron-los-humanos-de-esto" class="table-of-contents__link toc-highlight">¿Qué aprendieron los humanos de esto?</a><ul><li><a href="#nuevos-joseki-descubiertos-por-la-ia" class="table-of-contents__link toc-highlight">Nuevos joseki descubiertos por la IA</a></li><li><a href="#impacto-en-el-go-humano" class="table-of-contents__link toc-highlight">Impacto en el Go humano</a></li></ul></li><li><a href="#reflexiones-filosóficas" class="table-of-contents__link toc-highlight">Reflexiones filosóficas</a><ul><li><a href="#la-naturaleza-del-aprendizaje" class="table-of-contents__link toc-highlight">La naturaleza del aprendizaje</a></li><li><a href="#creatividad-y-descubrimiento" class="table-of-contents__link toc-highlight">Creatividad y descubrimiento</a></li><li><a href="#el-lugar-de-la-inteligencia-humana" class="table-of-contents__link toc-highlight">El lugar de la inteligencia humana</a></li></ul></li><li><a href="#correspondencia-con-animaciones" class="table-of-contents__link toc-highlight">Correspondencia con animaciones</a></li><li><a href="#resumen" class="table-of-contents__link toc-highlight">Resumen</a></li><li><a href="#lecturas-adicionales" class="table-of-contents__link toc-highlight">Lecturas adicionales</a></li><li><a href="#referencias" class="table-of-contents__link toc-highlight">Referencias</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>