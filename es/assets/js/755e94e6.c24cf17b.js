"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[495],{27474(e,n,a){a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"for-engineers/how-it-works/alphago-explained/value-network","title":"Value Network en detalle","description":"Comprensi\xf3n profunda de la arquitectura de la red de valores de AlphaGo, desaf\xedos de entrenamiento y su rol clave en MCTS","source":"@site/i18n/es/docusaurus-plugin-content-docs/current/for-engineers/how-it-works/alphago-explained/08-value-network.mdx","sourceDirName":"for-engineers/how-it-works/alphago-explained","slug":"/for-engineers/how-it-works/alphago-explained/value-network","permalink":"/es/docs/for-engineers/how-it-works/alphago-explained/value-network","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/08-value-network.mdx","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9,"title":"Value Network en detalle","description":"Comprensi\xf3n profunda de la arquitectura de la red de valores de AlphaGo, desaf\xedos de entrenamiento y su rol clave en MCTS"},"sidebar":"tutorialSidebar","previous":{"title":"Policy Network en detalle","permalink":"/es/docs/for-engineers/how-it-works/alphago-explained/policy-network"},"next":{"title":"Diseno de Caracteristicas de Entrada","permalink":"/es/docs/for-engineers/how-it-works/alphago-explained/input-features"}}');var r=a(62615),s=a(30416);const l={sidebar_position:9,title:"Value Network en detalle",description:"Comprensi\xf3n profunda de la arquitectura de la red de valores de AlphaGo, desaf\xedos de entrenamiento y su rol clave en MCTS"},d="Value Network en detalle",o={},c=[{value:"\xbfQu\xe9 es la Value Network?",id:"qu\xe9-es-la-value-network",level:2},{value:"Funci\xf3n principal",id:"funci\xf3n-principal",level:3},{value:"Significado de la salida",id:"significado-de-la-salida",level:3},{value:"\xbfPor qu\xe9 se necesita un solo valor?",id:"por-qu\xe9-se-necesita-un-solo-valor",level:3},{value:"Comparar diferentes opciones",id:"comparar-diferentes-opciones",level:4},{value:"Reemplazar muchas simulaciones",id:"reemplazar-muchas-simulaciones",level:4},{value:"Arquitectura de la red",id:"arquitectura-de-la-red",level:2},{value:"Similitud con la Policy Network",id:"similitud-con-la-policy-network",level:3},{value:"Capa de entrada",id:"capa-de-entrada",level:3},{value:"Capas convolucionales",id:"capas-convolucionales",level:3},{value:"Diferencia en la capa de salida",id:"diferencia-en-la-capa-de-salida",level:3},{value:"Salida de Policy Network",id:"salida-de-policy-network",level:4},{value:"Salida de Value Network",id:"salida-de-value-network",level:4},{value:"Funci\xf3n de activaci\xf3n Tanh",id:"funci\xf3n-de-activaci\xf3n-tanh",level:3},{value:"\xbfPor qu\xe9 Tanh en lugar de Sigmoid?",id:"por-qu\xe9-tanh-en-lugar-de-sigmoid",level:4},{value:"Diagrama de arquitectura completa",id:"diagrama-de-arquitectura-completa",level:3},{value:"Cantidad de par\xe1metros",id:"cantidad-de-par\xe1metros",level:3},{value:"Desaf\xedos del entrenamiento",id:"desaf\xedos-del-entrenamiento",level:2},{value:"Problema de sobreajuste",id:"problema-de-sobreajuste",level:3},{value:"\xbfQu\xe9 es el sobreajuste?",id:"qu\xe9-es-el-sobreajuste",level:4},{value:"\xbfPor qu\xe9 la Value Network es propensa al sobreajuste?",id:"por-qu\xe9-la-value-network-es-propensa-al-sobreajuste",level:4},{value:"Soluci\xf3n: datos de auto-juego",id:"soluci\xf3n-datos-de-auto-juego",level:3},{value:"\xbfPor qu\xe9 esto resuelve el sobreajuste?",id:"por-qu\xe9-esto-resuelve-el-sobreajuste",level:4},{value:"Generaci\xf3n de datos de entrenamiento",id:"generaci\xf3n-de-datos-de-entrenamiento",level:3},{value:"Objetivo y m\xe9todos de entrenamiento",id:"objetivo-y-m\xe9todos-de-entrenamiento",level:2},{value:"P\xe9rdida de error cuadr\xe1tico medio",id:"p\xe9rdida-de-error-cuadr\xe1tico-medio",level:3},{value:"\xbfPor qu\xe9 MSE en lugar de entrop\xeda cruzada?",id:"por-qu\xe9-mse-en-lugar-de-entrop\xeda-cruzada",level:4},{value:"Proceso de entrenamiento",id:"proceso-de-entrenamiento",level:3},{value:"An\xe1lisis de precisi\xf3n",id:"an\xe1lisis-de-precisi\xf3n",level:2},{value:"Comparaci\xf3n con simulaciones aleatorias",id:"comparaci\xf3n-con-simulaciones-aleatorias",level:3},{value:"Precisi\xf3n por etapa",id:"precisi\xf3n-por-etapa",level:3},{value:"Distribuci\xf3n de salida",id:"distribuci\xf3n-de-salida",level:3},{value:"Posiciones inciertas",id:"posiciones-inciertas",level:3},{value:"Rol en MCTS",id:"rol-en-mcts",level:2},{value:"Evaluaci\xf3n de nodos hoja",id:"evaluaci\xf3n-de-nodos-hoja",level:3},{value:"\xbfPor qu\xe9 combinar?",id:"por-qu\xe9-combinar",level:4},{value:"Simplificaci\xf3n de AlphaGo Zero",id:"simplificaci\xf3n-de-alphago-zero",level:3},{value:"Actualizaci\xf3n de retropropagaci\xf3n",id:"actualizaci\xf3n-de-retropropagaci\xf3n",level:3},{value:"An\xe1lisis visual",id:"an\xe1lisis-visual",level:2},{value:"Superficie de valor",id:"superficie-de-valor",level:3},{value:"Evoluci\xf3n durante el entrenamiento",id:"evoluci\xf3n-durante-el-entrenamiento",level:3},{value:"Identificaci\xf3n de posiciones dif\xedciles",id:"identificaci\xf3n-de-posiciones-dif\xedciles",level:3},{value:"Puntos clave de implementaci\xf3n",id:"puntos-clave-de-implementaci\xf3n",level:2},{value:"Implementaci\xf3n en PyTorch",id:"implementaci\xf3n-en-pytorch",level:3},{value:"Ciclo de entrenamiento",id:"ciclo-de-entrenamiento",level:3},{value:"T\xe9cnicas para evitar sobreajuste",id:"t\xe9cnicas-para-evitar-sobreajuste",level:3},{value:"Colaboraci\xf3n con Policy Network",id:"colaboraci\xf3n-con-policy-network",level:2},{value:"Relaci\xf3n complementaria",id:"relaci\xf3n-complementaria",level:3},{value:"Red unificada de dos cabezas",id:"red-unificada-de-dos-cabezas",level:3},{value:"Correspondencia de animaciones",id:"correspondencia-de-animaciones",level:2},{value:"Lecturas adicionales",id:"lecturas-adicionales",level:2},{value:"Puntos clave",id:"puntos-clave",level:2},{value:"Referencias",id:"referencias",level:2}];function t(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"value-network-en-detalle",children:"Value Network en detalle"})}),"\n",(0,r.jsx)(n.p,{children:'Si la Policy Network le dice a AlphaGo "d\xf3nde deber\xeda jugar el siguiente movimiento", la Value Network responde una pregunta m\xe1s fundamental:'}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"\xbfVoy a ganar esta partida?"'})}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"qu\xe9-es-la-value-network",children:"\xbfQu\xe9 es la Value Network?"}),"\n",(0,r.jsx)(n.h3,{id:"funci\xf3n-principal",children:"Funci\xf3n principal"}),"\n",(0,r.jsx)(n.p,{children:"La Value Network es una red neuronal convolucional profunda cuya tarea es:"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Dado el estado actual del tablero, predecir la probabilidad de victoria final"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Expresado matem\xe1ticamente:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"v = f_\u03b8(s)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Donde:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"s"}),": estado actual del tablero"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"f_\u03b8"}),": Value Network (\u03b8 son los par\xe1metros de la red)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"v"}),": un valor entre -1 y +1"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"significado-de-la-salida",children:"Significado de la salida"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Valor de salida"}),(0,r.jsx)(n.th,{children:"Significado"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"+1"}),(0,r.jsx)(n.td,{children:"El jugador actual gana con certeza"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"+0.5"}),(0,r.jsx)(n.td,{children:"El jugador actual tiene ~75% de probabilidad de ganar"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"0"}),(0,r.jsx)(n.td,{children:"Probabilidades iguales para ambos"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"-0.5"}),(0,r.jsx)(n.td,{children:"El jugador actual tiene ~25% de probabilidad de ganar"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"-1"}),(0,r.jsx)(n.td,{children:"El jugador actual pierde con certeza"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"por-qu\xe9-se-necesita-un-solo-valor",children:"\xbfPor qu\xe9 se necesita un solo valor?"}),"\n",(0,r.jsx)(n.h4,{id:"comparar-diferentes-opciones",children:"Comparar diferentes opciones"}),"\n",(0,r.jsx)(n.p,{children:"Al jugar, frecuentemente necesitamos elegir entre m\xfaltiples opciones. La Value Network simplifica esta comparaci\xf3n:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Valor de la posici\xf3n de opci\xf3n A: 0.3\nValor de la posici\xf3n de opci\xf3n B: 0.5\nValor de la posici\xf3n de opci\xf3n C: 0.2\n\n\u2192 Elegir B (el valor m\xe1s alto)\n"})}),"\n",(0,r.jsx)(n.p,{children:'Sin un solo valor, \xbfc\xf3mo comparamos "capturar un grupo del oponente" con "rodear un territorio grande"?'}),"\n",(0,r.jsx)(n.h4,{id:"reemplazar-muchas-simulaciones",children:"Reemplazar muchas simulaciones"}),"\n",(0,r.jsxs)(n.p,{children:["En el Monte Carlo Tree Search tradicional, evaluar una posici\xf3n requiere realizar ",(0,r.jsx)(n.strong,{children:"simulaciones aleatorias (rollout)"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Comenzar desde la posici\xf3n actual"}),"\n",(0,r.jsx)(n.li,{children:"Ambos jugadores juegan aleatoriamente hasta que termine el juego"}),"\n",(0,r.jsx)(n.li,{children:"Registrar victoria/derrota"}),"\n",(0,r.jsx)(n.li,{children:"Repetir miles de veces, calcular tasa de victoria"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Esto es muy lento. La Value Network puede dar una evaluaci\xf3n con ",(0,r.jsx)(n.strong,{children:"una sola propagaci\xf3n hacia adelante"}),", \xf3rdenes de magnitud m\xe1s r\xe1pido."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"M\xe9todo"}),(0,r.jsx)(n.th,{children:"Tiempo de evaluaci\xf3n"}),(0,r.jsx)(n.th,{children:"Precisi\xf3n"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1000 simulaciones aleatorias"}),(0,r.jsx)(n.td,{children:"~2000 ms"}),(0,r.jsx)(n.td,{children:"Baja"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"15000 simulaciones aleatorias"}),(0,r.jsx)(n.td,{children:"~30000 ms"}),(0,r.jsx)(n.td,{children:"Media"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Value Network"}),(0,r.jsx)(n.td,{children:"~3 ms"}),(0,r.jsx)(n.td,{children:"Alta (equivalente a 15000 simulaciones)"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"arquitectura-de-la-red",children:"Arquitectura de la red"}),"\n",(0,r.jsx)(n.h3,{id:"similitud-con-la-policy-network",children:"Similitud con la Policy Network"}),"\n",(0,r.jsx)(n.p,{children:"La arquitectura de la Value Network es muy similar a la Policy Network, ambas son redes neuronales convolucionales profundas:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Capa de entrada \u2192 Capas convolucionales \xd712 \u2192 Capa totalmente conectada \u2192 Salida\n       \u2193                   \u2193                          \u2193                    \u2193\n   19\xd719\xd748            19\xd719\xd7192                   256-dim            Un solo valor\n"})}),"\n",(0,r.jsx)(n.h3,{id:"capa-de-entrada",children:"Capa de entrada"}),"\n",(0,r.jsxs)(n.p,{children:["Similar a la Policy Network, la entrada es un tensor de caracter\xedsticas de ",(0,r.jsx)(n.strong,{children:"19\xd719\xd749"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"19\xd719"}),": tama\xf1o del tablero"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"49"}),": 48 planos de caracter\xedsticas + 1 plano indicando de qui\xe9n es el turno"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"El plano adicional es importante: la Value Network necesita saber de qui\xe9n es el turno, porque la misma posici\xf3n tiene valores opuestos para negro y blanco."}),"\n",(0,r.jsx)(n.h3,{id:"capas-convolucionales",children:"Capas convolucionales"}),"\n",(0,r.jsx)(n.p,{children:"Igual que la Policy Network:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"12 capas convolucionales"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"192 filtros"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kernel 3\xd73"})," (primera capa 5\xd75)"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Funci\xf3n de activaci\xf3n ReLU"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"diferencia-en-la-capa-de-salida",children:"Diferencia en la capa de salida"}),"\n",(0,r.jsx)(n.p,{children:"Esta es la diferencia clave entre Value Network y Policy Network:"}),"\n",(0,r.jsx)(n.h4,{id:"salida-de-policy-network",children:"Salida de Policy Network"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"19\xd719\xd7192 \u2192 Conv 1\xd71 \u2192 19\xd719\xd71 \u2192 Aplanar \u2192 361-dim \u2192 Softmax \u2192 Distribuci\xf3n de probabilidad\n"})}),"\n",(0,r.jsx)(n.h4,{id:"salida-de-value-network",children:"Salida de Value Network"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"19\xd719\xd7192 \u2192 Conv 1\xd71 \u2192 19\xd719\xd71 \u2192 Aplanar \u2192 361-dim \u2192 FC 256 \u2192 ReLU \u2192 FC 1 \u2192 Tanh \u2192 Un solo valor\n"})}),"\n",(0,r.jsx)(n.h3,{id:"funci\xf3n-de-activaci\xf3n-tanh",children:"Funci\xf3n de activaci\xf3n Tanh"}),"\n",(0,r.jsxs)(n.p,{children:["La \xfaltima capa de la Value Network usa la funci\xf3n ",(0,r.jsx)(n.strong,{children:"Tanh"})," (tangente hiperb\xf3lica):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n"})}),"\n",(0,r.jsxs)(n.p,{children:["El rango de salida de Tanh es ",(0,r.jsx)(n.strong,{children:"(-1, +1)"}),", correspondiendo exactamente a victoria/derrota."]}),"\n",(0,r.jsx)(n.h4,{id:"por-qu\xe9-tanh-en-lugar-de-sigmoid",children:"\xbfPor qu\xe9 Tanh en lugar de Sigmoid?"}),"\n",(0,r.jsx)(n.p,{children:"El rango de salida de Sigmoid es (0, 1), que tambi\xe9n puede representar tasa de victoria. Pero Tanh tiene varias ventajas:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simetr\xeda"}),": centrado en 0, salida puede ser positiva o negativa"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Mejor gradiente"}),": gradiente cercano a 1 alrededor de 0"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sem\xe1ntica clara"}),": positivo es ganar, negativo es perder, cero es empate"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"diagrama-de-arquitectura-completa",children:"Diagrama de arquitectura completa"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Entrada: 19\xd719\xd749\n        \u2193\n    Conv 5\xd75, 192 filtros\n        \u2193\n    ReLU\n        \u2193\n    Conv 3\xd73, 192 filtros (\xd711)\n        \u2193\n    ReLU\n        \u2193\n    Conv 1\xd71, 1 filtro\n        \u2193\n    Aplanar (361 dim)\n        \u2193\n    Totalmente conectada (256 dim)\n        \u2193\n    ReLU\n        \u2193\n    Totalmente conectada (1 dim)\n        \u2193\n    Tanh\n        \u2193\nSalida: [-1, +1]\n"})}),"\n",(0,r.jsx)(n.h3,{id:"cantidad-de-par\xe1metros",children:"Cantidad de par\xe1metros"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Capa"}),(0,r.jsx)(n.th,{children:"C\xe1lculo"}),(0,r.jsx)(n.th,{children:"Cantidad de par\xe1metros"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Capas conv"}),(0,r.jsx)(n.td,{children:"Igual que Policy Network"}),(0,r.jsx)(n.td,{children:"~3.9M"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Capa FC 1"}),(0,r.jsx)(n.td,{children:"361\xd7256 + 256"}),(0,r.jsx)(n.td,{children:"92,672"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Capa FC 2"}),(0,r.jsx)(n.td,{children:"256\xd71 + 1"}),(0,r.jsx)(n.td,{children:"257"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Total"})}),(0,r.jsx)(n.td,{}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"~4.0M"})})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Aproximadamente 4 millones de par\xe1metros, ligeramente m\xe1s que la Policy Network."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"desaf\xedos-del-entrenamiento",children:"Desaf\xedos del entrenamiento"}),"\n",(0,r.jsx)(n.h3,{id:"problema-de-sobreajuste",children:"Problema de sobreajuste"}),"\n",(0,r.jsxs)(n.p,{children:["El entrenamiento de la Value Network es mucho m\xe1s dif\xedcil que el de la Policy Network. El problema principal es el ",(0,r.jsx)(n.strong,{children:"sobreajuste"}),"."]}),"\n",(0,r.jsx)(n.h4,{id:"qu\xe9-es-el-sobreajuste",children:"\xbfQu\xe9 es el sobreajuste?"}),"\n",(0,r.jsx)(n.p,{children:'El sobreajuste es cuando el modelo "memoriza" los datos de entrenamiento en lugar de aprender a generalizar. Se manifiesta como:'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Muy buen rendimiento en el conjunto de entrenamiento"}),"\n",(0,r.jsx)(n.li,{children:"Mal rendimiento en el conjunto de prueba"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"por-qu\xe9-la-value-network-es-propensa-al-sobreajuste",children:"\xbfPor qu\xe9 la Value Network es propensa al sobreajuste?"}),"\n",(0,r.jsx)(n.p,{children:"Considera los datos de una partida:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Posici\xf3n 1 \u2192 Posici\xf3n 2 \u2192 Posici\xf3n 3 \u2192 ... \u2192 Posici\xf3n 200 \u2192 Resultado: Negro gana\n"})}),"\n",(0,r.jsx)(n.p,{children:"Si se entrena directamente con estos datos:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Estas 200 posiciones est\xe1n fuertemente correlacionadas"}),"\n",(0,r.jsx)(n.li,{children:"Vienen de la misma partida, con el mismo resultado"}),"\n",(0,r.jsx)(n.li,{children:'El modelo puede aprender a "reconocer" esta partida, en lugar de entender posiciones'}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"DeepMind descubri\xf3: si se entrenan Policy y Value Network con las mismas partidas humanas, la Value Network sufre severo sobreajuste."}),"\n",(0,r.jsx)(n.h3,{id:"soluci\xf3n-datos-de-auto-juego",children:"Soluci\xf3n: datos de auto-juego"}),"\n",(0,r.jsxs)(n.p,{children:["La soluci\xf3n de DeepMind fue usar ",(0,r.jsx)(n.strong,{children:"auto-juego"})," para generar nuevos datos de entrenamiento:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"1. Usar la RL Policy Network entrenada para auto-juego\n2. Tomar solo una posici\xf3n de cada partida (evitar correlaci\xf3n)\n3. La etiqueta de esta posici\xf3n es el resultado final de esa partida\n4. Generar 30 millones de tales muestras\n"})}),"\n",(0,r.jsx)(n.h4,{id:"por-qu\xe9-esto-resuelve-el-sobreajuste",children:"\xbfPor qu\xe9 esto resuelve el sobreajuste?"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gran cantidad de datos"}),": 30 millones de posiciones independientes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sin correlaci\xf3n"}),": solo una posici\xf3n por partida"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Distribuci\xf3n diferente"}),": la distribuci\xf3n de posiciones de auto-juego difiere de las partidas humanas"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"generaci\xf3n-de-datos-de-entrenamiento",children:"Generaci\xf3n de datos de entrenamiento"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Pseudoc\xf3digo\ntraining_data = []\n\nfor game_id in range(30_000_000):\n    # Auto-juego de una partida\n    states, result = self_play(rl_policy_network)\n\n    # Seleccionar aleatoriamente una posici\xf3n\n    random_index = random.randint(0, len(states) - 1)\n    state = states[random_index]\n\n    # Registrar posici\xf3n y resultado\n    training_data.append((state, result))\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"objetivo-y-m\xe9todos-de-entrenamiento",children:"Objetivo y m\xe9todos de entrenamiento"}),"\n",(0,r.jsx)(n.h3,{id:"p\xe9rdida-de-error-cuadr\xe1tico-medio",children:"P\xe9rdida de error cuadr\xe1tico medio"}),"\n",(0,r.jsxs)(n.p,{children:["La Value Network usa el ",(0,r.jsx)(n.strong,{children:"error cuadr\xe1tico medio (MSE)"})," como funci\xf3n de p\xe9rdida:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"L(\u03b8) = (1/n) \xd7 \u03a3 (v_\u03b8(s) - z)\xb2\n"})}),"\n",(0,r.jsx)(n.p,{children:"Donde:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"v_\u03b8(s)"}),": valor predicho por el modelo"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"z"}),": resultado real (+1 o -1)"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"por-qu\xe9-mse-en-lugar-de-entrop\xeda-cruzada",children:"\xbfPor qu\xe9 MSE en lugar de entrop\xeda cruzada?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Entrop\xeda cruzada"})," es adecuada para problemas de clasificaci\xf3n (etiquetas discretas)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"MSE"})," es adecuado para problemas de regresi\xf3n (valores continuos)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Aunque los resultados son solo +1 o -1, el modelo predice valores continuos (cualquier n\xfamero entre -1 y +1). MSE hace que el modelo aprenda a predecir valores cercanos a +1 o -1."}),"\n",(0,r.jsx)(n.h3,{id:"proceso-de-entrenamiento",children:"Proceso de entrenamiento"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Pseudoc\xf3digo\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        states, outcomes = batch\n\n        # Propagaci\xf3n hacia adelante\n        values = network(states)  # (batch, 1)\n\n        # Calcular p\xe9rdida (MSE)\n        loss = mse_loss(values, outcomes)\n\n        # Retropropagaci\xf3n\n        loss.backward()\n        optimizer.step()\n"})}),"\n",(0,r.jsx)(n.p,{children:"Detalles de entrenamiento:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Optimizador"}),": SGD con momentum"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tasa de aprendizaje"}),": 0.003"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tama\xf1o de lote"}),": 32"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tiempo de entrenamiento"}),": aproximadamente 1 semana (50 GPUs)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"an\xe1lisis-de-precisi\xf3n",children:"An\xe1lisis de precisi\xf3n"}),"\n",(0,r.jsx)(n.h3,{id:"comparaci\xf3n-con-simulaciones-aleatorias",children:"Comparaci\xf3n con simulaciones aleatorias"}),"\n",(0,r.jsx)(n.p,{children:"DeepMind realiz\xf3 una comparaci\xf3n detallada en el art\xedculo:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"M\xe9todo de evaluaci\xf3n"}),(0,r.jsx)(n.th,{children:"Error de predicci\xf3n"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1000 simulaciones aleatorias"}),(0,r.jsx)(n.td,{children:"Alto"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"15000 simulaciones aleatorias"}),(0,r.jsx)(n.td,{children:"Medio"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Value Network"}),(0,r.jsx)(n.td,{children:"Comparable a 15000 simulaciones"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Esto significa que una evaluaci\xf3n de Value Network \u2248 15000 simulaciones aleatorias, pero aproximadamente 1000 veces m\xe1s r\xe1pido."}),"\n",(0,r.jsx)(n.h3,{id:"precisi\xf3n-por-etapa",children:"Precisi\xf3n por etapa"}),"\n",(0,r.jsx)(n.p,{children:"La precisi\xf3n de la Value Network depende del progreso del juego:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Etapa"}),(0,r.jsx)(n.th,{children:"Movimientos restantes"}),(0,r.jsx)(n.th,{children:"Dificultad de predicci\xf3n"}),(0,r.jsx)(n.th,{children:"Precisi\xf3n"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Apertura"}),(0,r.jsx)(n.td,{children:"~300"}),(0,r.jsx)(n.td,{children:"Muy dif\xedcil"}),(0,r.jsx)(n.td,{children:"Baja"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Medio juego"}),(0,r.jsx)(n.td,{children:"~150"}),(0,r.jsx)(n.td,{children:"Dif\xedcil"}),(0,r.jsx)(n.td,{children:"Media"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Final"}),(0,r.jsx)(n.td,{children:"~50"}),(0,r.jsx)(n.td,{children:"M\xe1s f\xe1cil"}),(0,r.jsx)(n.td,{children:"Alta"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cierre"}),(0,r.jsx)(n.td,{children:"~10"}),(0,r.jsx)(n.td,{children:"Simple"}),(0,r.jsx)(n.td,{children:"Muy alta"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Esto es intuitivamente razonable: cuanto m\xe1s cerca del final del juego, m\xe1s determinado est\xe1 el resultado."}),"\n",(0,r.jsx)(n.h3,{id:"distribuci\xf3n-de-salida",children:"Distribuci\xf3n de salida"}),"\n",(0,r.jsx)(n.p,{children:"La distribuci\xf3n de salida de una Value Network bien entrenada:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"        Frecuencia\n          |\n          |    *\n          |   * *\n          |  *   *\n          | *     *\n          |*       *\n          +----+----+---- Valor de salida\n         -1    0   +1\n\nLa mayor\xeda de las salidas se concentran cerca de -1 y +1\n(porque la mayor\xeda de las posiciones tienen una tendencia clara de victoria/derrota)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"posiciones-inciertas",children:"Posiciones inciertas"}),"\n",(0,r.jsx)(n.p,{children:"Cuando la salida de la Value Network est\xe1 cerca de 0, indica una posici\xf3n muy compleja donde el resultado es dif\xedcil de predecir. Estas posiciones suelen ser:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"En medio de batallas grandes"}),"\n",(0,r.jsx)(n.li,{children:"Ambos lados igualmente equilibrados"}),"\n",(0,r.jsx)(n.li,{children:"Existen m\xfaltiples variaciones posibles"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"En MCTS, estos nodos reciben m\xe1s recursos de b\xfasqueda (debido a la alta incertidumbre)."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"rol-en-mcts",children:"Rol en MCTS"}),"\n",(0,r.jsx)(n.h3,{id:"evaluaci\xf3n-de-nodos-hoja",children:"Evaluaci\xf3n de nodos hoja"}),"\n",(0,r.jsxs)(n.p,{children:["La Value Network juega un rol clave en la fase de ",(0,r.jsx)(n.strong,{children:"Evaluaci\xf3n"})," de MCTS:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\xc1rbol de b\xfasqueda MCTS:\n\n        Nodo ra\xedz (posici\xf3n actual)\n           /    \\\n         A        B\n        /  \\    /  \\\n       A1  A2  B1  B2 \u2190 Nodos hoja\n        \u2193   \u2193   \u2193   \u2193\n      Eval Eval Eval Eval\n"})}),"\n",(0,r.jsx)(n.p,{children:"Cuando MCTS llega a un nodo hoja, necesita evaluar el valor de esta posici\xf3n. Hay dos m\xe9todos:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simulaci\xf3n aleatoria (Rollout)"}),": jugar aleatoriamente desde el nodo hoja hasta el final"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Evaluaci\xf3n por Value Network"}),": usar directamente la red neuronal para predecir"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo combina ambos:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"V(hoja) = (1-\u03bb) \xd7 V_network(hoja) + \u03bb \xd7 V_rollout(hoja)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Donde \u03bb = 0.5, es decir, peso igual para ambos."}),"\n",(0,r.jsx)(n.h4,{id:"por-qu\xe9-combinar",children:"\xbfPor qu\xe9 combinar?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Value Network"})," es m\xe1s precisa, pero puede tener sesgo sistem\xe1tico"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simulaci\xf3n aleatoria"})," es menos precisa, pero proporciona una estimaci\xf3n independiente"]}),"\n",(0,r.jsx)(n.li,{children:"Combinar ambas puede ser complementario"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"simplificaci\xf3n-de-alphago-zero",children:"Simplificaci\xf3n de AlphaGo Zero"}),"\n",(0,r.jsx)(n.p,{children:"El posterior AlphaGo Zero abandon\xf3 completamente las simulaciones aleatorias:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"V(hoja) = V_network(hoja)\n"})}),"\n",(0,r.jsx)(n.p,{children:'Esto simplific\xf3 enormemente el sistema, mientras que la fuerza de juego mejor\xf3. Esto demuestra que la Value Network es lo suficientemente confiable, sin necesidad del "seguro" de simulaciones aleatorias.'}),"\n",(0,r.jsx)(n.h3,{id:"actualizaci\xf3n-de-retropropagaci\xf3n",children:"Actualizaci\xf3n de retropropagaci\xf3n"}),"\n",(0,r.jsx)(n.p,{children:"Despu\xe9s de evaluar el nodo hoja, este valor se propaga hacia atr\xe1s a lo largo del camino:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"v3 = V(hoja) = 0.6\n      \u2191\nActualizar valor Q de A2\n      \u2191\nActualizar valor Q de A\n      \u2191\nActualizar estad\xedsticas del nodo ra\xedz\n"})}),"\n",(0,r.jsx)(n.p,{children:"El valor Q mantenido por cada nodo es el promedio de todas las evaluaciones de nodos hoja que pasaron por \xe9l:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Q(s, a) = (1/N(s,a)) \xd7 \u03a3 V(hoja)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"an\xe1lisis-visual",children:"An\xe1lisis visual"}),"\n",(0,r.jsx)(n.h3,{id:"superficie-de-valor",children:"Superficie de valor"}),"\n",(0,r.jsx)(n.p,{children:'Imagina un tablero simplificado de 3\xd73. La Value Network aprende una "superficie de valor":'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Superficie de valor (tablero simplificado 3x3)"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{style:{textAlign:"center"}}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Posici\xf3n de blanco 1"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Posici\xf3n de blanco 2"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Posici\xf3n de blanco 3"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{style:{textAlign:"center"},children:(0,r.jsx)(n.strong,{children:"Negro 1"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"+0.3"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"-0.1"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"+0.2"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{style:{textAlign:"center"},children:(0,r.jsx)(n.strong,{children:"Negro 2"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"-0.2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"+0.5"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"-0.3"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{style:{textAlign:"center"},children:(0,r.jsx)(n.strong,{children:"Negro 3"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"+0.1"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"-0.2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"+0.4"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Esta superficie nos dice el valor de cada combinaci\xf3n de posiciones. Valores positivos favorecen a negro, valores negativos favorecen a blanco."}),"\n",(0,r.jsx)(n.h3,{id:"evoluci\xf3n-durante-el-entrenamiento",children:"Evoluci\xf3n durante el entrenamiento"}),"\n",(0,r.jsx)(n.p,{children:"A medida que progresa el entrenamiento, las predicciones de la Value Network se vuelven m\xe1s precisas:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"       Error de predicci\xf3n\n          |\n     1.0  |*\n          | *\n     0.5  |  *\n          |   *\n     0.1  |    * * * * *\n          +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Pasos de entrenamiento\n          0   100K  500K  1M\n"})}),"\n",(0,r.jsx)(n.p,{children:"El error disminuye r\xe1pidamente, luego se estabiliza."}),"\n",(0,r.jsx)(n.h3,{id:"identificaci\xf3n-de-posiciones-dif\xedciles",children:"Identificaci\xf3n de posiciones dif\xedciles"}),"\n",(0,r.jsx)(n.p,{children:"La Value Network puede ayudar a identificar posiciones dif\xedciles:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Salida"}),(0,r.jsx)(n.th,{children:"Significado"}),(0,r.jsx)(n.th,{children:"Estrategia"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cerca de +1"}),(0,r.jsx)(n.td,{children:"Gran ventaja"}),(0,r.jsx)(n.td,{children:"Juego s\xf3lido"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cerca de -1"}),(0,r.jsx)(n.td,{children:"Gran desventaja"}),(0,r.jsx)(n.td,{children:"Buscar oportunidades de remontada"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cerca de 0"}),(0,r.jsx)(n.td,{children:"Posici\xf3n compleja"}),(0,r.jsx)(n.td,{children:"Necesita c\xe1lculo profundo"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo invierte m\xe1s tiempo de pensamiento en posiciones cercanas a 0."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"puntos-clave-de-implementaci\xf3n",children:"Puntos clave de implementaci\xf3n"}),"\n",(0,r.jsx)(n.h3,{id:"implementaci\xf3n-en-pytorch",children:"Implementaci\xf3n en PyTorch"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ValueNetwork(nn.Module):\n    def __init__(self, input_channels=49, num_filters=192, num_layers=12):\n        super().__init__()\n\n        # Primera capa convolucional (5\xd75)\n        self.conv1 = nn.Conv2d(input_channels, num_filters,\n                               kernel_size=5, padding=2)\n\n        # Capas convolucionales intermedias (3\xd73) \xd711\n        self.conv_layers = nn.ModuleList([\n            nn.Conv2d(num_filters, num_filters,\n                     kernel_size=3, padding=1)\n            for _ in range(num_layers - 1)\n        ])\n\n        # Capa convolucional de salida\n        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)\n\n        # Capas totalmente conectadas\n        self.fc1 = nn.Linear(361, 256)\n        self.fc2 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        # x: (batch, 49, 19, 19)\n\n        # Capas convolucionales\n        x = F.relu(self.conv1(x))\n        for conv in self.conv_layers:\n            x = F.relu(conv(x))\n        x = self.conv_out(x)\n\n        # Aplanar\n        x = x.view(x.size(0), -1)  # (batch, 361)\n\n        # Capas totalmente conectadas\n        x = F.relu(self.fc1(x))\n        x = torch.tanh(self.fc2(x))\n\n        return x.squeeze(-1)  # (batch,)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"ciclo-de-entrenamiento",children:"Ciclo de entrenamiento"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def train_value_network(model, optimizer, states, outcomes):\n    """\n    states: (batch, 49, 19, 19) - caracter\xedsticas del tablero\n    outcomes: (batch,) - resultado del juego (+1 o -1)\n    """\n    # Propagaci\xf3n hacia adelante\n    values = model(states)  # (batch,)\n\n    # P\xe9rdida MSE\n    loss = F.mse_loss(values, outcomes)\n\n    # Retropropagaci\xf3n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Calcular precisi\xf3n (predicci\xf3n correcta de victoria/derrota)\n    predictions = (values > 0).float() * 2 - 1  # Convertir a +1/-1\n    accuracy = (predictions == outcomes).float().mean()\n\n    return loss.item(), accuracy.item()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"t\xe9cnicas-para-evitar-sobreajuste",children:"T\xe9cnicas para evitar sobreajuste"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# 1. Aumento de datos (8 simetr\xedas)\ndef augment(state, outcome):\n    augmented = []\n    for rotation in [0, 90, 180, 270]:\n        s = rotate(state, rotation)\n        augmented.append((s, outcome))\n        augmented.append((flip(s), outcome))\n    return augmented\n\n# 2. Dropout\nclass ValueNetworkWithDropout(ValueNetwork):\n    def __init__(self, *args, dropout_rate=0.5, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        # ... capas conv ...\n        x = self.dropout(x)  # dropout antes de capas FC\n        # ... capas FC ...\n\n# 3. Detenci\xf3n temprana (Early Stopping)\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(max_epochs):\n    train_loss = train_one_epoch()\n    val_loss = evaluate()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        save_model()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping!\")\n            break\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"colaboraci\xf3n-con-policy-network",children:"Colaboraci\xf3n con Policy Network"}),"\n",(0,r.jsx)(n.h3,{id:"relaci\xf3n-complementaria",children:"Relaci\xf3n complementaria"}),"\n",(0,r.jsx)(n.p,{children:"Policy Network y Value Network son complementarias en AlphaGo:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Red"}),(0,r.jsx)(n.th,{children:"Pregunta que responde"}),(0,r.jsx)(n.th,{children:"Salida"}),(0,r.jsx)(n.th,{children:"Rol en MCTS"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Policy"}),(0,r.jsx)(n.td,{children:"\xbfD\xf3nde jugar?"}),(0,r.jsx)(n.td,{children:"Distribuci\xf3n de probabilidad"}),(0,r.jsx)(n.td,{children:"Guiar direcci\xf3n de b\xfasqueda"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Value"}),(0,r.jsx)(n.td,{children:"\xbfGanar\xe9?"}),(0,r.jsx)(n.td,{children:"Un solo valor"}),(0,r.jsx)(n.td,{children:"Evaluar nodos hoja"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"red-unificada-de-dos-cabezas",children:"Red unificada de dos cabezas"}),"\n",(0,r.jsxs)(n.p,{children:["En AlphaGo Zero, estas dos redes se fusionaron en una ",(0,r.jsx)(n.strong,{children:"red de dos cabezas"}),":"]}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart TB\n    Shared["Capas compartidas de extraccion de caracteristicas"]\n    PolicyHead["Cabeza Policy"]\n    ValueHead["Cabeza Value"]\n    PolicyOut["361 probabilidades"]\n    ValueOut["Un solo valor"]\n\n    Shared --\x3e PolicyHead\n    Shared --\x3e ValueHead\n    PolicyHead --\x3e PolicyOut\n    ValueHead --\x3e ValueOut'}),"\n",(0,r.jsx)(n.p,{children:"Ventajas de este dise\xf1o:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compartir par\xe1metros"}),": reduce c\xe1lculo"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compartir caracter\xedsticas"}),": Policy y Value usan las mismas caracter\xedsticas"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Entrenamiento m\xe1s estable"}),": los dos objetivos se regularizan mutuamente"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Ver m\xe1s en ",(0,r.jsx)(n.a,{href:"../dual-head-resnet",children:"Red de dos cabezas y red residual"}),"."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"correspondencia-de-animaciones",children:"Correspondencia de animaciones"}),"\n",(0,r.jsx)(n.p,{children:"Los conceptos principales de este art\xedculo y los n\xfameros de animaci\xf3n correspondientes:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"N\xfamero"}),(0,r.jsx)(n.th,{children:"Concepto"}),(0,r.jsx)(n.th,{children:"Correspondencia f\xedsica/matem\xe1tica"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"E2"}),(0,r.jsx)(n.td,{children:"Value Network"}),(0,r.jsx)(n.td,{children:"Superficie de energ\xeda potencial"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"D4"}),(0,r.jsx)(n.td,{children:"Funci\xf3n de valor"}),(0,r.jsx)(n.td,{children:"Retorno esperado"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"C6"}),(0,r.jsx)(n.td,{children:"Evaluaci\xf3n de nodo hoja"}),(0,r.jsx)(n.td,{children:"Aproximaci\xf3n de funci\xf3n"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"H3"}),(0,r.jsx)(n.td,{children:"Diferencia temporal"}),(0,r.jsx)(n.td,{children:"Aprendizaje bootstrap"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"lecturas-adicionales",children:"Lecturas adicionales"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Art\xedculo anterior"}),": ",(0,r.jsx)(n.a,{href:"../policy-network",children:"Policy Network en detalle"})," \u2014 C\xf3mo la red de pol\xedticas selecciona movimientos"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Siguiente art\xedculo"}),": ",(0,r.jsx)(n.a,{href:"../input-features",children:"Dise\xf1o de caracter\xedsticas de entrada"})," \u2014 Detalles de los 48 planos de caracter\xedsticas"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tema avanzado"}),": ",(0,r.jsx)(n.a,{href:"../mcts-neural-combo",children:"Combinaci\xf3n de MCTS con redes neuronales"})," \u2014 El proceso completo de b\xfasqueda"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"puntos-clave",children:"Puntos clave"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Value Network predice tasa de victoria"}),": salida un solo valor entre -1 y +1"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Salida Tanh"}),": asegura que la salida est\xe9 en el rango correcto"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"P\xe9rdida MSE"}),": aproxima el valor predicho al resultado real"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Desaf\xedo de sobreajuste"}),": necesita datos de auto-juego para evitar"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reemplaza simulaciones aleatorias"}),": una evaluaci\xf3n \u2248 15000 simulaciones"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:'La Value Network es el "juicio" de AlphaGo \u2014 permite que la IA eval\xfae si cualquier posici\xf3n es buena o mala, sin necesidad de agotar todas las posibilidades.'}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"referencias",children:"Referencias"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:['Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." ',(0,r.jsx)(n.em,{children:"Nature"}),", 529, 484-489."]}),"\n",(0,r.jsxs)(n.li,{children:['Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." ',(0,r.jsx)(n.em,{children:"Nature"}),", 551, 354-359."]}),"\n",(0,r.jsxs)(n.li,{children:["Sutton, R. S., & Barto, A. G. (2018). ",(0,r.jsx)(n.em,{children:"Reinforcement Learning: An Introduction"}),". MIT Press."]}),"\n",(0,r.jsxs)(n.li,{children:['Tesauro, G. (1995). "Temporal difference learning and TD-Gammon." ',(0,r.jsx)(n.em,{children:"Communications of the ACM"}),", 38(3), 58-68."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(t,{...e})}):t(e)}},30416(e,n,a){a.d(n,{R:()=>l,x:()=>d});var i=a(59471);const r={},s=i.createContext(r);function l(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);