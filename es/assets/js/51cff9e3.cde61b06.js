"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[7397],{37303(e,a,n){n.r(a),n.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>d,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"alphago/explained/alphago-zero","title":"Visi\xf3n General de AlphaGo Zero","description":"Desde cero, completamente autodidacta, c\xf3mo AlphaGo Zero super\xf3 todas las versiones anteriores sin usar partidas humanas","source":"@site/i18n/es/docusaurus-plugin-content-docs/current/alphago/explained/16-alphago-zero.mdx","sourceDirName":"alphago/explained","slug":"/alphago/explained/alphago-zero","permalink":"/es/docs/alphago/explained/alphago-zero","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/explained/16-alphago-zero.mdx","tags":[],"version":"current","sidebarPosition":17,"frontMatter":{"sidebar_position":17,"title":"Visi\xf3n General de AlphaGo Zero","description":"Desde cero, completamente autodidacta, c\xf3mo AlphaGo Zero super\xf3 todas las versiones anteriores sin usar partidas humanas","keywords":["AlphaGo Zero","auto-juego","aprendizaje por refuerzo","deep learning","Go AI","aprendizaje no supervisado"]},"sidebar":"tutorialSidebar","previous":{"title":"Explicaci\xf3n Detallada de la F\xf3rmula PUCT","permalink":"/es/docs/alphago/explained/puct-formula"},"next":{"title":"Red de Doble Cabeza y Redes Residuales","permalink":"/es/docs/alphago/explained/dual-head-resnet"}}');var s=n(62615),r=n(30416);const d={sidebar_position:17,title:"Visi\xf3n General de AlphaGo Zero",description:"Desde cero, completamente autodidacta, c\xf3mo AlphaGo Zero super\xf3 todas las versiones anteriores sin usar partidas humanas",keywords:["AlphaGo Zero","auto-juego","aprendizaje por refuerzo","deep learning","Go AI","aprendizaje no supervisado"]},o="Visi\xf3n General de AlphaGo Zero",l={},c=[{value:"\xbfPor Qu\xe9 No Se Necesitan Partidas Humanas?",id:"por-qu\xe9-no-se-necesitan-partidas-humanas",level:2},{value:"Limitaciones de las Partidas Humanas",id:"limitaciones-de-las-partidas-humanas",level:3},{value:"1. Las Partidas Humanas Tienen un L\xedmite Superior",id:"1-las-partidas-humanas-tienen-un-l\xedmite-superior",level:4},{value:"2. El Cuello de Botella del Aprendizaje Supervisado",id:"2-el-cuello-de-botella-del-aprendizaje-supervisado",level:4},{value:"3. Costo de Recolecci\xf3n de Datos",id:"3-costo-de-recolecci\xf3n-de-datos",level:4},{value:"El Avance de Zero",id:"el-avance-de-zero",level:3},{value:"Comparaci\xf3n con AlphaGo Original: 100:0",id:"comparaci\xf3n-con-alphago-original-1000",level:2},{value:"Victoria Aplastante",id:"victoria-aplastante",level:3},{value:"Menos Recursos, Mayor Fuerza",id:"menos-recursos-mayor-fuerza",level:3},{value:"\xbfPor Qu\xe9 Zero Es M\xe1s Fuerte?",id:"por-qu\xe9-zero-es-m\xe1s-fuerte",level:3},{value:"1. Aprendizaje Sin Sesgos",id:"1-aprendizaje-sin-sesgos",level:4},{value:"2. Objetivo de Aprendizaje Consistente",id:"2-objetivo-de-aprendizaje-consistente",level:4},{value:"3. Arquitectura M\xe1s Simple",id:"3-arquitectura-m\xe1s-simple",level:4},{value:"Caracter\xedsticas de Entrada Simplificadas: De 48 a 17",id:"caracter\xedsticas-de-entrada-simplificadas-de-48-a-17",level:2},{value:"Los 48 Planos de Caracter\xedsticas del AlphaGo Original",id:"los-48-planos-de-caracter\xedsticas-del-alphago-original",level:3},{value:"Los 17 Planos de Caracter\xedsticas de AlphaGo Zero",id:"los-17-planos-de-caracter\xedsticas-de-alphago-zero",level:3},{value:"\xbfPor Qu\xe9 la Simplificaci\xf3n Es Buena?",id:"por-qu\xe9-la-simplificaci\xf3n-es-buena",level:3},{value:"1. Dejar que la Red Descubra Caracter\xedsticas",id:"1-dejar-que-la-red-descubra-caracter\xedsticas",level:4},{value:"2. Mejor Generalizaci\xf3n",id:"2-mejor-generalizaci\xf3n",level:4},{value:"3. Reducir Errores Humanos",id:"3-reducir-errores-humanos",level:4},{value:"Arquitectura de Red \xdanica",id:"arquitectura-de-red-\xfanica",level:2},{value:"Dise\xf1o de Doble Red Original",id:"dise\xf1o-de-doble-red-original",level:3},{value:"Red de Doble Cabeza de Zero",id:"red-de-doble-cabeza-de-zero",level:3},{value:"1. Eficiencia de Par\xe1metros",id:"1-eficiencia-de-par\xe1metros",level:4},{value:"2. Compartici\xf3n de Caracter\xedsticas",id:"2-compartici\xf3n-de-caracter\xedsticas",level:4},{value:"3. Estabilidad de Entrenamiento",id:"3-estabilidad-de-entrenamiento",level:4},{value:"El Poder de las Redes Residuales",id:"el-poder-de-las-redes-residuales",level:3},{value:"Mejora en la Eficiencia del Entrenamiento",id:"mejora-en-la-eficiencia-del-entrenamiento",level:2},{value:"Crecimiento Exponencial del Auto-juego",id:"crecimiento-exponencial-del-auto-juego",level:3},{value:"\xbfPor Qu\xe9 Tan R\xe1pido?",id:"por-qu\xe9-tan-r\xe1pido",level:3},{value:"1. Gu\xeda de B\xfasqueda M\xe1s Fuerte",id:"1-gu\xeda-de-b\xfasqueda-m\xe1s-fuerte",level:4},{value:"2. Auto-juego M\xe1s R\xe1pido",id:"2-auto-juego-m\xe1s-r\xe1pido",level:4},{value:"3. Aprendizaje M\xe1s Efectivo",id:"3-aprendizaje-m\xe1s-efectivo",level:4},{value:"Comparaci\xf3n con Aprendizaje Humano",id:"comparaci\xf3n-con-aprendizaje-humano",level:3},{value:"Generalidad: Ajedrez, Shogi",id:"generalidad-ajedrez-shogi",level:2},{value:"El Nacimiento de AlphaZero",id:"el-nacimiento-de-alphazero",level:3},{value:"El Significado de la Generalidad",id:"el-significado-de-la-generalidad",level:3},{value:"Impacto en la IA Tradicional",id:"impacto-en-la-ia-tradicional",level:3},{value:"El Estilo de Juego de AlphaGo Zero",id:"el-estilo-de-juego-de-alphago-zero",level:2},{value:"Superando la Est\xe9tica Humana",id:"superando-la-est\xe9tica-humana",level:3},{value:"Redescubriendo la Teor\xeda del Go Humana",id:"redescubriendo-la-teor\xeda-del-go-humana",level:3},{value:"Innovaciones Que Superan a Humanos",id:"innovaciones-que-superan-a-humanos",level:3},{value:"Resumen de Detalles T\xe9cnicos",id:"resumen-de-detalles-t\xe9cnicos",level:2},{value:"Comparaci\xf3n Completa con AlphaGo Original",id:"comparaci\xf3n-completa-con-alphago-original",level:3},{value:"Algoritmo Central",id:"algoritmo-central",level:3},{value:"Implicaciones para la Investigaci\xf3n en IA",id:"implicaciones-para-la-investigaci\xf3n-en-ia",level:2},{value:"Aprendizaje desde Primeros Principios",id:"aprendizaje-desde-primeros-principios",level:3},{value:"El Poder del Auto-juego",id:"el-poder-del-auto-juego",level:3},{value:"La Importancia de la Simplificaci\xf3n",id:"la-importancia-de-la-simplificaci\xf3n",level:3},{value:"Correspondencia con Animaciones",id:"correspondencia-con-animaciones",level:2},{value:"Lecturas Adicionales",id:"lecturas-adicionales",level:2},{value:"Referencias",id:"referencias",level:2}];function t(e){const a={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.header,{children:(0,s.jsx)(a.h1,{id:"visi\xf3n-general-de-alphago-zero",children:"Visi\xf3n General de AlphaGo Zero"})}),"\n",(0,s.jsxs)(a.p,{children:["En octubre de 2017, DeepMind public\xf3 un resultado que conmocion\xf3 al mundo de la IA: ",(0,s.jsx)(a.strong,{children:"AlphaGo Zero"}),", entrenado desde un estado completamente aleatorio sin usar ninguna partida humana, super\xf3 al AlphaGo original que derrot\xf3 a Lee Sedol en solo tres d\xedas, ganando por un marcador de ",(0,s.jsx)(a.strong,{children:"100:0"}),"."]}),"\n",(0,s.jsxs)(a.p,{children:["Esto no es solo un avance num\xe9rico. Representa un paradigma completamente nuevo: ",(0,s.jsx)(a.strong,{children:"la IA no necesita conocimiento humano, puede descubrir todo desde cero"}),"."]}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"por-qu\xe9-no-se-necesitan-partidas-humanas",children:"\xbfPor Qu\xe9 No Se Necesitan Partidas Humanas?"}),"\n",(0,s.jsx)(a.h3,{id:"limitaciones-de-las-partidas-humanas",children:"Limitaciones de las Partidas Humanas"}),"\n",(0,s.jsx)(a.p,{children:"El proceso de entrenamiento del AlphaGo original se dividi\xf3 en dos etapas:"}),"\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Aprendizaje supervisado"}),": Entrenar la Policy Network con 30 millones de partidas humanas"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Aprendizaje por refuerzo"}),": Mejorar a\xfan m\xe1s a trav\xe9s del auto-juego"]}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"Este m\xe9todo tiene varios problemas fundamentales:"}),"\n",(0,s.jsx)(a.h4,{id:"1-las-partidas-humanas-tienen-un-l\xedmite-superior",children:"1. Las Partidas Humanas Tienen un L\xedmite Superior"}),"\n",(0,s.jsx)(a.p,{children:"La fuerza de los jugadores humanos tiene l\xedmites, y las partidas contienen la comprensi\xf3n humana, incluyendo errores y sesgos humanos. Cuando la IA aprende de partidas humanas, aprende:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Lo que los humanos creen que son buenos movimientos (pero no necesariamente \xf3ptimos)"}),"\n",(0,s.jsx)(a.li,{children:"Patrones de pensamiento humano (pero pueden limitar la innovaci\xf3n)"}),"\n",(0,s.jsx)(a.li,{children:"Errores humanos (que se aprenden como muestras correctas)"}),"\n"]}),"\n",(0,s.jsx)(a.h4,{id:"2-el-cuello-de-botella-del-aprendizaje-supervisado",children:"2. El Cuello de Botella del Aprendizaje Supervisado"}),"\n",(0,s.jsx)(a.p,{children:'El objetivo del aprendizaje supervisado es "imitar a humanos" -- predecir qu\xe9 movimiento jugar\xe1 un jugador humano. Esto significa que el l\xedmite superior de la capacidad de la IA est\xe1 limitado por la capacidad de los jugadores humanos.'}),"\n",(0,s.jsx)(a.p,{children:"Es como un aprendiz que solo puede imitar al maestro, nunca puede superar al maestro."}),"\n",(0,s.jsx)(a.h4,{id:"3-costo-de-recolecci\xf3n-de-datos",children:"3. Costo de Recolecci\xf3n de Datos"}),"\n",(0,s.jsx)(a.p,{children:'Las partidas humanas de alta calidad necesitan a\xf1os para acumularse, y solo existen para juegos con larga historia como Go. Si quisieras aplicar IA a nuevos dominios (como predicci\xf3n de estructuras de prote\xednas), simplemente no hay "partidas de expertos humanos" disponibles.'}),"\n",(0,s.jsx)(a.h3,{id:"el-avance-de-zero",children:"El Avance de Zero"}),"\n",(0,s.jsxs)(a.p,{children:["AlphaGo Zero omiti\xf3 completamente la etapa de aprendizaje supervisado, comenzando directamente desde ",(0,s.jsx)(a.strong,{children:"inicializaci\xf3n aleatoria"})," con auto-juego. Esto resolvi\xf3 todos los problemas mencionados:"]}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"Problema"}),(0,s.jsx)(a.th,{children:"AlphaGo Original"}),(0,s.jsx)(a.th,{children:"AlphaGo Zero"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"L\xedmite del conocimiento humano"}),(0,s.jsx)(a.td,{children:"Limitado por calidad de partidas"}),(0,s.jsx)(a.td,{children:"Sin esta limitaci\xf3n"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Objetivo de aprendizaje"}),(0,s.jsx)(a.td,{children:"Imitar humanos"}),(0,s.jsx)(a.td,{children:"Maximizar tasa de victoria"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Requisitos de datos"}),(0,s.jsx)(a.td,{children:"30 millones de partidas"}),(0,s.jsx)(a.td,{children:"0"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Generalizaci\xf3n"}),(0,s.jsx)(a.td,{children:"Solo Go"}),(0,s.jsx)(a.td,{children:"Generalizable a otros dominios"})]})]})]}),"\n",(0,s.jsx)(a.p,{children:'Este es un cambio de paradigma fundamental: de "aprender conocimiento humano" a "descubrir conocimiento desde primeros principios".'}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"comparaci\xf3n-con-alphago-original-1000",children:"Comparaci\xf3n con AlphaGo Original: 100:0"}),"\n",(0,s.jsx)(a.h3,{id:"victoria-aplastante",children:"Victoria Aplastante"}),"\n",(0,s.jsx)(a.p,{children:"DeepMind hizo que AlphaGo Zero entrenado jugara contra varias versiones de AlphaGo:"}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"Oponente"}),(0,s.jsx)(a.th,{children:"R\xe9cord de AlphaGo Zero"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"AlphaGo Fan (versi\xf3n que derrot\xf3 a Fan Hui)"}),(0,s.jsx)(a.td,{children:"100:0"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"AlphaGo Lee (versi\xf3n que derrot\xf3 a Lee Sedol)"}),(0,s.jsx)(a.td,{children:"100:0"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"AlphaGo Master (versi\xf3n 60 victorias consecutivas)"}),(0,s.jsx)(a.td,{children:"89:11"})]})]})]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"100:0"})," -- esto significa que en 100 partidas, el AlphaGo original no pudo ganar ni una sola."]}),"\n",(0,s.jsx)(a.h3,{id:"menos-recursos-mayor-fuerza",children:"Menos Recursos, Mayor Fuerza"}),"\n",(0,s.jsx)(a.p,{children:"No solo gan\xf3, AlphaGo Zero logr\xf3 mayor fuerza con menos recursos:"}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"M\xe9trica"}),(0,s.jsx)(a.th,{children:"AlphaGo Lee"}),(0,s.jsx)(a.th,{children:"AlphaGo Zero"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Tiempo de entrenamiento"}),(0,s.jsx)(a.td,{children:"Varios meses"}),(0,s.jsx)(a.td,{children:"40 d\xedas (3 d\xedas para superar a AlphaGo Lee)"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Partidas de entrenamiento"}),(0,s.jsx)(a.td,{children:"30 millones humanas + auto-juego"}),(0,s.jsx)(a.td,{children:"4.9 millones de auto-juego"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"TPUs (entrenamiento)"}),(0,s.jsx)(a.td,{children:"50+"}),(0,s.jsx)(a.td,{children:"4"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"TPUs (inferencia)"}),(0,s.jsx)(a.td,{children:"48"}),(0,s.jsx)(a.td,{children:"4"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Caracter\xedsticas de entrada"}),(0,s.jsx)(a.td,{children:"48 planos"}),(0,s.jsx)(a.td,{children:"17 planos"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Red neuronal"}),(0,s.jsx)(a.td,{children:"Redes SL + RL separadas"}),(0,s.jsx)(a.td,{children:"Red \xfanica de doble cabeza"})]})]})]}),"\n",(0,s.jsxs)(a.p,{children:["Esta es una mejora de eficiencia asombrosa: ",(0,s.jsx)(a.strong,{children:"m\xe1s de 10 veces menos recursos, pero fuerza significativamente mayor"}),"."]}),"\n",(0,s.jsx)(a.h3,{id:"por-qu\xe9-zero-es-m\xe1s-fuerte",children:"\xbfPor Qu\xe9 Zero Es M\xe1s Fuerte?"}),"\n",(0,s.jsx)(a.p,{children:"Las razones por las que AlphaGo Zero es m\xe1s fuerte se pueden entender desde varios \xe1ngulos:"}),"\n",(0,s.jsx)(a.h4,{id:"1-aprendizaje-sin-sesgos",children:"1. Aprendizaje Sin Sesgos"}),"\n",(0,s.jsx)(a.p,{children:"El AlphaGo original aprendi\xf3 de partidas humanas, heredando sesgos humanos. Por ejemplo, los jugadores humanos pueden sobrevalorar ciertas joseki, o tener evaluaciones incorrectas de ciertas posiciones."}),"\n",(0,s.jsx)(a.p,{children:"AlphaGo Zero no tiene esta carga. Comenz\xf3 desde una hoja en blanco, aprendiendo solo a trav\xe9s de resultados de victoria/derrota qu\xe9 es un buen movimiento. Esto le permiti\xf3 descubrir movimientos que los humanos nunca hab\xedan pensado."}),"\n",(0,s.jsx)(a.h4,{id:"2-objetivo-de-aprendizaje-consistente",children:"2. Objetivo de Aprendizaje Consistente"}),"\n",(0,s.jsx)(a.p,{children:"El entrenamiento del AlphaGo original ten\xeda dos objetivos diferentes:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Aprendizaje supervisado: Maximizar precisi\xf3n de predicci\xf3n de movimientos humanos"}),"\n",(0,s.jsx)(a.li,{children:"Aprendizaje por refuerzo: Maximizar tasa de victoria"}),"\n"]}),"\n",(0,s.jsxs)(a.p,{children:["Estos dos objetivos pueden entrar en conflicto. AlphaGo Zero tiene solo un objetivo: ",(0,s.jsx)(a.strong,{children:"maximizaci\xf3n de tasa de victoria"}),". Esto hace que el proceso de aprendizaje sea m\xe1s consistente y efectivo."]}),"\n",(0,s.jsx)(a.h4,{id:"3-arquitectura-m\xe1s-simple",children:"3. Arquitectura M\xe1s Simple"}),"\n",(0,s.jsx)(a.p,{children:"El AlphaGo original usaba Policy Network y Value Network separadas. AlphaGo Zero usa una red \xfanica de doble cabeza (ver siguiente art\xedculo), permitiendo compartir representaciones de caracter\xedsticas, mejorando la eficiencia del aprendizaje."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"caracter\xedsticas-de-entrada-simplificadas-de-48-a-17",children:"Caracter\xedsticas de Entrada Simplificadas: De 48 a 17"}),"\n",(0,s.jsx)(a.h3,{id:"los-48-planos-de-caracter\xedsticas-del-alphago-original",children:"Los 48 Planos de Caracter\xedsticas del AlphaGo Original"}),"\n",(0,s.jsx)(a.p,{children:"La entrada de la red neuronal del AlphaGo original inclu\xeda 48 planos de 19x19, codificando muchas caracter\xedsticas dise\xf1adas por humanos:"}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"Categor\xeda"}),(0,s.jsx)(a.th,{children:"N\xfamero de caracter\xedsticas"}),(0,s.jsx)(a.th,{children:"Contenido"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Posici\xf3n de piedras"}),(0,s.jsx)(a.td,{children:"3"}),(0,s.jsx)(a.td,{children:"Negras, blancas, vac\xedas"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Libertades"}),(0,s.jsx)(a.td,{children:"8"}),(0,s.jsx)(a.td,{children:"Grupos con 1-8 libertades"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Capturas"}),(0,s.jsx)(a.td,{children:"8"}),(0,s.jsx)(a.td,{children:"Puede capturar 1-8 piedras"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Ko"}),(0,s.jsx)(a.td,{children:"1"}),(0,s.jsx)(a.td,{children:"Posici\xf3n de ko"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Distancia al borde"}),(0,s.jsx)(a.td,{children:"4"}),(0,s.jsx)(a.td,{children:"Primera a cuarta l\xednea"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Legalidad de jugada"}),(0,s.jsx)(a.td,{children:"1"}),(0,s.jsx)(a.td,{children:"Qu\xe9 posiciones pueden jugarse"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Estado hist\xf3rico"}),(0,s.jsx)(a.td,{children:"8"}),(0,s.jsx)(a.td,{children:"Posiciones de \xfaltimos 8 movimientos"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Turno"}),(0,s.jsx)(a.td,{children:"1"}),(0,s.jsx)(a.td,{children:"Negro o blanco"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Otros"}),(0,s.jsx)(a.td,{children:"14"}),(0,s.jsx)(a.td,{children:"Escalera, ojos, etc."})]})]})]}),"\n",(0,s.jsx)(a.p,{children:"Estas 48 caracter\xedsticas fueron cuidadosamente dise\xf1adas por expertos en Go, conteniendo mucho conocimiento del dominio."}),"\n",(0,s.jsx)(a.h3,{id:"los-17-planos-de-caracter\xedsticas-de-alphago-zero",children:"Los 17 Planos de Caracter\xedsticas de AlphaGo Zero"}),"\n",(0,s.jsx)(a.p,{children:"AlphaGo Zero simplific\xf3 dram\xe1ticamente la entrada, usando solo 17 planos de caracter\xedsticas:"}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"N\xfamero de plano"}),(0,s.jsx)(a.th,{children:"Contenido"}),(0,s.jsx)(a.th,{children:"Cantidad"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"1-8"}),(0,s.jsx)(a.td,{children:"Posici\xf3n de negras (\xfaltimos 8 movimientos)"}),(0,s.jsx)(a.td,{children:"8"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"9-16"}),(0,s.jsx)(a.td,{children:"Posici\xf3n de blancas (\xfaltimos 8 movimientos)"}),(0,s.jsx)(a.td,{children:"8"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"17"}),(0,s.jsx)(a.td,{children:"Turno actual (todo 1 o todo 0)"}),(0,s.jsx)(a.td,{children:"1"})]})]})]}),"\n",(0,s.jsx)(a.p,{children:"Estos 17 planos solo incluyen:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Estado actual del tablero"}),": Cada posici\xf3n tiene piedra negra, blanca o vac\xeda"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Informaci\xf3n hist\xf3rica"}),": Estados del tablero de los \xfaltimos 8 movimientos"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Informaci\xf3n de turno"}),": Qui\xe9n juega"]}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:'Sin libertades, sin juicio de escalera, sin distancia al borde -- todo este "conocimiento de Go" lo aprende la red neuronal por s\xed misma.'}),"\n",(0,s.jsx)(a.h3,{id:"por-qu\xe9-la-simplificaci\xf3n-es-buena",children:"\xbfPor Qu\xe9 la Simplificaci\xf3n Es Buena?"}),"\n",(0,s.jsx)(a.h4,{id:"1-dejar-que-la-red-descubra-caracter\xedsticas",children:"1. Dejar que la Red Descubra Caracter\xedsticas"}),"\n",(0,s.jsx)(a.p,{children:"Caracter\xedsticas manuales complejas pueden perder informaci\xf3n importante, o codificar suposiciones err\xf3neas. Dejar que la red neuronal aprenda de datos crudos puede descubrir mejores representaciones de caracter\xedsticas."}),"\n",(0,s.jsx)(a.p,{children:"De hecho, AlphaGo Zero aprendi\xf3 todas las caracter\xedsticas dise\xf1adas por humanos (libertades, escalera, etc.), y tambi\xe9n aprendi\xf3 algunos patrones que los humanos no hab\xedan identificado conscientemente."}),"\n",(0,s.jsx)(a.h4,{id:"2-mejor-generalizaci\xf3n",children:"2. Mejor Generalizaci\xf3n"}),"\n",(0,s.jsx)(a.p,{children:"Muchas de las 48 caracter\xedsticas eran espec\xedficas de Go (como escalera, distancia al borde). Los 17 planos simplificados son universales -- cualquier juego de tablero puede codificarse de manera similar."}),"\n",(0,s.jsxs)(a.p,{children:["Esto sent\xf3 las bases para el posterior ",(0,s.jsx)(a.strong,{children:"AlphaZero"})," (IA de juegos general)."]}),"\n",(0,s.jsx)(a.h4,{id:"3-reducir-errores-humanos",children:"3. Reducir Errores Humanos"}),"\n",(0,s.jsx)(a.p,{children:"Las caracter\xedsticas dise\xf1adas manualmente pueden contener definiciones err\xf3neas o incompletas. La entrada simplificada elimina la posibilidad de tales problemas."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"arquitectura-de-red-\xfanica",children:"Arquitectura de Red \xdanica"}),"\n",(0,s.jsx)(a.h3,{id:"dise\xf1o-de-doble-red-original",children:"Dise\xf1o de Doble Red Original"}),"\n",(0,s.jsx)(a.p,{children:"El AlphaGo original usaba dos redes neuronales independientes:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{children:"Policy Network:  Entrada \u2192 CNN \u2192 Probabilidades de jugada 19x19\nValue Network:   Entrada \u2192 CNN \u2192 Evaluaci\xf3n de tasa de victoria (-1 a 1)\n"})}),"\n",(0,s.jsx)(a.p,{children:"Estas dos redes:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Ten\xedan arquitecturas diferentes (n\xfamero de capas, canales ligeramente diferentes)"}),"\n",(0,s.jsx)(a.li,{children:"Se entrenaban independientemente (primero Policy, luego Value)"}),"\n",(0,s.jsx)(a.li,{children:"No compart\xedan ning\xfan par\xe1metro"}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"red-de-doble-cabeza-de-zero",children:"Red de Doble Cabeza de Zero"}),"\n",(0,s.jsx)(a.p,{children:"AlphaGo Zero usa una red \xfanica, pero con dos cabezas de salida (heads):"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{children:"Entrada \u2192 ResNet Backbone Compartido \u2192 Policy Head \u2192 Probabilidades de jugada 19x19\n                                    \u2192 Value Head  \u2192 Evaluaci\xf3n de tasa de victoria\n"})}),"\n",(0,s.jsxs)(a.p,{children:["Las dos Heads comparten el mismo backbone ResNet (ver ",(0,s.jsx)(a.a,{href:"../dual-head-resnet",children:"siguiente art\xedculo: Red de Doble Cabeza y Redes Residuales"}),"), lo que trae varios beneficios:"]}),"\n",(0,s.jsx)(a.h4,{id:"1-eficiencia-de-par\xe1metros",children:"1. Eficiencia de Par\xe1metros"}),"\n",(0,s.jsx)(a.p,{children:"Compartir el backbone significa que la mayor\xeda de par\xe1metros son usados por ambas tareas. Esto reduce la cantidad total de par\xe1metros, disminuyendo el riesgo de sobreajuste."}),"\n",(0,s.jsx)(a.h4,{id:"2-compartici\xf3n-de-caracter\xedsticas",children:"2. Compartici\xf3n de Caracter\xedsticas"}),"\n",(0,s.jsx)(a.p,{children:'"D\xf3nde deber\xeda jugar" (Policy) y "Qui\xe9n ganar\xe1" (Value) necesitan entender patrones de tablero similares. El backbone compartido permite que estas caracter\xedsticas sean aprendidas y utilizadas simult\xe1neamente por ambas tareas.'}),"\n",(0,s.jsx)(a.h4,{id:"3-estabilidad-de-entrenamiento",children:"3. Estabilidad de Entrenamiento"}),"\n",(0,s.jsx)(a.p,{children:"El entrenamiento conjunto hace que las se\xf1ales de gradiente vengan de dos fuentes, proporcionando se\xf1ales de supervisi\xf3n m\xe1s ricas, haciendo el entrenamiento m\xe1s estable."}),"\n",(0,s.jsx)(a.h3,{id:"el-poder-de-las-redes-residuales",children:"El Poder de las Redes Residuales"}),"\n",(0,s.jsxs)(a.p,{children:["El backbone de AlphaGo Zero usa una ",(0,s.jsx)(a.strong,{children:"Red Residual de 40 capas (ResNet)"}),", mucho m\xe1s profunda que la CNN de 13 capas del AlphaGo original."]}),"\n",(0,s.jsx)(a.p,{children:"Las conexiones residuales (skip connections) permiten entrenar efectivamente redes profundas, evitando el problema de desvanecimiento de gradientes. Esta fue la tecnolog\xeda revolucionaria de la competencia ImageNet 2015, aplicada exitosamente por AlphaGo Zero al dominio del Go."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"mejora-en-la-eficiencia-del-entrenamiento",children:"Mejora en la Eficiencia del Entrenamiento"}),"\n",(0,s.jsx)(a.h3,{id:"crecimiento-exponencial-del-auto-juego",children:"Crecimiento Exponencial del Auto-juego"}),"\n",(0,s.jsx)(a.p,{children:"El proceso de entrenamiento de AlphaGo Zero mostr\xf3 una eficiencia asombrosa:"}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"Tiempo de entrenamiento"}),(0,s.jsx)(a.th,{children:"Puntuaci\xf3n ELO"}),(0,s.jsx)(a.th,{children:"Equivalente a"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"0 horas"}),(0,s.jsx)(a.td,{children:"0"}),(0,s.jsx)(a.td,{children:"Jugadas aleatorias"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"3 horas"}),(0,s.jsx)(a.td,{children:"~1000"}),(0,s.jsx)(a.td,{children:"Descubriendo reglas b\xe1sicas"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"12 horas"}),(0,s.jsx)(a.td,{children:"~3000"}),(0,s.jsx)(a.td,{children:"Descubriendo joseki"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"36 horas"}),(0,s.jsx)(a.td,{children:"~4500"}),(0,s.jsx)(a.td,{children:"Superando versi\xf3n Fan Hui"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"60 horas"}),(0,s.jsx)(a.td,{children:"~5200"}),(0,s.jsx)(a.td,{children:"Superando versi\xf3n Lee Sedol"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"72 horas"}),(0,s.jsx)(a.td,{children:"~5400"}),(0,s.jsx)(a.td,{children:"Superando AlphaGo original"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"40 d\xedas"}),(0,s.jsx)(a.td,{children:"~5600"}),(0,s.jsx)(a.td,{children:"Versi\xf3n m\xe1s fuerte"})]})]})]}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Tres d\xedas para superar humanos, tres d\xedas para superar IA que tom\xf3 meses entrenar"})," -- esta es una mejora de eficiencia exponencial."]}),"\n",(0,s.jsx)(a.h3,{id:"por-qu\xe9-tan-r\xe1pido",children:"\xbfPor Qu\xe9 Tan R\xe1pido?"}),"\n",(0,s.jsx)(a.h4,{id:"1-gu\xeda-de-b\xfasqueda-m\xe1s-fuerte",children:"1. Gu\xeda de B\xfasqueda M\xe1s Fuerte"}),"\n",(0,s.jsx)(a.p,{children:"El MCTS de AlphaGo Zero est\xe1 completamente guiado por la red neuronal, sin usar m\xe1s la pol\xedtica de rollout r\xe1pido. Esto hace la b\xfasqueda m\xe1s eficiente y precisa."}),"\n",(0,s.jsx)(a.h4,{id:"2-auto-juego-m\xe1s-r\xe1pido",children:"2. Auto-juego M\xe1s R\xe1pido"}),"\n",(0,s.jsx)(a.p,{children:"Ya que solo se necesita una red (en lugar de dos), el costo computacional de cada partida de auto-juego se reduce. Esto significa que se pueden generar m\xe1s datos de entrenamiento en el mismo tiempo."}),"\n",(0,s.jsx)(a.h4,{id:"3-aprendizaje-m\xe1s-efectivo",children:"3. Aprendizaje M\xe1s Efectivo"}),"\n",(0,s.jsx)(a.p,{children:"El entrenamiento conjunto de la red de doble cabeza hace que la informaci\xf3n de cada partida sea utilizada m\xe1s efectivamente. Los gradientes de Policy y Value se refuerzan mutuamente, acelerando la convergencia."}),"\n",(0,s.jsx)(a.h3,{id:"comparaci\xf3n-con-aprendizaje-humano",children:"Comparaci\xf3n con Aprendizaje Humano"}),"\n",(0,s.jsx)(a.p,{children:"\xbfCu\xe1nto tiempo necesitan los jugadores humanos para alcanzar diferentes niveles?"}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"Nivel"}),(0,s.jsx)(a.th,{children:"Tiempo requerido por humanos"}),(0,s.jsx)(a.th,{children:"AlphaGo Zero"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Principiante"}),(0,s.jsx)(a.td,{children:"Semanas"}),(0,s.jsx)(a.td,{children:"Minutos"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"1 dan amateur"}),(0,s.jsx)(a.td,{children:"A\xf1os"}),(0,s.jsx)(a.td,{children:"Horas"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Nivel profesional"}),(0,s.jsx)(a.td,{children:"10-20 a\xf1os"}),(0,s.jsx)(a.td,{children:"1-2 d\xedas"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Campe\xf3n mundial"}),(0,s.jsx)(a.td,{children:"20+ a\xf1os de dedicaci\xf3n a tiempo completo"}),(0,s.jsx)(a.td,{children:"3 d\xedas"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Superar humanos"}),(0,s.jsx)(a.td,{children:"Imposible"}),(0,s.jsx)(a.td,{children:"3 d\xedas"})]})]})]}),"\n",(0,s.jsx)(a.p,{children:"Esta comparaci\xf3n no pretende menospreciar a los jugadores humanos -- ellos usan neuronas biol\xf3gicas, mientras AlphaGo Zero usa TPUs especialmente dise\xf1ados y miles de vatios de electricidad. Pero s\xed demuestra cu\xe1n eficiente puede ser el m\xe9todo de aprendizaje correcto."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"generalidad-ajedrez-shogi",children:"Generalidad: Ajedrez, Shogi"}),"\n",(0,s.jsx)(a.h3,{id:"el-nacimiento-de-alphazero",children:"El Nacimiento de AlphaZero"}),"\n",(0,s.jsxs)(a.p,{children:["En diciembre de 2017, DeepMind public\xf3 ",(0,s.jsx)(a.strong,{children:"AlphaZero"})," -- la versi\xf3n general de AlphaGo Zero. El mismo algoritmo, solo cambiando las reglas del juego, alcanz\xf3 nivel mundial en tres juegos de tablero:"]}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"Juego"}),(0,s.jsx)(a.th,{children:"Tiempo de entrenamiento"}),(0,s.jsx)(a.th,{children:"Oponente"}),(0,s.jsx)(a.th,{children:"R\xe9cord"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Go"}),(0,s.jsx)(a.td,{children:"8 horas"}),(0,s.jsx)(a.td,{children:"AlphaGo Zero"}),(0,s.jsx)(a.td,{children:"60:40"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Ajedrez"}),(0,s.jsx)(a.td,{children:"4 horas"}),(0,s.jsx)(a.td,{children:"Stockfish 8"}),(0,s.jsx)(a.td,{children:"28 victorias 72 empates 0 derrotas"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"Shogi"}),(0,s.jsx)(a.td,{children:"2 horas"}),(0,s.jsx)(a.td,{children:"Elmo"}),(0,s.jsx)(a.td,{children:"90:8:2"})]})]})]}),"\n",(0,s.jsx)(a.p,{children:"Nota los oponentes aqu\xed:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Stockfish"})," era el motor de ajedrez m\xe1s fuerte entonces, usando d\xe9cadas de conocimiento humano y optimizaci\xf3n"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Elmo"})," era la IA de Shogi m\xe1s fuerte entonces"]}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"AlphaZero con unas pocas horas de entrenamiento super\xf3 estos sistemas especializados desarrollados durante a\xf1os."}),"\n",(0,s.jsx)(a.h3,{id:"el-significado-de-la-generalidad",children:"El Significado de la Generalidad"}),"\n",(0,s.jsx)(a.p,{children:"AlphaGo Zero / AlphaZero prob\xf3 algo importante:"}),"\n",(0,s.jsxs)(a.blockquote,{children:["\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.strong,{children:"El mismo algoritmo de aprendizaje puede alcanzar nivel sobrehumano en diferentes dominios."})}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"Estos no son tres IAs diferentes, sino un marco de aprendizaje general:"}),"\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Auto-juego"})," genera experiencia"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"B\xfasqueda de \xc1rbol Monte Carlo"})," explora posibilidades"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Redes neuronales"})," aprenden funciones de pol\xedtica y valor"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Aprendizaje por refuerzo"})," optimiza la funci\xf3n objetivo"]}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"Este marco no depende de conocimiento espec\xedfico del dominio, dando un paso importante hacia la generalizaci\xf3n de la IA."}),"\n",(0,s.jsx)(a.h3,{id:"impacto-en-la-ia-tradicional",children:"Impacto en la IA Tradicional"}),"\n",(0,s.jsx)(a.p,{children:'Antes de AlphaZero, las IAs m\xe1s fuertes de ajedrez y shogi eran estilo "sistema experto":'}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Mucho conocimiento humano"}),": Libros de apertura, tablas de finales, funciones de evaluaci\xf3n"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"D\xe9cadas de optimizaci\xf3n"}),": Esfuerzo de incontables jugadores e ingenieros"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Extremadamente especializadas"}),": Stockfish no puede jugar Go, Elmo no puede jugar ajedrez"]}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"AlphaZero super\xf3 todo esto con un algoritmo general en unas pocas horas. Esto hizo que muchos investigadores de IA reconsideraran:"}),"\n",(0,s.jsxs)(a.blockquote,{children:["\n",(0,s.jsx)(a.p,{children:'\xbfDeber\xedamos invertir m\xe1s esfuerzo en "algoritmos de aprendizaje general" o en "codificaci\xf3n de conocimiento experto"?'}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"La respuesta parece cada vez m\xe1s clara: dejar que la m\xe1quina aprenda por s\xed misma es m\xe1s efectivo que ense\xf1arle conocimiento."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"el-estilo-de-juego-de-alphago-zero",children:"El Estilo de Juego de AlphaGo Zero"}),"\n",(0,s.jsx)(a.h3,{id:"superando-la-est\xe9tica-humana",children:"Superando la Est\xe9tica Humana"}),"\n",(0,s.jsxs)(a.p,{children:["La comunidad del Go tiene una evaluaci\xf3n universal del estilo de juego de AlphaGo Zero: ",(0,s.jsx)(a.strong,{children:"m\xe1s elegante"}),"."]}),"\n",(0,s.jsx)(a.p,{children:'Los movimientos de AlphaGo Lee a veces parec\xedan "extra\xf1os" -- como el movimiento 37, los humanos necesitaron an\xe1lisis posterior para entender su brillantez. Pero los movimientos de AlphaGo Zero a menudo se evaluaban despu\xe9s como "obviamente buenos a primera vista".'}),"\n",(0,s.jsx)(a.p,{children:"Esto puede ser porque:"}),"\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Mayor fuerza de juego"}),": Zero puede ver m\xe1s profundo, jugando m\xe1s compuesto"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Sin sesgos humanos"}),": No restringido por joseki tradicional"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Objetivo consistente"}),": Solo persigue tasa de victoria, no imita humanos"]}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"redescubriendo-la-teor\xeda-del-go-humana",children:"Redescubriendo la Teor\xeda del Go Humana"}),"\n",(0,s.jsx)(a.p,{children:'Interesantemente, AlphaGo Zero "redescubri\xf3" durante el entrenamiento el conocimiento del Go acumulado por humanos durante miles de a\xf1os:'}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Joseki"}),": Zero descubri\xf3 muchas joseki comunes, porque estas son realmente las soluciones \xf3ptimas para ambos lados"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Principios de apertura"}),": Importancia de esquinas, lados, centro en ese orden"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Conocimiento de forma"}),": Diferencia entre mala forma y buena forma"]}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"Esto valid\xf3 la racionalidad de la teor\xeda del Go humana -- este conocimiento no es coincidencia, sino un reflejo de la naturaleza del Go."}),"\n",(0,s.jsx)(a.h3,{id:"innovaciones-que-superan-a-humanos",children:"Innovaciones Que Superan a Humanos"}),"\n",(0,s.jsx)(a.p,{children:"Pero Zero tambi\xe9n descubri\xf3 movimientos que los humanos nunca hab\xedan pensado:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Aperturas no convencionales"}),": Variaciones sobre aperturas tradicionales"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Sacrificios agresivos"}),": M\xe1s dispuesto que humanos a abandonar ventaja local por ventaja global"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Formas contra-intuitivas"}),': "Mala forma" superficial que en realidad es \xf3ptima']}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"Estas innovaciones est\xe1n cambiando la comprensi\xf3n humana del Go. Muchos jugadores profesionales dicen que estudiar las partidas de AlphaGo Zero les dio una comprensi\xf3n completamente nueva del Go."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"resumen-de-detalles-t\xe9cnicos",children:"Resumen de Detalles T\xe9cnicos"}),"\n",(0,s.jsx)(a.h3,{id:"comparaci\xf3n-completa-con-alphago-original",children:"Comparaci\xf3n Completa con AlphaGo Original"}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"Aspecto"}),(0,s.jsx)(a.th,{children:"AlphaGo (original)"}),(0,s.jsx)(a.th,{children:"AlphaGo Zero"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.strong,{children:"Datos de entrenamiento"})}),(0,s.jsx)(a.td,{children:"Partidas humanas + auto-juego"}),(0,s.jsx)(a.td,{children:"Auto-juego puro"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.strong,{children:"M\xe9todo de aprendizaje"})}),(0,s.jsx)(a.td,{children:"Supervisado + refuerzo"}),(0,s.jsx)(a.td,{children:"Refuerzo puro"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.strong,{children:"Caracter\xedsticas de entrada"})}),(0,s.jsx)(a.td,{children:"48 planos"}),(0,s.jsx)(a.td,{children:"17 planos"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.strong,{children:"Arquitectura de red"})}),(0,s.jsx)(a.td,{children:"Policy/Value separadas"}),(0,s.jsx)(a.td,{children:"ResNet de doble cabeza"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.strong,{children:"Profundidad de red"})}),(0,s.jsx)(a.td,{children:"13 capas"}),(0,s.jsx)(a.td,{children:"40 capas (o m\xe1s)"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.strong,{children:"Evaluaci\xf3n MCTS"})}),(0,s.jsx)(a.td,{children:"Red neuronal + Rollout"}),(0,s.jsx)(a.td,{children:"Red neuronal pura"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.strong,{children:"B\xfasquedas por movimiento"})}),(0,s.jsx)(a.td,{children:"~100,000"}),(0,s.jsx)(a.td,{children:"~1,600"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.strong,{children:"TPUs de entrenamiento"})}),(0,s.jsx)(a.td,{children:"50+"}),(0,s.jsx)(a.td,{children:"4"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.strong,{children:"TPUs de inferencia"})}),(0,s.jsx)(a.td,{children:"48"}),(0,s.jsx)(a.td,{children:"4 (escalable)"})]})]})]}),"\n",(0,s.jsx)(a.h3,{id:"algoritmo-central",children:"Algoritmo Central"}),"\n",(0,s.jsx)(a.p,{children:"El bucle de entrenamiento de AlphaGo Zero es muy conciso:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{children:"1. Auto-juego\n   - Usar red actual para MCTS\n   - Seleccionar movimientos seg\xfan probabilidades de b\xfasqueda MCTS\n   - Registrar cada movimiento (posici\xf3n, probabilidades MCTS, resultado)\n\n2. Entrenar red\n   - Muestrear del pool de experiencia\n   - Policy Head: Minimizar entrop\xeda cruzada con probabilidades MCTS\n   - Value Head: Minimizar error cuadr\xe1tico medio con resultado real\n   - Optimizar conjuntamente ambos objetivos\n\n3. Actualizar red\n   - Reemplazar red vieja con nueva (verificar que nueva red sea m\xe1s fuerte jugando)\n   - Volver al paso 1\n"})}),"\n",(0,s.jsx)(a.p,{children:"Este bucle se ejecuta continuamente, la red se vuelve m\xe1s fuerte constantemente. Sin datos humanos, sin conocimiento humano, solo reglas del juego y objetivo de victoria."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"implicaciones-para-la-investigaci\xf3n-en-ia",children:"Implicaciones para la Investigaci\xf3n en IA"}),"\n",(0,s.jsx)(a.h3,{id:"aprendizaje-desde-primeros-principios",children:"Aprendizaje desde Primeros Principios"}),"\n",(0,s.jsx)(a.p,{children:'AlphaGo Zero demostr\xf3 un m\xe9todo de aprendizaje de "primeros principios":'}),"\n",(0,s.jsxs)(a.blockquote,{children:["\n",(0,s.jsx)(a.p,{children:"No digas a la IA c\xf3mo hacerlo, solo dile cu\xe1l es el objetivo, deja que descubra el m\xe9todo por s\xed misma."}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"Esto contrasta fuertemente con el enfoque tradicional de sistemas expertos. Los sistemas expertos intentan codificar conocimiento humano en la IA, mientras AlphaGo Zero deja que la IA descubra el conocimiento por s\xed misma."}),"\n",(0,s.jsx)(a.p,{children:"El resultado es: el conocimiento que descubre la IA puede ser m\xe1s completo y preciso que el conocimiento humano."}),"\n",(0,s.jsx)(a.h3,{id:"el-poder-del-auto-juego",children:"El Poder del Auto-juego"}),"\n",(0,s.jsx)(a.p,{children:"AlphaGo Zero prob\xf3 que el auto-juego puede generar datos de entrenamiento infinitos, y la calidad de estos datos mejora a medida que la red mejora."}),"\n",(0,s.jsx)(a.p,{children:'Este es un "ciclo positivo":'}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Red m\xe1s fuerte \u2192 Mejores datos de auto-juego"}),"\n",(0,s.jsx)(a.li,{children:"Mejores datos \u2192 Red m\xe1s fuerte"}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"Este ciclo puede continuar ejecut\xe1ndose hasta alcanzar el l\xedmite te\xf3rico del juego (si existe)."}),"\n",(0,s.jsx)(a.h3,{id:"la-importancia-de-la-simplificaci\xf3n",children:"La Importancia de la Simplificaci\xf3n"}),"\n",(0,s.jsx)(a.p,{children:'El \xe9xito de AlphaGo Zero prob\xf3 la importancia de la "simplificaci\xf3n":'}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Simplificar entrada (48 \u2192 17)"}),"\n",(0,s.jsx)(a.li,{children:"Simplificar arquitectura (doble red \u2192 red \xfanica)"}),"\n",(0,s.jsx)(a.li,{children:"Simplificar entrenamiento (supervisado + refuerzo \u2192 refuerzo puro)"}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"Cada simplificaci\xf3n hizo el sistema m\xe1s poderoso. Esto nos dice: complejidad no es igual a bueno, la soluci\xf3n m\xe1s simple a menudo es la mejor."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"correspondencia-con-animaciones",children:"Correspondencia con Animaciones"}),"\n",(0,s.jsx)(a.p,{children:"Conceptos centrales cubiertos en este art\xedculo y sus n\xfameros de animaci\xf3n:"}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"N\xfamero"}),(0,s.jsx)(a.th,{children:"Concepto"}),(0,s.jsx)(a.th,{children:"Correspondencia F\xedsica/Matem\xe1tica"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"E7"}),(0,s.jsx)(a.td,{children:"Entrenamiento desde cero"}),(0,s.jsx)(a.td,{children:"Fen\xf3meno de auto-organizaci\xf3n"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"E5"}),(0,s.jsx)(a.td,{children:"Auto-juego"}),(0,s.jsx)(a.td,{children:"Convergencia de punto fijo"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"E12"}),(0,s.jsx)(a.td,{children:"Curva de crecimiento de fuerza"}),(0,s.jsx)(a.td,{children:"Crecimiento en S"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:"D12"}),(0,s.jsx)(a.td,{children:"Red residual"}),(0,s.jsx)(a.td,{children:"Autopista de gradientes"})]})]})]}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"lecturas-adicionales",children:"Lecturas Adicionales"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Siguiente art\xedculo"}),": ",(0,s.jsx)(a.a,{href:"../dual-head-resnet",children:"Red de Doble Cabeza y Redes Residuales"})," \u2014 Arquitectura de red neuronal de AlphaGo Zero en detalle"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Art\xedculo relacionado"}),": ",(0,s.jsx)(a.a,{href:"../self-play",children:"Auto-juego"})," \u2014 Por qu\xe9 el auto-juego puede producir nivel sobrehumano"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Profundizaci\xf3n t\xe9cnica"}),": ",(0,s.jsx)(a.a,{href:"../training-from-scratch",children:"Proceso de Entrenamiento desde Cero"})," \u2014 Evoluci\xf3n detallada del D\xeda 0-3"]}),"\n"]}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"referencias",children:"Referencias"}),"\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsxs)(a.li,{children:['Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." ',(0,s.jsx)(a.em,{children:"Nature"}),", 550, 354-359."]}),"\n",(0,s.jsxs)(a.li,{children:['Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." ',(0,s.jsx)(a.em,{children:"Science"}),", 362(6419), 1140-1144."]}),"\n",(0,s.jsxs)(a.li,{children:['DeepMind. (2017). "AlphaGo Zero: Starting from scratch." ',(0,s.jsx)(a.em,{children:"DeepMind Blog"}),"."]}),"\n",(0,s.jsxs)(a.li,{children:['Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." ',(0,s.jsx)(a.em,{children:"Nature"}),", 588, 604-609."]}),"\n"]})]})}function h(e={}){const{wrapper:a}={...(0,r.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(t,{...e})}):t(e)}},30416(e,a,n){n.d(a,{R:()=>d,x:()=>o});var i=n(59471);const s={},r=i.createContext(s);function d(e){const a=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:d(e.components),i.createElement(r.Provider,{value:a},e.children)}}}]);