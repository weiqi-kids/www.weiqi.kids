"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[5521],{41968(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>h,frontMatter:()=>l,metadata:()=>a,toc:()=>s});const a=JSON.parse('{"id":"tech/deep-dive/papers","title":"Guia de articulos clave","description":"Analisis de los puntos clave de los articulos hito de IA de Go como AlphaGo, AlphaZero y KataGo","source":"@site/i18n/es/docusaurus-plugin-content-docs/current/tech/deep-dive/papers.md","sourceDirName":"tech/deep-dive","slug":"/tech/deep-dive/papers","permalink":"/es/docs/tech/deep-dive/papers","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tech/deep-dive/papers.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11,"title":"Guia de articulos clave","description":"Analisis de los puntos clave de los articulos hito de IA de Go como AlphaGo, AlphaZero y KataGo"},"sidebar":"tutorialSidebar","previous":{"title":"Reglas personalizadas y variantes","permalink":"/es/docs/tech/deep-dive/custom-rules"},"next":{"title":"Construir una IA de Go desde cero","permalink":"/es/docs/tech/deep-dive/build-from-scratch"}}');var r=i(62615),o=i(30416);const l={sidebar_position:11,title:"Guia de articulos clave",description:"Analisis de los puntos clave de los articulos hito de IA de Go como AlphaGo, AlphaZero y KataGo"},d="Guia de articulos clave",c={},s=[{value:"Vision general de articulos",id:"vision-general-de-articulos",level:2},{value:"Linea temporal",id:"linea-temporal",level:3},{value:"Recomendaciones de lectura",id:"recomendaciones-de-lectura",level:3},{value:"1. El nacimiento de MCTS (2006)",id:"1-el-nacimiento-de-mcts-2006",level:2},{value:"Informacion del articulo",id:"informacion-del-articulo",level:3},{value:"Contribucion principal",id:"contribucion-principal",level:3},{value:"Conceptos clave",id:"conceptos-clave",level:3},{value:"Formula UCB1",id:"formula-ucb1",level:4},{value:"Cuatro pasos de MCTS",id:"cuatro-pasos-de-mcts",level:4},{value:"Impacto",id:"impacto",level:3},{value:"2. AlphaGo (2016)",id:"2-alphago-2016",level:2},{value:"Informacion del articulo",id:"informacion-del-articulo-1",level:3},{value:"Contribucion principal",id:"contribucion-principal-1",level:3},{value:"Arquitectura del sistema",id:"arquitectura-del-sistema",level:3},{value:"Puntos tecnicos",id:"puntos-tecnicos",level:3},{value:"1. Policy Network con aprendizaje supervisado",id:"1-policy-network-con-aprendizaje-supervisado",level:4},{value:"2. Mejora con aprendizaje por refuerzo",id:"2-mejora-con-aprendizaje-por-refuerzo",level:4},{value:"3. Entrenamiento de Value Network",id:"3-entrenamiento-de-value-network",level:4},{value:"4. Integracion con MCTS",id:"4-integracion-con-mcts",level:4},{value:"Datos clave",id:"datos-clave",level:3},{value:"3. AlphaGo Zero (2017)",id:"3-alphago-zero-2017",level:2},{value:"Informacion del articulo",id:"informacion-del-articulo-2",level:3},{value:"Contribucion principal",id:"contribucion-principal-2",level:3},{value:"Diferencias con AlphaGo",id:"diferencias-con-alphago",level:3},{value:"Innovaciones clave",id:"innovaciones-clave",level:3},{value:"1. Red unica de doble cabeza",id:"1-red-unica-de-doble-cabeza",level:4},{value:"2. Caracteristicas de entrada simplificadas",id:"2-caracteristicas-de-entrada-simplificadas",level:4},{value:"3. Evaluacion solo con Value Network",id:"3-evaluacion-solo-con-value-network",level:4},{value:"4. Proceso de entrenamiento",id:"4-proceso-de-entrenamiento",level:4},{value:"Curva de aprendizaje",id:"curva-de-aprendizaje",level:3},{value:"4. AlphaZero (2017)",id:"4-alphazero-2017",level:2},{value:"Informacion del articulo",id:"informacion-del-articulo-3",level:3},{value:"Contribucion principal",id:"contribucion-principal-3",level:3},{value:"Arquitectura general",id:"arquitectura-general",level:3},{value:"Adaptacion entre juegos",id:"adaptacion-entre-juegos",level:3},{value:"Mejoras de MCTS",id:"mejoras-de-mcts",level:3},{value:"Formula PUCT",id:"formula-puct",level:4},{value:"Ruido de exploracion",id:"ruido-de-exploracion",level:4},{value:"5. KataGo (2019)",id:"5-katago-2019",level:2},{value:"Informacion del articulo",id:"informacion-del-articulo-4",level:3},{value:"Contribucion principal",id:"contribucion-principal-4",level:3},{value:"Innovaciones clave",id:"innovaciones-clave-1",level:3},{value:"1. Objetivos de entrenamiento auxiliares",id:"1-objetivos-de-entrenamiento-auxiliares",level:4},{value:"2. Caracteristicas globales",id:"2-caracteristicas-globales",level:4},{value:"3. Aleatorizacion de Playout Cap",id:"3-aleatorizacion-de-playout-cap",level:4},{value:"4. Tamano de tablero progresivo",id:"4-tamano-de-tablero-progresivo",level:4},{value:"Comparacion de eficiencia",id:"comparacion-de-eficiencia",level:3},{value:"6. Articulos de extension",id:"6-articulos-de-extension",level:2},{value:"MuZero (2020)",id:"muzero-2020",level:3},{value:"EfficientZero (2021)",id:"efficientzero-2021",level:3},{value:"Gumbel AlphaZero (2022)",id:"gumbel-alphazero-2022",level:3},{value:"Recomendaciones de lectura de articulos",id:"recomendaciones-de-lectura-de-articulos",level:2},{value:"Orden para principiantes",id:"orden-para-principiantes",level:3},{value:"Orden avanzado",id:"orden-avanzado",level:3},{value:"Tecnicas de lectura",id:"tecnicas-de-lectura",level:3},{value:"Enlaces de recursos",id:"enlaces-de-recursos",level:2},{value:"PDFs de articulos",id:"pdfs-de-articulos",level:3},{value:"Implementaciones de codigo abierto",id:"implementaciones-de-codigo-abierto",level:3},{value:"Lectura adicional",id:"lectura-adicional",level:2}];function t(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"guia-de-articulos-clave",children:"Guia de articulos clave"})}),"\n",(0,r.jsx)(n.p,{children:"Este articulo resume los articulos mas importantes en la historia del desarrollo de IA de Go, proporcionando resumenes y puntos tecnicos clave para una comprension rapida."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"vision-general-de-articulos",children:"Vision general de articulos"}),"\n",(0,r.jsx)(n.h3,{id:"linea-temporal",children:"Linea temporal"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"2006  Coulom - MCTS aplicado por primera vez a Go\n2016  Silver et al. - AlphaGo (Nature)\n2017  Silver et al. - AlphaGo Zero (Nature)\n2017  Silver et al. - AlphaZero\n2019  Wu - KataGo\n2020+ Diversas mejoras y aplicaciones\n"})}),"\n",(0,r.jsx)(n.h3,{id:"recomendaciones-de-lectura",children:"Recomendaciones de lectura"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Objetivo"}),(0,r.jsx)(n.th,{children:"Articulo recomendado"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Entender lo basico"}),(0,r.jsx)(n.td,{children:"AlphaGo (2016)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Entender el auto-juego"}),(0,r.jsx)(n.td,{children:"AlphaGo Zero (2017)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Entender el metodo general"}),(0,r.jsx)(n.td,{children:"AlphaZero (2017)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Referencia de implementacion"}),(0,r.jsx)(n.td,{children:"KataGo (2019)"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"1-el-nacimiento-de-mcts-2006",children:"1. El nacimiento de MCTS (2006)"}),"\n",(0,r.jsx)(n.h3,{id:"informacion-del-articulo",children:"Informacion del articulo"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Titulo: Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search\nAutor: Remi Coulom\nPublicacion: Computers and Games 2006\n"})}),"\n",(0,r.jsx)(n.h3,{id:"contribucion-principal",children:"Contribucion principal"}),"\n",(0,r.jsx)(n.p,{children:"Primera aplicacion sistematica de metodos Monte Carlo a Go:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Antes: Simulacion puramente aleatoria, sin estructura de arbol\nDespues: Construir arbol de busqueda + seleccion UCB + retropropagacion de estadisticas\n"})}),"\n",(0,r.jsx)(n.h3,{id:"conceptos-clave",children:"Conceptos clave"}),"\n",(0,r.jsx)(n.h4,{id:"formula-ucb1",children:"Formula UCB1"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Puntuacion de seleccion = Tasa de victoria promedio + C * sqrt(ln(N) / n)\n\nDonde:\n- N: Numero de visitas del nodo padre\n- n: Numero de visitas del nodo hijo\n- C: Constante de exploracion\n"})}),"\n",(0,r.jsx)(n.h4,{id:"cuatro-pasos-de-mcts",children:"Cuatro pasos de MCTS"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"1. Selection: Seleccionar nodo usando UCB\n2. Expansion: Expandir nuevo nodo\n3. Simulation: Simular aleatoriamente hasta el final\n4. Backpropagation: Retropropagar victoria/derrota\n"})}),"\n",(0,r.jsx)(n.h3,{id:"impacto",children:"Impacto"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Llevo la IA de Go al nivel de dan amateur"}),"\n",(0,r.jsx)(n.li,{children:"Se convirtio en la base de todas las IAs de Go posteriores"}),"\n",(0,r.jsx)(n.li,{children:"El concepto UCB influyo en el desarrollo de PUCT"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"2-alphago-2016",children:"2. AlphaGo (2016)"}),"\n",(0,r.jsx)(n.h3,{id:"informacion-del-articulo-1",children:"Informacion del articulo"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Titulo: Mastering the game of Go with deep neural networks and tree search\nAutores: Silver, D., Huang, A., Maddison, C.J., et al.\nPublicacion: Nature, 2016\nDOI: 10.1038/nature16961\n"})}),"\n",(0,r.jsx)(n.h3,{id:"contribucion-principal-1",children:"Contribucion principal"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Primera combinacion de deep learning y MCTS"}),", derrotando al campeon mundial humano."]}),"\n",(0,r.jsx)(n.h3,{id:"arquitectura-del-sistema",children:"Arquitectura del sistema"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"+---------------------------------------------+\n|              Arquitectura AlphaGo           |\n+---------------------------------------------+\n|                                             |\n|   Policy Network (SL)                       |\n|   +-- Entrada: Estado del tablero           |\n|   |   (48 planos de caracteristicas)        |\n|   +-- Arquitectura: CNN de 13 capas         |\n|   +-- Salida: Probabilidad de 361 posiciones|\n|   +-- Entrenamiento: 30 millones de         |\n|       partidas humanas                      |\n|                                             |\n|   Policy Network (RL)                       |\n|   +-- Inicializado desde SL Policy          |\n|   +-- Aprendizaje por refuerzo con          |\n|       auto-juego                            |\n|                                             |\n|   Value Network                             |\n|   +-- Entrada: Estado del tablero           |\n|   +-- Salida: Valor de tasa de victoria     |\n|       unico                                 |\n|   +-- Entrenamiento: Posiciones generadas   |\n|       por auto-juego                        |\n|                                             |\n|   MCTS                                      |\n|   +-- Usa Policy Network para guiar         |\n|       busqueda                              |\n|   +-- Usa Value Network + Rollout para      |\n|       evaluacion                            |\n|                                             |\n+---------------------------------------------+\n"})}),"\n",(0,r.jsx)(n.h3,{id:"puntos-tecnicos",children:"Puntos tecnicos"}),"\n",(0,r.jsx)(n.h4,{id:"1-policy-network-con-aprendizaje-supervisado",children:"1. Policy Network con aprendizaje supervisado"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Caracteristicas de entrada (48 planos)\n- Posicion de piedras propias\n- Posicion de piedras del oponente\n- Numero de libertades\n- Estado despues de captura\n- Posiciones de movimientos legales\n- Posiciones de los ultimos movimientos\n...\n"})}),"\n",(0,r.jsx)(n.h4,{id:"2-mejora-con-aprendizaje-por-refuerzo",children:"2. Mejora con aprendizaje por refuerzo"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"SL Policy -> Auto-juego -> RL Policy\n\nRL Policy tiene ~80% tasa de victoria contra SL Policy\n"})}),"\n",(0,r.jsx)(n.h4,{id:"3-entrenamiento-de-value-network",children:"3. Entrenamiento de Value Network"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Clave para prevenir sobreajuste:\n- Tomar solo una posicion de cada partida\n- Evitar posiciones similares repetidas\n"})}),"\n",(0,r.jsx)(n.h4,{id:"4-integracion-con-mcts",children:"4. Integracion con MCTS"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Evaluacion de nodo hoja = 0.5 * Value Network + 0.5 * Rollout\n\nRollout usa Policy Network rapido (menor precision pero mas velocidad)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"datos-clave",children:"Datos clave"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Item"}),(0,r.jsx)(n.th,{children:"Valor"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Precision SL Policy"}),(0,r.jsx)(n.td,{children:"57%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Tasa de victoria RL Policy vs SL Policy"}),(0,r.jsx)(n.td,{children:"80%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"GPUs de entrenamiento"}),(0,r.jsx)(n.td,{children:"176"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"GPUs de partida"}),(0,r.jsx)(n.td,{children:"48 TPU"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"3-alphago-zero-2017",children:"3. AlphaGo Zero (2017)"}),"\n",(0,r.jsx)(n.h3,{id:"informacion-del-articulo-2",children:"Informacion del articulo"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Titulo: Mastering the game of Go without human knowledge\nAutores: Silver, D., Schrittwieser, J., Simonyan, K., et al.\nPublicacion: Nature, 2017\nDOI: 10.1038/nature24270\n"})}),"\n",(0,r.jsx)(n.h3,{id:"contribucion-principal-2",children:"Contribucion principal"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sin necesidad de partidas humanas"}),", aprendizaje desde cero."]}),"\n",(0,r.jsx)(n.h3,{id:"diferencias-con-alphago",children:"Diferencias con AlphaGo"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspecto"}),(0,r.jsx)(n.th,{children:"AlphaGo"}),(0,r.jsx)(n.th,{children:"AlphaGo Zero"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Partidas humanas"}),(0,r.jsx)(n.td,{children:"Necesarias"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"No necesarias"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Numero de redes"}),(0,r.jsx)(n.td,{children:"4"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"1 de doble cabeza"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Planos de entrada"}),(0,r.jsx)(n.td,{children:"48"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"17"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Rollout"}),(0,r.jsx)(n.td,{children:"Usado"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"No usado"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Red residual"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Si"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Tiempo de entrenamiento"}),(0,r.jsx)(n.td,{children:"Meses"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"3 dias"})})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"innovaciones-clave",children:"Innovaciones clave"}),"\n",(0,r.jsx)(n.h4,{id:"1-red-unica-de-doble-cabeza",children:"1. Red unica de doble cabeza"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"              Entrada (17 planos)\n                   |\n              +----+----+\n              | Torre   |\n              | residual|\n              | (19 o   |\n              |  39     |\n              | capas)  |\n              +----+----+\n           +-------+-------+\n           |               |\n        Policy          Value\n        (361)            (1)\n"})}),"\n",(0,r.jsx)(n.h4,{id:"2-caracteristicas-de-entrada-simplificadas",children:"2. Caracteristicas de entrada simplificadas"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Solo 17 planos de caracteristicas\nfeatures = [\n    current_player_stones,      # Piedras propias\n    opponent_stones,            # Piedras del oponente\n    history_1_player,           # Estado historico 1\n    history_1_opponent,\n    ...                         # Estados historicos 2-7\n    color_to_play               # Turno\n]\n"})}),"\n",(0,r.jsx)(n.h4,{id:"3-evaluacion-solo-con-value-network",children:"3. Evaluacion solo con Value Network"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Ya no usa Rollout\nEvaluacion de nodo hoja = Salida de Value Network\n\nMas simple, mas rapido\n"})}),"\n",(0,r.jsx)(n.h4,{id:"4-proceso-de-entrenamiento",children:"4. Proceso de entrenamiento"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Inicializar red aleatoria\n    |\n    v\n+-----------------------------+\n|  Auto-juego genera partidas | <--+\n+-------------+---------------+    |\n              |                    |\n              v                    |\n+-----------------------------+    |\n|  Entrenar red neuronal      |    |\n|  - Policy: Minimizar        |    |\n|    entropia cruzada         |    |\n|  - Value: Minimizar MSE     |    |\n+-------------+---------------+    |\n              |                    |\n              v                    |\n+-----------------------------+    |\n|  Evaluar nueva red          |    |\n|  Si es mejor, reemplazar    |----+\n+-----------------------------+\n"})}),"\n",(0,r.jsx)(n.h3,{id:"curva-de-aprendizaje",children:"Curva de aprendizaje"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Tiempo de entrenamiento    Elo\n-------------------------\n3 horas      Principiante\n24 horas     Supera a AlphaGo Lee\n72 horas     Supera a AlphaGo Master\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"4-alphazero-2017",children:"4. AlphaZero (2017)"}),"\n",(0,r.jsx)(n.h3,{id:"informacion-del-articulo-3",children:"Informacion del articulo"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Titulo: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\nAutores: Silver, D., Hubert, T., Schrittwieser, J., et al.\nPublicacion: arXiv:1712.01815 (posteriormente en Science, 2018)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"contribucion-principal-3",children:"Contribucion principal"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Generalizacion"}),": El mismo algoritmo aplicado a Go, ajedrez y shogi."]}),"\n",(0,r.jsx)(n.h3,{id:"arquitectura-general",children:"Arquitectura general"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Codificacion de entrada (especifica del juego) -> Red residual (general) -> Salida de doble cabeza (general)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"adaptacion-entre-juegos",children:"Adaptacion entre juegos"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Juego"}),(0,r.jsx)(n.th,{children:"Planos de entrada"}),(0,r.jsx)(n.th,{children:"Espacio de acciones"}),(0,r.jsx)(n.th,{children:"Tiempo de entrenamiento"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Go"}),(0,r.jsx)(n.td,{children:"17"}),(0,r.jsx)(n.td,{children:"362"}),(0,r.jsx)(n.td,{children:"40 dias"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Ajedrez"}),(0,r.jsx)(n.td,{children:"119"}),(0,r.jsx)(n.td,{children:"4672"}),(0,r.jsx)(n.td,{children:"9 horas"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Shogi"}),(0,r.jsx)(n.td,{children:"362"}),(0,r.jsx)(n.td,{children:"11259"}),(0,r.jsx)(n.td,{children:"12 horas"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"mejoras-de-mcts",children:"Mejoras de MCTS"}),"\n",(0,r.jsx)(n.h4,{id:"formula-puct",children:"Formula PUCT"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Puntuacion de seleccion = Q(s,a) + c(s) * P(s,a) * sqrt(N(s)) / (1 + N(s,a))\n\nc(s) = log((1 + N(s) + c_base) / c_base) + c_init\n"})}),"\n",(0,r.jsx)(n.h4,{id:"ruido-de-exploracion",children:"Ruido de exploracion"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Agregar ruido de Dirichlet en el nodo raiz\nP(s,a) = (1 - epsilon) * p_a + epsilon * eta_a\n\neta ~ Dir(alpha)\nalpha = 0.03 (Go), 0.3 (ajedrez), 0.15 (shogi)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"5-katago-2019",children:"5. KataGo (2019)"}),"\n",(0,r.jsx)(n.h3,{id:"informacion-del-articulo-4",children:"Informacion del articulo"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Titulo: Accelerating Self-Play Learning in Go\nAutor: David J. Wu\nPublicacion: arXiv:1902.10565\n"})}),"\n",(0,r.jsx)(n.h3,{id:"contribucion-principal-4",children:"Contribucion principal"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"50x de mejora en eficiencia"}),", permitiendo a desarrolladores individuales entrenar una potente IA de Go."]}),"\n",(0,r.jsx)(n.h3,{id:"innovaciones-clave-1",children:"Innovaciones clave"}),"\n",(0,r.jsx)(n.h4,{id:"1-objetivos-de-entrenamiento-auxiliares",children:"1. Objetivos de entrenamiento auxiliares"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Perdida total = Policy Loss + Value Loss +\n         Score Loss + Ownership Loss + ...\n\nLos objetivos auxiliares hacen que la red converja mas rapido\n"})}),"\n",(0,r.jsx)(n.h4,{id:"2-caracteristicas-globales",children:"2. Caracteristicas globales"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Capa de pooling global\nglobal_features = global_avg_pool(conv_features)\n# Combinar con caracteristicas locales\ncombined = concat(conv_features, broadcast(global_features))\n"})}),"\n",(0,r.jsx)(n.h4,{id:"3-aleatorizacion-de-playout-cap",children:"3. Aleatorizacion de Playout Cap"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Tradicional: Buscar N veces fijas\nKataGo: N muestreado aleatoriamente de una distribucion\n\nHace que la red aprenda a funcionar bien en varias profundidades de busqueda\n"})}),"\n",(0,r.jsx)(n.h4,{id:"4-tamano-de-tablero-progresivo",children:"4. Tamano de tablero progresivo"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if training_step < 1000000:\n    board_size = random.choice([9, 13, 19])\nelse:\n    board_size = 19\n"})}),"\n",(0,r.jsx)(n.h3,{id:"comparacion-de-eficiencia",children:"Comparacion de eficiencia"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Metrica"}),(0,r.jsx)(n.th,{children:"AlphaZero"}),(0,r.jsx)(n.th,{children:"KataGo"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Dias-GPU para nivel superhumano"}),(0,r.jsx)(n.td,{children:"5000"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"100"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Mejora de eficiencia"}),(0,r.jsx)(n.td,{children:"Base"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"50x"})})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"6-articulos-de-extension",children:"6. Articulos de extension"}),"\n",(0,r.jsx)(n.h3,{id:"muzero-2020",children:"MuZero (2020)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Titulo: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\nContribucion: Aprende el modelo de dinamica del entorno, no necesita reglas del juego\n"})}),"\n",(0,r.jsx)(n.h3,{id:"efficientzero-2021",children:"EfficientZero (2021)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Titulo: Mastering Atari Games with Limited Data\nContribucion: Gran mejora en eficiencia de muestras\n"})}),"\n",(0,r.jsx)(n.h3,{id:"gumbel-alphazero-2022",children:"Gumbel AlphaZero (2022)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Titulo: Policy Improvement by Planning with Gumbel\nContribucion: Metodo mejorado de mejora de politica\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"recomendaciones-de-lectura-de-articulos",children:"Recomendaciones de lectura de articulos"}),"\n",(0,r.jsx)(n.h3,{id:"orden-para-principiantes",children:"Orden para principiantes"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"1. AlphaGo (2016) - Entender arquitectura basica\n2. AlphaGo Zero (2017) - Entender auto-juego\n3. KataGo (2019) - Entender detalles de implementacion\n"})}),"\n",(0,r.jsx)(n.h3,{id:"orden-avanzado",children:"Orden avanzado"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"4. AlphaZero (2017) - Generalizacion\n5. MuZero (2020) - Aprender modelo del mundo\n6. Articulo original de MCTS - Entender fundamentos\n"})}),"\n",(0,r.jsx)(n.h3,{id:"tecnicas-de-lectura",children:"Tecnicas de lectura"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ver primero resumen y conclusiones"}),": Captar rapidamente la contribucion principal"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ver las figuras"}),": Entender la arquitectura general"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ver seccion de metodos"}),": Entender detalles tecnicos"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ver apendices"}),": Encontrar detalles de implementacion e hiperparametros"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"enlaces-de-recursos",children:"Enlaces de recursos"}),"\n",(0,r.jsx)(n.h3,{id:"pdfs-de-articulos",children:"PDFs de articulos"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Articulo"}),(0,r.jsx)(n.th,{children:"Enlace"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaGo"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://www.nature.com/articles/nature16961",children:"Nature"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaGo Zero"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://www.nature.com/articles/nature24270",children:"Nature"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaZero"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://www.science.org/doi/10.1126/science.aar6404",children:"Science"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"KataGo"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1902.10565",children:"arXiv"})})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"implementaciones-de-codigo-abierto",children:"Implementaciones de codigo abierto"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Proyecto"}),(0,r.jsx)(n.th,{children:"Enlace"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"KataGo"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/lightvector/KataGo",children:"GitHub"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Leela Zero"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/leela-zero/leela-zero",children:"GitHub"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MiniGo"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/tensorflow/minigo",children:"GitHub"})})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"lectura-adicional",children:"Lectura adicional"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"../neural-network",children:"Arquitectura de redes neuronales en detalle"})," \u2014 Entender a fondo el diseno de redes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"../mcts-implementation",children:"Detalles de implementacion de MCTS"})," \u2014 Implementacion del algoritmo de busqueda"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"../training",children:"Analisis del mecanismo de entrenamiento de KataGo"})," \u2014 Detalles del proceso de entrenamiento"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(t,{...e})}):t(e)}},30416(e,n,i){i.d(n,{R:()=>l,x:()=>d});var a=i(59471);const r={},o=a.createContext(r);function l(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);