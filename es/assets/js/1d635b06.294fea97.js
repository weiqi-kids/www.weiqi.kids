"use strict";(self.webpackChunktemp_docusaurus=self.webpackChunktemp_docusaurus||[]).push([[654],{1149:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"for-engineers/background-info/alphago","title":"Analisis del paper de AlphaGo","description":"Este articulo analiza en profundidad el paper clasico publicado en Nature por DeepMind \\"Mastering the game of Go with deep neural networks and tree search\\", asi como los papers posteriores de AlphaGo Zero y AlphaZero.","source":"@site/i18n/es/docusaurus-plugin-content-docs/current/for-engineers/background-info/alphago.md","sourceDirName":"for-engineers/background-info","slug":"/for-engineers/background-info/alphago","permalink":"/www.weiqi.kids/es/docs/for-engineers/background-info/alphago","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/background-info/alphago.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Analisis del paper de AlphaGo"},"sidebar":"tutorialSidebar","previous":{"title":"Conocimientos previos","permalink":"/www.weiqi.kids/es/docs/for-engineers/background-info/"},"next":{"title":"Analisis del paper de KataGo","permalink":"/www.weiqi.kids/es/docs/for-engineers/background-info/katago-paper"}}');var i=n(3420),s=n(5521);const o={sidebar_position:1,title:"Analisis del paper de AlphaGo"},d="Analisis del paper de AlphaGo",l={},c=[{value:"Significado historico de AlphaGo",id:"significado-historico-de-alphago",level:2},{value:"Eventos hito",id:"eventos-hito",level:3},{value:"Arquitectura tecnica central",id:"arquitectura-tecnica-central",level:2},{value:"Policy Network (Red de estrategia)",id:"policy-network-red-de-estrategia",level:3},{value:"Arquitectura de red",id:"arquitectura-de-red",level:4},{value:"Caracteristicas de entrada",id:"caracteristicas-de-entrada",level:4},{value:"Metodo de entrenamiento",id:"metodo-de-entrenamiento",level:4},{value:"Value Network (Red de valor)",id:"value-network-red-de-valor",level:3},{value:"Arquitectura de red",id:"arquitectura-de-red-1",level:4},{value:"Metodo de entrenamiento",id:"metodo-de-entrenamiento-1",level:4},{value:"Busqueda de Arbol Monte Carlo (MCTS)",id:"busqueda-de-arbol-monte-carlo-mcts",level:2},{value:"Cuatro pasos de MCTS",id:"cuatro-pasos-de-mcts",level:3},{value:"Formula de seleccion (PUCT)",id:"formula-de-seleccion-puct",level:3},{value:"Detalle del proceso de busqueda",id:"detalle-del-proceso-de-busqueda",level:3},{value:"Rollout (Simulacion rapida)",id:"rollout-simulacion-rapida",level:3},{value:"Metodo de entrenamiento Self-play",id:"metodo-de-entrenamiento-self-play",level:2},{value:"Ciclo de entrenamiento",id:"ciclo-de-entrenamiento",level:3},{value:"Por que Self-play es efectivo?",id:"por-que-self-play-es-efectivo",level:3},{value:"Mejoras de AlphaGo Zero",id:"mejoras-de-alphago-zero",level:2},{value:"Diferencias principales",id:"diferencias-principales",level:3},{value:"Simplificacion de arquitectura",id:"simplificacion-de-arquitectura",level:3},{value:"Caracteristicas de entrada simplificadas",id:"caracteristicas-de-entrada-simplificadas",level:3},{value:"Mejoras de entrenamiento",id:"mejoras-de-entrenamiento",level:3},{value:"Generalizacion de AlphaZero",id:"generalizacion-de-alphazero",level:2},{value:"Caracteristicas clave",id:"caracteristicas-clave",level:3},{value:"Diferencias con AlphaGo Zero",id:"diferencias-con-alphago-zero",level:3},{value:"Puntos clave de implementacion",id:"puntos-clave-de-implementacion",level:2},{value:"Recursos de computo",id:"recursos-de-computo",level:3},{value:"Hiperparametros clave",id:"hiperparametros-clave",level:3},{value:"Problemas comunes",id:"problemas-comunes",level:3},{value:"Lectura adicional",id:"lectura-adicional",level:2}];function t(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(a.header,{children:(0,i.jsx)(a.h1,{id:"analisis-del-paper-de-alphago",children:"Analisis del paper de AlphaGo"})}),"\n",(0,i.jsx)(a.p,{children:'Este articulo analiza en profundidad el paper clasico publicado en Nature por DeepMind "Mastering the game of Go with deep neural networks and tree search", asi como los papers posteriores de AlphaGo Zero y AlphaZero.'}),"\n",(0,i.jsx)(a.h2,{id:"significado-historico-de-alphago",children:"Significado historico de AlphaGo"}),"\n",(0,i.jsx)(a.p,{children:'El Go fue considerado durante mucho tiempo el desafio "santo grial" de la inteligencia artificial. A diferencia del ajedrez, el espacio de busqueda del Go es extremadamente enorme:'}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"Juego"}),(0,i.jsx)(a.th,{children:"Factor de ramificacion promedio"}),(0,i.jsx)(a.th,{children:"Longitud promedio del juego"}),(0,i.jsx)(a.th,{children:"Espacio de estados"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Ajedrez"}),(0,i.jsx)(a.td,{children:"~35"}),(0,i.jsx)(a.td,{children:"~80"}),(0,i.jsx)(a.td,{children:"~10^47"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Go"}),(0,i.jsx)(a.td,{children:"~250"}),(0,i.jsx)(a.td,{children:"~150"}),(0,i.jsx)(a.td,{children:"~10^170"})]})]})]}),"\n",(0,i.jsx)(a.p,{children:"Los metodos tradicionales de busqueda por fuerza bruta son completamente inviables en Go. En 2016, AlphaGo derroto a Lee Sedol, demostrando el poderoso poder de la combinacion del aprendizaje profundo y el aprendizaje por refuerzo."}),"\n",(0,i.jsx)(a.h3,{id:"eventos-hito",children:"Eventos hito"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Octubre 2015"}),": AlphaGo Fan derrota al campeon europeo Fan Hui (profesional 2-dan) 5:0"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Marzo 2016"}),": AlphaGo Lee derrota al campeon mundial Lee Sedol (profesional 9-dan) 4:1"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Mayo 2017"}),": AlphaGo Master derrota a Ke Jie, el numero uno del mundo, 3:0"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Octubre 2017"}),": Publicacion de AlphaGo Zero, self-play puro, supera todas las versiones anteriores"]}),"\n"]}),"\n",(0,i.jsx)(a.h2,{id:"arquitectura-tecnica-central",children:"Arquitectura tecnica central"}),"\n",(0,i.jsx)(a.p,{children:"La innovacion central de AlphaGo esta en la combinacion de tres tecnologias clave:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Arquitectura AlphaGo                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502   \u2502   Policy    \u2502    \u2502    Value    \u2502                   \u2502\n\u2502   \u2502   Network   \u2502    \u2502   Network   \u2502                   \u2502\n\u2502   \u2502 (estrategia)\u2502    \u2502  (evaluacion\u2502                   \u2502\n\u2502   \u2502             \u2502    \u2502  de victoria)\u2502                   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502          \u2502                  \u2502                          \u2502\n\u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2502                   \u2502                                    \u2502\n\u2502                   \u25bc                                    \u2502\n\u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u2502\n\u2502          \u2502      MCTS       \u2502                          \u2502\n\u2502          \u2502(Busqueda de arbol\u2502                          \u2502\n\u2502          \u2502  Monte Carlo)   \u2502                          \u2502\n\u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(a.h3,{id:"policy-network-red-de-estrategia",children:"Policy Network (Red de estrategia)"}),"\n",(0,i.jsx)(a.p,{children:"La Policy Network es responsable de predecir la probabilidad de jugar en cada posicion, usada para guiar la direccion de busqueda."}),"\n",(0,i.jsx)(a.h4,{id:"arquitectura-de-red",children:"Arquitectura de red"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"Capa de entrada: 19x19x48 planos de caracteristicas\n    \u2502\n    \u25bc\nCapa convolucional 1: kernel 5x5, 192 filtros\n    \u2502\n    \u25bc\nCapas convolucionales 2-12: kernel 3x3, 192 filtros\n    \u2502\n    \u25bc\nCapa de salida: distribucion de probabilidad 19x19 (softmax)\n"})}),"\n",(0,i.jsx)(a.h4,{id:"caracteristicas-de-entrada",children:"Caracteristicas de entrada"}),"\n",(0,i.jsx)(a.p,{children:"AlphaGo usa 48 planos de caracteristicas como entrada:"}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"Caracteristica"}),(0,i.jsx)(a.th,{children:"Numero de planos"}),(0,i.jsx)(a.th,{children:"Descripcion"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Color de piedra"}),(0,i.jsx)(a.td,{children:"3"}),(0,i.jsx)(a.td,{children:"Piedra negra, piedra blanca, punto vacio"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Numero de libertades"}),(0,i.jsx)(a.td,{children:"8"}),(0,i.jsx)(a.td,{children:"1 libertad, 2 libertades, ..., 8+ libertades"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Libertades despues de captura"}),(0,i.jsx)(a.td,{children:"8"}),(0,i.jsx)(a.td,{children:"Cuantas libertades tendria despues de capturar"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Numero de capturas"}),(0,i.jsx)(a.td,{children:"8"}),(0,i.jsx)(a.td,{children:"Cuantas piedras se pueden capturar en esa posicion"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Ko"}),(0,i.jsx)(a.td,{children:"1"}),(0,i.jsx)(a.td,{children:"Si es posicion de ko"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Legalidad de jugada"}),(0,i.jsx)(a.td,{children:"1"}),(0,i.jsx)(a.td,{children:"Si se puede jugar en esa posicion"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Posiciones de los ultimos 1-8 movimientos"}),(0,i.jsx)(a.td,{children:"8"}),(0,i.jsx)(a.td,{children:"Posiciones de jugadas anteriores"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Turno de jugar"}),(0,i.jsx)(a.td,{children:"1"}),(0,i.jsx)(a.td,{children:"Es turno de negro o blanco"})]})]})]}),"\n",(0,i.jsx)(a.h4,{id:"metodo-de-entrenamiento",children:"Metodo de entrenamiento"}),"\n",(0,i.jsx)(a.p,{children:"El entrenamiento de Policy Network se divide en dos etapas:"}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"Primera etapa: Aprendizaje supervisado (SL Policy Network)"})}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"Usa 30 millones de partidas del servidor de Go KGS"}),"\n",(0,i.jsx)(a.li,{children:"Objetivo: Predecir la siguiente jugada de jugadores humanos"}),"\n",(0,i.jsx)(a.li,{children:"Alcanza 57% de precision de prediccion"}),"\n"]}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"Segunda etapa: Aprendizaje por refuerzo (RL Policy Network)"})}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"Comienza desde SL Policy Network"}),"\n",(0,i.jsx)(a.li,{children:"Juega contra versiones anteriores de si mismo"}),"\n",(0,i.jsx)(a.li,{children:"Usa el algoritmo REINFORCE para optimizar"}),"\n"]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:"# Actualizacion simplificada de Policy Gradient\n# reward: +1 victoria, -1 derrota\nloss = -log(policy[action]) * reward\n"})}),"\n",(0,i.jsx)(a.h3,{id:"value-network-red-de-valor",children:"Value Network (Red de valor)"}),"\n",(0,i.jsx)(a.p,{children:"La Value Network evalua la tasa de victoria de la posicion actual, usada para reducir la profundidad de busqueda."}),"\n",(0,i.jsx)(a.h4,{id:"arquitectura-de-red-1",children:"Arquitectura de red"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"Capa de entrada: 19x19x48 planos de caracteristicas (igual que Policy Network)\n    \u2502\n    \u25bc\nCapas convolucionales 1-12: similar a Policy Network\n    \u2502\n    \u25bc\nCapa completamente conectada: 256 neuronas\n    \u2502\n    \u25bc\nCapa de salida: 1 neurona (tanh, rango [-1, 1])\n"})}),"\n",(0,i.jsx)(a.h4,{id:"metodo-de-entrenamiento-1",children:"Metodo de entrenamiento"}),"\n",(0,i.jsx)(a.p,{children:"Value Network se entrena con 30 millones de posiciones generadas por self-play de RL Policy Network:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"Se muestrea aleatoriamente una posicion de cada partida"}),"\n",(0,i.jsx)(a.li,{children:"Se usa el resultado final de victoria/derrota como etiqueta"}),"\n",(0,i.jsx)(a.li,{children:"Se usa funcion de perdida MSE"}),"\n"]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:"# Entrenamiento de Value Network\nvalue_prediction = value_network(position)\nloss = (value_prediction - game_outcome) ** 2\n"})}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"Por que solo se toma una muestra de cada partida?"})}),"\n",(0,i.jsx)(a.p,{children:"Si se toman multiples muestras, las posiciones adyacentes de la misma partida estaran altamente correlacionadas, causando sobreajuste. El muestreo aleatorio asegura la diversidad de los datos de entrenamiento."}),"\n",(0,i.jsx)(a.h2,{id:"busqueda-de-arbol-monte-carlo-mcts",children:"Busqueda de Arbol Monte Carlo (MCTS)"}),"\n",(0,i.jsx)(a.p,{children:"MCTS es el nucleo de decision de AlphaGo, combinando redes neuronales para buscar eficientemente la mejor jugada."}),"\n",(0,i.jsx)(a.h3,{id:"cuatro-pasos-de-mcts",children:"Cuatro pasos de MCTS"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"    (1) Seleccion          (2) Expansion\n         \u2502                      \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502 Seleccionar\u2502          \u2502 Expandir \u2502\n    \u2502 mejor ruta \u2502          \u2502nuevo nodo\u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n         \u2502                      \u2502\n         \u25bc                      \u25bc\n    (3) Evaluacion         (4) Retropropagacion\n         \u2502                      \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502  Red     \u2502            \u2502Propagar \u2502\n    \u2502 neuronal \u2502            \u2502 hacia   \u2502\n    \u2502 evalua   \u2502            \u2502 arriba  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(a.h3,{id:"formula-de-seleccion-puct",children:"Formula de seleccion (PUCT)"}),"\n",(0,i.jsx)(a.p,{children:"AlphaGo usa la formula PUCT (Predictor + UCT) para seleccionar que rama explorar:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"a = argmax[Q(s,a) + u(s,a)]\n\nu(s,a) = c_puct * P(s,a) * sqrt(N(s)) / (1 + N(s,a))\n"})}),"\n",(0,i.jsx)(a.p,{children:"Donde:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Q(s,a)"}),": Valor promedio de la accion a (explotacion)"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"P(s,a)"}),": Probabilidad a priori predicha por Policy Network"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"N(s)"}),": Numero de visitas al nodo padre"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"N(s,a)"}),": Numero de visitas a esa accion"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"c_puct"}),": Constante de exploracion, equilibra exploracion y explotacion"]}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"detalle-del-proceso-de-busqueda",children:"Detalle del proceso de busqueda"}),"\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Seleccion"}),": Desde el nodo raiz, usar formula PUCT para seleccionar acciones hasta llegar al nodo hoja"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Expansion"}),": Expandir nuevos nodos hijos en el nodo hoja, usar Policy Network para inicializar probabilidades a priori"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Evaluacion"}),": Combinar evaluacion de Value Network y simulacion rapida (Rollout) para evaluar el valor"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Retropropagacion"}),": Propagar el valor de evaluacion a lo largo del camino hacia atras, actualizar valores Q y N"]}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"rollout-simulacion-rapida",children:"Rollout (Simulacion rapida)"}),"\n",(0,i.jsx)(a.p,{children:"AlphaGo (version no Zero) tambien usa una pequena red de politica rapida para simulacion:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"Nodo hoja -> Jugadas rapidas aleatorias hasta el final -> Calcular resultado\n"})}),"\n",(0,i.jsx)(a.p,{children:"El valor de evaluacion final combina Value Network y Rollout:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"V = lambda * v_network + (1-lambda) * v_rollout\n"})}),"\n",(0,i.jsx)(a.p,{children:"AlphaGo usa lambda = 0.5, dando igual peso a ambos."}),"\n",(0,i.jsx)(a.h2,{id:"metodo-de-entrenamiento-self-play",children:"Metodo de entrenamiento Self-play"}),"\n",(0,i.jsx)(a.p,{children:"Self-play es la estrategia de entrenamiento central de AlphaGo, permitiendo que la IA mejore continuamente jugando contra si misma."}),"\n",(0,i.jsx)(a.h3,{id:"ciclo-de-entrenamiento",children:"Ciclo de entrenamiento"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                Ciclo de entrenamiento Self-play         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n\u2502   \u2502  Modelo \u2502 -> \u2502Self-play\u2502 -> \u2502 Generar \u2502           \u2502\n\u2502   \u2502  actual \u2502    \u2502         \u2502    \u2502  datos  \u2502           \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n\u2502        \u25b2                              \u2502                \u2502\n\u2502        \u2502                              \u25bc                \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n\u2502   \u2502  Nuevo  \u2502 <- \u2502Entrenar \u2502 <- \u2502  Buffer \u2502           \u2502\n\u2502   \u2502  modelo \u2502    \u2502         \u2502    \u2502 de datos\u2502           \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(a.h3,{id:"por-que-self-play-es-efectivo",children:"Por que Self-play es efectivo?"}),"\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Datos infinitos"}),": No esta limitado por la cantidad de partidas humanas"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Dificultad adaptativa"}),": La fuerza del oponente mejora sincronizadamente con la tuya"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Exploracion de innovaciones"}),": No esta limitado por patrones de pensamiento humanos"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Objetivo claro"}),": Optimiza directamente la tasa de victoria, no imita a humanos"]}),"\n"]}),"\n",(0,i.jsx)(a.h2,{id:"mejoras-de-alphago-zero",children:"Mejoras de AlphaGo Zero"}),"\n",(0,i.jsx)(a.p,{children:"AlphaGo Zero publicado en 2017 trajo mejoras revolucionarias:"}),"\n",(0,i.jsx)(a.h3,{id:"diferencias-principales",children:"Diferencias principales"}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"Caracteristica"}),(0,i.jsx)(a.th,{children:"AlphaGo"}),(0,i.jsx)(a.th,{children:"AlphaGo Zero"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Entrenamiento inicial"}),(0,i.jsx)(a.td,{children:"Aprendizaje supervisado con partidas humanas"}),(0,i.jsx)(a.td,{children:"Completamente desde cero"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Arquitectura de red"}),(0,i.jsx)(a.td,{children:"Policy/Value separados"}),(0,i.jsx)(a.td,{children:"Red unica de dos cabezas"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Estructura de red"}),(0,i.jsx)(a.td,{children:"CNN regular"}),(0,i.jsx)(a.td,{children:"ResNet"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Ingenieria de caracteristicas"}),(0,i.jsx)(a.td,{children:"48 caracteristicas manuales"}),(0,i.jsx)(a.td,{children:"17 caracteristicas simples"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Rollout"}),(0,i.jsx)(a.td,{children:"Necesario"}),(0,i.jsx)(a.td,{children:"No necesario"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Tiempo de entrenamiento"}),(0,i.jsx)(a.td,{children:"Varios meses"}),(0,i.jsx)(a.td,{children:"3 dias para superar humanos"})]})]})]}),"\n",(0,i.jsx)(a.h3,{id:"simplificacion-de-arquitectura",children:"Simplificacion de arquitectura"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Red de dos cabezas AlphaGo Zero              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502   Entrada: 19x19x17 (caracteristicas simplificadas)     \u2502\n\u2502                      \u2502                                  \u2502\n\u2502                      \u25bc                                  \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n\u2502              \u2502   ResNet      \u2502                         \u2502\n\u2502              \u2502  (40 bloques  \u2502                         \u2502\n\u2502              \u2502  residuales)  \u2502                         \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2502                      \u2502                                  \u2502\n\u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502          \u25bc                       \u25bc                     \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502   \u2502 Policy Head \u2502         \u2502 Value Head  \u2502             \u2502\n\u2502   \u2502 (19x19+1)   \u2502         \u2502   ([-1,1])  \u2502             \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(a.h3,{id:"caracteristicas-de-entrada-simplificadas",children:"Caracteristicas de entrada simplificadas"}),"\n",(0,i.jsx)(a.p,{children:"AlphaGo Zero solo usa 17 planos de caracteristicas:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"8 planos: Posiciones de tus piedras de los ultimos 8 movimientos"}),"\n",(0,i.jsx)(a.li,{children:"8 planos: Posiciones de piedras del oponente de los ultimos 8 movimientos"}),"\n",(0,i.jsx)(a.li,{children:"1 plano: Turno actual (todo 0 o todo 1)"}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"mejoras-de-entrenamiento",children:"Mejoras de entrenamiento"}),"\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Self-play puro"}),": No usa ningun dato humano"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Usa directamente probabilidades MCTS como objetivo de entrenamiento"}),": En lugar de victoria/derrota binaria"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Sin Rollout"}),": Depende completamente de Value Network"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Entrenamiento de red unica"}),": Policy y Value comparten parametros, se refuerzan mutuamente"]}),"\n"]}),"\n",(0,i.jsx)(a.h2,{id:"generalizacion-de-alphazero",children:"Generalizacion de AlphaZero"}),"\n",(0,i.jsx)(a.p,{children:"AlphaZero publicado a finales de 2017 aplico la misma arquitectura a Go, ajedrez y shogi:"}),"\n",(0,i.jsx)(a.h3,{id:"caracteristicas-clave",children:"Caracteristicas clave"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Cero conocimiento de dominio"}),": No usa ningun conocimiento especifico del dominio excepto las reglas del juego"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Arquitectura unificada"}),": El mismo algoritmo aplica a diferentes juegos de tablero"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Entrenamiento mas rapido"}),":","\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"Go: 8 horas para superar AlphaGo Lee"}),"\n",(0,i.jsx)(a.li,{children:"Ajedrez: 4 horas para superar Stockfish"}),"\n",(0,i.jsx)(a.li,{children:"Shogi: 2 horas para superar Elmo"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"diferencias-con-alphago-zero",children:"Diferencias con AlphaGo Zero"}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"Caracteristica"}),(0,i.jsx)(a.th,{children:"AlphaGo Zero"}),(0,i.jsx)(a.th,{children:"AlphaZero"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Juego objetivo"}),(0,i.jsx)(a.td,{children:"Solo Go"}),(0,i.jsx)(a.td,{children:"Go, ajedrez, shogi"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Uso de simetria"}),(0,i.jsx)(a.td,{children:"Usa simetria 8-fold del Go"}),(0,i.jsx)(a.td,{children:"No asume simetria"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Ajuste de hiperparametros"}),(0,i.jsx)(a.td,{children:"Optimizado para Go"}),(0,i.jsx)(a.td,{children:"Configuracion general"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Modo de entrenamiento"}),(0,i.jsx)(a.td,{children:"Self-play con mejor modelo"}),(0,i.jsx)(a.td,{children:"Self-play con modelo mas reciente"})]})]})]}),"\n",(0,i.jsx)(a.h2,{id:"puntos-clave-de-implementacion",children:"Puntos clave de implementacion"}),"\n",(0,i.jsx)(a.p,{children:"Si quieres implementar un sistema similar, estos son los puntos clave a considerar:"}),"\n",(0,i.jsx)(a.h3,{id:"recursos-de-computo",children:"Recursos de computo"}),"\n",(0,i.jsx)(a.p,{children:"El entrenamiento de AlphaGo requiere enormes recursos de computo:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"AlphaGo Lee"}),": 176 GPU + 48 TPU"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"AlphaGo Zero"}),": 4 TPU (entrenamiento) + 1 TPU (self-play)"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"AlphaZero"}),": 5000 TPU (entrenamiento)"]}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"hiperparametros-clave",children:"Hiperparametros clave"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:"# Relacionados con MCTS\nnum_simulations = 800     # Numero de simulaciones de busqueda por jugada\nc_puct = 1.5              # Constante de exploracion\ntemperature = 1.0         # Parametro de temperatura para seleccion de accion\n\n# Relacionados con entrenamiento\nbatch_size = 2048\nlearning_rate = 0.01      # Con decay\nl2_regularization = 1e-4\n"})}),"\n",(0,i.jsx)(a.h3,{id:"problemas-comunes",children:"Problemas comunes"}),"\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Entrenamiento inestable"}),": Usar learning rate mas pequeno, aumentar batch size"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Sobreajuste"}),": Asegurar diversidad de datos de entrenamiento, usar regularizacion"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Eficiencia de busqueda"}),": Optimizar inferencia batch GPU, paralelizar MCTS"]}),"\n"]}),"\n",(0,i.jsx)(a.h2,{id:"lectura-adicional",children:"Lectura adicional"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:(0,i.jsx)(a.a,{href:"https://www.nature.com/articles/nature16961",children:"Paper original: Mastering the game of Go with deep neural networks and tree search"})}),"\n",(0,i.jsx)(a.li,{children:(0,i.jsx)(a.a,{href:"https://www.nature.com/articles/nature24270",children:"Paper AlphaGo Zero: Mastering the game of Go without human knowledge"})}),"\n",(0,i.jsx)(a.li,{children:(0,i.jsx)(a.a,{href:"https://www.science.org/doi/10.1126/science.aar6404",children:"Paper AlphaZero: A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"})}),"\n"]}),"\n",(0,i.jsxs)(a.p,{children:["Despues de entender la tecnologia de AlphaGo, veamos como ",(0,i.jsx)(a.a,{href:"/www.weiqi.kids/es/docs/for-engineers/background-info/katago-paper",children:"KataGo hizo mejoras sobre esta base"}),"."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,i.jsx)(a,{...e,children:(0,i.jsx)(t,{...e})}):t(e)}},5521:(e,a,n)=>{n.d(a,{R:()=>o,x:()=>d});var r=n(6672);const i={},s=r.createContext(i);function o(e){const a=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function d(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(s.Provider,{value:a},e.children)}}}]);