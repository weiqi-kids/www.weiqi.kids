"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[797],{21864(e,a,n){n.r(a),n.d(a,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"alphago/explained/policy-network","title":"Policy Network en detalle","description":"Comprensi\xf3n profunda de la arquitectura de la red de pol\xedticas de AlphaGo, m\xe9todos de entrenamiento y aplicaciones pr\xe1cticas, desde 13 capas convolucionales hasta la salida Softmax","source":"@site/i18n/es/docusaurus-plugin-content-docs/current/alphago/explained/07-policy-network.mdx","sourceDirName":"alphago/explained","slug":"/alphago/explained/policy-network","permalink":"/es/docs/alphago/explained/policy-network","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/explained/07-policy-network.mdx","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"title":"Policy Network en detalle","description":"Comprensi\xf3n profunda de la arquitectura de la red de pol\xedticas de AlphaGo, m\xe9todos de entrenamiento y aplicaciones pr\xe1cticas, desde 13 capas convolucionales hasta la salida Softmax"},"sidebar":"tutorialSidebar","previous":{"title":"Representacion del Estado del Tablero","permalink":"/es/docs/alphago/explained/board-representation"},"next":{"title":"Value Network en detalle","permalink":"/es/docs/alphago/explained/value-network"}}');var r=n(62615),s=n(30416),t=n(45695);const l={sidebar_position:8,title:"Policy Network en detalle",description:"Comprensi\xf3n profunda de la arquitectura de la red de pol\xedticas de AlphaGo, m\xe9todos de entrenamiento y aplicaciones pr\xe1cticas, desde 13 capas convolucionales hasta la salida Softmax"},o="Policy Network en detalle",d={},c=[{value:"\xbfQu\xe9 es la Policy Network?",id:"qu\xe9-es-la-policy-network",level:2},{value:"Funci\xf3n principal",id:"funci\xf3n-principal",level:3},{value:"Comprensi\xf3n intuitiva",id:"comprensi\xf3n-intuitiva",level:3},{value:"\xbfPor qu\xe9 se necesita la Policy Network?",id:"por-qu\xe9-se-necesita-la-policy-network",level:3},{value:"Arquitectura de la red",id:"arquitectura-de-la-red",level:2},{value:"Estructura general",id:"estructura-general",level:3},{value:"Capa de entrada",id:"capa-de-entrada",level:3},{value:"Capas convolucionales",id:"capas-convolucionales",level:3},{value:"\xbfPor qu\xe9 192 filtros?",id:"por-qu\xe9-192-filtros",level:4},{value:"\xbfPor qu\xe9 kernels de 3\xd73?",id:"por-qu\xe9-kernels-de-33",level:4},{value:"\xbfPor qu\xe9 la primera capa usa 5\xd75?",id:"por-qu\xe9-la-primera-capa-usa-55",level:4},{value:"Funci\xf3n de activaci\xf3n ReLU",id:"funci\xf3n-de-activaci\xf3n-relu",level:3},{value:"Capa de salida",id:"capa-de-salida",level:3},{value:"Convoluci\xf3n 1\xd71",id:"convoluci\xf3n-11",level:4},{value:"Salida Softmax",id:"salida-softmax",level:4},{value:"Cantidad de par\xe1metros",id:"cantidad-de-par\xe1metros",level:3},{value:"Objetivo y m\xe9todos de entrenamiento",id:"objetivo-y-m\xe9todos-de-entrenamiento",level:2},{value:"Datos de entrenamiento",id:"datos-de-entrenamiento",level:3},{value:"Funci\xf3n de p\xe9rdida de entrop\xeda cruzada",id:"funci\xf3n-de-p\xe9rdida-de-entrop\xeda-cruzada",level:3},{value:"Comprensi\xf3n intuitiva",id:"comprensi\xf3n-intuitiva-1",level:4},{value:"Proceso de entrenamiento",id:"proceso-de-entrenamiento",level:3},{value:"Aumento de datos",id:"aumento-de-datos",level:3},{value:"Resultados del entrenamiento",id:"resultados-del-entrenamiento",level:2},{value:"57% de precisi\xf3n",id:"57-de-precisi\xf3n",level:3},{value:"\xbfEs alta esta precisi\xf3n?",id:"es-alta-esta-precisi\xf3n",level:4},{value:"Mejora en la fuerza de juego",id:"mejora-en-la-fuerza-de-juego",level:3},{value:"\xbfPor qu\xe9 solo 57%?",id:"por-qu\xe9-solo-57",level:3},{value:"1. M\xfaltiples buenos movimientos",id:"1-m\xfaltiples-buenos-movimientos",level:4},{value:"2. Diferencias de estilo",id:"2-diferencias-de-estilo",level:4},{value:"3. Los humanos tambi\xe9n cometen errores",id:"3-los-humanos-tambi\xe9n-cometen-errores",level:4},{value:"Rol en MCTS",id:"rol-en-mcts",level:2},{value:"1. Guiar la direcci\xf3n de b\xfasqueda",id:"1-guiar-la-direcci\xf3n-de-b\xfasqueda",level:3},{value:"2. Prior para expandir nodos",id:"2-prior-para-expandir-nodos",level:3},{value:"Versi\xf3n ligera vs. versi\xf3n completa",id:"versi\xf3n-ligera-vs-versi\xf3n-completa",level:2},{value:"Versi\xf3n completa (SL Policy Network)",id:"versi\xf3n-completa-sl-policy-network",level:3},{value:"Versi\xf3n ligera (Rollout Policy Network)",id:"versi\xf3n-ligera-rollout-policy-network",level:3},{value:"\xbfPor qu\xe9 se necesita la versi\xf3n ligera?",id:"por-qu\xe9-se-necesita-la-versi\xf3n-ligera",level:3},{value:"Caracter\xedsticas de la versi\xf3n ligera",id:"caracter\xedsticas-de-la-versi\xf3n-ligera",level:3},{value:"Mejoras de AlphaGo Zero",id:"mejoras-de-alphago-zero",level:3},{value:"Ajuste fino con aprendizaje por refuerzo (RL Policy Network)",id:"ajuste-fino-con-aprendizaje-por-refuerzo-rl-policy-network",level:2},{value:"Limitaciones del aprendizaje supervisado",id:"limitaciones-del-aprendizaje-supervisado",level:3},{value:"Auto-juego con refuerzo",id:"auto-juego-con-refuerzo",level:3},{value:"Algoritmo REINFORCE",id:"algoritmo-reinforce",level:3},{value:"Resultados",id:"resultados",level:3},{value:"De &quot;imitar&quot; a &quot;innovar&quot;",id:"de-imitar-a-innovar",level:3},{value:"An\xe1lisis visual",id:"an\xe1lisis-visual",level:2},{value:"Distribuci\xf3n de probabilidad en diferentes posiciones",id:"distribuci\xf3n-de-probabilidad-en-diferentes-posiciones",level:3},{value:"Apertura (fase de fuseki)",id:"apertura-fase-de-fuseki",level:4},{value:"Posici\xf3n de combate",id:"posici\xf3n-de-combate",level:4},{value:"Fase final (yose)",id:"fase-final-yose",level:4},{value:"\xbfQu\xe9 aprenden las capas ocultas?",id:"qu\xe9-aprenden-las-capas-ocultas",level:3},{value:"Puntos clave de implementaci\xf3n",id:"puntos-clave-de-implementaci\xf3n",level:2},{value:"Implementaci\xf3n en PyTorch",id:"implementaci\xf3n-en-pytorch",level:3},{value:"Ciclo de entrenamiento",id:"ciclo-de-entrenamiento",level:3},{value:"Consideraciones para inferencia",id:"consideraciones-para-inferencia",level:3},{value:"Correspondencia de animaciones",id:"correspondencia-de-animaciones",level:2},{value:"Lecturas adicionales",id:"lecturas-adicionales",level:2},{value:"Puntos clave",id:"puntos-clave",level:2},{value:"Referencias",id:"referencias",level:2}];function p(e){const a={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"policy-network-en-detalle",children:"Policy Network en detalle"})}),"\n",(0,r.jsx)(a.p,{children:"En cualquier posici\xf3n de Go, hay un promedio de 250 movimientos legales. Si la computadora elige al azar, nunca podr\xe1 jugar bien."}),"\n",(0,r.jsx)(a.p,{children:'El avance de AlphaGo fue: aprendi\xf3 a "mirar el tablero y saber qu\xe9 posiciones vale la pena considerar".'}),"\n",(0,r.jsxs)(a.p,{children:["Esta capacidad proviene de la ",(0,r.jsx)(a.strong,{children:"Policy Network (red de pol\xedticas)"}),"."]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"qu\xe9-es-la-policy-network",children:"\xbfQu\xe9 es la Policy Network?"}),"\n",(0,r.jsx)(a.h3,{id:"funci\xf3n-principal",children:"Funci\xf3n principal"}),"\n",(0,r.jsx)(a.p,{children:"La Policy Network es una red neuronal convolucional profunda cuya tarea es:"}),"\n",(0,r.jsxs)(a.blockquote,{children:["\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Dado el estado actual del tablero, generar la probabilidad de jugar en cada posici\xf3n"})}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Expresado matem\xe1ticamente:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"p = f_\u03b8(s)\n"})}),"\n",(0,r.jsx)(a.p,{children:"Donde:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.code,{children:"s"}),": estado actual del tablero (tablero de 19\xd719 + otras caracter\xedsticas)"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.code,{children:"f_\u03b8"}),": Policy Network (\u03b8 son los par\xe1metros de la red)"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.code,{children:"p"}),": distribuci\xf3n de probabilidad sobre 361 posiciones (incluyendo pasar)"]}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"comprensi\xf3n-intuitiva",children:"Comprensi\xf3n intuitiva"}),"\n",(0,r.jsx)(a.p,{children:'Imagina que eres un jugador profesional. Cuando ves una posici\xf3n, tu cerebro autom\xe1ticamente "ilumina" varias posiciones importantes \u2014 estas son las que intuitivamente consideras que vale la pena analizar.'}),"\n",(0,r.jsx)(a.p,{children:"La Policy Network simula este proceso."}),"\n",(0,r.jsx)(t.dW,{initialPosition:"corner",size:400}),"\n",(0,r.jsx)(a.p,{children:"El mapa de calor anterior muestra la salida de la Policy Network. Cuanto m\xe1s brillante es el color de una posici\xf3n, m\xe1s valiosa la considera el modelo."}),"\n",(0,r.jsx)(a.h3,{id:"por-qu\xe9-se-necesita-la-policy-network",children:"\xbfPor qu\xe9 se necesita la Policy Network?"}),"\n",(0,r.jsx)(a.p,{children:"El espacio de b\xfasqueda en Go es demasiado grande. Si se buscan todos los movimientos posibles sin filtrar:"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Estrategia"}),(0,r.jsx)(a.th,{children:"Movimientos por turno"}),(0,r.jsx)(a.th,{children:"Nodos al buscar 10 movimientos"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Considerar todos"}),(0,r.jsx)(a.td,{children:"361"}),(0,r.jsx)(a.td,{children:"361^10 \u2248 10^25"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Filtrado por Policy Network"}),(0,r.jsx)(a.td,{children:"~20"}),(0,r.jsx)(a.td,{children:"20^10 \u2248 10^13"})]})]})]}),"\n",(0,r.jsxs)(a.p,{children:["La Policy Network reduce el espacio de b\xfasqueda en ",(0,r.jsx)(a.strong,{children:"10^12 veces"})," (un bill\xf3n de veces)."]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"arquitectura-de-la-red",children:"Arquitectura de la red"}),"\n",(0,r.jsx)(a.h3,{id:"estructura-general",children:"Estructura general"}),"\n",(0,r.jsx)(a.p,{children:"La Policy Network de AlphaGo utiliza una arquitectura de red neuronal convolucional profunda (CNN):"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Capa de entrada \u2192 Capas convolucionales \xd712 \u2192 Capa convolucional de salida \u2192 Softmax\n       \u2193                 \u2193                            \u2193                    \u2193\n   19\xd719\xd748          19\xd719\xd7192                     19\xd719\xd71            362 probabilidades\n"})}),"\n",(0,r.jsx)(a.h3,{id:"capa-de-entrada",children:"Capa de entrada"}),"\n",(0,r.jsxs)(a.p,{children:["La entrada es un tensor de caracter\xedsticas de ",(0,r.jsx)(a.strong,{children:"19\xd719\xd748"}),":"]}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"19\xd719"}),": tama\xf1o del tablero"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"48"}),": 48 planos de caracter\xedsticas (ver ",(0,r.jsx)(a.a,{href:"../input-features",children:"Dise\xf1o de caracter\xedsticas de entrada"}),")"]}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Estos 48 planos incluyen:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Posiciones de piedras negras y blancas"}),"\n",(0,r.jsx)(a.li,{children:"Historial de los \xfaltimos 8 movimientos"}),"\n",(0,r.jsx)(a.li,{children:"Libertades, atari, escalera y otras caracter\xedsticas"}),"\n",(0,r.jsx)(a.li,{children:"Legalidad (qu\xe9 posiciones son jugables)"}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"capas-convolucionales",children:"Capas convolucionales"}),"\n",(0,r.jsxs)(a.p,{children:["La red contiene ",(0,r.jsx)(a.strong,{children:"12 capas convolucionales"}),", cada una con la siguiente configuraci\xf3n:"]}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Par\xe1metro"}),(0,r.jsx)(a.th,{children:"Valor"}),(0,r.jsx)(a.th,{children:"Descripci\xf3n"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"N\xfamero de filtros"}),(0,r.jsx)(a.td,{children:"192"}),(0,r.jsx)(a.td,{children:"Cada capa genera 192 mapas de caracter\xedsticas"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Tama\xf1o del kernel"}),(0,r.jsx)(a.td,{children:"3\xd73 (primera capa 5\xd75)"}),(0,r.jsx)(a.td,{children:"Cada vez observa un \xe1rea de 3\xd73"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Tipo de padding"}),(0,r.jsx)(a.td,{children:"same"}),(0,r.jsx)(a.td,{children:"Mantiene el tama\xf1o 19\xd719"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Funci\xf3n de activaci\xf3n"}),(0,r.jsx)(a.td,{children:"ReLU"}),(0,r.jsx)(a.td,{children:"max(0, x)"})]})]})]}),"\n",(0,r.jsx)(a.h4,{id:"por-qu\xe9-192-filtros",children:"\xbfPor qu\xe9 192 filtros?"}),"\n",(0,r.jsx)(a.p,{children:"Es un valor emp\xedrico. Muy pocos limitar\xedan la capacidad del modelo, demasiados aumentar\xedan el c\xe1lculo y el riesgo de sobreajuste. El equipo de DeepMind determin\xf3 mediante experimentos que 192 es un buen equilibrio."}),"\n",(0,r.jsx)(a.h4,{id:"por-qu\xe9-kernels-de-33",children:"\xbfPor qu\xe9 kernels de 3\xd73?"}),"\n",(0,r.jsx)(a.p,{children:"3\xd73 es el tama\xf1o m\xe1s com\xfan en redes neuronales convolucionales por estas razones:"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Suficiente para capturar patrones locales"}),": ojos, conexiones, cortes en Go est\xe1n dentro del rango 3\xd73"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Alta eficiencia computacional"}),": menos par\xe1metros que kernels grandes"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Apilables"}),": m\xfaltiples convoluciones 3\xd73 pueden lograr un campo receptivo grande"]}),"\n"]}),"\n",(0,r.jsx)(a.h4,{id:"por-qu\xe9-la-primera-capa-usa-55",children:"\xbfPor qu\xe9 la primera capa usa 5\xd75?"}),"\n",(0,r.jsx)(a.p,{children:"La primera capa usa un kernel de 5\xd75 m\xe1s grande para capturar patrones de mayor alcance desde la entrada (como saltos peque\xf1os). Esta es una decisi\xf3n de dise\xf1o; el posterior AlphaGo Zero usa uniformemente 3\xd73."}),"\n",(0,r.jsx)(a.h3,{id:"funci\xf3n-de-activaci\xf3n-relu",children:"Funci\xf3n de activaci\xf3n ReLU"}),"\n",(0,r.jsx)(a.p,{children:"Cada capa convolucional va seguida de la funci\xf3n de activaci\xf3n ReLU (Rectified Linear Unit):"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"ReLU(x) = max(0, x)\n"})}),"\n",(0,r.jsx)(a.p,{children:"\xbfPor qu\xe9 usar ReLU?"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"C\xe1lculo simple"}),": solo tomar el m\xe1ximo, mucho m\xe1s r\xe1pido que sigmoid"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Mitiga el desvanecimiento del gradiente"}),": gradiente constante de 1 en la regi\xf3n positiva"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Activaci\xf3n dispersa"}),": los valores negativos se reducen a cero, produciendo representaciones dispersas"]}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"capa-de-salida",children:"Capa de salida"}),"\n",(0,r.jsx)(a.p,{children:"La \xfaltima capa es una capa convolucional especial:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"19\xd719\xd7192 \u2192 Convoluci\xf3n(1\xd71, 1 filtro) \u2192 19\xd719\xd71 \u2192 Aplanar \u2192 Vector 362-dim \u2192 Softmax\n"})}),"\n",(0,r.jsx)(a.h4,{id:"convoluci\xf3n-11",children:"Convoluci\xf3n 1\xd71"}),"\n",(0,r.jsx)(a.p,{children:"La capa de salida usa convoluci\xf3n 1\xd71, comprimiendo 192 canales a 1. Esto equivale a hacer una combinaci\xf3n lineal de las 192 caracter\xedsticas dimensionales para cada posici\xf3n."}),"\n",(0,r.jsx)(a.h4,{id:"salida-softmax",children:"Salida Softmax"}),"\n",(0,r.jsx)(a.p,{children:"El vector de 362 dimensiones (361 posiciones del tablero + 1 para pasar) pasa por la funci\xf3n Softmax:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Softmax(z_i) = exp(z_i) / \u03a3_j exp(z_j)\n"})}),"\n",(0,r.jsx)(a.p,{children:"Softmax asegura que la salida sea una distribuci\xf3n de probabilidad v\xe1lida:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Todos los valores est\xe1n entre 0 y 1"}),"\n",(0,r.jsx)(a.li,{children:"La suma de todos los valores es 1"}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"cantidad-de-par\xe1metros",children:"Cantidad de par\xe1metros"}),"\n",(0,r.jsx)(a.p,{children:"Calculemos la cantidad total de par\xe1metros de la red:"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Capa"}),(0,r.jsx)(a.th,{children:"C\xe1lculo"}),(0,r.jsx)(a.th,{children:"Cantidad de par\xe1metros"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Primera capa conv"}),(0,r.jsx)(a.td,{children:"5\xd75\xd748\xd7192 + 192"}),(0,r.jsx)(a.td,{children:"230,592"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Capas conv intermedias \xd711"}),(0,r.jsx)(a.td,{children:"(3\xd73\xd7192\xd7192 + 192) \xd7 11"}),(0,r.jsx)(a.td,{children:"3,633,792"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Capa conv de salida"}),(0,r.jsx)(a.td,{children:"1\xd71\xd7192\xd71 + 1"}),(0,r.jsx)(a.td,{children:"193"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"Total"})}),(0,r.jsx)(a.td,{}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"~3.9M"})})]})]})]}),"\n",(0,r.jsxs)(a.p,{children:["Aproximadamente ",(0,r.jsx)(a.strong,{children:"3.9 millones de par\xe1metros"}),", una red peque\xf1a seg\xfan los est\xe1ndares actuales."]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"objetivo-y-m\xe9todos-de-entrenamiento",children:"Objetivo y m\xe9todos de entrenamiento"}),"\n",(0,r.jsx)(a.h3,{id:"datos-de-entrenamiento",children:"Datos de entrenamiento"}),"\n",(0,r.jsxs)(a.p,{children:["La Policy Network utiliza ",(0,r.jsx)(a.strong,{children:"aprendizaje supervisado"}),", aprendiendo de partidas humanas."]}),"\n",(0,r.jsx)(a.p,{children:"Fuentes de datos:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"KGS Go Server"}),": partidas de jugadores aficionados y profesionales"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Aproximadamente 30 millones de posiciones"}),": muestreadas de 160,000 partidas"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Etiquetas"}),": el siguiente movimiento humano correspondiente a cada posici\xf3n"]}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"funci\xf3n-de-p\xe9rdida-de-entrop\xeda-cruzada",children:"Funci\xf3n de p\xe9rdida de entrop\xeda cruzada"}),"\n",(0,r.jsx)(a.p,{children:"El objetivo de entrenamiento es maximizar la probabilidad de predecir los movimientos humanos. Usando la funci\xf3n de p\xe9rdida de entrop\xeda cruzada:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"L(\u03b8) = -\u03a3 log p_\u03b8(a | s)\n"})}),"\n",(0,r.jsx)(a.p,{children:"Donde:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.code,{children:"s"}),": estado del tablero"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.code,{children:"a"}),": posici\xf3n donde jug\xf3 el humano"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.code,{children:"p_\u03b8(a | s)"}),": probabilidad que el modelo predice para esa posici\xf3n"]}),"\n"]}),"\n",(0,r.jsx)(a.h4,{id:"comprensi\xf3n-intuitiva-1",children:"Comprensi\xf3n intuitiva"}),"\n",(0,r.jsx)(a.p,{children:"La p\xe9rdida de entrop\xeda cruzada tiene un significado simple:"}),"\n",(0,r.jsxs)(a.blockquote,{children:["\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Cuanto mayor sea la probabilidad que el modelo predice para la posici\xf3n correcta, menor ser\xe1 la p\xe9rdida"})}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Si el humano jug\xf3 en K10, y el modelo da a K10 una probabilidad de:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"0.9 \u2192 p\xe9rdida = -log(0.9) \u2248 0.1 (muy baja, bien)"}),"\n",(0,r.jsx)(a.li,{children:"0.1 \u2192 p\xe9rdida = -log(0.1) \u2248 2.3 (alta, mal)"}),"\n",(0,r.jsx)(a.li,{children:"0.01 \u2192 p\xe9rdida = -log(0.01) \u2248 4.6 (muy alta, muy mal)"}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"proceso-de-entrenamiento",children:"Proceso de entrenamiento"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# Pseudoc\xf3digo\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        states, actions = batch\n\n        # Propagaci\xf3n hacia adelante\n        policy = network(states)  # vector de probabilidad de 361 dim\n\n        # Calcular p\xe9rdida (entrop\xeda cruzada)\n        loss = cross_entropy(policy, actions)\n\n        # Retropropagaci\xf3n\n        loss.backward()\n        optimizer.step()\n"})}),"\n",(0,r.jsx)(a.p,{children:"Detalles de entrenamiento:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Optimizador"}),": SGD con momentum"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Tasa de aprendizaje"}),": inicial 0.003, decreciente gradualmente"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Tama\xf1o de lote"}),": 16"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Tiempo de entrenamiento"}),": aproximadamente 3 semanas (50 GPUs)"]}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"aumento-de-datos",children:"Aumento de datos"}),"\n",(0,r.jsx)(a.p,{children:"El tablero de Go tiene 8 simetr\xedas (4 rotaciones \xd7 2 reflexiones). Cada muestra de entrenamiento puede transformarse en 8 muestras equivalentes:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Original \u2192 Rotar 90\xb0 \u2192 Rotar 180\xb0 \u2192 Rotar 270\xb0\n   \u2193           \u2193           \u2193            \u2193\nVolteo horizontal \u2192 ...\n"})}),"\n",(0,r.jsx)(a.p,{children:"Esto aumenta los datos de entrenamiento efectivos 8 veces y asegura que los patrones aprendidos no dependan de la orientaci\xf3n."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"resultados-del-entrenamiento",children:"Resultados del entrenamiento"}),"\n",(0,r.jsx)(a.h3,{id:"57-de-precisi\xf3n",children:"57% de precisi\xf3n"}),"\n",(0,r.jsxs)(a.p,{children:["Despu\xe9s del entrenamiento, la Policy Network alcanz\xf3 una ",(0,r.jsx)(a.strong,{children:"precisi\xf3n top-1 del 57%"}),"."]}),"\n",(0,r.jsx)(a.p,{children:"Esto significa: dada cualquier posici\xf3n, el modelo tiene un 57% de probabilidad de predecir exactamente el movimiento que jug\xf3 el experto humano."}),"\n",(0,r.jsx)(a.h4,{id:"es-alta-esta-precisi\xf3n",children:"\xbfEs alta esta precisi\xf3n?"}),"\n",(0,r.jsx)(a.p,{children:"Considerando que cada posici\xf3n tiene en promedio 250 movimientos legales, adivinar al azar tendr\xeda solo 0.4% de precisi\xf3n."}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"M\xe9todo"}),(0,r.jsx)(a.th,{children:"Precisi\xf3n Top-1"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Adivinanza aleatoria"}),(0,r.jsx)(a.td,{children:"0.4%"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Mejor programa de Go anterior"}),(0,r.jsx)(a.td,{children:"~44%"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Policy Network de AlphaGo"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"57%"})})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"Un aumento de 13 puntos porcentuales puede parecer poco, pero es muy significativo."}),"\n",(0,r.jsx)(a.h3,{id:"mejora-en-la-fuerza-de-juego",children:"Mejora en la fuerza de juego"}),"\n",(0,r.jsx)(a.p,{children:"\xbfQu\xe9 nivel de juego se puede alcanzar usando solo la Policy Network (sin b\xfasqueda)?"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Configuraci\xf3n"}),(0,r.jsx)(a.th,{children:"Puntuaci\xf3n Elo"}),(0,r.jsx)(a.th,{children:"Nivel aproximado"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Mejor programa anterior (Pachi)"}),(0,r.jsx)(a.td,{children:"2,500"}),(0,r.jsx)(a.td,{children:"4-5 dan amateur"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Solo Policy Network"}),(0,r.jsx)(a.td,{children:"2,800"}),(0,r.jsx)(a.td,{children:"6-7 dan amateur"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"+ MCTS 1600 simulaciones"}),(0,r.jsx)(a.td,{children:"3,200+"}),(0,r.jsx)(a.td,{children:"Nivel profesional"})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"La Policy Network sola ya alcanza nivel alto amateur, y con MCTS salta a nivel profesional."}),"\n",(0,r.jsx)(a.h3,{id:"por-qu\xe9-solo-57",children:"\xbfPor qu\xe9 solo 57%?"}),"\n",(0,r.jsx)(a.p,{children:"Las partidas humanas tienen las siguientes caracter\xedsticas que limitan la precisi\xf3n:"}),"\n",(0,r.jsx)(a.h4,{id:"1-m\xfaltiples-buenos-movimientos",children:"1. M\xfaltiples buenos movimientos"}),"\n",(0,r.jsx)(a.p,{children:'Muchas posiciones tienen varios buenos movimientos. Por ejemplo, "acercarse a la esquina" y "defender la esquina" pueden ser ambas opciones correctas. Si el modelo elige otro buen movimiento, se cuenta como "error".'}),"\n",(0,r.jsx)(a.h4,{id:"2-diferencias-de-estilo",children:"2. Diferencias de estilo"}),"\n",(0,r.jsx)(a.p,{children:'Diferentes jugadores tienen diferentes estilos. Jugadores agresivos y jugadores s\xf3lidos pueden jugar diferente en la misma posici\xf3n. El modelo aprende un estilo "promedio".'}),"\n",(0,r.jsx)(a.h4,{id:"3-los-humanos-tambi\xe9n-cometen-errores",children:"3. Los humanos tambi\xe9n cometen errores"}),"\n",(0,r.jsx)(a.p,{children:'Los datos de KGS incluyen partidas de jugadores amateur, cuyas elecciones no siempre son \xf3ptimas. Es normal que el modelo aprenda algunos "errores".'}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"rol-en-mcts",children:"Rol en MCTS"}),"\n",(0,r.jsx)(a.p,{children:"La Policy Network desempe\xf1a dos roles clave en el MCTS de AlphaGo:"}),"\n",(0,r.jsx)(a.h3,{id:"1-guiar-la-direcci\xf3n-de-b\xfasqueda",children:"1. Guiar la direcci\xf3n de b\xfasqueda"}),"\n",(0,r.jsxs)(a.p,{children:["En la fase de ",(0,r.jsx)(a.strong,{children:"Selecci\xf3n"})," de MCTS, la salida de la Policy Network se usa para calcular UCB (Upper Confidence Bound):"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"UCB(s, a) = Q(s, a) + c_puct \xd7 P(s, a) \xd7 \u221a(N(s)) / (1 + N(s, a))\n"})}),"\n",(0,r.jsxs)(a.p,{children:["Donde ",(0,r.jsx)(a.code,{children:"P(s, a)"})," es la probabilidad dada por la Policy Network."]}),"\n",(0,r.jsx)(a.p,{children:"Esto significa:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.strong,{children:"Los movimientos de alta probabilidad se exploran primero"})}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Los movimientos de baja probabilidad tambi\xe9n tienen oportunidad de ser explorados"})," (debido al t\xe9rmino de exploraci\xf3n)"]}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"2-prior-para-expandir-nodos",children:"2. Prior para expandir nodos"}),"\n",(0,r.jsxs)(a.p,{children:["Cuando MCTS expande un nuevo nodo, la Policy Network proporciona las ",(0,r.jsx)(a.strong,{children:"probabilidades a priori"})," de todos los nodos hijos."]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Expandir nodo s:\n  for each action a:\n    child = Node()\n    child.prior = policy_network(s)[a]  # probabilidad a priori\n    child.value = 0\n    child.visits = 0\n"})}),"\n",(0,r.jsx)(a.p,{children:'Estas probabilidades a priori permiten que MCTS "sepa" qu\xe9 nodos hijos vale la pena explorar, incluso antes de visitarlos.'}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"versi\xf3n-ligera-vs-versi\xf3n-completa",children:"Versi\xf3n ligera vs. versi\xf3n completa"}),"\n",(0,r.jsx)(a.p,{children:"AlphaGo en realidad tiene dos Policy Networks:"}),"\n",(0,r.jsx)(a.h3,{id:"versi\xf3n-completa-sl-policy-network",children:"Versi\xf3n completa (SL Policy Network)"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Arquitectura"}),": CNN de 13 capas, 192 filtros"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Precisi\xf3n"}),": 57%"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Tiempo de inferencia"}),": aproximadamente 3 ms/posici\xf3n"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Uso"}),": Selecci\xf3n y Expansi\xf3n en MCTS"]}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"versi\xf3n-ligera-rollout-policy-network",children:"Versi\xf3n ligera (Rollout Policy Network)"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Arquitectura"}),": modelo lineal + caracter\xedsticas manuales"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Precisi\xf3n"}),": 24%"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Tiempo de inferencia"}),": aproximadamente 2 \u03bcs/posici\xf3n (1500 veces m\xe1s r\xe1pido)"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Uso"}),": simulaciones r\xe1pidas (rollout)"]}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"por-qu\xe9-se-necesita-la-versi\xf3n-ligera",children:"\xbfPor qu\xe9 se necesita la versi\xf3n ligera?"}),"\n",(0,r.jsxs)(a.p,{children:["En la fase de ",(0,r.jsx)(a.strong,{children:"Simulaci\xf3n"})," de MCTS, se necesita jugar desde el nodo actual hasta el final del juego, posiblemente 100+ movimientos. Si cada movimiento usa la Policy Network completa, es demasiado lento."]}),"\n",(0,r.jsx)(a.p,{children:"La versi\xf3n ligera tiene solo 24% de precisi\xf3n, pero es 1500 veces m\xe1s r\xe1pida. En rollouts, la velocidad importa m\xe1s que la precisi\xf3n."}),"\n",(0,r.jsx)(a.h3,{id:"caracter\xedsticas-de-la-versi\xf3n-ligera",children:"Caracter\xedsticas de la versi\xf3n ligera"}),"\n",(0,r.jsx)(a.p,{children:"La versi\xf3n ligera usa caracter\xedsticas dise\xf1adas manualmente, incluyendo:"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Tipo de caracter\xedstica"}),(0,r.jsx)(a.th,{children:"Ejemplo"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Patrones locales"}),(0,r.jsx)(a.td,{children:"Configuraci\xf3n de piedras en \xe1rea 3\xd73"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Caracter\xedsticas globales"}),(0,r.jsx)(a.td,{children:"Si est\xe1 en esquina/borde, puntos grandes"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Caracter\xedsticas t\xe1cticas"}),(0,r.jsx)(a.td,{children:"Atari, escalera, conexi\xf3n"})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"Estas caracter\xedsticas se introducen en un modelo lineal (sin capas ocultas), con c\xe1lculo extremadamente r\xe1pido."}),"\n",(0,r.jsx)(a.h3,{id:"mejoras-de-alphago-zero",children:"Mejoras de AlphaGo Zero"}),"\n",(0,r.jsx)(a.p,{children:"El posterior AlphaGo Zero abandon\xf3 completamente la versi\xf3n ligera y los rollouts. Eval\xfaa directamente los nodos hoja con la Value Network, sin necesidad de simulaciones r\xe1pidas. Esta fue una simplificaci\xf3n importante."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"ajuste-fino-con-aprendizaje-por-refuerzo-rl-policy-network",children:"Ajuste fino con aprendizaje por refuerzo (RL Policy Network)"}),"\n",(0,r.jsx)(a.h3,{id:"limitaciones-del-aprendizaje-supervisado",children:"Limitaciones del aprendizaje supervisado"}),"\n",(0,r.jsx)(a.p,{children:"La Policy Network entrenada con aprendizaje supervisado tiene un problema fundamental:"}),"\n",(0,r.jsxs)(a.blockquote,{children:["\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:'Aprende a "imitar humanos", no a "ganar"'})}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Esto significa que aprende malos h\xe1bitos de los humanos y tambi\xe9n tiene mal desempe\xf1o en posiciones que los humanos nunca encontraron."}),"\n",(0,r.jsx)(a.h3,{id:"auto-juego-con-refuerzo",children:"Auto-juego con refuerzo"}),"\n",(0,r.jsxs)(a.p,{children:["La soluci\xf3n de DeepMind fue usar el m\xe9todo de ",(0,r.jsx)(a.strong,{children:"gradiente de pol\xedticas"})," (Policy Gradient) para aprendizaje por refuerzo:"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"1. Hacer que Policy Network juegue contra s\xed misma\n2. Registrar todos los movimientos de cada partida\n3. Ajustar par\xe1metros seg\xfan victoria/derrota:\n   - Victoria \u2192 aumentar probabilidad de estos movimientos\n   - Derrota \u2192 disminuir probabilidad de estos movimientos\n"})}),"\n",(0,r.jsx)(a.h3,{id:"algoritmo-reinforce",children:"Algoritmo REINFORCE"}),"\n",(0,r.jsx)(a.p,{children:"Se usa espec\xedficamente el algoritmo REINFORCE:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"\u2207J(\u03b8) = E[\u03a3_t \u2207log \u03c0_\u03b8(a_t | s_t) \xd7 z]\n"})}),"\n",(0,r.jsx)(a.p,{children:"Donde:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.code,{children:"z"}),": resultado de la partida (+1 victoria, -1 derrota)"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.code,{children:"\u03c0_\u03b8(a_t | s_t)"}),": probabilidad de elegir la acci\xf3n ",(0,r.jsx)(a.code,{children:"a_t"})," en el estado ",(0,r.jsx)(a.code,{children:"s_t"})]}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"resultados",children:"Resultados"}),"\n",(0,r.jsx)(a.p,{children:"Despu\xe9s de aproximadamente 1 d\xeda de entrenamiento de auto-juego (1.28 millones de partidas), la RL Policy Network:"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"M\xe9trica"}),(0,r.jsx)(a.th,{children:"SL Policy"}),(0,r.jsx)(a.th,{children:"RL Policy"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Contra SL Policy"}),(0,r.jsx)(a.td,{children:"50%"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"80%"})})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Mejora Elo"}),(0,r.jsx)(a.td,{children:"-"}),(0,r.jsx)(a.td,{children:"+100"})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"La precisi\xf3n puede disminuir ligeramente (porque ya no imita completamente a los humanos), pero la tasa de victoria real mejora significativamente."}),"\n",(0,r.jsx)(a.h3,{id:"de-imitar-a-innovar",children:'De "imitar" a "innovar"'}),"\n",(0,r.jsx)(a.p,{children:"El aprendizaje por refuerzo permiti\xf3 que la Policy Network aprendiera algunos movimientos que los humanos nunca hab\xedan pensado. Estos movimientos nunca aparecieron en los datos de entrenamiento, pero son efectivos."}),"\n",(0,r.jsx)(a.p,{children:'Por eso AlphaGo puede hacer "movimientos divinos" \u2014 no est\xe1 limitado por la experiencia humana.'}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"an\xe1lisis-visual",children:"An\xe1lisis visual"}),"\n",(0,r.jsx)(a.h3,{id:"distribuci\xf3n-de-probabilidad-en-diferentes-posiciones",children:"Distribuci\xf3n de probabilidad en diferentes posiciones"}),"\n",(0,r.jsx)(a.p,{children:"Veamos la salida de la Policy Network en diferentes posiciones:"}),"\n",(0,r.jsx)(a.h4,{id:"apertura-fase-de-fuseki",children:"Apertura (fase de fuseki)"}),"\n",(0,r.jsx)(t.dW,{initialPosition:"opening",size:400}),"\n",(0,r.jsx)(a.p,{children:"En la apertura, la probabilidad se concentra principalmente en:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Esquinas (ocupar esquinas)"}),"\n",(0,r.jsx)(a.li,{children:"Bordes (acercamiento, defensa de esquina)"}),"\n",(0,r.jsx)(a.li,{children:'Posiciones de "puntos grandes"'}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Esto coincide con los principios b\xe1sicos de Go: esquina de oro, borde de plata, centro de hierba."}),"\n",(0,r.jsx)(a.h4,{id:"posici\xf3n-de-combate",children:"Posici\xf3n de combate"}),"\n",(0,r.jsx)(t.dW,{initialPosition:"fighting",size:400}),"\n",(0,r.jsx)(a.p,{children:"Durante el combate, la probabilidad se concentra en:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Puntos de corte cr\xedticos"}),"\n",(0,r.jsx)(a.li,{children:"Atari, conexiones"}),"\n",(0,r.jsx)(a.li,{children:"Hacer ojos, destruir ojos"}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Esto muestra que el modelo aprendi\xf3 t\xe1cticas locales."}),"\n",(0,r.jsx)(a.h4,{id:"fase-final-yose",children:"Fase final (yose)"}),"\n",(0,r.jsx)(t.dW,{initialPosition:"endgame",size:400}),"\n",(0,r.jsx)(a.p,{children:"En la fase final, la probabilidad se dispersa en varios puntos de yose, requiriendo c\xe1lculo preciso de territorio."}),"\n",(0,r.jsx)(a.h3,{id:"qu\xe9-aprenden-las-capas-ocultas",children:"\xbfQu\xe9 aprenden las capas ocultas?"}),"\n",(0,r.jsx)(a.p,{children:'Visualizando la salida de las capas convolucionales, podemos ver las "caracter\xedsticas" que el modelo aprendi\xf3:'}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Capas bajas"}),": formas b\xe1sicas (ojos, puntos de corte)"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Capas medias"}),": patrones t\xe1cticos (atari, escalera)"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Capas altas"}),": conceptos globales (influencia, grosor)"]}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Esto es muy similar a la estructura jer\xe1rquica de c\xf3mo los humanos entienden el Go."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"puntos-clave-de-implementaci\xf3n",children:"Puntos clave de implementaci\xf3n"}),"\n",(0,r.jsx)(a.h3,{id:"implementaci\xf3n-en-pytorch",children:"Implementaci\xf3n en PyTorch"}),"\n",(0,r.jsx)(a.p,{children:"Esta es una implementaci\xf3n simplificada de la Policy Network:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PolicyNetwork(nn.Module):\n    def __init__(self, input_channels=48, num_filters=192, num_layers=12):\n        super().__init__()\n\n        # Primera capa convolucional (5\xd75)\n        self.conv1 = nn.Conv2d(input_channels, num_filters,\n                               kernel_size=5, padding=2)\n\n        # Capas convolucionales intermedias (3\xd73) \xd711\n        self.conv_layers = nn.ModuleList([\n            nn.Conv2d(num_filters, num_filters,\n                     kernel_size=3, padding=1)\n            for _ in range(num_layers - 1)\n        ])\n\n        # Capa convolucional de salida (1\xd71)\n        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)\n\n    def forward(self, x):\n        # x: (batch, 48, 19, 19)\n\n        # Primera capa\n        x = F.relu(self.conv1(x))\n\n        # Capas intermedias\n        for conv in self.conv_layers:\n            x = F.relu(conv(x))\n\n        # Capa de salida\n        x = self.conv_out(x)  # (batch, 1, 19, 19)\n\n        # Aplanar + Softmax\n        x = x.view(x.size(0), -1)  # (batch, 361)\n        x = F.softmax(x, dim=1)\n\n        return x\n"})}),"\n",(0,r.jsx)(a.h3,{id:"ciclo-de-entrenamiento",children:"Ciclo de entrenamiento"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'def train_step(model, optimizer, states, actions):\n    """\n    states: (batch, 48, 19, 19) - caracter\xedsticas del tablero\n    actions: (batch,) - posici\xf3n donde jug\xf3 el humano (0-360)\n    """\n    # Propagaci\xf3n hacia adelante\n    policy = model(states)  # (batch, 361)\n\n    # P\xe9rdida de entrop\xeda cruzada\n    loss = F.cross_entropy(\n        torch.log(policy + 1e-8),  # prevenir log(0)\n        actions\n    )\n\n    # Retropropagaci\xf3n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Calcular precisi\xf3n\n    predictions = policy.argmax(dim=1)\n    accuracy = (predictions == actions).float().mean()\n\n    return loss.item(), accuracy.item()\n'})}),"\n",(0,r.jsx)(a.h3,{id:"consideraciones-para-inferencia",children:"Consideraciones para inferencia"}),"\n",(0,r.jsx)(a.p,{children:"Durante el juego real, hay que tener en cuenta:"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Filtrar movimientos ilegales"}),": establecer la probabilidad de posiciones ilegales a 0, luego renormalizar"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Ajuste de temperatura"}),': se puede usar un par\xe1metro de temperatura para controlar la "nitidez" de la distribuci\xf3n de probabilidad']}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Inferencia por lotes"}),": en MCTS se pueden procesar m\xfaltiples posiciones en lote"]}),"\n"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'def get_move_probabilities(model, state, legal_moves, temperature=1.0):\n    """Obtener distribuci\xf3n de probabilidad de movimientos legales"""\n    policy = model(state)  # (361,)\n\n    # Solo mantener movimientos legales\n    mask = torch.zeros(361)\n    mask[legal_moves] = 1\n    policy = policy * mask\n\n    # Ajuste de temperatura\n    if temperature != 1.0:\n        policy = policy ** (1 / temperature)\n\n    # Renormalizar\n    policy = policy / policy.sum()\n\n    return policy\n'})}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"correspondencia-de-animaciones",children:"Correspondencia de animaciones"}),"\n",(0,r.jsx)(a.p,{children:"Los conceptos principales de este art\xedculo y los n\xfameros de animaci\xf3n correspondientes:"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"N\xfamero"}),(0,r.jsx)(a.th,{children:"Concepto"}),(0,r.jsx)(a.th,{children:"Correspondencia f\xedsica/matem\xe1tica"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"E1"}),(0,r.jsx)(a.td,{children:"Policy Network"}),(0,r.jsx)(a.td,{children:"Campo de probabilidad"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"D9"}),(0,r.jsx)(a.td,{children:"Extracci\xf3n de caracter\xedsticas CNN"}),(0,r.jsx)(a.td,{children:"Respuesta de filtro"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"D3"}),(0,r.jsx)(a.td,{children:"Aprendizaje supervisado"}),(0,r.jsx)(a.td,{children:"Estimaci\xf3n de m\xe1xima verosimilitud"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"H4"}),(0,r.jsx)(a.td,{children:"Gradiente de pol\xedticas"}),(0,r.jsx)(a.td,{children:"Optimizaci\xf3n estoc\xe1stica"})]})]})]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"lecturas-adicionales",children:"Lecturas adicionales"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Siguiente art\xedculo"}),": ",(0,r.jsx)(a.a,{href:"../value-network",children:"Value Network en detalle"})," \u2014 C\xf3mo AlphaGo eval\xfaa posiciones"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Tema relacionado"}),": ",(0,r.jsx)(a.a,{href:"../input-features",children:"Dise\xf1o de caracter\xedsticas de entrada"})," \u2014 Detalles de los 48 planos de caracter\xedsticas"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Principios profundos"}),": ",(0,r.jsx)(a.a,{href:"../cnn-and-go",children:"CNN y Go"})," \u2014 Por qu\xe9 las redes neuronales convolucionales son adecuadas para tableros"]}),"\n"]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"puntos-clave",children:"Puntos clave"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Policy Network es un generador de distribuci\xf3n de probabilidad"}),": entrada tablero, salida probabilidades para 361 posiciones"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"13 capas CNN + Softmax"}),": convoluci\xf3n profunda para extraer caracter\xedsticas, Softmax para salida de probabilidad"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"57% de precisi\xf3n"}),": muy superior a programas de Go anteriores"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Dos versiones"}),": versi\xf3n completa para decisiones MCTS, versi\xf3n ligera para simulaciones r\xe1pidas"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Ajuste fino con RL"}),': evoluciona de "imitar humanos" a "buscar la victoria"']}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:'La Policy Network es la "intuici\xf3n" de AlphaGo \u2014 permite que la IA identifique r\xe1pidamente los movimientos que vale la pena considerar, como lo hacen los humanos.'}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"referencias",children:"Referencias"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:['Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." ',(0,r.jsx)(a.em,{children:"Nature"}),", 529, 484-489."]}),"\n",(0,r.jsxs)(a.li,{children:['Maddison, C. J., et al. (2014). "Move Evaluation in Go Using Deep Convolutional Neural Networks." ',(0,r.jsx)(a.em,{children:"arXiv:1412.6564"}),"."]}),"\n",(0,r.jsxs)(a.li,{children:["Sutton, R. S., & Barto, A. G. (2018). ",(0,r.jsx)(a.em,{children:"Reinforcement Learning: An Introduction"}),". MIT Press."]}),"\n",(0,r.jsxs)(a.li,{children:['LeCun, Y., Bengio, Y., & Hinton, G. (2015). "Deep learning." ',(0,r.jsx)(a.em,{children:"Nature"}),", 521, 436-444."]}),"\n"]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},42948(e,a,n){n.d(a,{A:()=>s});n(59471);var i=n(61785),r=n(62615);function s({children:e,fallback:a}){return(0,i.A)()?(0,r.jsx)(r.Fragment,{children:e?.()}):a??null}},45695(e,a,n){n.d(a,{$W:()=>N,tO:()=>o,u8:()=>v,dW:()=>x});var i=n(59471),r=n(90989),s=n(62615);const t=19,l=[[3,3],[3,9],[3,15],[9,3],[9,9],[9,15],[15,3],[15,9],[15,15]];function o({size:e=400,stones:a=[],highlights:n=[],labels:o=[],onCellClick:d=null,showCoordinates:c=!0}){const p=(0,i.useRef)(null),h=c?30:15,u=e-2*h,x=u/18;return(0,i.useEffect)(()=>{if(!p.current)return;const e=r.Ltv(p.current);e.selectAll("*").remove();const i=e.append("g").attr("transform",`translate(${h}, ${h})`);i.append("rect").attr("x",-x/2).attr("y",-x/2).attr("width",u+x).attr("height",u+x).attr("fill","#dcb35c").attr("rx",4);const s=i.append("g").attr("class","grid");for(let a=0;a<t;a++)s.append("line").attr("class","grid-line").attr("x1",0).attr("y1",a*x).attr("x2",18*x).attr("y2",a*x);for(let a=0;a<t;a++)s.append("line").attr("class","grid-line").attr("x1",a*x).attr("y1",0).attr("x2",a*x).attr("y2",18*x);const m=i.append("g").attr("class","star-points");if(l.forEach(([e,a])=>{m.append("circle").attr("class","star-point").attr("cx",e*x).attr("cy",a*x).attr("r",x/8)}),n.length>0){const e=i.append("g").attr("class","highlights");n.forEach(({x:a,y:n,intensity:i})=>{e.append("rect").attr("class","heatmap-cell").attr("x",a*x-x/2).attr("y",n*x-x/2).attr("width",x).attr("height",x).attr("fill",r.Q3(i)).attr("opacity",.7*i)})}const j=i.append("g").attr("class","stones");if(a.forEach(({x:e,y:a,color:n})=>{const i="black"===n?"stone-black":"stone-white";j.append("circle").attr("cx",e*x+2).attr("cy",a*x+2).attr("r",.45*x).attr("fill","rgba(0,0,0,0.2)"),j.append("circle").attr("class",i).attr("cx",e*x).attr("cy",a*x).attr("r",.45*x)}),o.length>0){const e=i.append("g").attr("class","labels");o.forEach(({x:n,y:i,text:r})=>{const s=a.find(e=>e.x===n&&e.y===i),t="black"===s?.color?"#fff":"#000";e.append("text").attr("x",n*x).attr("y",i*x).attr("dy","0.35em").attr("text-anchor","middle").attr("fill",t).attr("font-size",.5*x).attr("font-weight","bold").text(r)})}if(c){const a=e.append("g").attr("class","coordinates"),n="ABCDEFGHJKLMNOPQRST";for(let e=0;e<t;e++)a.append("text").attr("x",h+e*x).attr("y",h/2).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(n[e]);for(let e=0;e<t;e++)a.append("text").attr("x",h/2).attr("y",h+e*x).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(t-e)}d&&i.append("g").attr("class","click-targets").selectAll("rect").data(r.y17(361)).enter().append("rect").attr("x",e=>e%t*x-x/2).attr("y",e=>Math.floor(e/t)*x-x/2).attr("width",x).attr("height",x).attr("fill","transparent").attr("cursor","pointer").on("click",(e,a)=>{const n=a%t,i=Math.floor(a/t);d({x:n,y:i})})},[e,a,n,o,c,d,x,h,u]),(0,s.jsx)("div",{className:"go-board-container",children:(0,s.jsx)("svg",{ref:p,width:e,height:e,className:"go-board"})})}var d=n(42948);const c=19,p={empty:function(){const e=[];for(let a=0;a<c;a++)for(let n=0;n<c;n++)e.push({x:n,y:a,prob:1/361});return e}(),corner:function(){const e=[],a=[[3,3],[3,15],[15,3],[15,15]],n=[[2,4],[4,2],[2,14],[4,16],[14,2],[16,4],[14,16],[16,14]];for(let i=0;i<c;i++)for(let r=0;r<c;r++){let s=.001;a.some(([e,a])=>e===r&&a===i)?s=.15:n.some(([e,a])=>e===r&&a===i)?s=.05:0!==r&&18!==r&&0!==i&&18!==i||(s=5e-4),e.push({x:r,y:i,prob:s})}return h(e)}(),move37:function(){const e=[],a={x:9,y:4},n=[[3,2],[15,2],[10,10],[8,6]];for(let i=0;i<c;i++)for(let r=0;r<c;r++){let s=.001;r===a.x&&i===a.y?s=.08:n.some(([e,a])=>e===r&&a===i)?s=.12:r>=5&&r<=13&&i>=5&&i<=13&&(s=.005+.01*Math.random()),e.push({x:r,y:i,prob:s})}return h(e)}()};function h(e){const a=e.reduce((e,a)=>e+a.prob,0);return e.map(e=>({...e,prob:e.prob/a}))}function u({initialPosition:e="corner",stones:a=[],highlightMoves:n=[],size:t=450,showTopN:l=5,interactive:o=!0}){const d=(0,i.useRef)(null),h=(0,i.useRef)(null),[u,x]=(0,i.useState)(p[e]||p.corner),[m,j]=(0,i.useState)(null),v=35,g=t-70,f=g/18;(0,i.useEffect)(()=>{if(!d.current)return;const e=r.Ltv(d.current);e.selectAll("*").remove();const n=e.append("g").attr("transform","translate(35, 35)");n.append("rect").attr("x",-f/2).attr("y",-f/2).attr("width",g+f).attr("height",g+f).attr("fill","#dcb35c").attr("rx",4);const i=Math.max(...u.map(e=>e.prob)),s=r.exT(r.oKI).domain([0,i]);n.append("g").attr("class","heatmap").selectAll("rect").data(u).enter().append("rect").attr("class","heatmap-cell").attr("x",e=>e.x*f-f/2).attr("y",e=>e.y*f-f/2).attr("width",f).attr("height",f).attr("fill",e=>s(e.prob)).attr("opacity",e=>.3+e.prob/i*.6).attr("cursor",o?"pointer":"default").on("mouseover",function(e,a){if(!o)return;r.Ltv(this).attr("stroke","#333").attr("stroke-width",2);r.Ltv(h.current).style("display","block").style("left",`${e.pageX+10}px`).style("top",e.pageY-10+"px").html(`\u4f4d\u7f6e: ${String.fromCharCode(65+a.x)}${19-a.y}<br>\u6a5f\u7387: ${(100*a.prob).toFixed(2)}%`)}).on("mouseout",function(){r.Ltv(this).attr("stroke","none"),r.Ltv(h.current).style("display","none")}).on("click",function(e,a){o&&j(a)});const t=n.append("g").attr("class","grid");for(let a=0;a<c;a++)t.append("line").attr("class","grid-line").attr("x1",0).attr("y1",a*f).attr("x2",18*f).attr("y2",a*f).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5),t.append("line").attr("class","grid-line").attr("x1",a*f).attr("y1",0).attr("x2",a*f).attr("y2",18*f).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5);const p=n.append("g").attr("class","stones");a.forEach(({x:e,y:a,color:n})=>{p.append("circle").attr("cx",e*f).attr("cy",a*f).attr("r",.45*f).attr("fill","black"===n?"#1a1a1a":"#f5f5f5").attr("stroke","black"===n?"#000":"#333").attr("stroke-width",1)});const x=[...u].sort((e,a)=>a.prob-e.prob).slice(0,l),m=n.append("g").attr("class","top-labels");x.forEach((e,n)=>{a.some(a=>a.x===e.x&&a.y===e.y)||(m.append("circle").attr("cx",e.x*f).attr("cy",e.y*f).attr("r",.3*f).attr("fill","rgba(255,255,255,0.8)").attr("stroke","#e74c3c").attr("stroke-width",2),m.append("text").attr("x",e.x*f).attr("y",e.y*f).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#e74c3c").attr("font-size",.4*f).attr("font-weight","bold").text(n+1))});const b=e.append("g").attr("class","coordinates");for(let a=0;a<c;a++)b.append("text").attr("x",v+a*f).attr("y",17.5).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text("ABCDEFGHJKLMNOPQRST"[a]),b.append("text").attr("x",17.5).attr("y",v+a*f).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(c-a)},[u,a,l,o,f,v,g]);const b=e=>{x(p[e]||p.corner)};return(0,s.jsxs)("div",{children:[o&&(0,s.jsxs)("div",{className:"d3-controls",children:[(0,s.jsx)("button",{className:"empty"===e?"active":"",onClick:()=>b("empty"),children:"\u5747\u52fb\u5206\u5e03"}),(0,s.jsx)("button",{className:"corner"===e?"active":"",onClick:()=>b("corner"),children:"\u958b\u5c40\u661f\u4f4d"}),(0,s.jsx)("button",{className:"move37"===e?"active":"",onClick:()=>b("move37"),children:"\u7b2c 37 \u624b"})]}),(0,s.jsx)("div",{className:"go-board-container",children:(0,s.jsx)("svg",{ref:d,width:t,height:t,className:"go-board"})}),(0,s.jsx)("div",{ref:h,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),m&&(0,s.jsx)("div",{className:"d3-legend",children:(0,s.jsxs)("div",{className:"d3-legend-item",children:["\u5df2\u9078\u64c7: ",String.fromCharCode(65+m.x),19-m.y,"\u2014 \u6a5f\u7387: ",(100*m.prob).toFixed(2),"%"]})}),(0,s.jsxs)("div",{className:"d3-legend",children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#ffffb2"}}),"\u4f4e\u6a5f\u7387"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#fd8d3c"}}),"\u4e2d\u6a5f\u7387"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#bd0026"}}),"\u9ad8\u6a5f\u7387"]})]})]})}function x(e){return(0,s.jsx)(d.A,{fallback:(0,s.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,s.jsx)(u,{...e})})}const m={name:"Root",visits:1600,value:.55,prior:1,children:[{name:"D4",visits:800,value:.62,prior:.35,selected:!0,children:[{name:"Q16",visits:400,value:.58,prior:.3},{name:"R4",visits:300,value:.65,prior:.25,selected:!0},{name:"C16",visits:100,value:.55,prior:.2}]},{name:"Q4",visits:500,value:.52,prior:.3,children:[{name:"D16",visits:300,value:.5,prior:.28},{name:"Q16",visits:200,value:.54,prior:.22}]},{name:"D16",visits:200,value:.48,prior:.2},{name:"Q16",visits:100,value:.45,prior:.15}]};function j({data:e=m,width:a=700,height:n=450,showPUCT:t=!0,cPuct:l=1.5,interactive:o=!0}){const d=(0,i.useRef)(null),c=(0,i.useRef)(null),[p,h]=(0,i.useState)(null),[u,x]=(0,i.useState)(l),j=40,v=40,g=a-v-40,f=n-j-40;return(0,i.useEffect)(()=>{if(!d.current)return;const i=r.Ltv(d.current);i.selectAll("*").remove();const s=r.B22().size([g,f-50]),l=r.Sk5(e);s(l);const p=i.append("g").attr("transform",`translate(${v}, ${j})`);p.append("g").attr("class","links").selectAll("path").data(l.links()).enter().append("path").attr("class",e=>"link "+(e.target.data.selected?"selected":"")).attr("fill","none").attr("stroke",e=>e.target.data.selected?"#4a90d9":"#999").attr("stroke-width",e=>e.target.data.selected?3:1.5).attr("d",r.vu().x(e=>e.x).y(e=>e.y));const x=p.append("g").attr("class","nodes").selectAll("g").data(l.descendants()).enter().append("g").attr("class","node").attr("transform",e=>`translate(${e.x}, ${e.y})`).attr("cursor",o?"pointer":"default").on("mouseover",function(e,a){if(!o)return;r.Ltv(this).select("circle").transition().duration(200).attr("r",30);const n=a.parent?a.parent.data.visits:a.data.visits,i=((e,a)=>{if(!a)return 0;const n=e.value,i=e.prior,r=e.visits;return n+u*i*Math.sqrt(a)/(1+r)})(a.data,n);r.Ltv(c.current).style("display","block").style("left",`${e.pageX+15}px`).style("top",e.pageY-10+"px").html(`\n            <strong>${a.data.name}</strong><br>\n            \u8a2a\u554f\u6b21\u6578 (N): ${a.data.visits}<br>\n            \u5e73\u5747\u50f9\u503c (Q): ${a.data.value.toFixed(3)}<br>\n            \u5148\u9a57\u6a5f\u7387 (P): ${(100*a.data.prior).toFixed(1)}%<br>\n            ${t?`PUCT \u5206\u6578: ${i.toFixed(3)}`:""}\n          `)}).on("mouseout",function(){r.Ltv(this).select("circle").transition().duration(200).attr("r",25),r.Ltv(c.current).style("display","none")}).on("click",function(e,a){o&&h(a.data)});x.append("circle").attr("r",25).attr("fill",e=>e.data.selected?"#4a90d9":"#fff").attr("stroke",a=>{if(a.data.selected)return"#2c5282";const n=a.data.visits/e.visits;return r.dM(.3+.5*n)}).attr("stroke-width",e=>e.data.selected?3:2),x.append("text").attr("dy",-5).attr("text-anchor","middle").attr("fill",e=>e.data.selected?"#fff":"#333").attr("font-size",11).attr("font-weight","bold").text(e=>e.data.name),x.append("text").attr("dy",10).attr("text-anchor","middle").attr("fill",e=>e.data.selected?"#fff":"#666").attr("font-size",9).text(e=>`N=${e.data.visits}`),i.append("text").attr("x",a/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("MCTS \u641c\u7d22\u6a39"),t&&i.append("text").attr("x",a/2).attr("y",n-10).attr("text-anchor","middle").attr("font-size",11).attr("fill","#666").text("\u85cd\u8272\u8def\u5f91\uff1aPUCT \u9078\u64c7\u7684\u6700\u4f73\u8def\u5f91")},[e,a,n,t,u,o,g,f]),(0,s.jsxs)("div",{children:[t&&o&&(0,s.jsx)("div",{className:"d3-controls",children:(0,s.jsxs)("div",{className:"d3-slider",children:[(0,s.jsxs)("label",{children:["c_puct: ",u.toFixed(1)]}),(0,s.jsx)("input",{type:"range",min:"0.5",max:"3",step:"0.1",value:u,onChange:e=>x(parseFloat(e.target.value))})]})}),(0,s.jsx)("div",{className:"mcts-tree-container",children:(0,s.jsx)("svg",{ref:d,width:a,height:n,className:"mcts-tree"})}),(0,s.jsx)("div",{ref:c,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),p&&(0,s.jsxs)("div",{className:"d3-legend",style:{background:"#f5f5f5",padding:"1rem",borderRadius:"4px"},children:[(0,s.jsxs)("strong",{children:["\u5df2\u9078\u64c7\u7bc0\u9ede: ",p.name]}),(0,s.jsxs)("div",{children:["\u8a2a\u554f\u6b21\u6578: ",p.visits]}),(0,s.jsxs)("div",{children:["\u5e73\u5747\u50f9\u503c: ",p.value.toFixed(3)]}),(0,s.jsxs)("div",{children:["\u5148\u9a57\u6a5f\u7387: ",(100*p.prior).toFixed(1),"%"]})]}),(0,s.jsxs)("div",{className:"d3-legend",children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"\u9078\u4e2d\u8def\u5f91"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#fff",border:"2px solid #999"}}),"\u5176\u4ed6\u7bc0\u9ede"]}),(0,s.jsx)("div",{className:"d3-legend-item",children:(0,s.jsx)("span",{style:{fontSize:"12px"},children:"\u7bc0\u9ede\u5927\u5c0f \u221d \u8a2a\u554f\u6b21\u6578"})})]})]})}function v(e){return(0,s.jsx)(d.A,{fallback:(0,s.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,s.jsx)(j,{...e})})}const g=[{hours:0,elo:0,label:"\u96a8\u6a5f"},{hours:3,elo:1e3,label:"\u767c\u73fe\u898f\u5247"},{hours:6,elo:2e3},{hours:12,elo:3e3,label:"\u767c\u73fe\u5b9a\u5f0f"},{hours:24,elo:4e3},{hours:36,elo:4500,label:"\u8d85\u8d8a Fan Hui"},{hours:48,elo:5e3},{hours:60,elo:5200,label:"\u8d85\u8d8a Lee Sedol"},{hours:72,elo:5400,label:"\u8d85\u8d8a\u539f\u7248 AlphaGo"}],f=[{elo:2700,label:"\u696d\u9918\u5f37\u8c6a"},{elo:3500,label:"Fan Hui (\u8077\u696d\u4e8c\u6bb5)"},{elo:4500,label:"Lee Sedol (\u4e16\u754c\u51a0\u8ecd)"},{elo:5e3,label:"\u539f\u7248 AlphaGo"}],b=[{epochs:0,elo:0},{epochs:10,elo:1500},{epochs:20,elo:2500},{epochs:30,elo:3e3},{epochs:40,elo:3200},{epochs:50,elo:3300}],y=[{games:0,elo:3300},{games:1e3,elo:3800},{games:5e3,elo:4200},{games:1e4,elo:4500},{games:5e4,elo:4800},{games:1e5,elo:5e3}];function k({mode:e="zero",width:a=600,height:n=400,animated:t=!0,showMilestones:l=!0}){const o=(0,i.useRef)(null),[d,c]=(0,i.useState)(e),p=40,h=70,u=a-h-100,x=n-p-60;return(0,i.useEffect)(()=>{if(!o.current)return;const e=r.Ltv(o.current);let n,i,s;e.selectAll("*").remove(),"zero"===d?(n=g,i="\u8a13\u7df4\u6642\u9593\uff08\u5c0f\u6642\uff09",s=[0,80]):"sl"===d?(n=b,i="\u8a13\u7df4\u8f2a\u6578\uff08Epochs\uff09",s=[0,60]):(n=y,i="\u81ea\u6211\u5c0d\u5f08\u5c40\u6578",s=[0,12e4]);const c="selfplay"===d?r.ZEH().domain([1,s[1]]).range([0,u]):r.m4Y().domain(s).range([0,u]),m=r.m4Y().domain([0,6e3]).range([x,0]),j=e.append("g").attr("transform",`translate(${h}, ${p})`);if(j.append("g").attr("class","grid").selectAll(".grid-line-y").data(m.ticks(6)).enter().append("line").attr("class","grid-line-y").attr("x1",0).attr("x2",u).attr("y1",e=>m(e)).attr("y2",e=>m(e)).attr("stroke","#ddd").attr("stroke-dasharray","3,3"),l&&"zero"===d){const e=j.append("g").attr("class","human-levels");f.forEach(a=>{e.append("line").attr("x1",0).attr("x2",u).attr("y1",m(a.elo)).attr("y2",m(a.elo)).attr("stroke","#e74c3c").attr("stroke-dasharray","5,5").attr("opacity",.6),e.append("text").attr("x",u+5).attr("y",m(a.elo)).attr("dy","0.35em").attr("fill","#e74c3c").attr("font-size",10).text(a.label)})}const v=r.n8j().x(e=>c("zero"===d?e.hours:"sl"===d?e.epochs:Math.max(1,e.games))).y(e=>m(e.elo)).curve(r.nVG),k=r.Wcw().x(e=>c("zero"===d?e.hours:"sl"===d?e.epochs:Math.max(1,e.games))).y0(x).y1(e=>m(e.elo)).curve(r.nVG);j.append("path").datum(n).attr("class","area").attr("fill","#4a90d9").attr("opacity",.1).attr("d",k);const N=j.append("path").datum(n).attr("class","line").attr("fill","none").attr("stroke","#4a90d9").attr("stroke-width",3).attr("d",v);if(t){const e=N.node().getTotalLength();N.attr("stroke-dasharray",`${e} ${e}`).attr("stroke-dashoffset",e).transition().duration(2e3).ease(r.yfw).attr("stroke-dashoffset",0)}if(l&&"zero"===d){const e=n.filter(e=>e.label),a=j.append("g").attr("class","milestones");a.selectAll("circle").data(e).enter().append("circle").attr("cx",e=>c(e.hours)).attr("cy",e=>m(e.elo)).attr("r",6).attr("fill","#e74c3c").attr("stroke","#fff").attr("stroke-width",2),a.selectAll("text").data(e).enter().append("text").attr("x",e=>c(e.hours)).attr("y",e=>m(e.elo)-15).attr("text-anchor","middle").attr("fill","#333").attr("font-size",10).text(e=>e.label)}const C="selfplay"===d?r.l78(c).ticks(5,"~s"):r.l78(c);j.append("g").attr("class","x-axis").attr("transform",`translate(0, ${x})`).call(C),j.append("text").attr("class","axis-label").attr("x",u/2).attr("y",x+45).attr("text-anchor","middle").attr("fill","#666").text(i),j.append("g").attr("class","y-axis").call(r.V4s(m).ticks(6)),j.append("text").attr("class","axis-label").attr("transform","rotate(-90)").attr("x",-x/2).attr("y",-50).attr("text-anchor","middle").attr("fill","#666").text("ELO \u8a55\u5206"),e.append("text").attr("x",a/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("zero"===d?"AlphaGo Zero \u8a13\u7df4\u66f2\u7dda":"sl"===d?"\u76e3\u7763\u5b78\u7fd2\u68cb\u529b\u6210\u9577":"\u81ea\u6211\u5c0d\u5f08\u68cb\u529b\u6210\u9577")},[d,a,n,t,l,u,x]),(0,s.jsxs)("div",{children:[(0,s.jsxs)("div",{className:"d3-controls",children:[(0,s.jsx)("button",{className:"zero"===d?"active":"",onClick:()=>c("zero"),children:"AlphaGo Zero"}),(0,s.jsx)("button",{className:"sl"===d?"active":"",onClick:()=>c("sl"),children:"\u76e3\u7763\u5b78\u7fd2"}),(0,s.jsx)("button",{className:"selfplay"===d?"active":"",onClick:()=>c("selfplay"),children:"\u81ea\u6211\u5c0d\u5f08"})]}),(0,s.jsx)("div",{className:"elo-chart-container",children:(0,s.jsx)("svg",{ref:o,width:a,height:n,className:"elo-chart"})}),(0,s.jsxs)("div",{className:"d3-legend",children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"ELO \u8a55\u5206"]}),l&&"zero"===d&&(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c"}}),"\u91cc\u7a0b\u7891"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c",opacity:.3}}),"\u4eba\u985e\u6c34\u5e73\u53c3\u8003\u7dda"]})]})]})]})}function N(e){return(0,s.jsx)(d.A,{fallback:(0,s.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,s.jsx)(k,{...e})})}},30416(e,a,n){n.d(a,{R:()=>t,x:()=>l});var i=n(59471);const r={},s=i.createContext(r);function t(e){const a=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function l(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),i.createElement(s.Provider,{value:a},e.children)}}}]);