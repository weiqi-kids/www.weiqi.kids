"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[6726],{15489(e,a,n){n.r(a),n.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>d,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"alphago/supervised-learning","title":"Fase de aprendizaje supervisado","description":"C\xf3mo AlphaGo aprendi\xf3 de 30 millones de partidas humanas, alcanzando un 57% de precisi\xf3n en predicci\xf3n","source":"@site/i18n/es/docusaurus-plugin-content-docs/current/alphago/11-supervised-learning.mdx","sourceDirName":"alphago","slug":"/alphago/supervised-learning","permalink":"/es/docs/alphago/supervised-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/11-supervised-learning.mdx","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12,"title":"Fase de aprendizaje supervisado","description":"C\xf3mo AlphaGo aprendi\xf3 de 30 millones de partidas humanas, alcanzando un 57% de precisi\xf3n en predicci\xf3n"},"sidebar":"tutorialSidebar","previous":{"title":"CNN y Go","permalink":"/es/docs/alphago/cnn-and-go"},"next":{"title":"Introducci\xf3n al aprendizaje por refuerzo","permalink":"/es/docs/alphago/reinforcement-intro"}}');var r=n(62615),s=n(30416),i=n(45695);const d={sidebar_position:12,title:"Fase de aprendizaje supervisado",description:"C\xf3mo AlphaGo aprendi\xf3 de 30 millones de partidas humanas, alcanzando un 57% de precisi\xf3n en predicci\xf3n"},o="Fase de aprendizaje supervisado",l={},c=[{value:"\xbfPor qu\xe9 empezar con partidas humanas?",id:"por-qu\xe9-empezar-con-partidas-humanas",level:2},{value:"El punto de partida del aprendizaje",id:"el-punto-de-partida-del-aprendizaje",level:3},{value:"El valor de las partidas humanas",id:"el-valor-de-las-partidas-humanas",level:3},{value:"Fuente de datos de entrenamiento",id:"fuente-de-datos-de-entrenamiento",level:2},{value:"KGS Go Server",id:"kgs-go-server",level:3},{value:"Caracter\xedsticas de KGS",id:"caracter\xedsticas-de-kgs",level:4},{value:"\xbfPor qu\xe9 elegir KGS?",id:"por-qu\xe9-elegir-kgs",level:4},{value:"30 millones de posiciones",id:"30-millones-de-posiciones",level:3},{value:"Formato de datos",id:"formato-de-datos",level:3},{value:"Preprocesamiento de datos",id:"preprocesamiento-de-datos",level:2},{value:"An\xe1lisis de SGF",id:"an\xe1lisis-de-sgf",level:3},{value:"Extracci\xf3n de caracter\xedsticas",id:"extracci\xf3n-de-caracter\xedsticas",level:3},{value:"Aumento de datos",id:"aumento-de-datos",level:3},{value:"Funci\xf3n de p\xe9rdida",id:"funci\xf3n-de-p\xe9rdida",level:2},{value:"P\xe9rdida de entrop\xeda cruzada",id:"p\xe9rdida-de-entrop\xeda-cruzada",level:3},{value:"Comprensi\xf3n intuitiva",id:"comprensi\xf3n-intuitiva",level:3},{value:"Comparaci\xf3n con MSE",id:"comparaci\xf3n-con-mse",level:3},{value:"Proceso de entrenamiento",id:"proceso-de-entrenamiento",level:2},{value:"Configuraci\xf3n de hardware",id:"configuraci\xf3n-de-hardware",level:3},{value:"Optimizador",id:"optimizador",level:3},{value:"\xbfPor qu\xe9 SGD en lugar de Adam?",id:"por-qu\xe9-sgd-en-lugar-de-adam",level:4},{value:"Programaci\xf3n de tasa de aprendizaje",id:"programaci\xf3n-de-tasa-de-aprendizaje",level:3},{value:"Bucle de entrenamiento",id:"bucle-de-entrenamiento",level:3},{value:"Curva de entrenamiento",id:"curva-de-entrenamiento",level:3},{value:"An\xe1lisis de resultados",id:"an\xe1lisis-de-resultados",level:2},{value:"57% de precisi\xf3n",id:"57-de-precisi\xf3n",level:3},{value:"\xbfQu\xe9 es la precisi\xf3n top-1?",id:"qu\xe9-es-la-precisi\xf3n-top-1",level:4},{value:"Comparaci\xf3n con otros programas",id:"comparaci\xf3n-con-otros-programas",level:3},{value:"Evaluaci\xf3n de fuerza de juego",id:"evaluaci\xf3n-de-fuerza-de-juego",level:3},{value:"Precisi\xf3n vs fuerza de juego",id:"precisi\xf3n-vs-fuerza-de-juego",level:3},{value:"Limitaciones del aprendizaje supervisado",id:"limitaciones-del-aprendizaje-supervisado",level:2},{value:"Problema 1: Efecto techo",id:"problema-1-efecto-techo",level:3},{value:"Problema 2: No puede distinguir buenas de malas jugadas",id:"problema-2-no-puede-distinguir-buenas-de-malas-jugadas",level:3},{value:"Problema 3: Exploraci\xf3n insuficiente",id:"problema-3-exploraci\xf3n-insuficiente",level:3},{value:"Soluci\xf3n: Aprendizaje por refuerzo",id:"soluci\xf3n-aprendizaje-por-refuerzo",level:3},{value:"Puntos de implementaci\xf3n",id:"puntos-de-implementaci\xf3n",level:2},{value:"C\xf3digo de entrenamiento completo",id:"c\xf3digo-de-entrenamiento-completo",level:3},{value:"C\xf3digo de evaluaci\xf3n",id:"c\xf3digo-de-evaluaci\xf3n",level:3},{value:"Problemas comunes y soluciones",id:"problemas-comunes-y-soluciones",level:3},{value:"Correspondencia con animaciones",id:"correspondencia-con-animaciones",level:2},{value:"Lecturas adicionales",id:"lecturas-adicionales",level:2},{value:"Puntos clave",id:"puntos-clave",level:2},{value:"Referencias",id:"referencias",level:2}];function p(e){const a={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"fase-de-aprendizaje-supervisado",children:"Fase de aprendizaje supervisado"})}),"\n",(0,r.jsxs)(a.p,{children:['Antes de que AlphaGo pudiera jugar consigo mismo, primero necesitaba "ver" una gran cantidad de partidas humanas. Este proceso se llama ',(0,r.jsx)(a.strong,{children:"aprendizaje supervisado"}),"."]}),"\n",(0,r.jsxs)(a.p,{children:["Analizando 30 millones de posiciones de partidas humanas, la Policy Network de AlphaGo alcanz\xf3 una ",(0,r.jsx)(a.strong,{children:"precisi\xf3n de predicci\xf3n del 57%"}),", pudiendo adivinar el siguiente movimiento del experto humano en m\xe1s de la mitad de los casos."]}),"\n",(0,r.jsx)(a.p,{children:"Esto puede no parecer impresionante, pero considerando que cada posici\xf3n tiene un promedio de 250 movimientos legales, es un logro asombroso."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"por-qu\xe9-empezar-con-partidas-humanas",children:"\xbfPor qu\xe9 empezar con partidas humanas?"}),"\n",(0,r.jsx)(a.h3,{id:"el-punto-de-partida-del-aprendizaje",children:"El punto de partida del aprendizaje"}),"\n",(0,r.jsx)(a.p,{children:"Imagina que vas a ense\xf1ar Go a alguien que no sabe nada del juego. \xbfC\xf3mo lo har\xedas?"}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Opci\xf3n A: Exploraci\xf3n aleatoria"})}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Dejar que juegue al azar, descubriendo poco a poco qu\xe9 es una buena jugada\n\u2192 Extremadamente ineficiente, podr\xeda nunca aprender\n"})}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Opci\xf3n B: Ver c\xf3mo juegan los expertos"})}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Dejar que vea muchas partidas de jugadores profesionales, imitando su forma de jugar\n\u2192 Una vez que tenga una base, explorar por su cuenta\n"})}),"\n",(0,r.jsx)(a.p,{children:'AlphaGo eligi\xf3 la opci\xf3n B. El aprendizaje supervisado es la versi\xf3n matem\xe1tica de "ver c\xf3mo juegan los expertos".'}),"\n",(0,r.jsx)(a.h3,{id:"el-valor-de-las-partidas-humanas",children:"El valor de las partidas humanas"}),"\n",(0,r.jsx)(a.p,{children:"Los humanos han desarrollado la teor\xeda del Go durante miles de a\xf1os. Todo este conocimiento est\xe1 codificado en las partidas:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Joseki de apertura"}),": movimientos de apertura verificados a lo largo del tiempo"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"T\xe1cticas de medio juego"}),": sabidur\xeda de transiciones ataque-defensa"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"T\xe9cnicas de final"}),": la esencia del c\xe1lculo de puntos"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Visi\xf3n global"}),": intuici\xf3n de juicio general"]}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:'El aprendizaje supervisado permite que AlphaGo "herede" esta sabidur\xeda humana, sin necesidad de empezar desde cero.'}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"fuente-de-datos-de-entrenamiento",children:"Fuente de datos de entrenamiento"}),"\n",(0,r.jsx)(a.h3,{id:"kgs-go-server",children:"KGS Go Server"}),"\n",(0,r.jsxs)(a.p,{children:["Los datos de entrenamiento de AlphaGo provienen principalmente de ",(0,r.jsx)(a.strong,{children:"KGS Go Server"})," (tambi\xe9n conocido como Kiseido Go Server), una plataforma de Go en l\xednea muy conocida."]}),"\n",(0,r.jsx)(a.h4,{id:"caracter\xedsticas-de-kgs",children:"Caracter\xedsticas de KGS"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Caracter\xedstica"}),(0,r.jsx)(a.th,{children:"Descripci\xf3n"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Usuarios"}),(0,r.jsx)(a.td,{children:"Principalmente jugadores aficionados, tambi\xe9n algunos profesionales"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Rango de nivel"}),(0,r.jsx)(a.td,{children:"Desde principiantes hasta 9 dan profesional"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Registros de partidas"}),(0,r.jsx)(a.td,{children:"Archivos SGF completos guardados"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Per\xedodo activo"}),(0,r.jsx)(a.td,{children:"Desde 2000 hasta la actualidad"})]})]})]}),"\n",(0,r.jsx)(a.h4,{id:"por-qu\xe9-elegir-kgs",children:"\xbfPor qu\xe9 elegir KGS?"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Gran cantidad de datos"}),": millones de partidas"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Formato uniforme"}),": formato SGF f\xe1cil de analizar"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Etiquetas de nivel"}),": cada usuario tiene un rating"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Diversidad"}),": jugadores de diferentes estilos"]}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"30-millones-de-posiciones",children:"30 millones de posiciones"}),"\n",(0,r.jsxs)(a.p,{children:["De las partidas de KGS, DeepMind extrajo aproximadamente ",(0,r.jsx)(a.strong,{children:"30 millones de posiciones"}),":"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Datos originales:\n- Aproximadamente 160,000 partidas\n- Aproximadamente 200 movimientos por partida\n- Total ~32 millones de posiciones\n\nFiltrado de datos:\n- Filtrar partidas de bajo nivel\n- Filtrar posiciones de rendici\xf3n a mitad de partida\n- Aproximadamente 30 millones de posiciones de alta calidad finales\n"})}),"\n",(0,r.jsx)(a.h3,{id:"formato-de-datos",children:"Formato de datos"}),"\n",(0,r.jsx)(a.p,{children:"Cada muestra de entrenamiento contiene:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'{\n    "board_state": [[0, 1, 2, ...], ...],  # Tablero 19\xd719\n    "features": [...],                      # 48 planos de caracter\xedsticas\n    "next_move": 123,                       # Posici\xf3n jugada por el humano (0-360)\n    "game_result": 1,                       # 1=gana negro, -1=gana blanco\n    "player_rank": "5d",                    # Rango del jugador que hizo este movimiento\n}\n'})}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"preprocesamiento-de-datos",children:"Preprocesamiento de datos"}),"\n",(0,r.jsx)(a.h3,{id:"an\xe1lisis-de-sgf",children:"An\xe1lisis de SGF"}),"\n",(0,r.jsx)(a.p,{children:"SGF (Smart Game Format) es el formato est\xe1ndar para partidas de Go:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"(;GM[1]FF[4]CA[UTF-8]AP[CGoban:3]ST[2]\nRU[Japanese]SZ[19]KM[6.50]\nPW[White]PB[Black]\n;B[pd];W[dd];B[pq];W[dp];B[qk];W[nc]...\n)\n"})}),"\n",(0,r.jsx)(a.p,{children:"Necesita analizarse:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Tama\xf1o del tablero (SZ[19])"}),"\n",(0,r.jsx)(a.li,{children:"Cada movimiento (B[pd], W[dd]...)"}),"\n",(0,r.jsx)(a.li,{children:"Resultado de la partida (RE[B+2.5])"}),"\n"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"def parse_sgf(sgf_string):\n    \"\"\"Analizar archivo SGF\"\"\"\n    moves = []\n    # Extraer todos los movimientos\n    pattern = r';([BW])\\[([a-s]{2})\\]'\n    for match in re.finditer(pattern, sgf_string):\n        color = match.group(1)  # 'B' o 'W'\n        coord = match.group(2)  # 'pd', 'dd', etc.\n\n        # Convertir coordenadas\n        x = ord(coord[0]) - ord('a')\n        y = ord(coord[1]) - ord('a')\n\n        moves.append((color, x, y))\n\n    return moves\n"})}),"\n",(0,r.jsx)(a.h3,{id:"extracci\xf3n-de-caracter\xedsticas",children:"Extracci\xf3n de caracter\xedsticas"}),"\n",(0,r.jsxs)(a.p,{children:["Para cada posici\xf3n, se extraen 48 planos de caracter\xedsticas (ver ",(0,r.jsx)(a.a,{href:"../input-features",children:"Dise\xf1o de caracter\xedsticas de entrada"}),"):"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'def extract_features(board, history, current_player):\n    """Extraer 48 planos de caracter\xedsticas"""\n    features = np.zeros((48, 19, 19))\n\n    # Posiciones de piedras\n    features[0] = (board == 1)  # Piedras negras\n    features[1] = (board == 2)  # Piedras blancas\n    features[2] = (board == 0)  # Puntos vac\xedos\n\n    # Registro hist\xf3rico\n    for i, hist in enumerate(history[:8]):\n        features[3+i] = (hist == 1)\n        features[11+i] = (hist == 2)\n\n    # Libertades, atari, escalera, etc...\n    # (implementaci\xf3n detallada omitida)\n\n    return features\n'})}),"\n",(0,r.jsx)(a.h3,{id:"aumento-de-datos",children:"Aumento de datos"}),"\n",(0,r.jsxs)(a.p,{children:["El tablero de Go tiene ",(0,r.jsx)(a.strong,{children:"8 simetr\xedas"})," (4 rotaciones \xd7 2 reflexiones). Cada muestra original puede convertirse en 8:"]}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Transformaciones de simetria (8 en total):"})}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{style:{textAlign:"center"},children:"Transformacion"}),(0,r.jsx)(a.th,{style:{textAlign:"center"},children:"Posicion de la piedra"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Original"}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Arriba-izquierda"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Rotar 90 grados"}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Abajo-izquierda"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Rotar 180 grados"}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Abajo-derecha"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Rotar 270 grados"}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Arriba-derecha"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Volteo horizontal"}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Arriba-derecha"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Volteo + Rotar 90"}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Abajo-derecha"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Volteo + Rotar 180"}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Abajo-izquierda"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Volteo + Rotar 270"}),(0,r.jsx)(a.td,{style:{textAlign:"center"},children:"Arriba-izquierda"})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"Cada posicion original puede producir 8 muestras de entrenamiento equivalentes."}),"\n",(0,r.jsx)(a.p,{children:"Esto aumenta los datos de entrenamiento efectivos 8 veces, mientras asegura que los patrones aprendidos por el modelo no dependan de una orientaci\xf3n espec\xedfica."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'def augment(state, action):\n    """Aumento de 8 simetr\xedas"""\n    augmented = []\n\n    for rotation in [0, 1, 2, 3]:  # 0, 90, 180, 270 grados\n        rotated_state = np.rot90(state, rotation, axes=(1, 2))\n        rotated_action = rotate_action(action, rotation)\n        augmented.append((rotated_state, rotated_action))\n\n        # Volteo horizontal\n        flipped_state = np.flip(rotated_state, axis=2)\n        flipped_action = flip_action(rotated_action)\n        augmented.append((flipped_state, flipped_action))\n\n    return augmented\n'})}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"funci\xf3n-de-p\xe9rdida",children:"Funci\xf3n de p\xe9rdida"}),"\n",(0,r.jsx)(a.h3,{id:"p\xe9rdida-de-entrop\xeda-cruzada",children:"P\xe9rdida de entrop\xeda cruzada"}),"\n",(0,r.jsxs)(a.p,{children:["El aprendizaje supervisado usa ",(0,r.jsx)(a.strong,{children:"p\xe9rdida de entrop\xeda cruzada (Cross-Entropy Loss)"})," para entrenar la Policy Network:"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"L(\u03b8) = -\u03a3 log p_\u03b8(a | s)\n"})}),"\n",(0,r.jsx)(a.p,{children:"Donde:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.code,{children:"s"}),": estado del tablero"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.code,{children:"a"}),": posici\xf3n realmente jugada por el humano (etiqueta)"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.code,{children:"p_\u03b8(a | s)"}),": probabilidad predicha por el modelo para esa posici\xf3n"]}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"comprensi\xf3n-intuitiva",children:"Comprensi\xf3n intuitiva"}),"\n",(0,r.jsx)(a.p,{children:'La p\xe9rdida de entrop\xeda cruzada mide "la diferencia entre la predicci\xf3n del modelo y la etiqueta":'}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Escenario"}),(0,r.jsx)(a.th,{children:"Predicci\xf3n del modelo"}),(0,r.jsx)(a.th,{children:"P\xe9rdida"}),(0,r.jsx)(a.th,{children:"Descripci\xf3n"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Predicci\xf3n perfecta"}),(0,r.jsx)(a.td,{children:"probabilidad de a = 1.0"}),(0,r.jsx)(a.td,{children:"0"}),(0,r.jsx)(a.td,{children:"Mejor"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Confiado y correcto"}),(0,r.jsx)(a.td,{children:"probabilidad de a = 0.9"}),(0,r.jsx)(a.td,{children:"0.1"}),(0,r.jsx)(a.td,{children:"Muy bueno"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Incierto pero correcto"}),(0,r.jsx)(a.td,{children:"probabilidad de a = 0.5"}),(0,r.jsx)(a.td,{children:"0.7"}),(0,r.jsx)(a.td,{children:"Aceptable"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Predicci\xf3n incorrecta"}),(0,r.jsx)(a.td,{children:"probabilidad de a = 0.1"}),(0,r.jsx)(a.td,{children:"2.3"}),(0,r.jsx)(a.td,{children:"Malo"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Completamente incorrecto"}),(0,r.jsx)(a.td,{children:"probabilidad de a = 0.01"}),(0,r.jsx)(a.td,{children:"4.6"}),(0,r.jsx)(a.td,{children:"El peor"})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"La funci\xf3n de p\xe9rdida impulsa al modelo a aumentar la probabilidad de la posici\xf3n correcta."}),"\n",(0,r.jsx)(a.h3,{id:"comparaci\xf3n-con-mse",children:"Comparaci\xf3n con MSE"}),"\n",(0,r.jsx)(a.p,{children:"\xbfPor qu\xe9 no usar error cuadr\xe1tico medio (MSE)?"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# MSE:\nloss_mse = (prediction - target)^2\n\n# Cross-Entropy:\nloss_ce = -log(prediction[target])\n"})}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Caracter\xedstica"}),(0,r.jsx)(a.th,{children:"MSE"}),(0,r.jsx)(a.th,{children:"Cross-Entropy"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Tipo de objetivo"}),(0,r.jsx)(a.td,{children:"Regresi\xf3n (valor continuo)"}),(0,r.jsx)(a.td,{children:"Clasificaci\xf3n (distribuci\xf3n de probabilidad)"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Comportamiento del gradiente"}),(0,r.jsx)(a.td,{children:"Mayor error, mayor gradiente"}),(0,r.jsx)(a.td,{children:"Mayor gradiente cuando hay error con confianza"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Escenario adecuado"}),(0,r.jsx)(a.td,{children:"Value Network"}),(0,r.jsx)(a.td,{children:"Policy Network"})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"La Policy Network produce una distribuci\xf3n de probabilidad sobre 361 categor\xedas, la entrop\xeda cruzada es la elecci\xf3n natural."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"proceso-de-entrenamiento",children:"Proceso de entrenamiento"}),"\n",(0,r.jsx)(a.h3,{id:"configuraci\xf3n-de-hardware",children:"Configuraci\xf3n de hardware"}),"\n",(0,r.jsx)(a.p,{children:"DeepMind us\xf3 una gran cantidad de recursos computacionales:"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Recurso"}),(0,r.jsx)(a.th,{children:"Cantidad"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"GPU"}),(0,r.jsx)(a.td,{children:"50"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Tiempo de entrenamiento"}),(0,r.jsx)(a.td,{children:"Aproximadamente 3 semanas"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Tama\xf1o de lote"}),(0,r.jsx)(a.td,{children:"16"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Pasos totales de entrenamiento"}),(0,r.jsx)(a.td,{children:"~340M"})]})]})]}),"\n",(0,r.jsx)(a.h3,{id:"optimizador",children:"Optimizador"}),"\n",(0,r.jsxs)(a.p,{children:["Se usa ",(0,r.jsx)(a.strong,{children:"Descenso de Gradiente Estoc\xe1stico (SGD) + momentum"}),":"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"optimizer = torch.optim.SGD(\n    model.parameters(),\n    lr=0.003,         # Tasa de aprendizaje inicial\n    momentum=0.9,     # Coeficiente de momentum\n    weight_decay=1e-4 # Regularizaci\xf3n L2\n)\n"})}),"\n",(0,r.jsx)(a.h4,{id:"por-qu\xe9-sgd-en-lugar-de-adam",children:"\xbfPor qu\xe9 SGD en lugar de Adam?"}),"\n",(0,r.jsx)(a.p,{children:"En 2016, SGD + momentum segu\xeda siendo la opci\xf3n principal para tareas de imagen. De hecho, investigaciones posteriores (incluyendo KataGo) encontraron que los optimizadores tipo Adam podr\xedan ser mejores."}),"\n",(0,r.jsx)(a.h3,{id:"programaci\xf3n-de-tasa-de-aprendizaje",children:"Programaci\xf3n de tasa de aprendizaje"}),"\n",(0,r.jsx)(a.p,{children:"La tasa de aprendizaje se reduce gradualmente durante el entrenamiento:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=80_000_000,  # Cada 80M pasos\n    gamma=0.1              # Tasa de aprendizaje \xd7 0.1\n)\n"})}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Tasa de aprendizaje:\n0.003  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500 0.0003 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500 0.00003\n         80M pasos     160M pasos     240M pasos\n"})}),"\n",(0,r.jsx)(a.h3,{id:"bucle-de-entrenamiento",children:"Bucle de entrenamiento"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"def train_epoch(model, dataloader, optimizer):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    for batch in dataloader:\n        states, actions = batch\n\n        # Propagaci\xf3n hacia adelante\n        policy = model(states)  # (batch, 361)\n\n        # Calcular p\xe9rdida\n        loss = F.cross_entropy(policy, actions)\n\n        # Retropropagaci\xf3n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Estad\xedsticas\n        total_loss += loss.item()\n        predictions = policy.argmax(dim=1)\n        correct += (predictions == actions).sum().item()\n        total += actions.size(0)\n\n    accuracy = correct / total\n    avg_loss = total_loss / len(dataloader)\n\n    return avg_loss, accuracy\n"})}),"\n",(0,r.jsx)(a.h3,{id:"curva-de-entrenamiento",children:"Curva de entrenamiento"}),"\n",(0,r.jsx)(a.p,{children:"Proceso de entrenamiento t\xedpico:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Precisi\xf3n\n60% |                    ......**********\n    |              ......*\n50% |        ......*\n    |    ....*\n40% |  ..*\n    |..*\n30% |*\n    +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Pasos de entrenamiento\n    0       100M     200M     300M     340M\n"})}),"\n",(0,r.jsx)(a.p,{children:"La p\xe9rdida y la precisi\xf3n mejoran r\xe1pidamente, luego se estabilizan."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"an\xe1lisis-de-resultados",children:"An\xe1lisis de resultados"}),"\n",(0,r.jsx)(a.h3,{id:"57-de-precisi\xf3n",children:"57% de precisi\xf3n"}),"\n",(0,r.jsxs)(a.p,{children:["Despu\xe9s del entrenamiento completo, la Policy Network alcanz\xf3 una ",(0,r.jsx)(a.strong,{children:"precisi\xf3n top-1 del 57.0%"}),"."]}),"\n",(0,r.jsx)(a.h4,{id:"qu\xe9-es-la-precisi\xf3n-top-1",children:"\xbfQu\xe9 es la precisi\xf3n top-1?"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Predicci\xf3n: el modelo produce 361 probabilidades\nTop-1: la posici\xf3n con mayor probabilidad\nPrecisi\xf3n: proporci\xf3n en que esta posici\xf3n es igual a la jugada real del humano\n"})}),"\n",(0,r.jsx)(a.p,{children:"57% significa: el modelo tiene m\xe1s de la mitad de posibilidades de adivinar el siguiente movimiento del experto humano."}),"\n",(0,r.jsx)(a.h3,{id:"comparaci\xf3n-con-otros-programas",children:"Comparaci\xf3n con otros programas"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Programa"}),(0,r.jsx)(a.th,{children:"Precisi\xf3n Top-1"}),(0,r.jsx)(a.th,{children:"Descripci\xf3n"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Selecci\xf3n aleatoria"}),(0,r.jsx)(a.td,{children:"0.4%"}),(0,r.jsx)(a.td,{children:"L\xednea base"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Caracter\xedsticas tradicionales + modelo lineal"}),(0,r.jsx)(a.td,{children:"~24%"}),(0,r.jsx)(a.td,{children:"Nivel 2008"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"CNN superficial"}),(0,r.jsx)(a.td,{children:"~44%"}),(0,r.jsx)(a.td,{children:"Nivel 2014"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"Policy Network de AlphaGo"})}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"57%"})}),(0,r.jsx)(a.td,{children:"Avance de 2016"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"AlphaGo Zero"}),(0,r.jsx)(a.td,{children:"~60%"}),(0,r.jsx)(a.td,{children:"2017"})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"La CNN profunda de DeepMind mejor\xf3 13 puntos porcentuales sobre el mejor m\xe9todo anterior."}),"\n",(0,r.jsx)(a.h3,{id:"evaluaci\xf3n-de-fuerza-de-juego",children:"Evaluaci\xf3n de fuerza de juego"}),"\n",(0,r.jsx)(a.p,{children:"Fuerza de juego usando solo la Policy Network (sin b\xfasqueda):"}),"\n",(0,r.jsx)(i.$W,{mode:"training",width:600,height:350}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Configuraci\xf3n"}),(0,r.jsx)(a.th,{children:"Rating Elo"}),(0,r.jsx)(a.th,{children:"Nivel aproximado"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"M\xe1s fuerte tradicional (Pachi)"}),(0,r.jsx)(a.td,{children:"~2500"}),(0,r.jsx)(a.td,{children:"4-5 dan amateur"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"SL Policy Network"}),(0,r.jsx)(a.td,{children:"~2800"}),(0,r.jsx)(a.td,{children:"6-7 dan amateur"})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"Solo con aprendizaje supervisado ya se alcanz\xf3 el nivel amateur alto, lo cual fue un avance importante en 2016."}),"\n",(0,r.jsx)(a.h3,{id:"precisi\xf3n-vs-fuerza-de-juego",children:"Precisi\xf3n vs fuerza de juego"}),"\n",(0,r.jsx)(a.p,{children:"Curiosamente, la precisi\xf3n y la fuerza de juego no tienen una relaci\xf3n lineal:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Precisi\xf3n:  44% \u2192 57% (mejora del 13%)\nElo:       ~2500 \u2192 ~2800 (mejora de ~300)\n\nProporci\xf3n de mejora de precisi\xf3n: 13% / 44% \u2248 30%\nProporci\xf3n de mejora de Elo: 300 / 2500 \u2248 12%\n"})}),"\n",(0,r.jsx)(a.p,{children:"Peque\xf1as mejoras en la precisi\xf3n pueden traer mejoras significativas en la fuerza de juego, porque:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Las elecciones correctas en posiciones cr\xedticas son m\xe1s importantes"}),"\n",(0,r.jsx)(a.li,{children:"Evitar errores obvios es m\xe1s importante que jugar m\xe1s buenas jugadas"}),"\n"]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"limitaciones-del-aprendizaje-supervisado",children:"Limitaciones del aprendizaje supervisado"}),"\n",(0,r.jsx)(a.h3,{id:"problema-1-efecto-techo",children:"Problema 1: Efecto techo"}),"\n",(0,r.jsx)(a.p,{children:'El aprendizaje supervisado solo puede alcanzar el "nivel humano", no superarlo:'}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Objetivo de SL Policy: imitar humanos\n          \u2193\nSi los humanos tienen h\xe1bitos incorrectos\n          \u2193\nSL Policy tambi\xe9n aprender\xe1 estos errores\n"})}),"\n",(0,r.jsx)(a.p,{children:'Por ejemplo, si los jugadores en los datos de entrenamiento no juegan a menudo movimientos no tradicionales como el "Movimiento 37", la SL Policy tampoco los aprender\xe1.'}),"\n",(0,r.jsx)(a.h3,{id:"problema-2-no-puede-distinguir-buenas-de-malas-jugadas",children:"Problema 2: No puede distinguir buenas de malas jugadas"}),"\n",(0,r.jsx)(a.p,{children:'El aprendizaje supervisado solo ve "qu\xe9 jug\xf3 el humano", no si la jugada fue buena o mala:'}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Posici\xf3n A: el humano jug\xf3 K10 (en realidad una mala jugada)\nPosici\xf3n B: el humano jug\xf3 Q4 (buena jugada)\n\nSL Policy los trata por igual, tiene que aprender ambos\n"})}),"\n",(0,r.jsx)(a.p,{children:"Los datos de entrenamiento incluyen partidas de jugadores aficionados, que tienen muchos errores. SL Policy aprender\xe1 estos errores."}),"\n",(0,r.jsx)(a.h3,{id:"problema-3-exploraci\xf3n-insuficiente",children:"Problema 3: Exploraci\xf3n insuficiente"}),"\n",(0,r.jsx)(a.p,{children:"SL Policy solo aprende jugadas que los humanos ya conocen:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Conjunto de jugadas humanas: {A, B, C, D, E}\n           \u2193\nSL Policy solo elegir\xe1 entre estas jugadas\n           \u2193\nPodr\xeda existir una mejor jugada F, pero nunca fue descubierta\n"})}),"\n",(0,r.jsx)(a.p,{children:"Esta es la limitaci\xf3n fundamental del aprendizaje supervisado: solo puede aprender lo que est\xe1 en los datos de entrenamiento."}),"\n",(0,r.jsx)(a.h3,{id:"soluci\xf3n-aprendizaje-por-refuerzo",children:"Soluci\xf3n: Aprendizaje por refuerzo"}),"\n",(0,r.jsxs)(a.p,{children:["Para superar a los humanos, AlphaGo realiza ",(0,r.jsx)(a.strong,{children:"aprendizaje por refuerzo"})," despu\xe9s del aprendizaje supervisado:"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"SL Policy (nivel humano)\n      \u2193 auto-juego\nRL Policy (supera a humanos)\n"})}),"\n",(0,r.jsxs)(a.p,{children:["Ver ",(0,r.jsx)(a.a,{href:"../reinforcement-intro",children:"Introducci\xf3n al aprendizaje por refuerzo"})," y ",(0,r.jsx)(a.a,{href:"../self-play",children:"Auto-juego"})," para m\xe1s detalles."]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"puntos-de-implementaci\xf3n",children:"Puntos de implementaci\xf3n"}),"\n",(0,r.jsx)(a.h3,{id:"c\xf3digo-de-entrenamiento-completo",children:"C\xf3digo de entrenamiento completo"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\nclass GoDataset(Dataset):\n    def __init__(self, data_path):\n        # Cargar datos preprocesados\n        self.states = np.load(f"{data_path}/states.npy")\n        self.actions = np.load(f"{data_path}/actions.npy")\n\n    def __len__(self):\n        return len(self.states)\n\n    def __getitem__(self, idx):\n        state = torch.FloatTensor(self.states[idx])\n        action = torch.LongTensor([self.actions[idx]])[0]\n        return state, action\n\ndef train_policy_network():\n    # Modelo\n    model = PolicyNetwork(input_channels=48, num_filters=192, num_layers=12)\n    model = model.cuda()\n\n    # Datos\n    dataset = GoDataset("data/kgs")\n    dataloader = DataLoader(\n        dataset, batch_size=16, shuffle=True, num_workers=4\n    )\n\n    # Optimizador\n    optimizer = optim.SGD(\n        model.parameters(),\n        lr=0.003,\n        momentum=0.9,\n        weight_decay=1e-4\n    )\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=80_000_000, gamma=0.1)\n\n    # Bucle de entrenamiento\n    best_accuracy = 0\n\n    for epoch in range(100):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        for states, actions in dataloader:\n            states = states.cuda()\n            actions = actions.cuda()\n\n            # Propagaci\xf3n hacia adelante\n            policy = model(states)\n            loss = nn.functional.cross_entropy(policy, actions)\n\n            # Retropropagaci\xf3n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            # Estad\xedsticas\n            total_loss += loss.item()\n            predictions = policy.argmax(dim=1)\n            correct += (predictions == actions).sum().item()\n            total += actions.size(0)\n\n        accuracy = correct / total\n        print(f"Epoch {epoch}: Loss={total_loss/len(dataloader):.4f}, Acc={accuracy:.4f}")\n\n        # Guardar el mejor modelo\n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n            torch.save(model.state_dict(), "best_policy.pth")\n\n    print(f"Best accuracy: {best_accuracy:.4f}")\n'})}),"\n",(0,r.jsx)(a.h3,{id:"c\xf3digo-de-evaluaci\xf3n",children:"C\xf3digo de evaluaci\xf3n"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'def evaluate_policy(model, test_dataloader):\n    model.eval()\n\n    correct_top1 = 0\n    correct_top5 = 0\n    total = 0\n\n    with torch.no_grad():\n        for states, actions in test_dataloader:\n            states = states.cuda()\n            actions = actions.cuda()\n\n            policy = model(states)\n\n            # Precisi\xf3n Top-1\n            top1_pred = policy.argmax(dim=1)\n            correct_top1 += (top1_pred == actions).sum().item()\n\n            # Precisi\xf3n Top-5\n            top5_pred = policy.topk(5, dim=1)[1]\n            for i, action in enumerate(actions):\n                if action in top5_pred[i]:\n                    correct_top5 += 1\n\n            total += actions.size(0)\n\n    print(f"Top-1 Accuracy: {correct_top1/total:.4f}")\n    print(f"Top-5 Accuracy: {correct_top5/total:.4f}")\n'})}),"\n",(0,r.jsx)(a.h3,{id:"problemas-comunes-y-soluciones",children:"Problemas comunes y soluciones"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Problema"}),(0,r.jsx)(a.th,{children:"S\xedntoma"}),(0,r.jsx)(a.th,{children:"Soluci\xf3n"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Sobreajuste"}),(0,r.jsx)(a.td,{children:"Alta precisi\xf3n en entrenamiento, baja en prueba"}),(0,r.jsx)(a.td,{children:"M\xe1s aumento de datos, Dropout"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Entrenamiento inestable"}),(0,r.jsx)(a.td,{children:"P\xe9rdida fluct\xfaa violentamente"}),(0,r.jsx)(a.td,{children:"Reducir tasa de aprendizaje, aumentar tama\xf1o de lote"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Convergencia lenta"}),(0,r.jsx)(a.td,{children:"Precisi\xf3n estancada"}),(0,r.jsx)(a.td,{children:"Ajustar tasa de aprendizaje, verificar datos"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Memoria insuficiente"}),(0,r.jsx)(a.td,{children:"Error OOM"}),(0,r.jsx)(a.td,{children:"Reducir tama\xf1o de lote, usar precisi\xf3n mixta"})]})]})]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"correspondencia-con-animaciones",children:"Correspondencia con animaciones"}),"\n",(0,r.jsx)(a.p,{children:"Conceptos centrales de este art\xedculo y n\xfameros de animaci\xf3n:"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"N\xfamero"}),(0,r.jsx)(a.th,{children:"Concepto"}),(0,r.jsx)(a.th,{children:"Correspondencia f\xedsica/matem\xe1tica"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"D3"}),(0,r.jsx)(a.td,{children:"Aprendizaje supervisado"}),(0,r.jsx)(a.td,{children:"Estimaci\xf3n de m\xe1xima verosimilitud"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"D5"}),(0,r.jsx)(a.td,{children:"P\xe9rdida de entrop\xeda cruzada"}),(0,r.jsx)(a.td,{children:"Divergencia KL"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"D6"}),(0,r.jsx)(a.td,{children:"Descenso de gradiente"}),(0,r.jsx)(a.td,{children:"Optimizaci\xf3n"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"A6"}),(0,r.jsx)(a.td,{children:"Preprocesamiento de datos"}),(0,r.jsx)(a.td,{children:"Normalizaci\xf3n"})]})]})]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"lecturas-adicionales",children:"Lecturas adicionales"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Art\xedculo anterior"}),": ",(0,r.jsx)(a.a,{href:"../cnn-and-go",children:"CNN y Go"})," \u2014 C\xf3mo las redes neuronales convolucionales procesan el tablero"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Siguiente art\xedculo"}),": ",(0,r.jsx)(a.a,{href:"../reinforcement-intro",children:"Introducci\xf3n al aprendizaje por refuerzo"})," \u2014 La clave para superar a los humanos"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Tema relacionado"}),": ",(0,r.jsx)(a.a,{href:"../policy-network",children:"Detalles de la Policy Network"})," \u2014 Detalles de la arquitectura de red"]}),"\n"]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"puntos-clave",children:"Puntos clave"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Las partidas de KGS son la fuente de datos de entrenamiento"}),": aproximadamente 30 millones de posiciones de alta calidad"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"La p\xe9rdida de entrop\xeda cruzada impulsa el aprendizaje"}),": hace que el modelo aumente la probabilidad de la posici\xf3n correcta"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"57% de precisi\xf3n fue un avance importante"}),": superando el mejor m\xe9todo anterior por 13 puntos porcentuales"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Aumento de 8 simetr\xedas"}),": aumenta efectivamente los datos de entrenamiento"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"El aprendizaje supervisado tiene un techo"}),": no puede superar el nivel de los datos de entrenamiento"]}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:'El aprendizaje supervisado es el "punto de partida" de AlphaGo: hereda miles de a\xf1os de sabidur\xeda humana del Go, sentando las bases para el aprendizaje por refuerzo posterior.'}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"referencias",children:"Referencias"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:['Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." ',(0,r.jsx)(a.em,{children:"Nature"}),", 529, 484-489."]}),"\n",(0,r.jsxs)(a.li,{children:['Maddison, C. J., et al. (2014). "Move Evaluation in Go Using Deep Convolutional Neural Networks." ',(0,r.jsx)(a.em,{children:"arXiv:1412.6564"}),"."]}),"\n",(0,r.jsxs)(a.li,{children:['Clark, C., & Storkey, A. (2015). "Training Deep Convolutional Neural Networks to Play Go." ',(0,r.jsx)(a.em,{children:"ICML"}),"."]}),"\n",(0,r.jsxs)(a.li,{children:["KGS Game Archives: ",(0,r.jsx)(a.a,{href:"https://www.gokgs.com/archives.jsp",children:"https://www.gokgs.com/archives.jsp"})]}),"\n"]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},42948(e,a,n){n.d(a,{A:()=>s});n(59471);var t=n(61785),r=n(62615);function s({children:e,fallback:a}){return(0,t.A)()?(0,r.jsx)(r.Fragment,{children:e?.()}):a??null}},45695(e,a,n){n.d(a,{$W:()=>k,tO:()=>o,u8:()=>g,dW:()=>x});var t=n(59471),r=n(90989),s=n(62615);const i=19,d=[[3,3],[3,9],[3,15],[9,3],[9,9],[9,15],[15,3],[15,9],[15,15]];function o({size:e=400,stones:a=[],highlights:n=[],labels:o=[],onCellClick:l=null,showCoordinates:c=!0}){const p=(0,t.useRef)(null),h=c?30:15,u=e-2*h,x=u/18;return(0,t.useEffect)(()=>{if(!p.current)return;const e=r.Ltv(p.current);e.selectAll("*").remove();const t=e.append("g").attr("transform",`translate(${h}, ${h})`);t.append("rect").attr("x",-x/2).attr("y",-x/2).attr("width",u+x).attr("height",u+x).attr("fill","#dcb35c").attr("rx",4);const s=t.append("g").attr("class","grid");for(let a=0;a<i;a++)s.append("line").attr("class","grid-line").attr("x1",0).attr("y1",a*x).attr("x2",18*x).attr("y2",a*x);for(let a=0;a<i;a++)s.append("line").attr("class","grid-line").attr("x1",a*x).attr("y1",0).attr("x2",a*x).attr("y2",18*x);const m=t.append("g").attr("class","star-points");if(d.forEach(([e,a])=>{m.append("circle").attr("class","star-point").attr("cx",e*x).attr("cy",a*x).attr("r",x/8)}),n.length>0){const e=t.append("g").attr("class","highlights");n.forEach(({x:a,y:n,intensity:t})=>{e.append("rect").attr("class","heatmap-cell").attr("x",a*x-x/2).attr("y",n*x-x/2).attr("width",x).attr("height",x).attr("fill",r.Q3(t)).attr("opacity",.7*t)})}const j=t.append("g").attr("class","stones");if(a.forEach(({x:e,y:a,color:n})=>{const t="black"===n?"stone-black":"stone-white";j.append("circle").attr("cx",e*x+2).attr("cy",a*x+2).attr("r",.45*x).attr("fill","rgba(0,0,0,0.2)"),j.append("circle").attr("class",t).attr("cx",e*x).attr("cy",a*x).attr("r",.45*x)}),o.length>0){const e=t.append("g").attr("class","labels");o.forEach(({x:n,y:t,text:r})=>{const s=a.find(e=>e.x===n&&e.y===t),i="black"===s?.color?"#fff":"#000";e.append("text").attr("x",n*x).attr("y",t*x).attr("dy","0.35em").attr("text-anchor","middle").attr("fill",i).attr("font-size",.5*x).attr("font-weight","bold").text(r)})}if(c){const a=e.append("g").attr("class","coordinates"),n="ABCDEFGHJKLMNOPQRST";for(let e=0;e<i;e++)a.append("text").attr("x",h+e*x).attr("y",h/2).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(n[e]);for(let e=0;e<i;e++)a.append("text").attr("x",h/2).attr("y",h+e*x).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(i-e)}l&&t.append("g").attr("class","click-targets").selectAll("rect").data(r.y17(361)).enter().append("rect").attr("x",e=>e%i*x-x/2).attr("y",e=>Math.floor(e/i)*x-x/2).attr("width",x).attr("height",x).attr("fill","transparent").attr("cursor","pointer").on("click",(e,a)=>{const n=a%i,t=Math.floor(a/i);l({x:n,y:t})})},[e,a,n,o,c,l,x,h,u]),(0,s.jsx)("div",{className:"go-board-container",children:(0,s.jsx)("svg",{ref:p,width:e,height:e,className:"go-board"})})}var l=n(42948);const c=19,p={empty:function(){const e=[];for(let a=0;a<c;a++)for(let n=0;n<c;n++)e.push({x:n,y:a,prob:1/361});return e}(),corner:function(){const e=[],a=[[3,3],[3,15],[15,3],[15,15]],n=[[2,4],[4,2],[2,14],[4,16],[14,2],[16,4],[14,16],[16,14]];for(let t=0;t<c;t++)for(let r=0;r<c;r++){let s=.001;a.some(([e,a])=>e===r&&a===t)?s=.15:n.some(([e,a])=>e===r&&a===t)?s=.05:0!==r&&18!==r&&0!==t&&18!==t||(s=5e-4),e.push({x:r,y:t,prob:s})}return h(e)}(),move37:function(){const e=[],a={x:9,y:4},n=[[3,2],[15,2],[10,10],[8,6]];for(let t=0;t<c;t++)for(let r=0;r<c;r++){let s=.001;r===a.x&&t===a.y?s=.08:n.some(([e,a])=>e===r&&a===t)?s=.12:r>=5&&r<=13&&t>=5&&t<=13&&(s=.005+.01*Math.random()),e.push({x:r,y:t,prob:s})}return h(e)}()};function h(e){const a=e.reduce((e,a)=>e+a.prob,0);return e.map(e=>({...e,prob:e.prob/a}))}function u({initialPosition:e="corner",stones:a=[],highlightMoves:n=[],size:i=450,showTopN:d=5,interactive:o=!0}){const l=(0,t.useRef)(null),h=(0,t.useRef)(null),[u,x]=(0,t.useState)(p[e]||p.corner),[m,j]=(0,t.useState)(null),g=35,f=i-70,v=f/18;(0,t.useEffect)(()=>{if(!l.current)return;const e=r.Ltv(l.current);e.selectAll("*").remove();const n=e.append("g").attr("transform","translate(35, 35)");n.append("rect").attr("x",-v/2).attr("y",-v/2).attr("width",f+v).attr("height",f+v).attr("fill","#dcb35c").attr("rx",4);const t=Math.max(...u.map(e=>e.prob)),s=r.exT(r.oKI).domain([0,t]);n.append("g").attr("class","heatmap").selectAll("rect").data(u).enter().append("rect").attr("class","heatmap-cell").attr("x",e=>e.x*v-v/2).attr("y",e=>e.y*v-v/2).attr("width",v).attr("height",v).attr("fill",e=>s(e.prob)).attr("opacity",e=>.3+e.prob/t*.6).attr("cursor",o?"pointer":"default").on("mouseover",function(e,a){if(!o)return;r.Ltv(this).attr("stroke","#333").attr("stroke-width",2);r.Ltv(h.current).style("display","block").style("left",`${e.pageX+10}px`).style("top",e.pageY-10+"px").html(`\u4f4d\u7f6e: ${String.fromCharCode(65+a.x)}${19-a.y}<br>\u6a5f\u7387: ${(100*a.prob).toFixed(2)}%`)}).on("mouseout",function(){r.Ltv(this).attr("stroke","none"),r.Ltv(h.current).style("display","none")}).on("click",function(e,a){o&&j(a)});const i=n.append("g").attr("class","grid");for(let a=0;a<c;a++)i.append("line").attr("class","grid-line").attr("x1",0).attr("y1",a*v).attr("x2",18*v).attr("y2",a*v).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5),i.append("line").attr("class","grid-line").attr("x1",a*v).attr("y1",0).attr("x2",a*v).attr("y2",18*v).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5);const p=n.append("g").attr("class","stones");a.forEach(({x:e,y:a,color:n})=>{p.append("circle").attr("cx",e*v).attr("cy",a*v).attr("r",.45*v).attr("fill","black"===n?"#1a1a1a":"#f5f5f5").attr("stroke","black"===n?"#000":"#333").attr("stroke-width",1)});const x=[...u].sort((e,a)=>a.prob-e.prob).slice(0,d),m=n.append("g").attr("class","top-labels");x.forEach((e,n)=>{a.some(a=>a.x===e.x&&a.y===e.y)||(m.append("circle").attr("cx",e.x*v).attr("cy",e.y*v).attr("r",.3*v).attr("fill","rgba(255,255,255,0.8)").attr("stroke","#e74c3c").attr("stroke-width",2),m.append("text").attr("x",e.x*v).attr("y",e.y*v).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#e74c3c").attr("font-size",.4*v).attr("font-weight","bold").text(n+1))});const y=e.append("g").attr("class","coordinates");for(let a=0;a<c;a++)y.append("text").attr("x",g+a*v).attr("y",17.5).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text("ABCDEFGHJKLMNOPQRST"[a]),y.append("text").attr("x",17.5).attr("y",g+a*v).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(c-a)},[u,a,d,o,v,g,f]);const y=e=>{x(p[e]||p.corner)};return(0,s.jsxs)("div",{children:[o&&(0,s.jsxs)("div",{className:"d3-controls",children:[(0,s.jsx)("button",{className:"empty"===e?"active":"",onClick:()=>y("empty"),children:"\u5747\u52fb\u5206\u5e03"}),(0,s.jsx)("button",{className:"corner"===e?"active":"",onClick:()=>y("corner"),children:"\u958b\u5c40\u661f\u4f4d"}),(0,s.jsx)("button",{className:"move37"===e?"active":"",onClick:()=>y("move37"),children:"\u7b2c 37 \u624b"})]}),(0,s.jsx)("div",{className:"go-board-container",children:(0,s.jsx)("svg",{ref:l,width:i,height:i,className:"go-board"})}),(0,s.jsx)("div",{ref:h,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),m&&(0,s.jsx)("div",{className:"d3-legend",children:(0,s.jsxs)("div",{className:"d3-legend-item",children:["\u5df2\u9078\u64c7: ",String.fromCharCode(65+m.x),19-m.y,"\u2014 \u6a5f\u7387: ",(100*m.prob).toFixed(2),"%"]})}),(0,s.jsxs)("div",{className:"d3-legend",children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#ffffb2"}}),"\u4f4e\u6a5f\u7387"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#fd8d3c"}}),"\u4e2d\u6a5f\u7387"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#bd0026"}}),"\u9ad8\u6a5f\u7387"]})]})]})}function x(e){return(0,s.jsx)(l.A,{fallback:(0,s.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,s.jsx)(u,{...e})})}const m={name:"Root",visits:1600,value:.55,prior:1,children:[{name:"D4",visits:800,value:.62,prior:.35,selected:!0,children:[{name:"Q16",visits:400,value:.58,prior:.3},{name:"R4",visits:300,value:.65,prior:.25,selected:!0},{name:"C16",visits:100,value:.55,prior:.2}]},{name:"Q4",visits:500,value:.52,prior:.3,children:[{name:"D16",visits:300,value:.5,prior:.28},{name:"Q16",visits:200,value:.54,prior:.22}]},{name:"D16",visits:200,value:.48,prior:.2},{name:"Q16",visits:100,value:.45,prior:.15}]};function j({data:e=m,width:a=700,height:n=450,showPUCT:i=!0,cPuct:d=1.5,interactive:o=!0}){const l=(0,t.useRef)(null),c=(0,t.useRef)(null),[p,h]=(0,t.useState)(null),[u,x]=(0,t.useState)(d),j=40,g=40,f=a-g-40,v=n-j-40;return(0,t.useEffect)(()=>{if(!l.current)return;const t=r.Ltv(l.current);t.selectAll("*").remove();const s=r.B22().size([f,v-50]),d=r.Sk5(e);s(d);const p=t.append("g").attr("transform",`translate(${g}, ${j})`);p.append("g").attr("class","links").selectAll("path").data(d.links()).enter().append("path").attr("class",e=>"link "+(e.target.data.selected?"selected":"")).attr("fill","none").attr("stroke",e=>e.target.data.selected?"#4a90d9":"#999").attr("stroke-width",e=>e.target.data.selected?3:1.5).attr("d",r.vu().x(e=>e.x).y(e=>e.y));const x=p.append("g").attr("class","nodes").selectAll("g").data(d.descendants()).enter().append("g").attr("class","node").attr("transform",e=>`translate(${e.x}, ${e.y})`).attr("cursor",o?"pointer":"default").on("mouseover",function(e,a){if(!o)return;r.Ltv(this).select("circle").transition().duration(200).attr("r",30);const n=a.parent?a.parent.data.visits:a.data.visits,t=((e,a)=>{if(!a)return 0;const n=e.value,t=e.prior,r=e.visits;return n+u*t*Math.sqrt(a)/(1+r)})(a.data,n);r.Ltv(c.current).style("display","block").style("left",`${e.pageX+15}px`).style("top",e.pageY-10+"px").html(`\n            <strong>${a.data.name}</strong><br>\n            \u8a2a\u554f\u6b21\u6578 (N): ${a.data.visits}<br>\n            \u5e73\u5747\u50f9\u503c (Q): ${a.data.value.toFixed(3)}<br>\n            \u5148\u9a57\u6a5f\u7387 (P): ${(100*a.data.prior).toFixed(1)}%<br>\n            ${i?`PUCT \u5206\u6578: ${t.toFixed(3)}`:""}\n          `)}).on("mouseout",function(){r.Ltv(this).select("circle").transition().duration(200).attr("r",25),r.Ltv(c.current).style("display","none")}).on("click",function(e,a){o&&h(a.data)});x.append("circle").attr("r",25).attr("fill",e=>e.data.selected?"#4a90d9":"#fff").attr("stroke",a=>{if(a.data.selected)return"#2c5282";const n=a.data.visits/e.visits;return r.dM(.3+.5*n)}).attr("stroke-width",e=>e.data.selected?3:2),x.append("text").attr("dy",-5).attr("text-anchor","middle").attr("fill",e=>e.data.selected?"#fff":"#333").attr("font-size",11).attr("font-weight","bold").text(e=>e.data.name),x.append("text").attr("dy",10).attr("text-anchor","middle").attr("fill",e=>e.data.selected?"#fff":"#666").attr("font-size",9).text(e=>`N=${e.data.visits}`),t.append("text").attr("x",a/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("MCTS \u641c\u7d22\u6a39"),i&&t.append("text").attr("x",a/2).attr("y",n-10).attr("text-anchor","middle").attr("font-size",11).attr("fill","#666").text("\u85cd\u8272\u8def\u5f91\uff1aPUCT \u9078\u64c7\u7684\u6700\u4f73\u8def\u5f91")},[e,a,n,i,u,o,f,v]),(0,s.jsxs)("div",{children:[i&&o&&(0,s.jsx)("div",{className:"d3-controls",children:(0,s.jsxs)("div",{className:"d3-slider",children:[(0,s.jsxs)("label",{children:["c_puct: ",u.toFixed(1)]}),(0,s.jsx)("input",{type:"range",min:"0.5",max:"3",step:"0.1",value:u,onChange:e=>x(parseFloat(e.target.value))})]})}),(0,s.jsx)("div",{className:"mcts-tree-container",children:(0,s.jsx)("svg",{ref:l,width:a,height:n,className:"mcts-tree"})}),(0,s.jsx)("div",{ref:c,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),p&&(0,s.jsxs)("div",{className:"d3-legend",style:{background:"#f5f5f5",padding:"1rem",borderRadius:"4px"},children:[(0,s.jsxs)("strong",{children:["\u5df2\u9078\u64c7\u7bc0\u9ede: ",p.name]}),(0,s.jsxs)("div",{children:["\u8a2a\u554f\u6b21\u6578: ",p.visits]}),(0,s.jsxs)("div",{children:["\u5e73\u5747\u50f9\u503c: ",p.value.toFixed(3)]}),(0,s.jsxs)("div",{children:["\u5148\u9a57\u6a5f\u7387: ",(100*p.prior).toFixed(1),"%"]})]}),(0,s.jsxs)("div",{className:"d3-legend",children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"\u9078\u4e2d\u8def\u5f91"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#fff",border:"2px solid #999"}}),"\u5176\u4ed6\u7bc0\u9ede"]}),(0,s.jsx)("div",{className:"d3-legend-item",children:(0,s.jsx)("span",{style:{fontSize:"12px"},children:"\u7bc0\u9ede\u5927\u5c0f \u221d \u8a2a\u554f\u6b21\u6578"})})]})]})}function g(e){return(0,s.jsx)(l.A,{fallback:(0,s.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,s.jsx)(j,{...e})})}const f=[{hours:0,elo:0,label:"\u96a8\u6a5f"},{hours:3,elo:1e3,label:"\u767c\u73fe\u898f\u5247"},{hours:6,elo:2e3},{hours:12,elo:3e3,label:"\u767c\u73fe\u5b9a\u5f0f"},{hours:24,elo:4e3},{hours:36,elo:4500,label:"\u8d85\u8d8a Fan Hui"},{hours:48,elo:5e3},{hours:60,elo:5200,label:"\u8d85\u8d8a Lee Sedol"},{hours:72,elo:5400,label:"\u8d85\u8d8a\u539f\u7248 AlphaGo"}],v=[{elo:2700,label:"\u696d\u9918\u5f37\u8c6a"},{elo:3500,label:"Fan Hui (\u8077\u696d\u4e8c\u6bb5)"},{elo:4500,label:"Lee Sedol (\u4e16\u754c\u51a0\u8ecd)"},{elo:5e3,label:"\u539f\u7248 AlphaGo"}],y=[{epochs:0,elo:0},{epochs:10,elo:1500},{epochs:20,elo:2500},{epochs:30,elo:3e3},{epochs:40,elo:3200},{epochs:50,elo:3300}],b=[{games:0,elo:3300},{games:1e3,elo:3800},{games:5e3,elo:4200},{games:1e4,elo:4500},{games:5e4,elo:4800},{games:1e5,elo:5e3}];function z({mode:e="zero",width:a=600,height:n=400,animated:i=!0,showMilestones:d=!0}){const o=(0,t.useRef)(null),[l,c]=(0,t.useState)(e),p=40,h=70,u=a-h-100,x=n-p-60;return(0,t.useEffect)(()=>{if(!o.current)return;const e=r.Ltv(o.current);let n,t,s;e.selectAll("*").remove(),"zero"===l?(n=f,t="\u8a13\u7df4\u6642\u9593\uff08\u5c0f\u6642\uff09",s=[0,80]):"sl"===l?(n=y,t="\u8a13\u7df4\u8f2a\u6578\uff08Epochs\uff09",s=[0,60]):(n=b,t="\u81ea\u6211\u5c0d\u5f08\u5c40\u6578",s=[0,12e4]);const c="selfplay"===l?r.ZEH().domain([1,s[1]]).range([0,u]):r.m4Y().domain(s).range([0,u]),m=r.m4Y().domain([0,6e3]).range([x,0]),j=e.append("g").attr("transform",`translate(${h}, ${p})`);if(j.append("g").attr("class","grid").selectAll(".grid-line-y").data(m.ticks(6)).enter().append("line").attr("class","grid-line-y").attr("x1",0).attr("x2",u).attr("y1",e=>m(e)).attr("y2",e=>m(e)).attr("stroke","#ddd").attr("stroke-dasharray","3,3"),d&&"zero"===l){const e=j.append("g").attr("class","human-levels");v.forEach(a=>{e.append("line").attr("x1",0).attr("x2",u).attr("y1",m(a.elo)).attr("y2",m(a.elo)).attr("stroke","#e74c3c").attr("stroke-dasharray","5,5").attr("opacity",.6),e.append("text").attr("x",u+5).attr("y",m(a.elo)).attr("dy","0.35em").attr("fill","#e74c3c").attr("font-size",10).text(a.label)})}const g=r.n8j().x(e=>c("zero"===l?e.hours:"sl"===l?e.epochs:Math.max(1,e.games))).y(e=>m(e.elo)).curve(r.nVG),z=r.Wcw().x(e=>c("zero"===l?e.hours:"sl"===l?e.epochs:Math.max(1,e.games))).y0(x).y1(e=>m(e.elo)).curve(r.nVG);j.append("path").datum(n).attr("class","area").attr("fill","#4a90d9").attr("opacity",.1).attr("d",z);const k=j.append("path").datum(n).attr("class","line").attr("fill","none").attr("stroke","#4a90d9").attr("stroke-width",3).attr("d",g);if(i){const e=k.node().getTotalLength();k.attr("stroke-dasharray",`${e} ${e}`).attr("stroke-dashoffset",e).transition().duration(2e3).ease(r.yfw).attr("stroke-dashoffset",0)}if(d&&"zero"===l){const e=n.filter(e=>e.label),a=j.append("g").attr("class","milestones");a.selectAll("circle").data(e).enter().append("circle").attr("cx",e=>c(e.hours)).attr("cy",e=>m(e.elo)).attr("r",6).attr("fill","#e74c3c").attr("stroke","#fff").attr("stroke-width",2),a.selectAll("text").data(e).enter().append("text").attr("x",e=>c(e.hours)).attr("y",e=>m(e.elo)-15).attr("text-anchor","middle").attr("fill","#333").attr("font-size",10).text(e=>e.label)}const P="selfplay"===l?r.l78(c).ticks(5,"~s"):r.l78(c);j.append("g").attr("class","x-axis").attr("transform",`translate(0, ${x})`).call(P),j.append("text").attr("class","axis-label").attr("x",u/2).attr("y",x+45).attr("text-anchor","middle").attr("fill","#666").text(t),j.append("g").attr("class","y-axis").call(r.V4s(m).ticks(6)),j.append("text").attr("class","axis-label").attr("transform","rotate(-90)").attr("x",-x/2).attr("y",-50).attr("text-anchor","middle").attr("fill","#666").text("ELO \u8a55\u5206"),e.append("text").attr("x",a/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("zero"===l?"AlphaGo Zero \u8a13\u7df4\u66f2\u7dda":"sl"===l?"\u76e3\u7763\u5b78\u7fd2\u68cb\u529b\u6210\u9577":"\u81ea\u6211\u5c0d\u5f08\u68cb\u529b\u6210\u9577")},[l,a,n,i,d,u,x]),(0,s.jsxs)("div",{children:[(0,s.jsxs)("div",{className:"d3-controls",children:[(0,s.jsx)("button",{className:"zero"===l?"active":"",onClick:()=>c("zero"),children:"AlphaGo Zero"}),(0,s.jsx)("button",{className:"sl"===l?"active":"",onClick:()=>c("sl"),children:"\u76e3\u7763\u5b78\u7fd2"}),(0,s.jsx)("button",{className:"selfplay"===l?"active":"",onClick:()=>c("selfplay"),children:"\u81ea\u6211\u5c0d\u5f08"})]}),(0,s.jsx)("div",{className:"elo-chart-container",children:(0,s.jsx)("svg",{ref:o,width:a,height:n,className:"elo-chart"})}),(0,s.jsxs)("div",{className:"d3-legend",children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"ELO \u8a55\u5206"]}),d&&"zero"===l&&(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c"}}),"\u91cc\u7a0b\u7891"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c",opacity:.3}}),"\u4eba\u985e\u6c34\u5e73\u53c3\u8003\u7dda"]})]})]})]})}function k(e){return(0,s.jsx)(l.A,{fallback:(0,s.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,s.jsx)(z,{...e})})}},30416(e,a,n){n.d(a,{R:()=>i,x:()=>d});var t=n(59471);const r={},s=t.createContext(r);function i(e){const a=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function d(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),t.createElement(s.Provider,{value:a},e.children)}}}]);