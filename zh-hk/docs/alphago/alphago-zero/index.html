<!doctype html>
<html lang="zh-hk" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-alphago/alphago-zero" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">AlphaGo Zero 概述 | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/zh-hk/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/zh-hk/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/zh-hk/docs/alphago/alphago-zero/"><meta data-rh="true" property="og:locale" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="zh-hk"><meta data-rh="true" name="docsearch:language" content="zh-hk"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="AlphaGo Zero 概述 | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="由零開始、完全自學，AlphaGo Zero 點樣喺無人類棋譜嘅情況下超越所有前代版本"><meta data-rh="true" property="og:description" content="由零開始、完全自學，AlphaGo Zero 點樣喺無人類棋譜嘅情況下超越所有前代版本"><meta data-rh="true" name="keywords" content="AlphaGo Zero,自我對弈,強化學習,深度學習,圍棋 AI,無監督學習"><link data-rh="true" rel="icon" href="/zh-hk/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/zh-hk/docs/alphago/alphago-zero/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/alphago-zero/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/alphago/alphago-zero/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/alphago/alphago-zero/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/alphago/alphago-zero/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/alphago/alphago-zero/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/alphago/alphago-zero/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/alphago/alphago-zero/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/alphago/alphago-zero/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/alphago/alphago-zero/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/alphago/alphago-zero/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/alphago/alphago-zero/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/alphago-zero/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AlphaGo","item":"https://www.weiqi.kids/zh-hk/docs/alphago/"},{"@type":"ListItem","position":2,"name":"AlphaGo Zero 概述","item":"https://www.weiqi.kids/zh-hk/docs/alphago/alphago-zero"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/zh-hk/assets/css/styles.f23bf74b.css">
<script src="/zh-hk/assets/js/runtime~main.ff09f996.js" defer="defer"></script>
<script src="/zh-hk/assets/js/main.5904d3b4.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/zh-hk/img/logo.svg"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zh-hk/"><div class="navbar__logo"><img src="/zh-hk/img/logo.svg" alt="好棋寶寶協會標誌" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/zh-hk/img/logo.svg" alt="好棋寶寶協會標誌" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">圍棋寶寶</b></a><a class="navbar__item navbar__link" href="/zh-hk/docs/learn/">學圍棋</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/zh-hk/docs/alphago/">AlphaGo</a><a class="navbar__item navbar__link" href="/zh-hk/docs/animations/">動畫教室</a><a class="navbar__item navbar__link" href="/zh-hk/docs/tech/">技術文件</a><a class="navbar__item navbar__link" href="/zh-hk/docs/about/">關於我哋</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>粵語（香港）</a><ul class="dropdown__menu"><li><a href="/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/zh-hk/docs/intro/"><span title="使用指南" class="linkLabel_REp1">使用指南</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/zh-hk/docs/alphago/"><span title="AlphaGo" class="categoryLinkLabel_ezQx">AlphaGo</span></a><button aria-label="收起側邊欄分類 &#x27;AlphaGo&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/birth-of-alphago/"><span title="AlphaGo 嘅誕生" class="linkLabel_REp1">AlphaGo 嘅誕生</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/key-matches/"><span title="關鍵對局回顧" class="linkLabel_REp1">關鍵對局回顧</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/move-37/"><span title="「神之一手」深度分析" class="linkLabel_REp1">「神之一手」深度分析</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/why-go-is-hard/"><span title="圍棋為甚麼難？" class="linkLabel_REp1">圍棋為甚麼難？</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/traditional-limits/"><span title="傳統方法的極限" class="linkLabel_REp1">傳統方法的極限</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/board-representation/"><span title="棋盤狀態表示" class="linkLabel_REp1">棋盤狀態表示</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/policy-network/"><span title="Policy Network 詳解" class="linkLabel_REp1">Policy Network 詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/value-network/"><span title="Value Network 詳解" class="linkLabel_REp1">Value Network 詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/input-features/"><span title="輸入特徵設計" class="linkLabel_REp1">輸入特徵設計</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/cnn-and-go/"><span title="CNN 與圍棋嘅結合" class="linkLabel_REp1">CNN 與圍棋嘅結合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/supervised-learning/"><span title="監督學習階段" class="linkLabel_REp1">監督學習階段</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/reinforcement-intro/"><span title="強化學習入門" class="linkLabel_REp1">強化學習入門</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/self-play/"><span title="自我對弈" class="linkLabel_REp1">自我對弈</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/mcts-neural-combo/"><span title="MCTS 與神經網絡嘅結合" class="linkLabel_REp1">MCTS 與神經網絡嘅結合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/puct-formula/"><span title="PUCT 公式詳解" class="linkLabel_REp1">PUCT 公式詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/zh-hk/docs/alphago/alphago-zero/"><span title="AlphaGo Zero 概述" class="linkLabel_REp1">AlphaGo Zero 概述</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/dual-head-resnet/"><span title="雙頭網絡與殘差網絡" class="linkLabel_REp1">雙頭網絡與殘差網絡</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/training-from-scratch/"><span title="從零訓練嘅過程" class="linkLabel_REp1">從零訓練嘅過程</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/distributed-systems/"><span title="分散式系統與 TPU" class="linkLabel_REp1">分散式系統與 TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-hk/docs/alphago/legacy-and-impact/"><span title="AlphaGo 嘅遺產" class="linkLabel_REp1">AlphaGo 嘅遺產</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/zh-hk/docs/learn/"><span title="學圍棋" class="categoryLinkLabel_ezQx">學圍棋</span></a><button aria-label="展開側邊欄分類 &#x27;學圍棋&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/zh-hk/docs/animations/"><span title="動畫教室" class="linkLabel_REp1">動畫教室</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/zh-hk/docs/tech/"><span title="技術文件" class="categoryLinkLabel_ezQx">技術文件</span></a><button aria-label="展開側邊欄分類 &#x27;技術文件&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/zh-hk/docs/about/"><span title="關於我們" class="categoryLinkLabel_ezQx">關於我們</span></a><button aria-label="展開側邊欄分類 &#x27;關於我們&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="頁面路徑"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/zh-hk/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/zh-hk/docs/alphago/"><span>AlphaGo</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">AlphaGo Zero 概述</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">本頁導覽</button></div><div class="theme-doc-markdown markdown"><header><h1>AlphaGo Zero 概述</h1></header>
<p>2017 年 10 月，DeepMind 發表咗一個震驚 AI 界嘅成果：<strong>AlphaGo Zero</strong> 喺無使用任何人類棋譜嘅情況下，由完全隨機嘅狀態開始訓練，淨係三日就超越咗擊敗李世乭嘅原版 AlphaGo，並以 <strong>100:0</strong> 嘅比數完勝。</p>
<p>呢個唔淨止係數字上嘅進步。呢個代表一個全新嘅範式：<strong>AI 唔需要人類知識，可以由零發現一切</strong>。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="點解唔需要人類棋譜">點解唔需要人類棋譜？<a href="#點解唔需要人類棋譜" class="hash-link" aria-label="點解唔需要人類棋譜？的直接連結" title="點解唔需要人類棋譜？的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="人類棋譜嘅限制">人類棋譜嘅限制<a href="#人類棋譜嘅限制" class="hash-link" aria-label="人類棋譜嘅限制的直接連結" title="人類棋譜嘅限制的直接連結" translate="no">​</a></h3>
<p>原版 AlphaGo 嘅訓練過程分為兩個階段：</p>
<ol>
<li class=""><strong>監督學習</strong>：用 3000 萬局人類棋譜訓練 Policy Network</li>
<li class=""><strong>強化學習</strong>：透過自我對弈進一步提升</li>
</ol>
<p>呢個方法有幾個根本性嘅問題：</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-人類棋譜有上限">1. 人類棋譜有上限<a href="#1-人類棋譜有上限" class="hash-link" aria-label="1. 人類棋譜有上限的直接連結" title="1. 人類棋譜有上限的直接連結" translate="no">​</a></h4>
<p>人類棋手嘅棋力有極限，棋譜入面包含嘅係人類嘅理解，都包含人類嘅錯誤同偏見。當 AI 由人類棋譜學習嗰陣，佢學到嘅係：</p>
<ul>
<li class="">人類認為好嘅著法（但唔一定係最優嘅）</li>
<li class="">人類嘅思維模式（但可能限制創新）</li>
<li class="">人類嘅錯誤（會被當作正確嘅樣本學習）</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-監督學習嘅瓶頸">2. 監督學習嘅瓶頸<a href="#2-監督學習嘅瓶頸" class="hash-link" aria-label="2. 監督學習嘅瓶頸的直接連結" title="2. 監督學習嘅瓶頸的直接連結" translate="no">​</a></h4>
<p>監督學習嘅目標係「模仿人類」——預測人類棋手會落邊一步。呢個意味住 AI 嘅能力上限被人類棋手嘅能力所限制。</p>
<p>就好似一個學徒只能模仿師傅，永遠無法超越師傅一樣。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-資料收集成本">3. 資料收集成本<a href="#3-資料收集成本" class="hash-link" aria-label="3. 資料收集成本的直接連結" title="3. 資料收集成本的直接連結" translate="no">​</a></h4>
<p>高質素嘅人類棋譜需要多年累積，而且只存在於圍棋呢類有悠久歷史嘅遊戲入面。如果要將 AI 應用到新領域（好似蛋白質結構預測），根本無「人類專家棋譜」可用。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="zero-嘅突破">Zero 嘅突破<a href="#zero-嘅突破" class="hash-link" aria-label="Zero 嘅突破的直接連結" title="Zero 嘅突破的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 完全跳過監督學習階段，直接由<strong>隨機初始化</strong>開始自我對弈。呢個解決咗上述所有問題：</p>
<table><thead><tr><th>問題</th><th>原版 AlphaGo</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>人類知識上限</td><td>受限於棋譜質素</td><td>無此限制</td></tr><tr><td>學習目標</td><td>模仿人類</td><td>最大化勝率</td></tr><tr><td>資料需求</td><td>3000 萬局棋譜</td><td>0</td></tr><tr><td>可推廣性</td><td>只限圍棋</td><td>可推廣至其他領域</td></tr></tbody></table>
<p>呢個係一個根本性嘅範式轉變：由「學習人類知識」轉向「由第一性原理發現知識」。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="與原版-alphago-嘅對比1000">與原版 AlphaGo 嘅對比：100:0<a href="#與原版-alphago-嘅對比1000" class="hash-link" aria-label="與原版 AlphaGo 嘅對比：100:0的直接連結" title="與原版 AlphaGo 嘅對比：100:0的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="碾壓性嘅勝利">碾壓性嘅勝利<a href="#碾壓性嘅勝利" class="hash-link" aria-label="碾壓性嘅勝利的直接連結" title="碾壓性嘅勝利的直接連結" translate="no">​</a></h3>
<p>DeepMind 令訓練完成嘅 AlphaGo Zero 與各個版本嘅 AlphaGo 對弈：</p>
<table><thead><tr><th>對手</th><th>AlphaGo Zero 戰績</th></tr></thead><tbody><tr><td>AlphaGo Fan（擊敗樊麾版本）</td><td>100:0</td></tr><tr><td>AlphaGo Lee（擊敗李世乭版本）</td><td>100:0</td></tr><tr><td>AlphaGo Master（60 連勝版本）</td><td>89:11</td></tr></tbody></table>
<p><strong>100:0</strong>——呢個意味住喺 100 盤比賽入面，原版 AlphaGo 連一盤都贏唔到。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="更少嘅資源更強嘅棋力">更少嘅資源，更強嘅棋力<a href="#更少嘅資源更強嘅棋力" class="hash-link" aria-label="更少嘅資源，更強嘅棋力的直接連結" title="更少嘅資源，更強嘅棋力的直接連結" translate="no">​</a></h3>
<p>唔淨止係贏，AlphaGo Zero 仲用更少嘅資源達成更強嘅棋力：</p>
<table><thead><tr><th>指標</th><th>AlphaGo Lee</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>訓練時間</td><td>數個月</td><td>40 日（3 日超越 AlphaGo Lee）</td></tr><tr><td>訓練局數</td><td>3000 萬人類棋譜 + 自我對弈</td><td>490 萬局自我對弈</td></tr><tr><td>TPU 數量（訓練）</td><td>50+</td><td>4</td></tr><tr><td>TPU 數量（推理）</td><td>48</td><td>4</td></tr><tr><td>輸入特徵</td><td>48 個平面</td><td>17 個平面</td></tr><tr><td>神經網絡</td><td>SL + RL 雙網絡</td><td>單一雙頭網絡</td></tr></tbody></table>
<p>呢個係一個驚人嘅效率提升：<strong>資源減少 10 倍以上，棋力卻大幅提升</strong>。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="點解-zero-更強">點解 Zero 更強？<a href="#點解-zero-更強" class="hash-link" aria-label="點解 Zero 更強？的直接連結" title="點解 Zero 更強？的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 更強嘅原因可以由幾個角度理解：</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-無偏見嘅學習">1. 無偏見嘅學習<a href="#1-無偏見嘅學習" class="hash-link" aria-label="1. 無偏見嘅學習的直接連結" title="1. 無偏見嘅學習的直接連結" translate="no">​</a></h4>
<p>原版 AlphaGo 由人類棋譜學習，繼承咗人類嘅偏見。例如，人類棋手可能過度重視某啲定式，或者對某啲局面有錯誤嘅評估。</p>
<p>AlphaGo Zero 無呢啲包袱。佢由白紙開始，只透過勝負結果嚟學習乜嘢係好棋。呢個令佢能夠發現人類從未諗過嘅著法。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-一致嘅學習目標">2. 一致嘅學習目標<a href="#2-一致嘅學習目標" class="hash-link" aria-label="2. 一致嘅學習目標的直接連結" title="2. 一致嘅學習目標的直接連結" translate="no">​</a></h4>
<p>原版 AlphaGo 嘅訓練有兩個唔同嘅目標：</p>
<ul>
<li class="">監督學習：最大化對人類落子嘅預測準確率</li>
<li class="">強化學習：最大化勝率</li>
</ul>
<p>呢兩個目標可能互相衝突。AlphaGo Zero 只有一個目標：<strong>勝率最大化</strong>。呢個令學習過程更加一致同有效。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-更簡潔嘅架構">3. 更簡潔嘅架構<a href="#3-更簡潔嘅架構" class="hash-link" aria-label="3. 更簡潔嘅架構的直接連結" title="3. 更簡潔嘅架構的直接連結" translate="no">​</a></h4>
<p>原版 AlphaGo 使用分離嘅 Policy Network 同 Value Network。AlphaGo Zero 使用單一嘅雙頭網絡（詳見下一篇），令特徵表示能夠被共享，提高咗學習效率。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="簡化嘅輸入特徵由-48-到-17">簡化嘅輸入特徵：由 48 到 17<a href="#簡化嘅輸入特徵由-48-到-17" class="hash-link" aria-label="簡化嘅輸入特徵：由 48 到 17的直接連結" title="簡化嘅輸入特徵：由 48 到 17的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="原版-alphago-嘅-48-個特徵平面">原版 AlphaGo 嘅 48 個特徵平面<a href="#原版-alphago-嘅-48-個特徵平面" class="hash-link" aria-label="原版 AlphaGo 嘅 48 個特徵平面的直接連結" title="原版 AlphaGo 嘅 48 個特徵平面的直接連結" translate="no">​</a></h3>
<p>原版 AlphaGo 嘅神經網絡輸入包含 48 個 19x19 嘅特徵平面，編碼咗大量人類設計嘅特徵：</p>
<table><thead><tr><th>類別</th><th>特徵數</th><th>內容</th></tr></thead><tbody><tr><td>棋子位置</td><td>3</td><td>黑子、白子、空點</td></tr><tr><td>氣數</td><td>8</td><td>1-8 氣嘅棋串</td></tr><tr><td>提子</td><td>8</td><td>能提 1-8 粒子</td></tr><tr><td>打劫</td><td>1</td><td>劫爭位置</td></tr><tr><td>邊線距離</td><td>4</td><td>一線到四線</td></tr><tr><td>落子合法性</td><td>1</td><td>邊啲位置可以落</td></tr><tr><td>歷史狀態</td><td>8</td><td>過去 8 手嘅位置</td></tr><tr><td>輪次</td><td>1</td><td>黑方或白方</td></tr><tr><td>其他</td><td>14</td><td>征子、眼位等</td></tr></tbody></table>
<p>呢 48 個特徵係圍棋專家精心設計嘅，包含咗大量領域知識。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero-嘅-17-個特徵平面">AlphaGo Zero 嘅 17 個特徵平面<a href="#alphago-zero-嘅-17-個特徵平面" class="hash-link" aria-label="AlphaGo Zero 嘅 17 個特徵平面的直接連結" title="AlphaGo Zero 嘅 17 個特徵平面的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 大幅簡化咗輸入，只使用 17 個特徵平面：</p>
<table><thead><tr><th>平面編號</th><th>內容</th><th>數量</th></tr></thead><tbody><tr><td>1-8</td><td>黑子位置（最近 8 步）</td><td>8</td></tr><tr><td>9-16</td><td>白子位置（最近 8 步）</td><td>8</td></tr><tr><td>17</td><td>當前輪次（全 1 或全 0）</td><td>1</td></tr></tbody></table>
<p>呢 17 個特徵只包含：</p>
<ul>
<li class=""><strong>當前棋盤狀態</strong>：每個位置有黑子、白子或空</li>
<li class=""><strong>歷史資訊</strong>：過去 8 步嘅棋盤狀態</li>
<li class=""><strong>輪次資訊</strong>：輪到邊個落</li>
</ul>
<p>無氣數、無征子判斷、無邊線距離——所有呢啲「圍棋知識」都畀神經網絡自己學習。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="點解簡化係好嘅">點解簡化係好嘅？<a href="#點解簡化係好嘅" class="hash-link" aria-label="點解簡化係好嘅？的直接連結" title="點解簡化係好嘅？的直接連結" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-令網絡自己發現特徵">1. 令網絡自己發現特徵<a href="#1-令網絡自己發現特徵" class="hash-link" aria-label="1. 令網絡自己發現特徵的直接連結" title="1. 令網絡自己發現特徵的直接連結" translate="no">​</a></h4>
<p>複雜嘅手工特徵可能遺漏重要資訊，或者編碼錯誤嘅假設。令神經網絡由原始資料學習，佢可能發現更好嘅特徵表示。</p>
<p>事實證明，AlphaGo Zero 學識咗人類設計嘅所有特徵（氣數、征子等），仲學到咗一啲人類無明確意識到嘅模式。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-更好嘅可推廣性">2. 更好嘅可推廣性<a href="#2-更好嘅可推廣性" class="hash-link" aria-label="2. 更好嘅可推廣性的直接連結" title="2. 更好嘅可推廣性的直接連結" translate="no">​</a></h4>
<p>48 個特徵入面嘅好多係圍棋專用嘅（好似征子、邊線距離）。17 個簡化特徵則係通用嘅——任何棋盤遊戲都可以用類似嘅方式編碼。</p>
<p>呢個為後嚟嘅 <strong>AlphaZero</strong>（通用遊戲 AI）奠定咗基礎。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-減少人為錯誤">3. 減少人為錯誤<a href="#3-減少人為錯誤" class="hash-link" aria-label="3. 減少人為錯誤的直接連結" title="3. 減少人為錯誤的直接連結" translate="no">​</a></h4>
<p>手工設計嘅特徵可能包含錯誤或者唔完整嘅定義。簡化輸入消除咗呢類問題嘅可能性。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="單一網絡架構">單一網絡架構<a href="#單一網絡架構" class="hash-link" aria-label="單一網絡架構的直接連結" title="單一網絡架構的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="原版嘅雙網絡設計">原版嘅雙網絡設計<a href="#原版嘅雙網絡設計" class="hash-link" aria-label="原版嘅雙網絡設計的直接連結" title="原版嘅雙網絡設計的直接連結" translate="no">​</a></h3>
<p>原版 AlphaGo 使用兩個獨立嘅神經網絡：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Policy Network:  輸入 → CNN → 19x19 落子機率</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Value Network:   輸入 → CNN → 勝率評估（-1 到 1）</span><br></span></code></pre></div></div>
<p>呢兩個網絡：</p>
<ul>
<li class="">有唔同嘅架構（層數、通道數略有唔同）</li>
<li class="">獨立訓練（先訓練 Policy，再訓練 Value）</li>
<li class="">唔共享任何參數</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="zero-嘅雙頭網絡">Zero 嘅雙頭網絡<a href="#zero-嘅雙頭網絡" class="hash-link" aria-label="Zero 嘅雙頭網絡的直接連結" title="Zero 嘅雙頭網絡的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 使用單一網絡，但有兩個輸出頭（heads）：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">輸入 → ResNet 共享主幹 → Policy Head → 19x19 落子機率</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                       → Value Head  → 勝率評估</span><br></span></code></pre></div></div>
<p>兩個 Head 共享同一個 ResNet 主幹（詳見<a class="" href="/zh-hk/docs/alphago/dual-head-resnet/">下一篇：雙頭網絡與殘差網絡</a>），呢個帶嚟幾個好處：</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-參數效率">1. 參數效率<a href="#1-參數效率" class="hash-link" aria-label="1. 參數效率的直接連結" title="1. 參數效率的直接連結" translate="no">​</a></h4>
<p>共享主幹意味住大部分參數被兩個任務共用。呢個減少咗總參數量，降低咗過擬合風險。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-特徵共享">2. 特徵共享<a href="#2-特徵共享" class="hash-link" aria-label="2. 特徵共享的直接連結" title="2. 特徵共享的直接連結" translate="no">​</a></h4>
<p>「應該落邊度」（Policy）同「邊個會贏」（Value）需要理解類似嘅棋盤模式。共享主幹令呢啲特徵能被兩個任務同時學習同利用。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-訓練穩定性">3. 訓練穩定性<a href="#3-訓練穩定性" class="hash-link" aria-label="3. 訓練穩定性的直接連結" title="3. 訓練穩定性的直接連結" translate="no">​</a></h4>
<p>聯合訓練令梯度訊號嚟自兩個來源，提供咗更豐富嘅監督訊號，令訓練更加穩定。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="殘差網絡嘅威力">殘差網絡嘅威力<a href="#殘差網絡嘅威力" class="hash-link" aria-label="殘差網絡嘅威力的直接連結" title="殘差網絡嘅威力的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 嘅主幹使用 <strong>40 層殘差網絡（ResNet）</strong>，比原版 AlphaGo 嘅 13 層 CNN 深得多。</p>
<p>殘差連接（skip connections）令深層網絡得以有效訓練，避免咗梯度消失問題。呢個係 2015 年 ImageNet 競賽嘅突破性技術，被 AlphaGo Zero 成功應用到圍棋領域。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="訓練效率嘅提升">訓練效率嘅提升<a href="#訓練效率嘅提升" class="hash-link" aria-label="訓練效率嘅提升的直接連結" title="訓練效率嘅提升的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="自我對弈嘅指數增長">自我對弈嘅指數增長<a href="#自我對弈嘅指數增長" class="hash-link" aria-label="自我對弈嘅指數增長的直接連結" title="自我對弈嘅指數增長的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 嘅訓練過程展示咗令人驚嘆嘅效率：</p>
<table><thead><tr><th>訓練時間</th><th>ELO 評分</th><th>相當於</th></tr></thead><tbody><tr><td>0 小時</td><td>0</td><td>隨機亂落</td></tr><tr><td>3 小時</td><td>~1000</td><td>發現基本規則</td></tr><tr><td>12 小時</td><td>~3000</td><td>發現定式</td></tr><tr><td>36 小時</td><td>~4500</td><td>超越樊麾版</td></tr><tr><td>60 小時</td><td>~5200</td><td>超越李世乭版</td></tr><tr><td>72 小時</td><td>~5400</td><td>超越原版 AlphaGo</td></tr><tr><td>40 日</td><td>~5600</td><td>最強版本</td></tr></tbody></table>
<p><strong>三日超越人類、三日超越之前花費數個月訓練嘅 AI</strong>——呢個係指數級嘅效率提升。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="點解咁快">點解咁快？<a href="#點解咁快" class="hash-link" aria-label="點解咁快？的直接連結" title="點解咁快？的直接連結" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-更強嘅搜索引導">1. 更強嘅搜索引導<a href="#1-更強嘅搜索引導" class="hash-link" aria-label="1. 更強嘅搜索引導的直接連結" title="1. 更強嘅搜索引導的直接連結" translate="no">​</a></h4>
<p>AlphaGo Zero 嘅 MCTS 完全由神經網絡引導，唔再使用快速走子策略（rollout）。呢個令搜索更加高效同準確。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-更快嘅自我對弈">2. 更快嘅自我對弈<a href="#2-更快嘅自我對弈" class="hash-link" aria-label="2. 更快嘅自我對弈的直接連結" title="2. 更快嘅自我對弈的直接連結" translate="no">​</a></h4>
<p>由於只需要一個網絡（而唔係兩個），每局自我對弈嘅計算成本降低。呢個意味住喺相同時間內可以產生更多訓練資料。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-更有效嘅學習">3. 更有效嘅學習<a href="#3-更有效嘅學習" class="hash-link" aria-label="3. 更有效嘅學習的直接連結" title="3. 更有效嘅學習的直接連結" translate="no">​</a></h4>
<p>雙頭網絡嘅聯合訓練令每一局棋嘅資訊被更有效噉利用。Policy 同 Value 嘅梯度相互強化，加速咗收斂。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="與人類學習嘅對比">與人類學習嘅對比<a href="#與人類學習嘅對比" class="hash-link" aria-label="與人類學習嘅對比的直接連結" title="與人類學習嘅對比的直接連結" translate="no">​</a></h3>
<p>人類棋手需要幾耐時間達到唔同水平？</p>
<table><thead><tr><th>水平</th><th>人類所需時間</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>入門</td><td>數週</td><td>幾分鐘</td></tr><tr><td>業餘初段</td><td>數年</td><td>數小時</td></tr><tr><td>職業水平</td><td>10-20 年</td><td>1-2 日</td></tr><tr><td>世界冠軍</td><td>20+ 年全職投入</td><td>3 日</td></tr><tr><td>超越人類</td><td>唔可能</td><td>3 日</td></tr></tbody></table>
<p>呢個對比唔係要貶低人類棋手——佢哋用嘅係生物神經元，而 AlphaGo Zero 用嘅係專門設計嘅 TPU 同幾千瓦嘅電力。但佢確實展示咗正確嘅學習方法可以幾咁高效。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="通用性國際象棋將棋">通用性：國際象棋、將棋<a href="#通用性國際象棋將棋" class="hash-link" aria-label="通用性：國際象棋、將棋的直接連結" title="通用性：國際象棋、將棋的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphazero-嘅誕生">AlphaZero 嘅誕生<a href="#alphazero-嘅誕生" class="hash-link" aria-label="AlphaZero 嘅誕生的直接連結" title="AlphaZero 嘅誕生的直接連結" translate="no">​</a></h3>
<p>2017 年 12 月，DeepMind 發表咗 <strong>AlphaZero</strong>——AlphaGo Zero 嘅通用版本。同一套演算法，只需修改遊戲規則，就能喺三種棋類遊戲入面達到世界頂級水平：</p>
<table><thead><tr><th>遊戲</th><th>訓練時間</th><th>對手</th><th>戰績</th></tr></thead><tbody><tr><td>圍棋</td><td>8 小時</td><td>AlphaGo Zero</td><td>60:40</td></tr><tr><td>國際象棋</td><td>4 小時</td><td>Stockfish 8</td><td>28 勝 72 和 0 負</td></tr><tr><td>將棋</td><td>2 小時</td><td>Elmo</td><td>90:8:2</td></tr></tbody></table>
<p>留意呢度嘅對手：</p>
<ul>
<li class=""><strong>Stockfish</strong> 係當時最強嘅國際象棋引擎，使用幾十年人類知識同優化</li>
<li class=""><strong>Elmo</strong> 係當時最強嘅將棋 AI</li>
</ul>
<p>AlphaZero 用幾小時訓練，就超越咗呢啲耗費多年開發嘅專用系統。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="通用性嘅意義">通用性嘅意義<a href="#通用性嘅意義" class="hash-link" aria-label="通用性嘅意義的直接連結" title="通用性嘅意義的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero / AlphaZero 證明咗一件重要嘅事：</p>
<blockquote>
<p><strong>同一套學習演算法，可以喺唔同領域達到超人水平。</strong></p>
</blockquote>
<p>呢個唔係三個唔同嘅 AI，而係一個通用嘅學習框架：</p>
<ol>
<li class=""><strong>自我對弈</strong>產生經驗</li>
<li class=""><strong>蒙地卡羅樹搜索</strong>探索可能性</li>
<li class=""><strong>神經網絡</strong>學習策略同價值函數</li>
<li class=""><strong>強化學習</strong>優化目標函數</li>
</ol>
<p>呢個框架唔依賴領域特定嘅知識，呢個為 AI 嘅通用化邁出咗重要一步。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="對傳統-ai-嘅衝擊">對傳統 AI 嘅衝擊<a href="#對傳統-ai-嘅衝擊" class="hash-link" aria-label="對傳統 AI 嘅衝擊的直接連結" title="對傳統 AI 嘅衝擊的直接連結" translate="no">​</a></h3>
<p>喺 AlphaZero 之前，國際象棋同將棋嘅最強 AI 都係「專家系統」風格嘅：</p>
<ul>
<li class=""><strong>大量人類知識</strong>：開局庫、殘局庫、評估函數</li>
<li class=""><strong>數十年優化</strong>：無數棋手同工程師嘅心血</li>
<li class=""><strong>極度專業化</strong>：Stockfish 唔識落圍棋，Elmo 唔識落國際象棋</li>
</ul>
<p>AlphaZero 用一個通用演算法喺幾小時內超越咗呢一切。呢個令好多 AI 研究者重新思考：</p>
<blockquote>
<p>我哋應該投入更多精力喺「通用學習演算法」，定係「專家知識編碼」？</p>
</blockquote>
<p>答案似乎越嚟越清楚：令機器自己學習，比教佢知識更有效。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero-嘅落棋風格">AlphaGo Zero 嘅落棋風格<a href="#alphago-zero-嘅落棋風格" class="hash-link" aria-label="AlphaGo Zero 嘅落棋風格的直接連結" title="AlphaGo Zero 嘅落棋風格的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="超越人類嘅審美">超越人類嘅審美<a href="#超越人類嘅審美" class="hash-link" aria-label="超越人類嘅審美的直接連結" title="超越人類嘅審美的直接連結" translate="no">​</a></h3>
<p>圍棋界對 AlphaGo Zero 嘅著法有一個普遍評價：<strong>更加優美</strong>。</p>
<p>AlphaGo Lee 嘅著法有時顯得「怪異」——好似第 37 手嗰類落子，人類需要事後分析先能理解佢嘅妙處。但 AlphaGo Zero 嘅著法常常喺事後被評價為「一眼就知道係好棋」。</p>
<p>呢個可能係因為：</p>
<ol>
<li class=""><strong>更強嘅棋力</strong>：Zero 能睇得更深，落子更加從容</li>
<li class=""><strong>無人類偏見</strong>：唔受傳統定式嘅束縛</li>
<li class=""><strong>一致嘅目標</strong>：只追求勝率，唔模仿人類</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="重新發現人類棋理">重新發現人類棋理<a href="#重新發現人類棋理" class="hash-link" aria-label="重新發現人類棋理的直接連結" title="重新發現人類棋理的直接連結" translate="no">​</a></h3>
<p>有趣嘅係，AlphaGo Zero 喺訓練過程入面「重新發現」咗人類數千年累積嘅圍棋知識：</p>
<ul>
<li class=""><strong>定式</strong>：Zero 自己發現咗好多常見定式，因為呢啲確實係雙方最優解</li>
<li class=""><strong>佈局原則</strong>：角、邊、中央嘅重要性順序</li>
<li class=""><strong>棋形知識</strong>：愚形同好形嘅區別</li>
</ul>
<p>呢個驗證咗人類棋理嘅合理性——呢啲知識唔係偶然嘅，而係圍棋本質嘅反映。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="超越人類嘅創新">超越人類嘅創新<a href="#超越人類嘅創新" class="hash-link" aria-label="超越人類嘅創新的直接連結" title="超越人類嘅創新的直接連結" translate="no">​</a></h3>
<p>但 Zero 都發現咗人類從未諗過嘅著法：</p>
<ul>
<li class=""><strong>非常規開局</strong>：喺傳統開局基礎上嘅變化</li>
<li class=""><strong>激進嘅棄子</strong>：比人類更願意放棄局部換取全局優勢</li>
<li class=""><strong>反直覺嘅形狀</strong>：表面上嘅「壞形」其實係最優解</li>
</ul>
<p>呢啲創新正喺改變人類對圍棋嘅理解。好多職業棋手表示，研究 AlphaGo Zero 嘅棋譜令佢哋對圍棋有咗全新嘅認識。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="技術細節總結">技術細節總結<a href="#技術細節總結" class="hash-link" aria-label="技術細節總結的直接連結" title="技術細節總結的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="與原版-alphago-嘅完整對比">與原版 AlphaGo 嘅完整對比<a href="#與原版-alphago-嘅完整對比" class="hash-link" aria-label="與原版 AlphaGo 嘅完整對比的直接連結" title="與原版 AlphaGo 嘅完整對比的直接連結" translate="no">​</a></h3>
<table><thead><tr><th>方面</th><th>AlphaGo（原版）</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td><strong>訓練資料</strong></td><td>人類棋譜 + 自我對弈</td><td>純自我對弈</td></tr><tr><td><strong>學習方法</strong></td><td>監督學習 + 強化學習</td><td>純強化學習</td></tr><tr><td><strong>輸入特徵</strong></td><td>48 個平面</td><td>17 個平面</td></tr><tr><td><strong>網絡架構</strong></td><td>分離嘅 Policy/Value</td><td>雙頭 ResNet</td></tr><tr><td><strong>網絡深度</strong></td><td>13 層</td><td>40 層（或更多）</td></tr><tr><td><strong>MCTS 評估</strong></td><td>神經網絡 + Rollout</td><td>純神經網絡</td></tr><tr><td><strong>搜索次數</strong></td><td>每步 ~100,000</td><td>每步 ~1,600</td></tr><tr><td><strong>訓練 TPU</strong></td><td>50+</td><td>4</td></tr><tr><td><strong>推理 TPU</strong></td><td>48</td><td>4（可擴展）</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="核心算法">核心算法<a href="#核心算法" class="hash-link" aria-label="核心算法的直接連結" title="核心算法的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 嘅訓練循環非常簡潔：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. 自我對弈</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 用當前網絡進行 MCTS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 按 MCTS 搜索機率揀選落子</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 記錄每一步嘅 (局面, MCTS機率, 勝負結果)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. 訓練網絡</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 由經驗池入面取樣</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Policy Head：最小化與 MCTS 機率嘅交叉熵</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Value Head：最小化與實際勝負嘅均方誤差</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 聯合優化兩個目標</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. 更新網絡</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 用新網絡替換舊網絡（通過對弈驗證新網絡更強）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 返回步驟 1</span><br></span></code></pre></div></div>
<p>呢個循環持續運行，網絡不斷變強。無人類數據、無人類知識，只有遊戲規則同勝負目標。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="對-ai-研究嘅啟示">對 AI 研究嘅啟示<a href="#對-ai-研究嘅啟示" class="hash-link" aria-label="對 AI 研究嘅啟示的直接連結" title="對 AI 研究嘅啟示的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="第一性原理學習">第一性原理學習<a href="#第一性原理學習" class="hash-link" aria-label="第一性原理學習的直接連結" title="第一性原理學習的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 展示咗一種「第一性原理」嘅學習方法：</p>
<blockquote>
<p>唔好話 AI 點樣做，只話佢目標係乜嘢，令佢自己發現方法。</p>
</blockquote>
<p>呢個同傳統嘅專家系統方法形成鮮明對比。專家系統嘗試將人類知識編碼落 AI，而 AlphaGo Zero 令 AI 自己發現知識。</p>
<p>結果係：AI 發現嘅知識可能比人類知識更完整、更準確。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="自我對弈嘅威力">自我對弈嘅威力<a href="#自我對弈嘅威力" class="hash-link" aria-label="自我對弈嘅威力的直接連結" title="自我對弈嘅威力的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 證明咗自我對弈可以產生無限嘅訓練資料，而且呢啲資料嘅質素會隨住網絡嘅提升而提升。</p>
<p>呢個係一個「正向循環」：</p>
<ul>
<li class="">更強嘅網絡 → 更好嘅自我對弈資料</li>
<li class="">更好嘅資料 → 更強嘅網絡</li>
</ul>
<p>呢個循環可以持續運行，直到達到遊戲嘅理論上限（如果存在嘅話）。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="簡化嘅重要性">簡化嘅重要性<a href="#簡化嘅重要性" class="hash-link" aria-label="簡化嘅重要性的直接連結" title="簡化嘅重要性的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 嘅成功證明咗「簡化」嘅重要性：</p>
<ul>
<li class="">簡化輸入（48 → 17）</li>
<li class="">簡化架構（雙網絡 → 單網絡）</li>
<li class="">簡化訓練（監督 + 強化 → 純強化）</li>
</ul>
<p>每一次簡化都令系統更加強大。呢個話畀我哋知：複雜唔等於好，最簡單嘅解決方案往往係最好嘅。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="動畫對應">動畫對應<a href="#動畫對應" class="hash-link" aria-label="動畫對應的直接連結" title="動畫對應的直接連結" translate="no">​</a></h2>
<p>本文涉及嘅核心概念與動畫編號：</p>
<table><thead><tr><th>編號</th><th>概念</th><th>物理/數學對應</th></tr></thead><tbody><tr><td>🎬 E7</td><td>由零開始訓練</td><td>自組織現象</td></tr><tr><td>🎬 E5</td><td>自我對弈</td><td>不動點收斂</td></tr><tr><td>🎬 E12</td><td>棋力成長曲線</td><td>S 型增長</td></tr><tr><td>🎬 D12</td><td>殘差網絡</td><td>梯度高速公路</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="延伸閱讀">延伸閱讀<a href="#延伸閱讀" class="hash-link" aria-label="延伸閱讀的直接連結" title="延伸閱讀的直接連結" translate="no">​</a></h2>
<ul>
<li class=""><strong>下一篇</strong>：<a class="" href="/zh-hk/docs/alphago/dual-head-resnet/">雙頭網絡與殘差網絡</a> — 詳解 AlphaGo Zero 嘅神經網絡架構</li>
<li class=""><strong>相關文章</strong>：<a class="" href="/zh-hk/docs/alphago/self-play/">自我對弈</a> — 點解自我對弈能產生超人水平</li>
<li class=""><strong>技術深入</strong>：<a class="" href="/zh-hk/docs/alphago/training-from-scratch/">由零訓練嘅過程</a> — Day 0-3 嘅詳細演進</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="參考資料">參考資料<a href="#參考資料" class="hash-link" aria-label="參考資料的直接連結" title="參考資料的直接連結" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">Silver, D., et al. (2018). &quot;A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.&quot; <em>Science</em>, 362(6419), 1140-1144.</li>
<li class="">DeepMind. (2017). &quot;AlphaGo Zero: Starting from scratch.&quot; <em>DeepMind Blog</em>.</li>
<li class="">Schrittwieser, J., et al. (2020). &quot;Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.&quot; <em>Nature</em>, 588, 604-609.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/16-alphago-zero.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>編輯此頁</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/zh-hk/docs/alphago/puct-formula/"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">PUCT 公式詳解</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/zh-hk/docs/alphago/dual-head-resnet/"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">雙頭網絡與殘差網絡</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#點解唔需要人類棋譜" class="table-of-contents__link toc-highlight">點解唔需要人類棋譜？</a><ul><li><a href="#人類棋譜嘅限制" class="table-of-contents__link toc-highlight">人類棋譜嘅限制</a></li><li><a href="#zero-嘅突破" class="table-of-contents__link toc-highlight">Zero 嘅突破</a></li></ul></li><li><a href="#與原版-alphago-嘅對比1000" class="table-of-contents__link toc-highlight">與原版 AlphaGo 嘅對比：100:0</a><ul><li><a href="#碾壓性嘅勝利" class="table-of-contents__link toc-highlight">碾壓性嘅勝利</a></li><li><a href="#更少嘅資源更強嘅棋力" class="table-of-contents__link toc-highlight">更少嘅資源，更強嘅棋力</a></li><li><a href="#點解-zero-更強" class="table-of-contents__link toc-highlight">點解 Zero 更強？</a></li></ul></li><li><a href="#簡化嘅輸入特徵由-48-到-17" class="table-of-contents__link toc-highlight">簡化嘅輸入特徵：由 48 到 17</a><ul><li><a href="#原版-alphago-嘅-48-個特徵平面" class="table-of-contents__link toc-highlight">原版 AlphaGo 嘅 48 個特徵平面</a></li><li><a href="#alphago-zero-嘅-17-個特徵平面" class="table-of-contents__link toc-highlight">AlphaGo Zero 嘅 17 個特徵平面</a></li><li><a href="#點解簡化係好嘅" class="table-of-contents__link toc-highlight">點解簡化係好嘅？</a></li></ul></li><li><a href="#單一網絡架構" class="table-of-contents__link toc-highlight">單一網絡架構</a><ul><li><a href="#原版嘅雙網絡設計" class="table-of-contents__link toc-highlight">原版嘅雙網絡設計</a></li><li><a href="#zero-嘅雙頭網絡" class="table-of-contents__link toc-highlight">Zero 嘅雙頭網絡</a></li><li><a href="#殘差網絡嘅威力" class="table-of-contents__link toc-highlight">殘差網絡嘅威力</a></li></ul></li><li><a href="#訓練效率嘅提升" class="table-of-contents__link toc-highlight">訓練效率嘅提升</a><ul><li><a href="#自我對弈嘅指數增長" class="table-of-contents__link toc-highlight">自我對弈嘅指數增長</a></li><li><a href="#點解咁快" class="table-of-contents__link toc-highlight">點解咁快？</a></li><li><a href="#與人類學習嘅對比" class="table-of-contents__link toc-highlight">與人類學習嘅對比</a></li></ul></li><li><a href="#通用性國際象棋將棋" class="table-of-contents__link toc-highlight">通用性：國際象棋、將棋</a><ul><li><a href="#alphazero-嘅誕生" class="table-of-contents__link toc-highlight">AlphaZero 嘅誕生</a></li><li><a href="#通用性嘅意義" class="table-of-contents__link toc-highlight">通用性嘅意義</a></li><li><a href="#對傳統-ai-嘅衝擊" class="table-of-contents__link toc-highlight">對傳統 AI 嘅衝擊</a></li></ul></li><li><a href="#alphago-zero-嘅落棋風格" class="table-of-contents__link toc-highlight">AlphaGo Zero 嘅落棋風格</a><ul><li><a href="#超越人類嘅審美" class="table-of-contents__link toc-highlight">超越人類嘅審美</a></li><li><a href="#重新發現人類棋理" class="table-of-contents__link toc-highlight">重新發現人類棋理</a></li><li><a href="#超越人類嘅創新" class="table-of-contents__link toc-highlight">超越人類嘅創新</a></li></ul></li><li><a href="#技術細節總結" class="table-of-contents__link toc-highlight">技術細節總結</a><ul><li><a href="#與原版-alphago-嘅完整對比" class="table-of-contents__link toc-highlight">與原版 AlphaGo 嘅完整對比</a></li><li><a href="#核心算法" class="table-of-contents__link toc-highlight">核心算法</a></li></ul></li><li><a href="#對-ai-研究嘅啟示" class="table-of-contents__link toc-highlight">對 AI 研究嘅啟示</a><ul><li><a href="#第一性原理學習" class="table-of-contents__link toc-highlight">第一性原理學習</a></li><li><a href="#自我對弈嘅威力" class="table-of-contents__link toc-highlight">自我對弈嘅威力</a></li><li><a href="#簡化嘅重要性" class="table-of-contents__link toc-highlight">簡化嘅重要性</a></li></ul></li><li><a href="#動畫對應" class="table-of-contents__link toc-highlight">動畫對應</a></li><li><a href="#延伸閱讀" class="table-of-contents__link toc-highlight">延伸閱讀</a></li><li><a href="#參考資料" class="table-of-contents__link toc-highlight">參考資料</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>