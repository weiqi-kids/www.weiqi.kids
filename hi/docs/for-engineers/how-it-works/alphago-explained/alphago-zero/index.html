<!doctype html>
<html lang="hi" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-for-engineers/how-it-works/alphago-explained/alphago-zero" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">AlphaGo Zero अवलोकन | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/hi/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/hi/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/hi/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><meta data-rh="true" property="og:locale" content="hi"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="hi"><meta data-rh="true" name="docsearch:language" content="hi"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="AlphaGo Zero अवलोकन | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="शून्य से शुरू, पूरी तरह स्व-शिक्षित, AlphaGo Zero ने बिना मानव शतरंज रिकॉर्ड के सभी पूर्ववर्ती संस्करणों को कैसे पछाड़ा"><meta data-rh="true" property="og:description" content="शून्य से शुरू, पूरी तरह स्व-शिक्षित, AlphaGo Zero ने बिना मानव शतरंज रिकॉर्ड के सभी पूर्ववर्ती संस्करणों को कैसे पछाड़ा"><meta data-rh="true" name="keywords" content="AlphaGo Zero,सेल्फ-प्ले,रीइन्फोर्समेंट लर्निंग,डीप लर्निंग,Go AI,अनसुपरवाइज्ड लर्निंग"><link data-rh="true" rel="icon" href="/hi/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/hi/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"給工程師的圍棋 AI 指南","item":"https://www.weiqi.kids/hi/docs/for-engineers/"},{"@type":"ListItem","position":2,"name":"一篇文章搞懂圍棋 AI","item":"https://www.weiqi.kids/hi/docs/for-engineers/how-it-works/"},{"@type":"ListItem","position":3,"name":"AlphaGo 完整解析","item":"https://www.weiqi.kids/hi/docs/for-engineers/how-it-works/alphago-explained/"},{"@type":"ListItem","position":4,"name":"AlphaGo Zero अवलोकन","item":"https://www.weiqi.kids/hi/docs/for-engineers/how-it-works/alphago-explained/alphago-zero"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/hi/assets/css/styles.f23bf74b.css">
<script src="/hi/assets/js/runtime~main.8f66f108.js" defer="defer"></script>
<script src="/hi/assets/js/main.ac318873.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/hi/img/logo.svg"><div role="region" aria-label="मुख्य कंटेंट तक स्किप करें"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">मुख्य कंटेंट तक स्किप करें</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/hi/"><div class="navbar__logo"><img src="/hi/img/logo.svg" alt="वेइकी किड्स एसोसिएशन लोगो" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/hi/img/logo.svg" alt="वेइकी किड्स एसोसिएशन लोगो" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">वेइकी किड्स</b></a><a class="navbar__item navbar__link" href="/hi/docs/for-players/">खिलाड़ियों के लिए</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/hi/docs/for-engineers/">इंजीनियरों के लिए</a><a class="navbar__item navbar__link" href="/hi/docs/about/">हमारे बारे में</a><a class="navbar__item navbar__link" href="/hi/docs/activities/">गतिविधियां</a><a class="navbar__item navbar__link" href="/hi/docs/references/">संदर्भ</a><a class="navbar__item navbar__link" href="/hi/docs/sop/">मानक प्रक्रियाएं</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>हिन्दी</a><ul class="dropdown__menu"><li><a href="/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="hi">हिन्दी</a></li><li><a href="/id/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/hi/docs/intro/"><span title="उपयोग मार्गदर्शिका" class="linkLabel_REp1">उपयोग मार्गदर्शिका</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/hi/docs/about/"><span title="關於協會" class="categoryLinkLabel_ezQx">關於協會</span></a><button aria-label="Expand sidebar category &#x27;關於協會&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/hi/docs/activities/"><span title="活動實績" class="categoryLinkLabel_ezQx">活動實績</span></a><button aria-label="Expand sidebar category &#x27;活動實績&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/hi/docs/for-players/"><span title="गो खिलाड़ियों के लिए" class="categoryLinkLabel_ezQx">गो खिलाड़ियों के लिए</span></a><button aria-label="Expand sidebar category &#x27;गो खिलाड़ियों के लिए&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/hi/docs/references/"><span title="參考資料" class="categoryLinkLabel_ezQx">參考資料</span></a><button aria-label="Expand sidebar category &#x27;參考資料&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/hi/docs/sop/"><span title="標準作業流程" class="categoryLinkLabel_ezQx">標準作業流程</span></a><button aria-label="Expand sidebar category &#x27;標準作業流程&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/hi/docs/for-engineers/"><span title="給工程師的圍棋 AI 指南" class="categoryLinkLabel_ezQx">給工程師的圍棋 AI 指南</span></a><button aria-label="Collapse sidebar category &#x27;給工程師的圍棋 AI 指南&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/hi/docs/for-engineers/deep-dive/"><span title="गहन अध्ययन के इच्छुक लोगों के लिए" class="categoryLinkLabel_ezQx">गहन अध्ययन के इच्छुक लोगों के लिए</span></a><button aria-label="Expand sidebar category &#x27;गहन अध्ययन के इच्छुक लोगों के लिए&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/hi/docs/for-engineers/hands-on/"><span title="30 分鐘跑起第一個圍棋 AI" class="categoryLinkLabel_ezQx">30 分鐘跑起第一個圍棋 AI</span></a><button aria-label="Expand sidebar category &#x27;30 分鐘跑起第一個圍棋 AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/hi/docs/for-engineers/how-it-works/"><span title="一篇文章搞懂圍棋 AI" class="categoryLinkLabel_ezQx">一篇文章搞懂圍棋 AI</span></a><button aria-label="Collapse sidebar category &#x27;一篇文章搞懂圍棋 AI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/"><span title="AlphaGo 完整解析" class="categoryLinkLabel_ezQx">AlphaGo 完整解析</span></a><button aria-label="Collapse sidebar category &#x27;AlphaGo 完整解析&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/birth-of-alphago/"><span title="AlphaGo का जन्म" class="linkLabel_REp1">AlphaGo का जन्म</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/key-matches/"><span title="महत्वपूर्ण मैचों की समीक्षा" class="linkLabel_REp1">महत्वपूर्ण मैचों की समीक्षा</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/move-37/"><span title="&quot;दिव्य चाल&quot; का गहन विश्लेषण" class="linkLabel_REp1">&quot;दिव्य चाल&quot; का गहन विश्लेषण</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/why-go-is-hard/"><span title="गो कठिन क्यों है?" class="linkLabel_REp1">गो कठिन क्यों है?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/traditional-limits/"><span title="पारंपरिक तरीकों की सीमाएँ" class="linkLabel_REp1">पारंपरिक तरीकों की सीमाएँ</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/board-representation/"><span title="बोर्ड स्थिति प्रतिनिधित्व" class="linkLabel_REp1">बोर्ड स्थिति प्रतिनिधित्व</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/policy-network/"><span title="Policy Network विस्तृत विश्लेषण" class="linkLabel_REp1">Policy Network विस्तृत विश्लेषण</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/value-network/"><span title="Value Network विस्तृत विश्लेषण" class="linkLabel_REp1">Value Network विस्तृत विश्लेषण</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/input-features/"><span title="इनपुट विशेषता डिज़ाइन" class="linkLabel_REp1">इनपुट विशेषता डिज़ाइन</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/cnn-and-go/"><span title="CNN और गो का संयोजन" class="linkLabel_REp1">CNN और गो का संयोजन</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/supervised-learning/"><span title="सुपरवाइज़्ड लर्निंग चरण" class="linkLabel_REp1">सुपरवाइज़्ड लर्निंग चरण</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/"><span title="रीइन्फोर्समेंट लर्निंग परिचय" class="linkLabel_REp1">रीइन्फोर्समेंट लर्निंग परिचय</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/self-play/"><span title="सेल्फ-प्ले" class="linkLabel_REp1">सेल्फ-प्ले</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/"><span title="MCTS और न्यूरल नेटवर्क का संयोजन" class="linkLabel_REp1">MCTS और न्यूरल नेटवर्क का संयोजन</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/puct-formula/"><span title="PUCT सूत्र का विस्तृत विवरण" class="linkLabel_REp1">PUCT सूत्र का विस्तृत विवरण</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><span title="AlphaGo Zero अवलोकन" class="linkLabel_REp1">AlphaGo Zero अवलोकन</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/"><span title="डुअल-हेड नेटवर्क और रेसिड्युअल नेटवर्क" class="linkLabel_REp1">डुअल-हेड नेटवर्क और रेसिड्युअल नेटवर्क</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/training-from-scratch/"><span title="शून्य से प्रशिक्षण की प्रक्रिया" class="linkLabel_REp1">शून्य से प्रशिक्षण की प्रक्रिया</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/distributed-systems/"><span title="वितरित प्रणाली और TPU" class="linkLabel_REp1">वितरित प्रणाली और TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/alphago-explained/legacy-and-impact/"><span title="AlphaGo की विरासत" class="linkLabel_REp1">AlphaGo की विरासत</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/katago-innovations/"><span title="KataGo 的關鍵創新" class="linkLabel_REp1">KataGo 的關鍵創新</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hi/docs/for-engineers/how-it-works/concepts/"><span title="概念速查表" class="linkLabel_REp1">概念速查表</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/hi/docs/for-engineers/industry/"><span title="圍棋 AI 產業現況" class="categoryLinkLabel_ezQx">圍棋 AI 產業現況</span></a><button aria-label="Expand sidebar category &#x27;圍棋 AI 產業現況&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/hi/docs/for-engineers/overview/"><span title="圍棋 AI 能做什麼？" class="categoryLinkLabel_ezQx">圍棋 AI 能做什麼？</span></a><button aria-label="Expand sidebar category &#x27;圍棋 AI 能做什麼？&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/hi/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/hi/docs/for-engineers/"><span>給工程師的圍棋 AI 指南</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/hi/docs/for-engineers/how-it-works/"><span>一篇文章搞懂圍棋 AI</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/hi/docs/for-engineers/how-it-works/alphago-explained/"><span>AlphaGo 完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">AlphaGo Zero अवलोकन</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">इस पेज पर</button></div><div class="theme-doc-markdown markdown"><header><h1>AlphaGo Zero अवलोकन</h1></header>
<p>2017 के अक्टूबर में, DeepMind ने AI जगत को चौंकाने वाला एक परिणाम प्रकाशित किया: <strong>AlphaGo Zero</strong> ने बिना किसी मानव शतरंज रिकॉर्ड के, पूरी तरह यादृच्छिक स्थिति से प्रशिक्षण शुरू करके, केवल तीन दिनों में ली से-दोल को हराने वाले मूल AlphaGo को पार कर लिया, और <strong>100:0</strong> के स्कोर से जीता।</p>
<p>यह केवल संख्यात्मक प्रगति नहीं है। यह एक पूरी तरह नई प्रतिमान का प्रतिनिधित्व करता है: <strong>AI को मानव ज्ञान की आवश्यकता नहीं है, यह शून्य से सब कुछ खोज सकता है</strong>।</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="मानव-शतरंज-रिकॉर्ड-की-आवश्यकता-क्यों-नहीं">मानव शतरंज रिकॉर्ड की आवश्यकता क्यों नहीं?<a href="#मानव-शतरंज-रिकॉर्ड-की-आवश्यकता-क्यों-नहीं" class="hash-link" aria-label="मानव शतरंज रिकॉर्ड की आवश्यकता क्यों नहीं? का सीधा लिंक" title="मानव शतरंज रिकॉर्ड की आवश्यकता क्यों नहीं? का सीधा लिंक" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="मानव-शतरंज-रिकॉर्ड-की-सीमाएं">मानव शतरंज रिकॉर्ड की सीमाएं<a href="#मानव-शतरंज-रिकॉर्ड-की-सीमाएं" class="hash-link" aria-label="मानव शतरंज रिकॉर्ड की सीमाएं का सीधा लिंक" title="मानव शतरंज रिकॉर्ड की सीमाएं का सीधा लिंक" translate="no">​</a></h3>
<p>मूल AlphaGo की प्रशिक्षण प्रक्रिया दो चरणों में विभाजित थी:</p>
<ol>
<li class=""><strong>सुपरवाइज्ड लर्निंग</strong>: 3 करोड़ मानव खेलों से Policy Network को प्रशिक्षित किया</li>
<li class=""><strong>रीइन्फोर्समेंट लर्निंग</strong>: सेल्फ-प्ले के माध्यम से और सुधार</li>
</ol>
<p>इस विधि में कई मौलिक समस्याएं हैं:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-मानव-शतरंज-रिकॉर्ड-की-सीमा-है">1. मानव शतरंज रिकॉर्ड की सीमा है<a href="#1-मानव-शतरंज-रिकॉर्ड-की-सीमा-है" class="hash-link" aria-label="1. मानव शतरंज रिकॉर्ड की सीमा है का सीधा लिंक" title="1. मानव शतरंज रिकॉर्ड की सीमा है का सीधा लिंक" translate="no">​</a></h4>
<p>मानव खिलाड़ियों की क्षमता की एक सीमा है, शतरंज रिकॉर्ड में मानव की समझ शामिल है, साथ ही मानव की गलतियां और पूर्वाग्रह भी। जब AI मानव शतरंज रिकॉर्ड से सीखता है, वह सीखता है:</p>
<ul>
<li class="">मानव जो अच्छा मानते हैं (लेकिन जरूरी नहीं कि सर्वोत्तम हो)</li>
<li class="">मानव की सोच के पैटर्न (लेकिन नवाचार को सीमित कर सकते हैं)</li>
<li class="">मानव की गलतियां (सही नमूनों के रूप में सीखी जाती हैं)</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-सुपरवाइज्ड-लर्निंग-की-बाधा">2. सुपरवाइज्ड लर्निंग की बाधा<a href="#2-सुपरवाइज्ड-लर्निंग-की-बाधा" class="hash-link" aria-label="2. सुपरवाइज्ड लर्निंग की बाधा का सीधा लिंक" title="2. सुपरवाइज्ड लर्निंग की बाधा का सीधा लिंक" translate="no">​</a></h4>
<p>सुपरवाइज्ड लर्निंग का लक्ष्य &quot;मानव की नकल करना&quot; है—यह भविष्यवाणी करना कि मानव खिलाड़ी कौन सी चाल चलेगा। इसका मतलब है कि AI की क्षमता की ऊपरी सीमा मानव खिलाड़ी की क्षमता द्वारा सीमित है।</p>
<p>जैसे एक शिष्य केवल गुरु की नकल कर सकता है, गुरु से आगे कभी नहीं जा सकता।</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-डेटा-संग्रह-की-लागत">3. डेटा संग्रह की लागत<a href="#3-डेटा-संग्रह-की-लागत" class="hash-link" aria-label="3. डेटा संग्रह की लागत का सीधा लिंक" title="3. डेटा संग्रह की लागत का सीधा लिंक" translate="no">​</a></h4>
<p>उच्च गुणवत्ता वाले मानव शतरंज रिकॉर्ड को संचित करने में वर्षों लगते हैं, और केवल Go जैसे लंबे इतिहास वाले खेलों में ही मौजूद हैं। यदि AI को नए क्षेत्रों में लागू करना है (जैसे प्रोटीन संरचना भविष्यवाणी), &quot;मानव विशेषज्ञ शतरंज रिकॉर्ड&quot; उपलब्ध नहीं हैं।</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="zero-की-सफलता">Zero की सफलता<a href="#zero-की-सफलता" class="hash-link" aria-label="Zero की सफलता का सीधा लिंक" title="Zero की सफलता का सीधा लिंक" translate="no">​</a></h3>
<p>AlphaGo Zero ने सुपरवाइज्ड लर्निंग चरण को पूरी तरह छोड़ दिया, सीधे <strong>यादृच्छिक आरंभीकरण</strong> से सेल्फ-प्ले शुरू किया। इसने उपरोक्त सभी समस्याओं का समाधान किया:</p>
<table><thead><tr><th>समस्या</th><th>मूल AlphaGo</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>मानव ज्ञान की सीमा</td><td>शतरंज रिकॉर्ड गुणवत्ता से सीमित</td><td>कोई सीमा नहीं</td></tr><tr><td>सीखने का लक्ष्य</td><td>मानव की नकल</td><td>जीत दर अधिकतम करना</td></tr><tr><td>डेटा आवश्यकता</td><td>3 करोड़ खेल</td><td>0</td></tr><tr><td>विस्तारणीयता</td><td>केवल Go</td><td>अन्य क्षेत्रों में विस्तार योग्य</td></tr></tbody></table>
<p>यह एक मौलिक प्रतिमान परिवर्तन है: &quot;मानव ज्ञान सीखना&quot; से &quot;प्रथम सिद्धांतों से ज्ञान खोजना&quot;।</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="मूल-alphago-से-तुलना-1000">मूल AlphaGo से तुलना: 100:0<a href="#मूल-alphago-से-तुलना-1000" class="hash-link" aria-label="मूल AlphaGo से तुलना: 100:0 का सीधा लिंक" title="मूल AlphaGo से तुलना: 100:0 का सीधा लिंक" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="कुचलने-वाली-जीत">कुचलने वाली जीत<a href="#कुचलने-वाली-जीत" class="hash-link" aria-label="कुचलने वाली जीत का सीधा लिंक" title="कुचलने वाली जीत का सीधा लिंक" translate="no">​</a></h3>
<p>DeepMind ने प्रशिक्षित AlphaGo Zero को AlphaGo के विभिन्न संस्करणों से खेलाया:</p>
<table><thead><tr><th>प्रतिद्वंद्वी</th><th>AlphaGo Zero का रिकॉर्ड</th></tr></thead><tbody><tr><td>AlphaGo Fan (फान हुई को हराने वाला संस्करण)</td><td>100:0</td></tr><tr><td>AlphaGo Lee (ली से-दोल को हराने वाला संस्करण)</td><td>100:0</td></tr><tr><td>AlphaGo Master (60 लगातार जीत वाला संस्करण)</td><td>89:11</td></tr></tbody></table>
<p><strong>100:0</strong>—इसका मतलब है कि 100 खेलों में, मूल AlphaGo एक भी नहीं जीत सका।</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="कम-संसाधन-मजबूत-खेल">कम संसाधन, मजबूत खेल<a href="#कम-संसाधन-मजबूत-खेल" class="hash-link" aria-label="कम संसाधन, मजबूत खेल का सीधा लिंक" title="कम संसाधन, मजबूत खेल का सीधा लिंक" translate="no">​</a></h3>
<p>केवल जीतना नहीं, AlphaGo Zero ने कम संसाधनों के साथ मजबूत खेल हासिल किया:</p>
<table><thead><tr><th>सूचक</th><th>AlphaGo Lee</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>प्रशिक्षण समय</td><td>कई महीने</td><td>40 दिन (3 दिन में AlphaGo Lee को पार)</td></tr><tr><td>प्रशिक्षण खेल</td><td>3 करोड़ मानव शतरंज रिकॉर्ड + सेल्फ-प्ले</td><td>49 लाख सेल्फ-प्ले खेल</td></tr><tr><td>TPU संख्या (प्रशिक्षण)</td><td>50+</td><td>4</td></tr><tr><td>TPU संख्या (इन्फरेंस)</td><td>48</td><td>4</td></tr><tr><td>इनपुट विशेषताएं</td><td>48 प्लेन</td><td>17 प्लेन</td></tr><tr><td>न्यूरल नेटवर्क</td><td>SL + RL दोहरा नेटवर्क</td><td>एकल डुअल-हेड नेटवर्क</td></tr></tbody></table>
<p>यह एक आश्चर्यजनक दक्षता सुधार है: <strong>संसाधन 10 गुना से अधिक कम, लेकिन खेल क्षमता में काफी वृद्धि</strong>।</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="zero-इतना-मजबूत-क्यों-है">Zero इतना मजबूत क्यों है?<a href="#zero-इतना-मजबूत-क्यों-है" class="hash-link" aria-label="Zero इतना मजबूत क्यों है? का सीधा लिंक" title="Zero इतना मजबूत क्यों है? का सीधा लिंक" translate="no">​</a></h3>
<p>AlphaGo Zero के मजबूत होने के कारणों को कई दृष्टिकोणों से समझा जा सकता है:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-पूर्वाग्रह-मुक्त-सीखना">1. पूर्वाग्रह-मुक्त सीखना<a href="#1-पूर्वाग्रह-मुक्त-सीखना" class="hash-link" aria-label="1. पूर्वाग्रह-मुक्त सीखना का सीधा लिंक" title="1. पूर्वाग्रह-मुक्त सीखना का सीधा लिंक" translate="no">​</a></h4>
<p>मूल AlphaGo मानव शतरंज रिकॉर्ड से सीखा, मानव के पूर्वाग्रहों को विरासत में लिया। उदाहरण के लिए, मानव खिलाड़ी कुछ जोसेकी को अधिक महत्व दे सकते हैं, या कुछ स्थितियों का गलत मूल्यांकन कर सकते हैं।</p>
<p>AlphaGo Zero के पास यह बोझ नहीं है। यह खाली स्लेट से शुरू करता है, केवल जीत-हार परिणाम से सीखता है कि अच्छी चाल क्या है। इससे यह उन चालों की खोज कर सका जो मानव ने कभी नहीं सोची थीं।</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-एकसमान-सीखने-का-लक्ष्य">2. एकसमान सीखने का लक्ष्य<a href="#2-एकसमान-सीखने-का-लक्ष्य" class="hash-link" aria-label="2. एकसमान सीखने का लक्ष्य का सीधा लिंक" title="2. एकसमान सीखने का लक्ष्य का सीधा लिंक" translate="no">​</a></h4>
<p>मूल AlphaGo के प्रशिक्षण में दो अलग-अलग लक्ष्य थे:</p>
<ul>
<li class="">सुपरवाइज्ड लर्निंग: मानव चाल भविष्यवाणी सटीकता अधिकतम करना</li>
<li class="">रीइन्फोर्समेंट लर्निंग: जीत दर अधिकतम करना</li>
</ul>
<p>ये दो लक्ष्य परस्पर विरोधी हो सकते हैं। AlphaGo Zero का केवल एक लक्ष्य है: <strong>जीत दर अधिकतम करना</strong>। इससे सीखने की प्रक्रिया अधिक एकसमान और प्रभावी है।</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-सरल-आर्किटेक्चर">3. सरल आर्किटेक्चर<a href="#3-सरल-आर्किटेक्चर" class="hash-link" aria-label="3. सरल आर्किटेक्चर का सीधा लिंक" title="3. सरल आर्किटेक्चर का सीधा लिंक" translate="no">​</a></h4>
<p>मूल AlphaGo अलग Policy Network और Value Network का उपयोग करता था। AlphaGo Zero एकल डुअल-हेड नेटवर्क का उपयोग करता है (अगले लेख में विस्तार से), जिससे फीचर प्रतिनिधित्व साझा होता है, सीखने की दक्षता बढ़ती है।</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="सरलीकृत-इनपुट-विशेषताएं-48-से-17">सरलीकृत इनपुट विशेषताएं: 48 से 17<a href="#सरलीकृत-इनपुट-विशेषताएं-48-से-17" class="hash-link" aria-label="सरलीकृत इनपुट विशेषताएं: 48 से 17 का सीधा लिंक" title="सरलीकृत इनपुट विशेषताएं: 48 से 17 का सीधा लिंक" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="मूल-alphago-के-48-फीचर-प्लेन">मूल AlphaGo के 48 फीचर प्लेन<a href="#मूल-alphago-के-48-फीचर-प्लेन" class="hash-link" aria-label="मूल AlphaGo के 48 फीचर प्लेन का सीधा लिंक" title="मूल AlphaGo के 48 फीचर प्लेन का सीधा लिंक" translate="no">​</a></h3>
<p>मूल AlphaGo के न्यूरल नेटवर्क इनपुट में 48 19x19 फीचर प्लेन थे, जिसमें मानव द्वारा डिज़ाइन की गई कई विशेषताएं एनकोड थीं:</p>
<table><thead><tr><th>श्रेणी</th><th>फीचर संख्या</th><th>सामग्री</th></tr></thead><tbody><tr><td>पत्थर की स्थिति</td><td>3</td><td>काला पत्थर, सफेद पत्थर, खाली बिंदु</td></tr><tr><td>लिबर्टी</td><td>8</td><td>1-8 लिबर्टी वाले स्ट्रिंग्स</td></tr><tr><td>कैप्चर</td><td>8</td><td>1-8 पत्थर कैप्चर करने योग्य</td></tr><tr><td>को</td><td>1</td><td>को स्थिति</td></tr><tr><td>किनारे की दूरी</td><td>4</td><td>पहली से चौथी लाइन</td></tr><tr><td>चाल वैधता</td><td>1</td><td>कहां चल सकते हैं</td></tr><tr><td>ऐतिहासिक स्थिति</td><td>8</td><td>पिछली 8 चालों की स्थिति</td></tr><tr><td>बारी</td><td>1</td><td>काले या सफेद की बारी</td></tr><tr><td>अन्य</td><td>14</td><td>लैडर, आई आदि</td></tr></tbody></table>
<p>ये 48 विशेषताएं Go विशेषज्ञों द्वारा सावधानीपूर्वक डिज़ाइन की गई थीं, जिसमें बहुत सारा डोमेन ज्ञान था।</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero-के-17-फीचर-प्लेन">AlphaGo Zero के 17 फीचर प्लेन<a href="#alphago-zero-के-17-फीचर-प्लेन" class="hash-link" aria-label="AlphaGo Zero के 17 फीचर प्लेन का सीधा लिंक" title="AlphaGo Zero के 17 फीचर प्लेन का सीधा लिंक" translate="no">​</a></h3>
<p>AlphaGo Zero ने इनपुट को काफी सरल किया, केवल 17 फीचर प्लेन:</p>
<table><thead><tr><th>प्लेन नंबर</th><th>सामग्री</th><th>संख्या</th></tr></thead><tbody><tr><td>1-8</td><td>काले पत्थर की स्थिति (पिछली 8 चालें)</td><td>8</td></tr><tr><td>9-16</td><td>सफेद पत्थर की स्थिति (पिछली 8 चालें)</td><td>8</td></tr><tr><td>17</td><td>वर्तमान बारी (सब 1 या सब 0)</td><td>1</td></tr></tbody></table>
<p>इन 17 विशेषताओं में केवल शामिल है:</p>
<ul>
<li class=""><strong>वर्तमान बोर्ड स्थिति</strong>: प्रत्येक स्थान पर काला पत्थर, सफेद पत्थर या खाली</li>
<li class=""><strong>ऐतिहासिक जानकारी</strong>: पिछली 8 चालों की बोर्ड स्थिति</li>
<li class=""><strong>बारी जानकारी</strong>: किसकी बारी है</li>
</ul>
<p>कोई लिबर्टी नहीं, कोई लैडर निर्णय नहीं, कोई किनारे की दूरी नहीं—यह सारा &quot;Go ज्ञान&quot; न्यूरल नेटवर्क को स्वयं सीखने दिया गया।</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="सरलीकरण-अच्छा-क्यों-है">सरलीकरण अच्छा क्यों है?<a href="#सरलीकरण-अच्छा-क्यों-है" class="hash-link" aria-label="सरलीकरण अच्छा क्यों है? का सीधा लिंक" title="सरलीकरण अच्छा क्यों है? का सीधा लिंक" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-नेटवर्क-को-स्वयं-विशेषताएं-खोजने-दें">1. नेटवर्क को स्वयं विशेषताएं खोजने दें<a href="#1-नेटवर्क-को-स्वयं-विशेषताएं-खोजने-दें" class="hash-link" aria-label="1. नेटवर्क को स्वयं विशेषताएं खोजने दें का सीधा लिंक" title="1. नेटवर्क को स्वयं विशेषताएं खोजने दें का सीधा लिंक" translate="no">​</a></h4>
<p>जटिल हाथ से बनाई गई विशेषताएं महत्वपूर्ण जानकारी छोड़ सकती हैं, या गलत धारणाएं एनकोड कर सकती हैं। न्यूरल नेटवर्क को कच्चे डेटा से सीखने दें, यह बेहतर फीचर प्रतिनिधित्व खोज सकता है।</p>
<p>वास्तव में, AlphaGo Zero ने मानव द्वारा डिज़ाइन की गई सभी विशेषताएं (लिबर्टी, लैडर आदि) सीखीं, और कुछ ऐसे पैटर्न भी सीखे जिनके बारे में मानव को स्पष्ट जागरूकता नहीं थी।</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-बेहतर-विस्तारणीयता">2. बेहतर विस्तारणीयता<a href="#2-बेहतर-विस्तारणीयता" class="hash-link" aria-label="2. बेहतर विस्तारणीयता का सीधा लिंक" title="2. बेहतर विस्तारणीयता का सीधा लिंक" translate="no">​</a></h4>
<p>48 विशेषताओं में से कई Go-विशिष्ट हैं (जैसे लैडर, किनारे की दूरी)। 17 सरलीकृत विशेषताएं सार्वभौमिक हैं—किसी भी बोर्ड गेम को समान तरीके से एनकोड किया जा सकता है।</p>
<p>इसने बाद के <strong>AlphaZero</strong> (सार्वभौमिक गेम AI) की नींव रखी।</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-मानवीय-त्रुटियों-को-कम-करना">3. मानवीय त्रुटियों को कम करना<a href="#3-मानवीय-त्रुटियों-को-कम-करना" class="hash-link" aria-label="3. मानवीय त्रुटियों को कम करना का सीधा लिंक" title="3. मानवीय त्रुटियों को कम करना का सीधा लिंक" translate="no">​</a></h4>
<p>हाथ से डिज़ाइन की गई विशेषताओं में गलतियां या अधूरी परिभाषाएं हो सकती हैं। इनपुट का सरलीकरण इस प्रकार की समस्याओं की संभावना को समाप्त करता है।</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="एकल-नेटवर्क-आर्किटेक्चर">एकल नेटवर्क आर्किटेक्चर<a href="#एकल-नेटवर्क-आर्किटेक्चर" class="hash-link" aria-label="एकल नेटवर्क आर्किटेक्चर का सीधा लिंक" title="एकल नेटवर्क आर्किटेक्चर का सीधा लिंक" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="मूल-का-दोहरा-नेटवर्क-डिज़ाइन">मूल का दोहरा नेटवर्क डिज़ाइन<a href="#मूल-का-दोहरा-नेटवर्क-डिज़ाइन" class="hash-link" aria-label="मूल का दोहरा नेटवर्क डिज़ाइन का सीधा लिंक" title="मूल का दोहरा नेटवर्क डिज़ाइन का सीधा लिंक" translate="no">​</a></h3>
<p>मूल AlphaGo दो स्वतंत्र न्यूरल नेटवर्क का उपयोग करता था:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Policy Network:  इनपुट → CNN → 19x19 चाल संभाव्यता</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Value Network:   इनपुट → CNN → जीत दर मूल्यांकन (-1 से 1)</span><br></span></code></pre></div></div>
<p>ये दो नेटवर्क:</p>
<ul>
<li class="">अलग-अलग आर्किटेक्चर थे (लेयर संख्या, चैनल संख्या थोड़ी अलग)</li>
<li class="">स्वतंत्र रूप से प्रशिक्षित (पहले Policy, फिर Value)</li>
<li class="">कोई पैरामीटर साझा नहीं</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="zero-का-डुअल-हेड-नेटवर्क">Zero का डुअल-हेड नेटवर्क<a href="#zero-का-डुअल-हेड-नेटवर्क" class="hash-link" aria-label="Zero का डुअल-हेड नेटवर्क का सीधा लिंक" title="Zero का डुअल-हेड नेटवर्क का सीधा लिंक" translate="no">​</a></h3>
<p>AlphaGo Zero एक नेटवर्क का उपयोग करता है, लेकिन दो आउटपुट हेड्स के साथ:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">इनपुट → ResNet साझा बैकबोन → Policy Head → 19x19 चाल संभाव्यता</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                           → Value Head  → जीत दर मूल्यांकन</span><br></span></code></pre></div></div>
<p>दोनों Head एक ही ResNet बैकबोन साझा करते हैं (विस्तार के लिए <a class="" href="/hi/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/">अगला लेख: डुअल-हेड नेटवर्क और रेसिड्युअल नेटवर्क</a> देखें), जिसके कई फायदे हैं:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-पैरामीटर-दक्षता">1. पैरामीटर दक्षता<a href="#1-पैरामीटर-दक्षता" class="hash-link" aria-label="1. पैरामीटर दक्षता का सीधा लिंक" title="1. पैरामीटर दक्षता का सीधा लिंक" translate="no">​</a></h4>
<p>साझा बैकबोन का मतलब है कि अधिकांश पैरामीटर दोनों कार्यों द्वारा साझा किए जाते हैं। इससे कुल पैरामीटर संख्या कम होती है, ओवरफिटिंग का जोखिम कम होता है।</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-फीचर-साझाकरण">2. फीचर साझाकरण<a href="#2-फीचर-साझाकरण" class="hash-link" aria-label="2. फीचर साझाकरण का सीधा लिंक" title="2. फीचर साझाकरण का सीधा लिंक" translate="no">​</a></h4>
<p>&quot;कहां चलना चाहिए&quot; (Policy) और &quot;कौन जीतेगा&quot; (Value) को समान बोर्ड पैटर्न समझने की आवश्यकता है। साझा बैकबोन इन विशेषताओं को दोनों कार्यों द्वारा एक साथ सीखने और उपयोग करने देता है।</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-प्रशिक्षण-स्थिरता">3. प्रशिक्षण स्थिरता<a href="#3-प्रशिक्षण-स्थिरता" class="hash-link" aria-label="3. प्रशिक्षण स्थिरता का सीधा लिंक" title="3. प्रशिक्षण स्थिरता का सीधा लिंक" translate="no">​</a></h4>
<p>संयुक्त प्रशिक्षण ग्रेडिएंट सिग्नल को दो स्रोतों से लाता है, समृद्ध पर्यवेक्षण सिग्नल प्रदान करता है, प्रशिक्षण को अधिक स्थिर बनाता है।</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="रेसिड्युअल-नेटवर्क-की-शक्ति">रेसिड्युअल नेटवर्क की शक्ति<a href="#रेसिड्युअल-नेटवर्क-की-शक्ति" class="hash-link" aria-label="रेसिड्युअल नेटवर्क की शक्ति का सीधा लिंक" title="रेसिड्युअल नेटवर्क की शक्ति का सीधा लिंक" translate="no">​</a></h3>
<p>AlphaGo Zero का बैकबोन <strong>40 लेयर रेसिड्युअल नेटवर्क (ResNet)</strong> का उपयोग करता है, जो मूल AlphaGo के 13 लेयर CNN से बहुत गहरा है।</p>
<p>रेसिड्युअल कनेक्शन (skip connections) गहरे नेटवर्क को प्रभावी ढंग से प्रशिक्षित करने देते हैं, ग्रेडिएंट विलुप्ति की समस्या से बचाते हैं। यह 2015 ImageNet प्रतियोगिता की सफलता वाली तकनीक है, जिसे AlphaGo Zero ने Go क्षेत्र में सफलतापूर्वक लागू किया।</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="प्रशिक्षण-दक्षता-में-सुधार">प्रशिक्षण दक्षता में सुधार<a href="#प्रशिक्षण-दक्षता-में-सुधार" class="hash-link" aria-label="प्रशिक्षण दक्षता में सुधार का सीधा लिंक" title="प्रशिक्षण दक्षता में सुधार का सीधा लिंक" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="सेल्फ-प्ले-की-घातांकीय-वृद्धि">सेल्फ-प्ले की घातांकीय वृद्धि<a href="#सेल्फ-प्ले-की-घातांकीय-वृद्धि" class="hash-link" aria-label="सेल्फ-प्ले की घातांकीय वृद्धि का सीधा लिंक" title="सेल्फ-प्ले की घातांकीय वृद्धि का सीधा लिंक" translate="no">​</a></h3>
<p>AlphaGo Zero की प्रशिक्षण प्रक्रिया आश्चर्यजनक दक्षता दिखाती है:</p>
<table><thead><tr><th>प्रशिक्षण समय</th><th>ELO रेटिंग</th><th>के समकक्ष</th></tr></thead><tbody><tr><td>0 घंटे</td><td>0</td><td>यादृच्छिक चालें</td></tr><tr><td>3 घंटे</td><td>~1000</td><td>बुनियादी नियम खोजे</td></tr><tr><td>12 घंटे</td><td>~3000</td><td>जोसेकी खोजे</td></tr><tr><td>36 घंटे</td><td>~4500</td><td>फान हुई संस्करण को पार</td></tr><tr><td>60 घंटे</td><td>~5200</td><td>ली से-दोल संस्करण को पार</td></tr><tr><td>72 घंटे</td><td>~5400</td><td>मूल AlphaGo को पार</td></tr><tr><td>40 दिन</td><td>~5600</td><td>सबसे मजबूत संस्करण</td></tr></tbody></table>
<p><strong>तीन दिन में मानव को पार, तीन दिन में महीनों के प्रशिक्षण वाले AI को पार</strong>—यह घातांकीय दक्षता सुधार है।</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="इतना-तेज़-क्यों">इतना तेज़ क्यों?<a href="#इतना-तेज़-क्यों" class="hash-link" aria-label="इतना तेज़ क्यों? का सीधा लिंक" title="इतना तेज़ क्यों? का सीधा लिंक" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-मजबूत-खोज-मार्गदर्शन">1. मजबूत खोज मार्गदर्शन<a href="#1-मजबूत-खोज-मार्गदर्शन" class="hash-link" aria-label="1. मजबूत खोज मार्गदर्शन का सीधा लिंक" title="1. मजबूत खोज मार्गदर्शन का सीधा लिंक" translate="no">​</a></h4>
<p>AlphaGo Zero का MCTS पूरी तरह न्यूरल नेटवर्क द्वारा मार्गदर्शित है, अब तेज़ चाल रणनीति (rollout) का उपयोग नहीं करता। इससे खोज अधिक कुशल और सटीक है।</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-तेज़-सेल्फ-प्ले">2. तेज़ सेल्फ-प्ले<a href="#2-तेज़-सेल्फ-प्ले" class="hash-link" aria-label="2. तेज़ सेल्फ-प्ले का सीधा लिंक" title="2. तेज़ सेल्फ-प्ले का सीधा लिंक" translate="no">​</a></h4>
<p>केवल एक नेटवर्क की आवश्यकता (दो के बजाय) होने से, प्रत्येक सेल्फ-प्ले खेल की गणना लागत कम होती है। इसका मतलब है कि समान समय में अधिक प्रशिक्षण डेटा उत्पन्न हो सकता है।</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-अधिक-प्रभावी-सीखना">3. अधिक प्रभावी सीखना<a href="#3-अधिक-प्रभावी-सीखना" class="hash-link" aria-label="3. अधिक प्रभावी सीखना का सीधा लिंक" title="3. अधिक प्रभावी सीखना का सीधा लिंक" translate="no">​</a></h4>
<p>डुअल-हेड नेटवर्क का संयुक्त प्रशिक्षण प्रत्येक खेल की जानकारी का अधिक प्रभावी उपयोग करता है। Policy और Value के ग्रेडिएंट एक-दूसरे को मजबूत करते हैं, कन्वर्जेंस को तेज़ करते हैं।</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="मानव-सीखने-से-तुलना">मानव सीखने से तुलना<a href="#मानव-सीखने-से-तुलना" class="hash-link" aria-label="मानव सीखने से तुलना का सीधा लिंक" title="मानव सीखने से तुलना का सीधा लिंक" translate="no">​</a></h3>
<p>मानव खिलाड़ियों को विभिन्न स्तरों तक पहुंचने में कितना समय लगता है?</p>
<table><thead><tr><th>स्तर</th><th>मानव के लिए समय</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>शुरुआती</td><td>कुछ सप्ताह</td><td>कुछ मिनट</td></tr><tr><td>शौकिया 1 दान</td><td>कई वर्ष</td><td>कुछ घंटे</td></tr><tr><td>पेशेवर स्तर</td><td>10-20 वर्ष</td><td>1-2 दिन</td></tr><tr><td>विश्व चैंपियन</td><td>20+ वर्ष पूर्णकालिक</td><td>3 दिन</td></tr><tr><td>मानव से परे</td><td>असंभव</td><td>3 दिन</td></tr></tbody></table>
<p>यह तुलना मानव खिलाड़ियों को कम आंकने के लिए नहीं है—वे जैविक न्यूरॉन्स का उपयोग करते हैं, जबकि AlphaGo Zero विशेष रूप से डिज़ाइन किए गए TPU और कई किलोवाट बिजली का उपयोग करता है। लेकिन यह वास्तव में दिखाता है कि सही सीखने की विधि कितनी कुशल हो सकती है।</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="सार्वभौमिकता-शतरंज-शोगी">सार्वभौमिकता: शतरंज, शोगी<a href="#सार्वभौमिकता-शतरंज-शोगी" class="hash-link" aria-label="सार्वभौमिकता: शतरंज, शोगी का सीधा लिंक" title="सार्वभौमिकता: शतरंज, शोगी का सीधा लिंक" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphazero-का-जन्म">AlphaZero का जन्म<a href="#alphazero-का-जन्म" class="hash-link" aria-label="AlphaZero का जन्म का सीधा लिंक" title="AlphaZero का जन्म का सीधा लिंक" translate="no">​</a></h3>
<p>2017 के दिसंबर में, DeepMind ने <strong>AlphaZero</strong> प्रकाशित किया—AlphaGo Zero का सार्वभौमिक संस्करण। एक ही एल्गोरिथम, केवल खेल नियमों को बदलकर, तीन बोर्ड खेलों में विश्व स्तरीय क्षमता प्राप्त कर सकता है:</p>
<table><thead><tr><th>खेल</th><th>प्रशिक्षण समय</th><th>प्रतिद्वंद्वी</th><th>रिकॉर्ड</th></tr></thead><tbody><tr><td>Go</td><td>8 घंटे</td><td>AlphaGo Zero</td><td>60:40</td></tr><tr><td>शतरंज</td><td>4 घंटे</td><td>Stockfish 8</td><td>28 जीत 72 ड्रॉ 0 हार</td></tr><tr><td>शोगी</td><td>2 घंटे</td><td>Elmo</td><td>90:8:2</td></tr></tbody></table>
<p>इन प्रतिद्वंद्वियों पर ध्यान दें:</p>
<ul>
<li class=""><strong>Stockfish</strong> उस समय का सबसे मजबूत शतरंज इंजन था, दशकों के मानव ज्ञान और अनुकूलन का उपयोग</li>
<li class=""><strong>Elmo</strong> उस समय का सबसे मजबूत शोगी AI था</li>
</ul>
<p>AlphaZero ने कुछ घंटों के प्रशिक्षण से इन वर्षों में विकसित विशेष प्रणालियों को पार कर लिया।</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="सार्वभौमिकता-का-महत्व">सार्वभौमिकता का महत्व<a href="#सार्वभौमिकता-का-महत्व" class="hash-link" aria-label="सार्वभौमिकता का महत्व का सीधा लिंक" title="सार्वभौमिकता का महत्व का सीधा लिंक" translate="no">​</a></h3>
<p>AlphaGo Zero / AlphaZero ने एक महत्वपूर्ण बात साबित की:</p>
<blockquote>
<p><strong>एक ही सीखने का एल्गोरिथम विभिन्न क्षेत्रों में अतिमानवीय स्तर प्राप्त कर सकता है।</strong></p>
</blockquote>
<p>यह तीन अलग-अलग AI नहीं हैं, बल्कि एक सार्वभौमिक सीखने की रूपरेखा है:</p>
<ol>
<li class=""><strong>सेल्फ-प्ले</strong> अनुभव उत्पन्न करता है</li>
<li class=""><strong>मोंटे कार्लो ट्री सर्च</strong> संभावनाओं का अन्वेषण करता है</li>
<li class=""><strong>न्यूरल नेटवर्क</strong> रणनीति और मूल्य फ़ंक्शन सीखता है</li>
<li class=""><strong>रीइन्फोर्समेंट लर्निंग</strong> उद्देश्य फ़ंक्शन को अनुकूलित करता है</li>
</ol>
<p>यह रूपरेखा डोमेन-विशिष्ट ज्ञान पर निर्भर नहीं करती, AI के सार्वभौमिकरण की दिशा में यह एक महत्वपूर्ण कदम है।</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="पारंपरिक-ai-पर-प्रभाव">पारंपरिक AI पर प्रभाव<a href="#पारंपरिक-ai-पर-प्रभाव" class="hash-link" aria-label="पारंपरिक AI पर प्रभाव का सीधा लिंक" title="पारंपरिक AI पर प्रभाव का सीधा लिंक" translate="no">​</a></h3>
<p>AlphaZero से पहले, शतरंज और शोगी के सबसे मजबूत AI &quot;विशेषज्ञ प्रणाली&quot; शैली के थे:</p>
<ul>
<li class=""><strong>बहुत सारा मानव ज्ञान</strong>: ओपनिंग बुक, एंडगेम बुक, मूल्यांकन फ़ंक्शन</li>
<li class=""><strong>दशकों का अनुकूलन</strong>: अनगिनत खिलाड़ियों और इंजीनियरों का परिश्रम</li>
<li class=""><strong>अत्यधिक विशेषज्ञता</strong>: Stockfish Go नहीं खेल सकता, Elmo शतरंज नहीं खेल सकता</li>
</ul>
<p>AlphaZero ने एक सार्वभौमिक एल्गोरिथम से कुछ घंटों में यह सब पार कर लिया। इसने कई AI शोधकर्ताओं को पुनर्विचार कराया:</p>
<blockquote>
<p>क्या हमें &quot;सार्वभौमिक सीखने के एल्गोरिथम&quot; में अधिक प्रयास लगाना चाहिए, या &quot;विशेषज्ञ ज्ञान एन्कोडिंग&quot; में?</p>
</blockquote>
<p>उत्तर तेजी से स्पष्ट होता जा रहा है: मशीन को स्वयं सीखने देना उसे ज्ञान सिखाने से अधिक प्रभावी है।</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero-की-खेल-शैली">AlphaGo Zero की खेल शैली<a href="#alphago-zero-की-खेल-शैली" class="hash-link" aria-label="AlphaGo Zero की खेल शैली का सीधा लिंक" title="AlphaGo Zero की खेल शैली का सीधा लिंक" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="मानव-सौंदर्यबोध-से-परे">मानव सौंदर्यबोध से परे<a href="#मानव-सौंदर्यबोध-से-परे" class="hash-link" aria-label="मानव सौंदर्यबोध से परे का सीधा लिंक" title="मानव सौंदर्यबोध से परे का सीधा लिंक" translate="no">​</a></h3>
<p>Go समुदाय में AlphaGo Zero की चालों का एक सामान्य मूल्यांकन है: <strong>अधिक सुंदर</strong>।</p>
<p>AlphaGo Lee की चालें कभी-कभी &quot;अजीब&quot; लगती थीं—जैसे 37वीं चाल, मानव को इसकी सुंदरता समझने के लिए बाद में विश्लेषण की आवश्यकता थी। लेकिन AlphaGo Zero की चालों को अक्सर &quot;पहली नज़र में अच्छी चाल&quot; के रूप में आंका जाता है।</p>
<p>यह शायद इसलिए है:</p>
<ol>
<li class=""><strong>मजबूत खेल</strong>: Zero अधिक गहराई से देख सकता है, चालें अधिक आत्मविश्वास से</li>
<li class=""><strong>कोई मानव पूर्वाग्रह नहीं</strong>: पारंपरिक जोसेकी से बंधा नहीं</li>
<li class=""><strong>एकसमान लक्ष्य</strong>: केवल जीत दर का पीछा, मानव की नकल नहीं</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="मानव-go-सिद्धांतों-की-पुनः-खोज">मानव Go सिद्धांतों की पुनः खोज<a href="#मानव-go-सिद्धांतों-की-पुनः-खोज" class="hash-link" aria-label="मानव Go सिद्धांतों की पुनः खोज का सीधा लिंक" title="मानव Go सिद्धांतों की पुनः खोज का सीधा लिंक" translate="no">​</a></h3>
<p>दिलचस्प बात यह है कि AlphaGo Zero ने प्रशिक्षण प्रक्रिया में मानव के हज़ारों वर्षों के संचित Go ज्ञान को &quot;पुनः खोजा&quot;:</p>
<ul>
<li class=""><strong>जोसेकी</strong>: Zero ने स्वयं कई सामान्य जोसेकी खोजे, क्योंकि ये वास्तव में दोनों पक्षों के लिए सर्वोत्तम हैं</li>
<li class=""><strong>लेआउट सिद्धांत</strong>: कॉर्नर, साइड, सेंटर का महत्व क्रम</li>
<li class=""><strong>आकार ज्ञान</strong>: बुरे और अच्छे आकारों में अंतर</li>
</ul>
<p>इसने मानव Go सिद्धांतों की तर्कसंगतता को प्रमाणित किया—यह ज्ञान आकस्मिक नहीं है, बल्कि Go के सार का प्रतिबिंब है।</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="मानव-से-परे-नवाचार">मानव से परे नवाचार<a href="#मानव-से-परे-नवाचार" class="hash-link" aria-label="मानव से परे नवाचार का सीधा लिंक" title="मानव से परे नवाचार का सीधा लिंक" translate="no">​</a></h3>
<p>लेकिन Zero ने ऐसी चालें भी खोजीं जो मानव ने कभी नहीं सोची थीं:</p>
<ul>
<li class=""><strong>अपरंपरागत ओपनिंग</strong>: पारंपरिक ओपनिंग पर विविधताएं</li>
<li class=""><strong>आक्रामक त्याग</strong>: स्थानीय को छोड़ने के लिए मानव से अधिक तैयार, वैश्विक लाभ के लिए</li>
<li class=""><strong>प्रतिकूल आकार</strong>: सतह पर &quot;बुरा आकार&quot; वास्तव में सर्वोत्तम है</li>
</ul>
<p>ये नवाचार Go की मानव समझ को बदल रहे हैं। कई पेशेवर खिलाड़ी कहते हैं कि AlphaGo Zero के शतरंज रिकॉर्ड का अध्ययन करने से उन्हें Go की पूरी तरह नई समझ मिली।</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="तकनीकी-विवरण-सारांश">तकनीकी विवरण सारांश<a href="#तकनीकी-विवरण-सारांश" class="hash-link" aria-label="तकनीकी विवरण सारांश का सीधा लिंक" title="तकनीकी विवरण सारांश का सीधा लिंक" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="मूल-alphago-से-पूर्ण-तुलना">मूल AlphaGo से पूर्ण तुलना<a href="#मूल-alphago-से-पूर्ण-तुलना" class="hash-link" aria-label="मूल AlphaGo से पूर्ण तुलना का सीधा लिंक" title="मूल AlphaGo से पूर्ण तुलना का सीधा लिंक" translate="no">​</a></h3>
<table><thead><tr><th>पहलू</th><th>AlphaGo (मूल)</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td><strong>प्रशिक्षण डेटा</strong></td><td>मानव शतरंज रिकॉर्ड + सेल्फ-प्ले</td><td>शुद्ध सेल्फ-प्ले</td></tr><tr><td><strong>सीखने की विधि</strong></td><td>सुपरवाइज्ड + रीइन्फोर्समेंट</td><td>शुद्ध रीइन्फोर्समेंट</td></tr><tr><td><strong>इनपुट विशेषताएं</strong></td><td>48 प्लेन</td><td>17 प्लेन</td></tr><tr><td><strong>नेटवर्क आर्किटेक्चर</strong></td><td>अलग Policy/Value</td><td>डुअल-हेड ResNet</td></tr><tr><td><strong>नेटवर्क गहराई</strong></td><td>13 लेयर</td><td>40 लेयर (या अधिक)</td></tr><tr><td><strong>MCTS मूल्यांकन</strong></td><td>न्यूरल नेटवर्क + Rollout</td><td>शुद्ध न्यूरल नेटवर्क</td></tr><tr><td><strong>खोज संख्या</strong></td><td>प्रति चाल ~100,000</td><td>प्रति चाल ~1,600</td></tr><tr><td><strong>प्रशिक्षण TPU</strong></td><td>50+</td><td>4</td></tr><tr><td><strong>इन्फरेंस TPU</strong></td><td>48</td><td>4 (स्केलेबल)</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="मुख्य-एल्गोरिथम">मुख्य एल्गोरिथम<a href="#मुख्य-एल्गोरिथम" class="hash-link" aria-label="मुख्य एल्गोरिथम का सीधा लिंक" title="मुख्य एल्गोरिथम का सीधा लिंक" translate="no">​</a></h3>
<p>AlphaGo Zero का प्रशिक्षण चक्र बहुत संक्षिप्त है:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. सेल्फ-प्ले</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - वर्तमान नेटवर्क से MCTS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - MCTS खोज संभाव्यता के अनुसार चाल चुनें</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - प्रत्येक चाल का (स्थिति, MCTS संभाव्यता, जीत-हार परिणाम) रिकॉर्ड करें</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. नेटवर्क प्रशिक्षण</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - अनुभव पूल से नमूने लें</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Policy Head: MCTS संभाव्यता के साथ क्रॉस-एंट्रॉपी न्यूनतम करें</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Value Head: वास्तविक जीत-हार के साथ MSE न्यूनतम करें</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - दोनों लक्ष्यों का संयुक्त अनुकूलन</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. नेटवर्क अपडेट</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - नए नेटवर्क से पुराने को बदलें (खेल द्वारा सत्यापित कि नया मजबूत है)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - चरण 1 पर वापस जाएं</span><br></span></code></pre></div></div>
<p>यह चक्र लगातार चलता है, नेटवर्क लगातार मजबूत होता है। कोई मानव डेटा नहीं, कोई मानव ज्ञान नहीं, केवल खेल नियम और जीत-हार लक्ष्य।</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="ai-अनुसंधान-के-लिए-प्रेरणा">AI अनुसंधान के लिए प्रेरणा<a href="#ai-अनुसंधान-के-लिए-प्रेरणा" class="hash-link" aria-label="AI अनुसंधान के लिए प्रेरणा का सीधा लिंक" title="AI अनुसंधान के लिए प्रेरणा का सीधा लिंक" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="प्रथम-सिद्धांत-सीखना">प्रथम सिद्धांत सीखना<a href="#प्रथम-सिद्धांत-सीखना" class="hash-link" aria-label="प्रथम सिद्धांत सीखना का सीधा लिंक" title="प्रथम सिद्धांत सीखना का सीधा लिंक" translate="no">​</a></h3>
<p>AlphaGo Zero ने &quot;प्रथम सिद्धांत&quot; सीखने की विधि दिखाई:</p>
<blockquote>
<p>AI को कैसे करना है मत बताओ, केवल लक्ष्य बताओ, इसे स्वयं विधि खोजने दो।</p>
</blockquote>
<p>यह पारंपरिक विशेषज्ञ प्रणाली विधि से बिल्कुल अलग है। विशेषज्ञ प्रणालियां मानव ज्ञान को AI में एन्कोड करने का प्रयास करती हैं, जबकि AlphaGo Zero AI को स्वयं ज्ञान खोजने देता है।</p>
<p>परिणाम है: AI द्वारा खोजा गया ज्ञान मानव ज्ञान से अधिक पूर्ण और सटीक हो सकता है।</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="सेल्फ-प्ले-की-शक्ति">सेल्फ-प्ले की शक्ति<a href="#सेल्फ-प्ले-की-शक्ति" class="hash-link" aria-label="सेल्फ-प्ले की शक्ति का सीधा लिंक" title="सेल्फ-प्ले की शक्ति का सीधा लिंक" translate="no">​</a></h3>
<p>AlphaGo Zero ने साबित किया कि सेल्फ-प्ले असीमित प्रशिक्षण डेटा उत्पन्न कर सकता है, और इस डेटा की गुणवत्ता नेटवर्क के सुधार के साथ सुधरती है।</p>
<p>यह एक &quot;सकारात्मक चक्र&quot; है:</p>
<ul>
<li class="">मजबूत नेटवर्क → बेहतर सेल्फ-प्ले डेटा</li>
<li class="">बेहतर डेटा → मजबूत नेटवर्क</li>
</ul>
<p>यह चक्र तब तक चल सकता है जब तक खेल की सैद्धांतिक सीमा (यदि मौजूद हो) तक नहीं पहुंच जाता।</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="सरलीकरण-का-महत्व">सरलीकरण का महत्व<a href="#सरलीकरण-का-महत्व" class="hash-link" aria-label="सरलीकरण का महत्व का सीधा लिंक" title="सरलीकरण का महत्व का सीधा लिंक" translate="no">​</a></h3>
<p>AlphaGo Zero की सफलता ने &quot;सरलीकरण&quot; का महत्व साबित किया:</p>
<ul>
<li class="">इनपुट का सरलीकरण (48 → 17)</li>
<li class="">आर्किटेक्चर का सरलीकरण (दोहरा नेटवर्क → एकल नेटवर्क)</li>
<li class="">प्रशिक्षण का सरलीकरण (सुपरवाइज्ड + रीइन्फोर्समेंट → शुद्ध रीइन्फोर्समेंट)</li>
</ul>
<p>प्रत्येक सरलीकरण ने सिस्टम को अधिक शक्तिशाली बनाया। यह हमें बताता है: जटिल अच्छा नहीं है, सबसे सरल समाधान अक्सर सबसे अच्छा होता है।</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="एनिमेशन-संदर्भ">एनिमेशन संदर्भ<a href="#एनिमेशन-संदर्भ" class="hash-link" aria-label="एनिमेशन संदर्भ का सीधा लिंक" title="एनिमेशन संदर्भ का सीधा लिंक" translate="no">​</a></h2>
<p>इस लेख में शामिल मुख्य अवधारणाएं और एनिमेशन नंबर:</p>
<table><thead><tr><th>नंबर</th><th>अवधारणा</th><th>भौतिकी/गणित समकक्ष</th></tr></thead><tbody><tr><td>E7</td><td>शून्य से प्रशिक्षण</td><td>स्व-संगठन घटना</td></tr><tr><td>E5</td><td>सेल्फ-प्ले</td><td>स्थिर बिंदु कन्वर्जेंस</td></tr><tr><td>E12</td><td>खेल क्षमता वृद्धि वक्र</td><td>S-आकार वृद्धि</td></tr><tr><td>D12</td><td>रेसिड्युअल नेटवर्क</td><td>ग्रेडिएंट हाईवे</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="आगे-पढ़ने-के-लिए">आगे पढ़ने के लिए<a href="#आगे-पढ़ने-के-लिए" class="hash-link" aria-label="आगे पढ़ने के लिए का सीधा लिंक" title="आगे पढ़ने के लिए का सीधा लिंक" translate="no">​</a></h2>
<ul>
<li class=""><strong>अगला लेख</strong>: <a class="" href="/hi/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/">डुअल-हेड नेटवर्क और रेसिड्युअल नेटवर्क</a> — AlphaGo Zero के न्यूरल नेटवर्क आर्किटेक्चर का विस्तृत विवरण</li>
<li class=""><strong>संबंधित लेख</strong>: <a class="" href="/hi/docs/for-engineers/how-it-works/alphago-explained/self-play/">सेल्फ-प्ले</a> — सेल्फ-प्ले अतिमानवीय स्तर क्यों उत्पन्न कर सकता है</li>
<li class=""><strong>तकनीकी गहराई</strong>: <a class="" href="/hi/docs/for-engineers/how-it-works/alphago-explained/training-from-scratch/">शून्य से प्रशिक्षण प्रक्रिया</a> — Day 0-3 का विस्तृत विकास</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="संदर्भ">संदर्भ<a href="#संदर्भ" class="hash-link" aria-label="संदर्भ का सीधा लिंक" title="संदर्भ का सीधा लिंक" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">Silver, D., et al. (2018). &quot;A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.&quot; <em>Science</em>, 362(6419), 1140-1144.</li>
<li class="">DeepMind. (2017). &quot;AlphaGo Zero: Starting from scratch.&quot; <em>DeepMind Blog</em>.</li>
<li class="">Schrittwieser, J., et al. (2020). &quot;Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.&quot; <em>Nature</em>, 588, 604-609.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/16-alphago-zero.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>इस पेज को बदलें</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="डॉक्स पेज"><a class="pagination-nav__link pagination-nav__link--prev" href="/hi/docs/for-engineers/how-it-works/alphago-explained/puct-formula/"><div class="pagination-nav__sublabel">पिछ्ला</div><div class="pagination-nav__label">PUCT सूत्र का विस्तृत विवरण</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/hi/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/"><div class="pagination-nav__sublabel">अगला</div><div class="pagination-nav__label">डुअल-हेड नेटवर्क और रेसिड्युअल नेटवर्क</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#मानव-शतरंज-रिकॉर्ड-की-आवश्यकता-क्यों-नहीं" class="table-of-contents__link toc-highlight">मानव शतरंज रिकॉर्ड की आवश्यकता क्यों नहीं?</a><ul><li><a href="#मानव-शतरंज-रिकॉर्ड-की-सीमाएं" class="table-of-contents__link toc-highlight">मानव शतरंज रिकॉर्ड की सीमाएं</a></li><li><a href="#zero-की-सफलता" class="table-of-contents__link toc-highlight">Zero की सफलता</a></li></ul></li><li><a href="#मूल-alphago-से-तुलना-1000" class="table-of-contents__link toc-highlight">मूल AlphaGo से तुलना: 100:0</a><ul><li><a href="#कुचलने-वाली-जीत" class="table-of-contents__link toc-highlight">कुचलने वाली जीत</a></li><li><a href="#कम-संसाधन-मजबूत-खेल" class="table-of-contents__link toc-highlight">कम संसाधन, मजबूत खेल</a></li><li><a href="#zero-इतना-मजबूत-क्यों-है" class="table-of-contents__link toc-highlight">Zero इतना मजबूत क्यों है?</a></li></ul></li><li><a href="#सरलीकृत-इनपुट-विशेषताएं-48-से-17" class="table-of-contents__link toc-highlight">सरलीकृत इनपुट विशेषताएं: 48 से 17</a><ul><li><a href="#मूल-alphago-के-48-फीचर-प्लेन" class="table-of-contents__link toc-highlight">मूल AlphaGo के 48 फीचर प्लेन</a></li><li><a href="#alphago-zero-के-17-फीचर-प्लेन" class="table-of-contents__link toc-highlight">AlphaGo Zero के 17 फीचर प्लेन</a></li><li><a href="#सरलीकरण-अच्छा-क्यों-है" class="table-of-contents__link toc-highlight">सरलीकरण अच्छा क्यों है?</a></li></ul></li><li><a href="#एकल-नेटवर्क-आर्किटेक्चर" class="table-of-contents__link toc-highlight">एकल नेटवर्क आर्किटेक्चर</a><ul><li><a href="#मूल-का-दोहरा-नेटवर्क-डिज़ाइन" class="table-of-contents__link toc-highlight">मूल का दोहरा नेटवर्क डिज़ाइन</a></li><li><a href="#zero-का-डुअल-हेड-नेटवर्क" class="table-of-contents__link toc-highlight">Zero का डुअल-हेड नेटवर्क</a></li><li><a href="#रेसिड्युअल-नेटवर्क-की-शक्ति" class="table-of-contents__link toc-highlight">रेसिड्युअल नेटवर्क की शक्ति</a></li></ul></li><li><a href="#प्रशिक्षण-दक्षता-में-सुधार" class="table-of-contents__link toc-highlight">प्रशिक्षण दक्षता में सुधार</a><ul><li><a href="#सेल्फ-प्ले-की-घातांकीय-वृद्धि" class="table-of-contents__link toc-highlight">सेल्फ-प्ले की घातांकीय वृद्धि</a></li><li><a href="#इतना-तेज़-क्यों" class="table-of-contents__link toc-highlight">इतना तेज़ क्यों?</a></li><li><a href="#मानव-सीखने-से-तुलना" class="table-of-contents__link toc-highlight">मानव सीखने से तुलना</a></li></ul></li><li><a href="#सार्वभौमिकता-शतरंज-शोगी" class="table-of-contents__link toc-highlight">सार्वभौमिकता: शतरंज, शोगी</a><ul><li><a href="#alphazero-का-जन्म" class="table-of-contents__link toc-highlight">AlphaZero का जन्म</a></li><li><a href="#सार्वभौमिकता-का-महत्व" class="table-of-contents__link toc-highlight">सार्वभौमिकता का महत्व</a></li><li><a href="#पारंपरिक-ai-पर-प्रभाव" class="table-of-contents__link toc-highlight">पारंपरिक AI पर प्रभाव</a></li></ul></li><li><a href="#alphago-zero-की-खेल-शैली" class="table-of-contents__link toc-highlight">AlphaGo Zero की खेल शैली</a><ul><li><a href="#मानव-सौंदर्यबोध-से-परे" class="table-of-contents__link toc-highlight">मानव सौंदर्यबोध से परे</a></li><li><a href="#मानव-go-सिद्धांतों-की-पुनः-खोज" class="table-of-contents__link toc-highlight">मानव Go सिद्धांतों की पुनः खोज</a></li><li><a href="#मानव-से-परे-नवाचार" class="table-of-contents__link toc-highlight">मानव से परे नवाचार</a></li></ul></li><li><a href="#तकनीकी-विवरण-सारांश" class="table-of-contents__link toc-highlight">तकनीकी विवरण सारांश</a><ul><li><a href="#मूल-alphago-से-पूर्ण-तुलना" class="table-of-contents__link toc-highlight">मूल AlphaGo से पूर्ण तुलना</a></li><li><a href="#मुख्य-एल्गोरिथम" class="table-of-contents__link toc-highlight">मुख्य एल्गोरिथम</a></li></ul></li><li><a href="#ai-अनुसंधान-के-लिए-प्रेरणा" class="table-of-contents__link toc-highlight">AI अनुसंधान के लिए प्रेरणा</a><ul><li><a href="#प्रथम-सिद्धांत-सीखना" class="table-of-contents__link toc-highlight">प्रथम सिद्धांत सीखना</a></li><li><a href="#सेल्फ-प्ले-की-शक्ति" class="table-of-contents__link toc-highlight">सेल्फ-प्ले की शक्ति</a></li><li><a href="#सरलीकरण-का-महत्व" class="table-of-contents__link toc-highlight">सरलीकरण का महत्व</a></li></ul></li><li><a href="#एनिमेशन-संदर्भ" class="table-of-contents__link toc-highlight">एनिमेशन संदर्भ</a></li><li><a href="#आगे-पढ़ने-के-लिए" class="table-of-contents__link toc-highlight">आगे पढ़ने के लिए</a></li><li><a href="#संदर्भ" class="table-of-contents__link toc-highlight">संदर्भ</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>