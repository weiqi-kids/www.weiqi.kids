<!doctype html>
<html lang="ko" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-for-engineers/background-info/alphago" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">AlphaGo 논문 해독 | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/ko/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/ko/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/ko/docs/for-engineers/background-info/alphago/"><meta data-rh="true" property="og:locale" content="ko"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="ko"><meta data-rh="true" name="docsearch:language" content="ko"><meta data-rh="true" name="keywords" content="圍棋, Go, 好棋寶寶, AI, KataGo, AlphaGo, 圍棋教學, 圍棋入門"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="AlphaGo 논문 해독 | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="본문은 DeepMind가 Nature에 발표한 고전 논문 《Mastering the game of Go with deep neural networks and tree search》와 후속 AlphaGo Zero, AlphaZero 논문을 심층 해석합니다."><meta data-rh="true" property="og:description" content="본문은 DeepMind가 Nature에 발표한 고전 논문 《Mastering the game of Go with deep neural networks and tree search》와 후속 AlphaGo Zero, AlphaZero 논문을 심층 해석합니다."><link data-rh="true" rel="icon" href="/ko/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/ko/docs/for-engineers/background-info/alphago/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/background-info/alphago/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/for-engineers/background-info/alphago/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/for-engineers/background-info/alphago/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/for-engineers/background-info/alphago/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/for-engineers/background-info/alphago/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/for-engineers/background-info/alphago/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/for-engineers/background-info/alphago/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/for-engineers/background-info/alphago/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/for-engineers/background-info/alphago/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/for-engineers/background-info/alphago/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/for-engineers/background-info/alphago/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/background-info/alphago/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"엔지니어를 위한 바둑 AI 가이드","item":"https://www.weiqi.kids/ko/docs/for-engineers/"},{"@type":"ListItem","position":2,"name":"배경 지식","item":"https://www.weiqi.kids/ko/docs/for-engineers/background-info/"},{"@type":"ListItem","position":3,"name":"AlphaGo 논문 해독","item":"https://www.weiqi.kids/ko/docs/for-engineers/background-info/alphago"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會"><link rel="stylesheet" href="/ko/assets/css/styles.c6458d7f.css">
<script src="/ko/assets/js/runtime~main.d7869339.js" defer="defer"></script>
<script src="/ko/assets/js/main.09765ea3.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ko/img/logo.svg"><div role="region" aria-label="본문으로 건너뛰기"><a class="skipToContent_bFSG" href="#__docusaurus_skipToContent_fallback">본문으로 건너뛰기</a></div><nav aria-label="메인" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="사이드바 펼치거나 접기" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ko/"><div class="navbar__logo"><img src="/ko/img/logo.svg" alt="호기보보협회 로고" class="themedComponent_Wuuq themedComponent--light_w2On"><img src="/ko/img/logo.svg" alt="호기보보협회 로고" class="themedComponent_Wuuq themedComponent--dark_zC0W"></div><b class="navbar__title text--truncate">바둑 키즈</b></a><a class="navbar__item navbar__link" href="/ko/docs/for-players/">바둑 플레이어</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ko/docs/for-engineers/">엔지니어</a><a class="navbar__item navbar__link" href="/ko/docs/about/">협회 소개</a><a class="navbar__item navbar__link" href="/ko/docs/activities/">활동 실적</a><a class="navbar__item navbar__link" href="/ko/docs/references/">참고 자료</a><a class="navbar__item navbar__link" href="/ko/docs/sop/">표준 운영 절차</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_Zh_g"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>한국어</a><ul class="dropdown__menu"><li><a href="/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="ko">한국어</a></li><li><a href="/es/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/for-engineers/background-info/alphago/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_pmRL"><div class="navbar__search searchBarContainer_nNQu" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_jmZR" value=""><div class="loadingRing_zZTX searchBarLoadingRing_JhV3"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_WlJ3"><div class="docsWrapper_ramL"><button aria-label="맨 위로 스크롤하기" class="clean-btn theme-back-to-top-button backToTopButton_MqxJ" type="button"></button><div class="docRoot_UZ1h"><aside class="theme-doc-sidebar-container docSidebarContainer_y0QC"><div class="sidebarViewport_jAkm"><div class="sidebar_HjkB"><nav aria-label="문서 사이드바" class="menu thin-scrollbar menu_jqYH"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ko/docs/intro/">이용 안내</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/ko/docs/about/">關於協會</a><button aria-label="사이드바 분류 &#x27;關於協會&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/ko/docs/activities/">活動實績</a><button aria-label="사이드바 분류 &#x27;活動實績&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/ko/docs/for-players/">바둑 기사를 위한 자료</a><button aria-label="사이드바 분류 &#x27;바둑 기사를 위한 자료&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/ko/docs/references/">參考資料</a><button aria-label="사이드바 분류 &#x27;參考資料&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/ko/docs/sop/">標準作業流程</a><button aria-label="사이드바 분류 &#x27;標準作業流程&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/ko/docs/for-engineers/">엔지니어를 위한 바둑 AI 가이드</a><button aria-label="사이드바 분류 &#x27;엔지니어를 위한 바둑 AI 가이드&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ko/docs/for-engineers/background-info/">배경 지식</a><button aria-label="사이드바 분류 &#x27;배경 지식&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ko/docs/for-engineers/background-info/alphago/">AlphaGo 논문 해독</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/background-info/katago-paper/">KataGo 논문 해독</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/background-info/zen/">기타 바둑 AI 소개</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ko/docs/for-engineers/katago-source/">KataGo 실전 입문</a><button aria-label="사이드바 분류 &#x27;KataGo 실전 입문&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_qMQS"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_Wlt9"><div class="docItemContainer_oLHG"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_M4ek" aria-label="탐색 경로"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="홈" class="breadcrumbs__link" href="/ko/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_pL7f"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ko/docs/for-engineers/"><span>엔지니어를 위한 바둑 AI 가이드</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ko/docs/for-engineers/background-info/"><span>배경 지식</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">AlphaGo 논문 해독</span></li></ul></nav><div class="tocCollapsible_K0FP theme-doc-toc-mobile tocMobile_PzQu"><button type="button" class="clean-btn tocCollapsibleButton_E0oR">이 페이지에서</button></div><div class="theme-doc-markdown markdown"><header><h1>AlphaGo 논문 해독</h1></header>
<p>본문은 DeepMind가 Nature에 발표한 고전 논문 《Mastering the game of Go with deep neural networks and tree search》와 후속 AlphaGo Zero, AlphaZero 논문을 심층 해석합니다.</p>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="alphago의-역사적-의의">AlphaGo의 역사적 의의<a href="#alphago의-역사적-의의" class="hash-link" aria-label="AlphaGo의 역사적 의의에 대한 직접 링크" title="AlphaGo의 역사적 의의에 대한 직접 링크">​</a></h2>
<p>바둑은 오랫동안 인공지능의 &#x27;성배&#x27; 도전으로 여겨졌습니다. 체스와 달리 바둑의 탐색 공간은 극도로 방대합니다:</p>
<table><thead><tr><th>게임</th><th>평균 분기 인자</th><th>평균 게임 길이</th><th>상태 공간</th></tr></thead><tbody><tr><td>체스</td><td>~35</td><td>~80</td><td>~10^47</td></tr><tr><td>바둑</td><td>~250</td><td>~150</td><td>~10^170</td></tr></tbody></table>
<p>전통적인 완전 탐색 방법은 바둑에서 전혀 불가능합니다. 2016년 AlphaGo가 이세돌을 꺾으며 딥러닝과 강화학습 결합의 강력한 위력을 증명했습니다.</p>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="이정표-사건">이정표 사건<a href="#이정표-사건" class="hash-link" aria-label="이정표 사건에 대한 직접 링크" title="이정표 사건에 대한 직접 링크">​</a></h3>
<ul>
<li><strong>2015년 10월</strong>: AlphaGo Fan이 유럽 챔피언 판후이(프로 2단) 5:0 격파</li>
<li><strong>2016년 3월</strong>: AlphaGo Lee가 세계 챔피언 이세돌(프로 9단) 4:1 격파</li>
<li><strong>2017년 5월</strong>: AlphaGo Master가 세계 랭킹 1위 커제 3:0 격파</li>
<li><strong>2017년 10월</strong>: AlphaGo Zero 발표, 순수 자가대국 훈련, 모든 이전 버전 초월</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="핵심-기술-아키텍처">핵심 기술 아키텍처<a href="#핵심-기술-아키텍처" class="hash-link" aria-label="핵심 기술 아키텍처에 대한 직접 링크" title="핵심 기술 아키텍처에 대한 직접 링크">​</a></h2>
<p>AlphaGo의 핵심 혁신은 세 가지 핵심 기술의 결합입니다:</p>
<!-- -->
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="policy-network정책-네트워크">Policy Network(정책 네트워크)<a href="#policy-network정책-네트워크" class="hash-link" aria-label="Policy Network(정책 네트워크)에 대한 직접 링크" title="Policy Network(정책 네트워크)에 대한 직접 링크">​</a></h3>
<p>Policy Network는 각 위치의 착점 확률을 예측하여 탐색 방향을 안내합니다.</p>
<h4 class="anchor anchorWithStickyNavbar_BVHq" id="네트워크-아키텍처">네트워크 아키텍처<a href="#네트워크-아키텍처" class="hash-link" aria-label="네트워크 아키텍처에 대한 직접 링크" title="네트워크 아키텍처에 대한 직접 링크">​</a></h4>
<!-- -->
<h4 class="anchor anchorWithStickyNavbar_BVHq" id="입력-특징">입력 특징<a href="#입력-특징" class="hash-link" aria-label="입력 특징에 대한 직접 링크" title="입력 특징에 대한 직접 링크">​</a></h4>
<p>AlphaGo는 48개의 특징 평면을 입력으로 사용합니다:</p>
<table><thead><tr><th>특징</th><th>평면 수</th><th>설명</th></tr></thead><tbody><tr><td>돌 색상</td><td>3</td><td>흑돌, 백돌, 빈 점</td></tr><tr><td>활로 수</td><td>8</td><td>1활로, 2활로, ..., 8활로 이상</td></tr><tr><td>단수 후 활로 수</td><td>8</td><td>잡은 후 몇 활로가 되는지</td></tr><tr><td>잡는 돌 수</td><td>8</td><td>해당 위치에서 몇 점 잡을 수 있는지</td></tr><tr><td>패</td><td>1</td><td>패 위치인지</td></tr><tr><td>착수 합법성</td><td>1</td><td>해당 위치에 둘 수 있는지</td></tr><tr><td>최근 1-8수 위치</td><td>8</td><td>이전 몇 수의 착점 위치</td></tr><tr><td>차례</td><td>1</td><td>현재 흑/백 차례</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_BVHq" id="훈련-방식">훈련 방식<a href="#훈련-방식" class="hash-link" aria-label="훈련 방식에 대한 직접 링크" title="훈련 방식에 대한 직접 링크">​</a></h4>
<p>Policy Network 훈련은 두 단계로 나뉩니다:</p>
<p><strong>1단계: 지도학습(SL Policy Network)</strong></p>
<ul>
<li>KGS 바둑 서버의 3000만 국 기보 사용</li>
<li>목표: 인간 기사의 다음 수 예측</li>
<li>57% 예측 정확도 달성</li>
</ul>
<p><strong>2단계: 강화학습(RL Policy Network)</strong></p>
<ul>
<li>SL Policy Network부터 시작</li>
<li>이전 버전의 자신과 대국</li>
<li>REINFORCE 알고리즘으로 최적화</li>
</ul>
<div class="language-python codeBlockContainer_WV0T theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_GRu9"><pre tabindex="0" class="prism-code language-python codeBlock_md1K thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_L4YE"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 간소화된 Policy Gradient 업데이트</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># reward: +1 승리, -1 패배</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">log</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">policy</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">action</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> reward</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="value-network가치-네트워크">Value Network(가치 네트워크)<a href="#value-network가치-네트워크" class="hash-link" aria-label="Value Network(가치 네트워크)에 대한 직접 링크" title="Value Network(가치 네트워크)에 대한 직접 링크">​</a></h3>
<p>Value Network는 현재 국면의 승률을 평가하여 탐색 깊이를 줄입니다.</p>
<h4 class="anchor anchorWithStickyNavbar_BVHq" id="네트워크-아키텍처-1">네트워크 아키텍처<a href="#네트워크-아키텍처-1" class="hash-link" aria-label="네트워크 아키텍처에 대한 직접 링크" title="네트워크 아키텍처에 대한 직접 링크">​</a></h4>
<!-- -->
<h4 class="anchor anchorWithStickyNavbar_BVHq" id="훈련-방식-1">훈련 방식<a href="#훈련-방식-1" class="hash-link" aria-label="훈련 방식에 대한 직접 링크" title="훈련 방식에 대한 직접 링크">​</a></h4>
<p>Value Network는 RL Policy Network 자가대국으로 생성된 3000만 국면으로 훈련:</p>
<ul>
<li>각 대국에서 무작위로 하나의 국면 샘플링</li>
<li>최종 승패를 레이블로 사용</li>
<li>MSE 손실 함수 사용</li>
</ul>
<div class="language-python codeBlockContainer_WV0T theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_GRu9"><pre tabindex="0" class="prism-code language-python codeBlock_md1K thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_L4YE"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Value Network 훈련</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">value_prediction </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> value_network</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">position</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">value_prediction </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> game_outcome</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">**</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><br></span></code></pre></div></div>
<p><strong>왜 각 대국에서 하나만 샘플링하는가?</strong></p>
<p>여러 샘플을 취하면 같은 대국의 인접 국면이 고도로 상관되어 과적합을 초래합니다. 무작위 샘플링이 훈련 데이터의 다양성을 보장합니다.</p>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="몬테카를로-트리-탐색mcts">몬테카를로 트리 탐색(MCTS)<a href="#몬테카를로-트리-탐색mcts" class="hash-link" aria-label="몬테카를로 트리 탐색(MCTS)에 대한 직접 링크" title="몬테카를로 트리 탐색(MCTS)에 대한 직접 링크">​</a></h2>
<p>MCTS는 AlphaGo의 의사결정 핵심으로, 신경망과 결합하여 최선수를 효율적으로 탐색합니다.</p>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="mcts-4단계">MCTS 4단계<a href="#mcts-4단계" class="hash-link" aria-label="MCTS 4단계에 대한 직접 링크" title="MCTS 4단계에 대한 직접 링크">​</a></h3>
<!-- -->
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="선택-공식puct">선택 공식(PUCT)<a href="#선택-공식puct" class="hash-link" aria-label="선택 공식(PUCT)에 대한 직접 링크" title="선택 공식(PUCT)에 대한 직접 링크">​</a></h3>
<p>AlphaGo는 PUCT(Predictor + UCT) 공식으로 탐색할 분기를 선택합니다:</p>
<div class="language-text codeBlockContainer_WV0T theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_GRu9"><pre tabindex="0" class="prism-code language-text codeBlock_md1K thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_L4YE"><span class="token-line" style="color:#393A34"><span class="token plain">a = argmax[Q(s,a) + u(s,a)]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">u(s,a) = c_puct * P(s,a) * sqrt(N(s)) / (1 + N(s,a))</span><br></span></code></pre></div></div>
<p>여기서:</p>
<ul>
<li><strong>Q(s,a)</strong>: 행동 a의 평균 가치(exploitation)</li>
<li><strong>P(s,a)</strong>: Policy Network가 예측한 사전 확률</li>
<li><strong>N(s)</strong>: 부모 노드의 방문 횟수</li>
<li><strong>N(s,a)</strong>: 해당 행동의 방문 횟수</li>
<li><strong>c_puct</strong>: 탐색 상수, exploration과 exploitation 균형</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="탐색-과정-상세">탐색 과정 상세<a href="#탐색-과정-상세" class="hash-link" aria-label="탐색 과정 상세에 대한 직접 링크" title="탐색 과정 상세에 대한 직접 링크">​</a></h3>
<ol>
<li><strong>Selection</strong>: 루트 노드에서 시작하여 PUCT 공식으로 행동 선택, 리프 노드까지</li>
<li><strong>Expansion</strong>: 리프 노드에서 새 자식 노드 확장, Policy Network로 사전 확률 초기화</li>
<li><strong>Evaluation</strong>: Value Network 평가와 빠른 rollout 시뮬레이션 결합하여 가치 평가</li>
<li><strong>Backpropagation</strong>: 평가값을 경로를 따라 역전파하여 Q값과 N값 업데이트</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="rollout빠른-착수">Rollout(빠른 착수)<a href="#rollout빠른-착수" class="hash-link" aria-label="Rollout(빠른 착수)에 대한 직접 링크" title="Rollout(빠른 착수)에 대한 직접 링크">​</a></h3>
<p>AlphaGo(Zero가 아닌 버전)는 작은 빠른 정책 네트워크로 시뮬레이션도 수행합니다:</p>
<div class="language-text codeBlockContainer_WV0T theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_GRu9"><pre tabindex="0" class="prism-code language-text codeBlock_md1K thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_L4YE"><span class="token-line" style="color:#393A34"><span class="token plain">리프 노드 → 종국까지 빠른 무작위 착수 → 승패 계산</span><br></span></code></pre></div></div>
<p>최종 평가값은 Value Network와 Rollout 결합:</p>
<div class="language-text codeBlockContainer_WV0T theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_GRu9"><pre tabindex="0" class="prism-code language-text codeBlock_md1K thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_L4YE"><span class="token-line" style="color:#393A34"><span class="token plain">V = λ * v_network + (1-λ) * v_rollout</span><br></span></code></pre></div></div>
<p>AlphaGo는 λ = 0.5를 사용하여 둘에게 동일한 가중치를 부여합니다.</p>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="self-play-훈련-방법">Self-play 훈련 방법<a href="#self-play-훈련-방법" class="hash-link" aria-label="Self-play 훈련 방법에 대한 직접 링크" title="Self-play 훈련 방법에 대한 직접 링크">​</a></h2>
<p>Self-play는 AlphaGo의 핵심 훈련 전략으로, AI가 자가대국을 통해 지속적으로 향상됩니다.</p>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="훈련-순환">훈련 순환<a href="#훈련-순환" class="hash-link" aria-label="훈련 순환에 대한 직접 링크" title="훈련 순환에 대한 직접 링크">​</a></h3>
<!-- -->
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="self-play가-효과적인-이유">Self-play가 효과적인 이유<a href="#self-play가-효과적인-이유" class="hash-link" aria-label="Self-play가 효과적인 이유에 대한 직접 링크" title="Self-play가 효과적인 이유에 대한 직접 링크">​</a></h3>
<ol>
<li><strong>무한 데이터</strong>: 인간 기보 수에 제한받지 않음</li>
<li><strong>적응적 난이도</strong>: 상대 강도가 자신과 동시에 향상</li>
<li><strong>탐색 혁신</strong>: 인간의 고정된 사고 패턴에 제한받지 않음</li>
<li><strong>명확한 목표</strong>: 승률 직접 최적화, 인간 모방 아님</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="alphago-zero의-개선">AlphaGo Zero의 개선<a href="#alphago-zero의-개선" class="hash-link" aria-label="AlphaGo Zero의 개선에 대한 직접 링크" title="AlphaGo Zero의 개선에 대한 직접 링크">​</a></h2>
<p>2017년 발표된 AlphaGo Zero는 혁명적인 개선을 가져왔습니다:</p>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="주요-차이점">주요 차이점<a href="#주요-차이점" class="hash-link" aria-label="주요 차이점에 대한 직접 링크" title="주요 차이점에 대한 직접 링크">​</a></h3>
<table><thead><tr><th>특성</th><th>AlphaGo</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>초기 훈련</td><td>인간 기보 지도학습</td><td>완전히 처음부터</td></tr><tr><td>네트워크 아키텍처</td><td>분리된 Policy/Value</td><td>단일 듀얼헤드 네트워크</td></tr><tr><td>네트워크 구조</td><td>일반 CNN</td><td>ResNet</td></tr><tr><td>특징 공학</td><td>48개 수작업 특징</td><td>17개 단순 특징</td></tr><tr><td>Rollout</td><td>필요</td><td>불필요</td></tr><tr><td>훈련 시간</td><td>수개월</td><td>3일만에 인간 초월</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="아키텍처-단순화">아키텍처 단순화<a href="#아키텍처-단순화" class="hash-link" aria-label="아키텍처 단순화에 대한 직접 링크" title="아키텍처 단순화에 대한 직접 링크">​</a></h3>
<!-- -->
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="단순화된-입력-특징">단순화된 입력 특징<a href="#단순화된-입력-특징" class="hash-link" aria-label="단순화된 입력 특징에 대한 직접 링크" title="단순화된 입력 특징에 대한 직접 링크">​</a></h3>
<p>AlphaGo Zero는 17개의 특징 평면만 사용합니다:</p>
<ul>
<li>8개 평면: 자신의 최근 8수 돌 위치</li>
<li>8개 평면: 상대의 최근 8수 돌 위치</li>
<li>1개 평면: 현재 차례(전부 0 또는 전부 1)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="훈련-개선">훈련 개선<a href="#훈련-개선" class="hash-link" aria-label="훈련 개선에 대한 직접 링크" title="훈련 개선에 대한 직접 링크">​</a></h3>
<ol>
<li><strong>순수 Self-play</strong>: 인간 데이터 사용 안 함</li>
<li><strong>MCTS 확률을 직접 훈련 목표로 사용</strong>: 이진 승패 아님</li>
<li><strong>Rollout 없음</strong>: Value Network에 완전 의존</li>
<li><strong>단일 네트워크 훈련</strong>: Policy와 Value가 파라미터 공유, 상호 강화</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="alphazero의-범용화">AlphaZero의 범용화<a href="#alphazero의-범용화" class="hash-link" aria-label="AlphaZero의 범용화에 대한 직접 링크" title="AlphaZero의 범용화에 대한 직접 링크">​</a></h2>
<p>2017년 말 발표된 AlphaZero는 동일 아키텍처를 바둑, 체스, 장기에 적용했습니다:</p>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="핵심-특징">핵심 특징<a href="#핵심-특징" class="hash-link" aria-label="핵심 특징에 대한 직접 링크" title="핵심 특징에 대한 직접 링크">​</a></h3>
<ul>
<li><strong>제로 도메인 지식</strong>: 게임 규칙 외에 도메인 특정 지식 사용 안 함</li>
<li><strong>통일 아키텍처</strong>: 동일 알고리즘이 다른 게임에 적용</li>
<li><strong>더 빠른 훈련</strong>:<!-- -->
<ul>
<li>바둑: 8시간만에 AlphaGo Lee 초월</li>
<li>체스: 4시간만에 Stockfish 초월</li>
<li>장기: 2시간만에 Elmo 초월</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="alphago-zero와의-차이">AlphaGo Zero와의 차이<a href="#alphago-zero와의-차이" class="hash-link" aria-label="AlphaGo Zero와의 차이에 대한 직접 링크" title="AlphaGo Zero와의 차이에 대한 직접 링크">​</a></h3>
<table><thead><tr><th>특성</th><th>AlphaGo Zero</th><th>AlphaZero</th></tr></thead><tbody><tr><td>대상 게임</td><td>바둑만</td><td>바둑, 체스, 장기</td></tr><tr><td>대칭성 활용</td><td>바둑 8중 대칭 활용</td><td>대칭성 가정 안 함</td></tr><tr><td>하이퍼파라미터 조정</td><td>바둑에 최적화</td><td>범용 설정</td></tr><tr><td>훈련 방식</td><td>최적 모델 자가대국</td><td>최신 모델 자가대국</td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="구현-핵심">구현 핵심<a href="#구현-핵심" class="hash-link" aria-label="구현 핵심에 대한 직접 링크" title="구현 핵심에 대한 직접 링크">​</a></h2>
<p>유사한 시스템을 구현하고 싶다면 다음이 핵심 고려사항입니다:</p>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="계산-자원">계산 자원<a href="#계산-자원" class="hash-link" aria-label="계산 자원에 대한 직접 링크" title="계산 자원에 대한 직접 링크">​</a></h3>
<p>AlphaGo 훈련에는 방대한 계산 자원이 필요합니다:</p>
<ul>
<li><strong>AlphaGo Lee</strong>: 176 GPU + 48 TPU</li>
<li><strong>AlphaGo Zero</strong>: 4 TPU(훈련) + 1 TPU(자가대국)</li>
<li><strong>AlphaZero</strong>: 5000 TPU(훈련)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="핵심-하이퍼파라미터">핵심 하이퍼파라미터<a href="#핵심-하이퍼파라미터" class="hash-link" aria-label="핵심 하이퍼파라미터에 대한 직접 링크" title="핵심 하이퍼파라미터에 대한 직접 링크">​</a></h3>
<div class="language-python codeBlockContainer_WV0T theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_GRu9"><pre tabindex="0" class="prism-code language-python codeBlock_md1K thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_L4YE"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># MCTS 관련</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">num_simulations </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">800</span><span class="token plain">     </span><span class="token comment" style="color:#999988;font-style:italic"># 매 수 탐색 시뮬레이션 횟수</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">c_puct </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.5</span><span class="token plain">              </span><span class="token comment" style="color:#999988;font-style:italic"># 탐색 상수</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">temperature </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.0</span><span class="token plain">         </span><span class="token comment" style="color:#999988;font-style:italic"># 행동 선택 온도 파라미터</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 훈련 관련</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">batch_size </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2048</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">learning_rate </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.01</span><span class="token plain">      </span><span class="token comment" style="color:#999988;font-style:italic"># 감쇠 포함</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">l2_regularization </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1e-4</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_BVHq" id="흔한-문제">흔한 문제<a href="#흔한-문제" class="hash-link" aria-label="흔한 문제에 대한 직접 링크" title="흔한 문제에 대한 직접 링크">​</a></h3>
<ol>
<li><strong>훈련 불안정</strong>: 작은 학습률 사용, batch size 증가</li>
<li><strong>과적합</strong>: 훈련 데이터 다양성 확보, 정규화 사용</li>
<li><strong>탐색 효율</strong>: GPU 배치 추론 최적화, MCTS 병렬화</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_BVHq" id="추가-읽기">추가 읽기<a href="#추가-읽기" class="hash-link" aria-label="추가 읽기에 대한 직접 링크" title="추가 읽기에 대한 직접 링크">​</a></h2>
<ul>
<li><a href="https://www.nature.com/articles/nature16961" target="_blank" rel="noopener noreferrer">원본 논문: Mastering the game of Go with deep neural networks and tree search</a></li>
<li><a href="https://www.nature.com/articles/nature24270" target="_blank" rel="noopener noreferrer">AlphaGo Zero 논문: Mastering the game of Go without human knowledge</a></li>
<li><a href="https://www.science.org/doi/10.1126/science.aar6404" target="_blank" rel="noopener noreferrer">AlphaZero 논문: A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</a></li>
</ul>
<p>AlphaGo의 기술을 이해했다면, 이제 <a href="/ko/docs/for-engineers/background-info/katago-paper/">KataGo가 이 기반에서 어떤 개선을 했는지</a> 살펴봅시다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/background-info/alphago.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_O1mQ" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>페이지 편집</a></div><div class="col lastUpdated_hyfO"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="문서 페이지"><a class="pagination-nav__link pagination-nav__link--prev" href="/ko/docs/for-engineers/background-info/"><div class="pagination-nav__sublabel">이전</div><div class="pagination-nav__label">배경 지식</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ko/docs/for-engineers/background-info/katago-paper/"><div class="pagination-nav__sublabel">다음</div><div class="pagination-nav__label">KataGo 논문 해독</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_tqik thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#alphago의-역사적-의의" class="table-of-contents__link toc-highlight">AlphaGo의 역사적 의의</a><ul><li><a href="#이정표-사건" class="table-of-contents__link toc-highlight">이정표 사건</a></li></ul></li><li><a href="#핵심-기술-아키텍처" class="table-of-contents__link toc-highlight">핵심 기술 아키텍처</a><ul><li><a href="#policy-network정책-네트워크" class="table-of-contents__link toc-highlight">Policy Network(정책 네트워크)</a></li><li><a href="#value-network가치-네트워크" class="table-of-contents__link toc-highlight">Value Network(가치 네트워크)</a></li></ul></li><li><a href="#몬테카를로-트리-탐색mcts" class="table-of-contents__link toc-highlight">몬테카를로 트리 탐색(MCTS)</a><ul><li><a href="#mcts-4단계" class="table-of-contents__link toc-highlight">MCTS 4단계</a></li><li><a href="#선택-공식puct" class="table-of-contents__link toc-highlight">선택 공식(PUCT)</a></li><li><a href="#탐색-과정-상세" class="table-of-contents__link toc-highlight">탐색 과정 상세</a></li><li><a href="#rollout빠른-착수" class="table-of-contents__link toc-highlight">Rollout(빠른 착수)</a></li></ul></li><li><a href="#self-play-훈련-방법" class="table-of-contents__link toc-highlight">Self-play 훈련 방법</a><ul><li><a href="#훈련-순환" class="table-of-contents__link toc-highlight">훈련 순환</a></li><li><a href="#self-play가-효과적인-이유" class="table-of-contents__link toc-highlight">Self-play가 효과적인 이유</a></li></ul></li><li><a href="#alphago-zero의-개선" class="table-of-contents__link toc-highlight">AlphaGo Zero의 개선</a><ul><li><a href="#주요-차이점" class="table-of-contents__link toc-highlight">주요 차이점</a></li><li><a href="#아키텍처-단순화" class="table-of-contents__link toc-highlight">아키텍처 단순화</a></li><li><a href="#단순화된-입력-특징" class="table-of-contents__link toc-highlight">단순화된 입력 특징</a></li><li><a href="#훈련-개선" class="table-of-contents__link toc-highlight">훈련 개선</a></li></ul></li><li><a href="#alphazero의-범용화" class="table-of-contents__link toc-highlight">AlphaZero의 범용화</a><ul><li><a href="#핵심-특징" class="table-of-contents__link toc-highlight">핵심 특징</a></li><li><a href="#alphago-zero와의-차이" class="table-of-contents__link toc-highlight">AlphaGo Zero와의 차이</a></li></ul></li><li><a href="#구현-핵심" class="table-of-contents__link toc-highlight">구현 핵심</a><ul><li><a href="#계산-자원" class="table-of-contents__link toc-highlight">계산 자원</a></li><li><a href="#핵심-하이퍼파라미터" class="table-of-contents__link toc-highlight">핵심 하이퍼파라미터</a></li><li><a href="#흔한-문제" class="table-of-contents__link toc-highlight">흔한 문제</a></li></ul></li><li><a href="#추가-읽기" class="table-of-contents__link toc-highlight">추가 읽기</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>