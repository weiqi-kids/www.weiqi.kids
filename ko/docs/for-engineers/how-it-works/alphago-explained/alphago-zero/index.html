<!doctype html>
<html lang="ko" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-for-engineers/how-it-works/alphago-explained/alphago-zero" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">AlphaGo Zero 개요 | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/ko/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/ko/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/ko/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><meta data-rh="true" property="og:locale" content="ko"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="ko"><meta data-rh="true" name="docsearch:language" content="ko"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="AlphaGo Zero 개요 | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="처음부터 완전히 자기 학습하는 AlphaGo Zero가 인간 기보 없이 어떻게 모든 이전 버전을 초월했는가"><meta data-rh="true" property="og:description" content="처음부터 완전히 자기 학습하는 AlphaGo Zero가 인간 기보 없이 어떻게 모든 이전 버전을 초월했는가"><meta data-rh="true" name="keywords" content="AlphaGo Zero,자기 대국,강화 학습,딥러닝,바둑 AI,비지도 학습"><link data-rh="true" rel="icon" href="/ko/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/ko/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"給工程師的圍棋 AI 指南","item":"https://www.weiqi.kids/ko/docs/for-engineers/"},{"@type":"ListItem","position":2,"name":"一篇文章搞懂圍棋 AI","item":"https://www.weiqi.kids/ko/docs/for-engineers/how-it-works/"},{"@type":"ListItem","position":3,"name":"AlphaGo 完整解析","item":"https://www.weiqi.kids/ko/docs/for-engineers/how-it-works/alphago-explained/"},{"@type":"ListItem","position":4,"name":"AlphaGo Zero 개요","item":"https://www.weiqi.kids/ko/docs/for-engineers/how-it-works/alphago-explained/alphago-zero"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/ko/assets/css/styles.f23bf74b.css">
<script src="/ko/assets/js/runtime~main.b152227b.js" defer="defer"></script>
<script src="/ko/assets/js/main.7cf510df.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ko/img/logo.svg"><div role="region" aria-label="본문으로 건너뛰기"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">본문으로 건너뛰기</a></div><nav aria-label="메인" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="사이드바 펼치거나 접기" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ko/"><div class="navbar__logo"><img src="/ko/img/logo.svg" alt="호기보보협회 로고" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/ko/img/logo.svg" alt="호기보보협회 로고" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">바둑 키즈</b></a><a class="navbar__item navbar__link" href="/ko/docs/for-players/">바둑 플레이어</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ko/docs/for-engineers/">엔지니어</a><a class="navbar__item navbar__link" href="/ko/docs/about/">협회 소개</a><a class="navbar__item navbar__link" href="/ko/docs/activities/">활동 실적</a><a class="navbar__item navbar__link" href="/ko/docs/references/">참고 자료</a><a class="navbar__item navbar__link" href="/ko/docs/sop/">표준 운영 절차</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>한국어</a><ul class="dropdown__menu"><li><a href="/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="ko">한국어</a></li><li><a href="/es/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="맨 위로 스크롤하기" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="문서 사이드바" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ko/docs/intro/"><span title="이용 안내" class="linkLabel_REp1">이용 안내</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/ko/docs/about/"><span title="關於協會" class="categoryLinkLabel_ezQx">關於協會</span></a><button aria-label="사이드바 분류 &#x27;關於協會&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/ko/docs/activities/"><span title="活動實績" class="categoryLinkLabel_ezQx">活動實績</span></a><button aria-label="사이드바 분류 &#x27;活動實績&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/ko/docs/for-players/"><span title="바둑 기사를 위한 자료" class="categoryLinkLabel_ezQx">바둑 기사를 위한 자료</span></a><button aria-label="사이드바 분류 &#x27;바둑 기사를 위한 자료&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/ko/docs/references/"><span title="參考資料" class="categoryLinkLabel_ezQx">參考資料</span></a><button aria-label="사이드바 분류 &#x27;參考資料&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/ko/docs/sop/"><span title="標準作業流程" class="categoryLinkLabel_ezQx">標準作業流程</span></a><button aria-label="사이드바 분류 &#x27;標準作業流程&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/ko/docs/for-engineers/"><span title="給工程師的圍棋 AI 指南" class="categoryLinkLabel_ezQx">給工程師的圍棋 AI 指南</span></a><button aria-label="사이드바 분류 &#x27;給工程師的圍棋 AI 指南&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/ko/docs/for-engineers/deep-dive/"><span title="심층 연구를 원하는 분들을 위해" class="categoryLinkLabel_ezQx">심층 연구를 원하는 분들을 위해</span></a><button aria-label="사이드바 분류 &#x27;심층 연구를 원하는 분들을 위해&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/ko/docs/for-engineers/hands-on/"><span title="30 分鐘跑起第一個圍棋 AI" class="categoryLinkLabel_ezQx">30 分鐘跑起第一個圍棋 AI</span></a><button aria-label="사이드바 분류 &#x27;30 分鐘跑起第一個圍棋 AI&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ko/docs/for-engineers/how-it-works/"><span title="一篇文章搞懂圍棋 AI" class="categoryLinkLabel_ezQx">一篇文章搞懂圍棋 AI</span></a><button aria-label="사이드바 분류 &#x27;一篇文章搞懂圍棋 AI&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/"><span title="AlphaGo 完整解析" class="categoryLinkLabel_ezQx">AlphaGo 完整解析</span></a><button aria-label="사이드바 분류 &#x27;AlphaGo 完整解析&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/birth-of-alphago/"><span title="AlphaGo의 탄생" class="linkLabel_REp1">AlphaGo의 탄생</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/key-matches/"><span title="주요 대국 리뷰" class="linkLabel_REp1">주요 대국 리뷰</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/move-37/"><span title="&quot;신의 한 수&quot; 심층 분석" class="linkLabel_REp1">&quot;신의 한 수&quot; 심층 분석</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/why-go-is-hard/"><span title="바둑은 왜 어려운가?" class="linkLabel_REp1">바둑은 왜 어려운가?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/traditional-limits/"><span title="전통적 방법의 한계" class="linkLabel_REp1">전통적 방법의 한계</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/board-representation/"><span title="바둑판 상태 표현" class="linkLabel_REp1">바둑판 상태 표현</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/policy-network/"><span title="Policy Network 상세 해설" class="linkLabel_REp1">Policy Network 상세 해설</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/value-network/"><span title="Value Network 상세 해설" class="linkLabel_REp1">Value Network 상세 해설</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/input-features/"><span title="입력 특성 설계" class="linkLabel_REp1">입력 특성 설계</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/cnn-and-go/"><span title="CNN과 바둑의 결합" class="linkLabel_REp1">CNN과 바둑의 결합</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/supervised-learning/"><span title="지도 학습 단계" class="linkLabel_REp1">지도 학습 단계</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/"><span title="강화 학습 입문" class="linkLabel_REp1">강화 학습 입문</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/self-play/"><span title="자기 대국" class="linkLabel_REp1">자기 대국</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/"><span title="MCTS와 신경망의 결합" class="linkLabel_REp1">MCTS와 신경망의 결합</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/puct-formula/"><span title="PUCT 공식 상세" class="linkLabel_REp1">PUCT 공식 상세</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><span title="AlphaGo Zero 개요" class="linkLabel_REp1">AlphaGo Zero 개요</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/"><span title="이중 헤드 네트워크와 잔차 네트워크" class="linkLabel_REp1">이중 헤드 네트워크와 잔차 네트워크</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/training-from-scratch/"><span title="처음부터 훈련하는 과정" class="linkLabel_REp1">처음부터 훈련하는 과정</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/distributed-systems/"><span title="분산 시스템과 TPU" class="linkLabel_REp1">분산 시스템과 TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/alphago-explained/legacy-and-impact/"><span title="AlphaGo의 유산" class="linkLabel_REp1">AlphaGo의 유산</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/katago-innovations/"><span title="KataGo 的關鍵創新" class="linkLabel_REp1">KataGo 的關鍵創新</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ko/docs/for-engineers/how-it-works/concepts/"><span title="概念速查表" class="linkLabel_REp1">概念速查表</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/ko/docs/for-engineers/industry/"><span title="圍棋 AI 產業現況" class="categoryLinkLabel_ezQx">圍棋 AI 產業現況</span></a><button aria-label="사이드바 분류 &#x27;圍棋 AI 產業現況&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/ko/docs/for-engineers/overview/"><span title="圍棋 AI 能做什麼？" class="categoryLinkLabel_ezQx">圍棋 AI 能做什麼？</span></a><button aria-label="사이드바 분류 &#x27;圍棋 AI 能做什麼？&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="탐색 경로"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="홈" class="breadcrumbs__link" href="/ko/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ko/docs/for-engineers/"><span>給工程師的圍棋 AI 指南</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ko/docs/for-engineers/how-it-works/"><span>一篇文章搞懂圍棋 AI</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ko/docs/for-engineers/how-it-works/alphago-explained/"><span>AlphaGo 完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">AlphaGo Zero 개요</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">이 페이지에서</button></div><div class="theme-doc-markdown markdown"><header><h1>AlphaGo Zero 개요</h1></header>
<p>2017년 10월, DeepMind는 AI 분야를 충격에 빠뜨린 성과를 발표했습니다: <strong>AlphaGo Zero</strong>는 어떤 인간 기보도 사용하지 않고 완전히 무작위 상태에서 훈련을 시작하여, 단 3일 만에 이세돌을 이긴 원본 AlphaGo를 초월하고, <strong>100:0</strong>의 점수로 완승했습니다.</p>
<p>이것은 단순한 숫자상의 진보가 아닙니다. 이것은 완전히 새로운 패러다임을 대표합니다: <strong>AI는 인간 지식 없이도 처음부터 모든 것을 발견할 수 있습니다</strong>.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="왜-인간-기보가-필요-없는가">왜 인간 기보가 필요 없는가?<a href="#왜-인간-기보가-필요-없는가" class="hash-link" aria-label="왜 인간 기보가 필요 없는가?에 대한 직접 링크" title="왜 인간 기보가 필요 없는가?에 대한 직접 링크" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="인간-기보의-한계">인간 기보의 한계<a href="#인간-기보의-한계" class="hash-link" aria-label="인간 기보의 한계에 대한 직접 링크" title="인간 기보의 한계에 대한 직접 링크" translate="no">​</a></h3>
<p>원본 AlphaGo의 훈련 과정은 두 단계로 나뉩니다:</p>
<ol>
<li class=""><strong>지도 학습</strong>: 3천만 국의 인간 기보로 Policy Network 훈련</li>
<li class=""><strong>강화 학습</strong>: 자기 대국을 통해 더욱 향상</li>
</ol>
<p>이 방법에는 몇 가지 근본적인 문제가 있습니다:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-인간-기보에는-상한이-있다">1. 인간 기보에는 상한이 있다<a href="#1-인간-기보에는-상한이-있다" class="hash-link" aria-label="1. 인간 기보에는 상한이 있다에 대한 직접 링크" title="1. 인간 기보에는 상한이 있다에 대한 직접 링크" translate="no">​</a></h4>
<p>인간 기사의 기력에는 극한이 있으며, 기보에는 인간의 이해뿐만 아니라 인간의 실수와 편견도 포함됩니다. AI가 인간 기보에서 배울 때, 배우는 것은:</p>
<ul>
<li class="">인간이 좋다고 생각하는 수법(최적이 아닐 수 있음)</li>
<li class="">인간의 사고 방식(혁신을 제한할 수 있음)</li>
<li class="">인간의 실수(올바른 샘플로 학습됨)</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-지도-학습의-병목">2. 지도 학습의 병목<a href="#2-지도-학습의-병목" class="hash-link" aria-label="2. 지도 학습의 병목에 대한 직접 링크" title="2. 지도 학습의 병목에 대한 직접 링크" translate="no">​</a></h4>
<p>지도 학습의 목표는 &#x27;인간 모방&#x27;—인간 기사가 어디에 둘지 예측하는 것입니다. 이는 AI의 능력 상한이 인간 기사의 능력에 의해 제한된다는 것을 의미합니다.</p>
<p>마치 도제가 스승만 모방할 수 있어서 영원히 스승을 초월할 수 없는 것과 같습니다.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-데이터-수집-비용">3. 데이터 수집 비용<a href="#3-데이터-수집-비용" class="hash-link" aria-label="3. 데이터 수집 비용에 대한 직접 링크" title="3. 데이터 수집 비용에 대한 직접 링크" translate="no">​</a></h4>
<p>고품질 인간 기보는 수년간의 축적이 필요하며, 바둑처럼 오랜 역사를 가진 게임에만 존재합니다. AI를 새로운 영역(예: 단백질 구조 예측)에 적용하려면 &#x27;인간 전문가 기보&#x27;가 전혀 없습니다.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="zero의-돌파구">Zero의 돌파구<a href="#zero의-돌파구" class="hash-link" aria-label="Zero의 돌파구에 대한 직접 링크" title="Zero의 돌파구에 대한 직접 링크" translate="no">​</a></h3>
<p>AlphaGo Zero는 지도 학습 단계를 완전히 건너뛰고 <strong>무작위 초기화</strong>에서 직접 자기 대국을 시작합니다. 이로써 위의 모든 문제가 해결됩니다:</p>
<table><thead><tr><th>문제</th><th>원본 AlphaGo</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>인간 지식 상한</td><td>기보 품질에 제한됨</td><td>제한 없음</td></tr><tr><td>학습 목표</td><td>인간 모방</td><td>승률 최대화</td></tr><tr><td>데이터 요구량</td><td>3천만 국의 기보</td><td>0</td></tr><tr><td>일반화 가능성</td><td>바둑에만 한정</td><td>다른 영역으로 확장 가능</td></tr></tbody></table>
<p>이것은 근본적인 패러다임 전환입니다: &#x27;인간 지식 학습&#x27;에서 &#x27;제1원리에서 지식 발견&#x27;으로.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="원본-alphago와의-대결-1000">원본 AlphaGo와의 대결: 100:0<a href="#원본-alphago와의-대결-1000" class="hash-link" aria-label="원본 AlphaGo와의 대결: 100:0에 대한 직접 링크" title="원본 AlphaGo와의 대결: 100:0에 대한 직접 링크" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="압도적-승리">압도적 승리<a href="#압도적-승리" class="hash-link" aria-label="압도적 승리에 대한 직접 링크" title="압도적 승리에 대한 직접 링크" translate="no">​</a></h3>
<p>DeepMind는 훈련을 마친 AlphaGo Zero를 각 버전의 AlphaGo와 대국시켰습니다:</p>
<table><thead><tr><th>상대</th><th>AlphaGo Zero 전적</th></tr></thead><tbody><tr><td>AlphaGo Fan(판후이를 이긴 버전)</td><td>100:0</td></tr><tr><td>AlphaGo Lee(이세돌을 이긴 버전)</td><td>100:0</td></tr><tr><td>AlphaGo Master(60연승 버전)</td><td>89:11</td></tr></tbody></table>
<p><strong>100:0</strong>—이것은 100판 경기에서 원본 AlphaGo가 한 판도 이기지 못했다는 것을 의미합니다.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="더-적은-자원으로-더-강한-기력">더 적은 자원으로 더 강한 기력<a href="#더-적은-자원으로-더-강한-기력" class="hash-link" aria-label="더 적은 자원으로 더 강한 기력에 대한 직접 링크" title="더 적은 자원으로 더 강한 기력에 대한 직접 링크" translate="no">​</a></h3>
<p>이기기만 한 것이 아니라, AlphaGo Zero는 더 적은 자원으로 더 강한 기력을 달성했습니다:</p>
<table><thead><tr><th>지표</th><th>AlphaGo Lee</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>훈련 시간</td><td>수개월</td><td>40일(3일 만에 AlphaGo Lee 초월)</td></tr><tr><td>훈련 대국 수</td><td>3천만 인간 기보 + 자기 대국</td><td>490만 국 자기 대국</td></tr><tr><td>TPU 수(훈련)</td><td>50+</td><td>4</td></tr><tr><td>TPU 수(추론)</td><td>48</td><td>4</td></tr><tr><td>입력 특징</td><td>48개 평면</td><td>17개 평면</td></tr><tr><td>신경망</td><td>SL + RL 이중 네트워크</td><td>단일 이중 헤드 네트워크</td></tr></tbody></table>
<p>이것은 놀라운 효율성 향상입니다: <strong>자원 10배 이상 감소, 기력은 대폭 향상</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="왜-zero가-더-강한가">왜 Zero가 더 강한가?<a href="#왜-zero가-더-강한가" class="hash-link" aria-label="왜 Zero가 더 강한가?에 대한 직접 링크" title="왜 Zero가 더 강한가?에 대한 직접 링크" translate="no">​</a></h3>
<p>AlphaGo Zero가 더 강한 이유는 여러 관점에서 이해할 수 있습니다:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-편견-없는-학습">1. 편견 없는 학습<a href="#1-편견-없는-학습" class="hash-link" aria-label="1. 편견 없는 학습에 대한 직접 링크" title="1. 편견 없는 학습에 대한 직접 링크" translate="no">​</a></h4>
<p>원본 AlphaGo는 인간 기보에서 학습하여 인간의 편견을 계승했습니다. 예를 들어, 인간 기사는 특정 정석을 과도하게 중시하거나 특정 국면에 대해 잘못된 평가를 할 수 있습니다.</p>
<p>AlphaGo Zero는 이러한 짐이 없습니다. 백지에서 시작하여 승패 결과만으로 좋은 수가 무엇인지 학습합니다. 이로써 인간이 생각하지 못한 수법을 발견할 수 있습니다.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-일관된-학습-목표">2. 일관된 학습 목표<a href="#2-일관된-학습-목표" class="hash-link" aria-label="2. 일관된 학습 목표에 대한 직접 링크" title="2. 일관된 학습 목표에 대한 직접 링크" translate="no">​</a></h4>
<p>원본 AlphaGo의 훈련에는 두 가지 다른 목표가 있습니다:</p>
<ul>
<li class="">지도 학습: 인간 착점 예측 정확도 최대화</li>
<li class="">강화 학습: 승률 최대화</li>
</ul>
<p>이 두 목표는 서로 충돌할 수 있습니다. AlphaGo Zero는 단 하나의 목표만 있습니다: <strong>승률 최대화</strong>. 이로써 학습 과정이 더 일관되고 효과적입니다.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-더-간결한-아키텍처">3. 더 간결한 아키텍처<a href="#3-더-간결한-아키텍처" class="hash-link" aria-label="3. 더 간결한 아키텍처에 대한 직접 링크" title="3. 더 간결한 아키텍처에 대한 직접 링크" translate="no">​</a></h4>
<p>원본 AlphaGo는 분리된 Policy Network와 Value Network를 사용합니다. AlphaGo Zero는 단일 이중 헤드 네트워크를 사용하여(자세한 내용은 <a class="" href="/ko/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/">다음 글: 이중 헤드 네트워크와 잔차 네트워크</a> 참조) 특징 표현이 공유되어 학습 효율이 향상됩니다.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="단순화된-입력-특징-48에서-17로">단순화된 입력 특징: 48에서 17로<a href="#단순화된-입력-특징-48에서-17로" class="hash-link" aria-label="단순화된 입력 특징: 48에서 17로에 대한 직접 링크" title="단순화된 입력 특징: 48에서 17로에 대한 직접 링크" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="원본-alphago의-48개-특징-평면">원본 AlphaGo의 48개 특징 평면<a href="#원본-alphago의-48개-특징-평면" class="hash-link" aria-label="원본 AlphaGo의 48개 특징 평면에 대한 직접 링크" title="원본 AlphaGo의 48개 특징 평면에 대한 직접 링크" translate="no">​</a></h3>
<p>원본 AlphaGo의 신경망 입력은 48개의 19x19 특징 평면을 포함하며, 인간이 설계한 많은 특징을 인코딩합니다:</p>
<table><thead><tr><th>카테고리</th><th>특징 수</th><th>내용</th></tr></thead><tbody><tr><td>돌 위치</td><td>3</td><td>흑돌, 백돌, 빈 점</td></tr><tr><td>활로</td><td>8</td><td>1-8 활로의 돌 연결</td></tr><tr><td>따냄</td><td>8</td><td>1-8개 돌 따낼 수 있음</td></tr><tr><td>패</td><td>1</td><td>패싸움 위치</td></tr><tr><td>변까지 거리</td><td>4</td><td>1선에서 4선</td></tr><tr><td>착점 합법성</td><td>1</td><td>어떤 위치에 둘 수 있는지</td></tr><tr><td>히스토리</td><td>8</td><td>과거 8수의 위치</td></tr><tr><td>차례</td><td>1</td><td>흑 또는 백</td></tr><tr><td>기타</td><td>14</td><td>축, 눈 등</td></tr></tbody></table>
<p>이 48개 특징은 바둑 전문가가 정성껏 설계한 것으로, 많은 도메인 지식을 포함합니다.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero의-17개-특징-평면">AlphaGo Zero의 17개 특징 평면<a href="#alphago-zero의-17개-특징-평면" class="hash-link" aria-label="AlphaGo Zero의 17개 특징 평면에 대한 직접 링크" title="AlphaGo Zero의 17개 특징 평면에 대한 직접 링크" translate="no">​</a></h3>
<p>AlphaGo Zero는 입력을 대폭 단순화하여 17개 특징 평면만 사용합니다:</p>
<table><thead><tr><th>평면 번호</th><th>내용</th><th>수량</th></tr></thead><tbody><tr><td>1-8</td><td>흑돌 위치(최근 8수)</td><td>8</td></tr><tr><td>9-16</td><td>백돌 위치(최근 8수)</td><td>8</td></tr><tr><td>17</td><td>현재 차례(전부 1 또는 전부 0)</td><td>1</td></tr></tbody></table>
<p>이 17개 특징은 다음만 포함합니다:</p>
<ul>
<li class=""><strong>현재 바둑판 상태</strong>: 각 위치에 흑돌, 백돌 또는 빈 곳</li>
<li class=""><strong>히스토리 정보</strong>: 과거 8수의 바둑판 상태</li>
<li class=""><strong>차례 정보</strong>: 누구 차례인지</li>
</ul>
<p>활로 없음, 축 판단 없음, 변까지 거리 없음—이 모든 &#x27;바둑 지식&#x27;은 신경망이 스스로 학습하도록 합니다.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="왜-단순화가-좋은가">왜 단순화가 좋은가?<a href="#왜-단순화가-좋은가" class="hash-link" aria-label="왜 단순화가 좋은가?에 대한 직접 링크" title="왜 단순화가 좋은가?에 대한 직접 링크" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-네트워크가-스스로-특징-발견">1. 네트워크가 스스로 특징 발견<a href="#1-네트워크가-스스로-특징-발견" class="hash-link" aria-label="1. 네트워크가 스스로 특징 발견에 대한 직접 링크" title="1. 네트워크가 스스로 특징 발견에 대한 직접 링크" translate="no">​</a></h4>
<p>복잡한 수작업 특징은 중요한 정보를 놓치거나 잘못된 가정을 인코딩할 수 있습니다. 신경망이 원시 데이터에서 학습하면 더 나은 특징 표현을 발견할 수 있습니다.</p>
<p>사실 AlphaGo Zero는 인간이 설계한 모든 특징(활로, 축 등)을 학습했고, 인간이 명확히 인식하지 못한 일부 패턴도 학습했습니다.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-더-나은-일반화-가능성">2. 더 나은 일반화 가능성<a href="#2-더-나은-일반화-가능성" class="hash-link" aria-label="2. 더 나은 일반화 가능성에 대한 직접 링크" title="2. 더 나은 일반화 가능성에 대한 직접 링크" translate="no">​</a></h4>
<p>48개 특징 중 많은 것이 바둑 전용입니다(예: 축, 변까지 거리). 17개 단순화된 특징은 범용적입니다—모든 보드 게임을 유사한 방식으로 인코딩할 수 있습니다.</p>
<p>이것이 나중의 <strong>AlphaZero</strong>(범용 게임 AI)의 기초가 되었습니다.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-인위적-오류-감소">3. 인위적 오류 감소<a href="#3-인위적-오류-감소" class="hash-link" aria-label="3. 인위적 오류 감소에 대한 직접 링크" title="3. 인위적 오류 감소에 대한 직접 링크" translate="no">​</a></h4>
<p>수작업 설계 특징에는 오류나 불완전한 정의가 포함될 수 있습니다. 입력 단순화로 이러한 문제의 가능성이 제거됩니다.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="단일-네트워크-아키텍처">단일 네트워크 아키텍처<a href="#단일-네트워크-아키텍처" class="hash-link" aria-label="단일 네트워크 아키텍처에 대한 직접 링크" title="단일 네트워크 아키텍처에 대한 직접 링크" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="원본의-이중-네트워크-설계">원본의 이중 네트워크 설계<a href="#원본의-이중-네트워크-설계" class="hash-link" aria-label="원본의 이중 네트워크 설계에 대한 직접 링크" title="원본의 이중 네트워크 설계에 대한 직접 링크" translate="no">​</a></h3>
<p>원본 AlphaGo는 두 개의 독립적인 신경망을 사용합니다:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Policy Network:  입력 → CNN → 19x19 착점 확률</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Value Network:   입력 → CNN → 승률 평가(-1 ~ 1)</span><br></span></code></pre></div></div>
<p>이 두 네트워크는:</p>
<ul>
<li class="">다른 아키텍처(층 수, 채널 수 약간 다름)</li>
<li class="">독립적으로 훈련(먼저 Policy 훈련, 그 다음 Value 훈련)</li>
<li class="">어떤 매개변수도 공유하지 않음</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="zero의-이중-헤드-네트워크">Zero의 이중 헤드 네트워크<a href="#zero의-이중-헤드-네트워크" class="hash-link" aria-label="Zero의 이중 헤드 네트워크에 대한 직접 링크" title="Zero의 이중 헤드 네트워크에 대한 직접 링크" translate="no">​</a></h3>
<p>AlphaGo Zero는 단일 네트워크를 사용하지만 두 개의 출력 헤드(heads)가 있습니다:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">입력 → ResNet 공유 백본 → Policy Head → 19x19 착점 확률</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                       → Value Head  → 승률 평가</span><br></span></code></pre></div></div>
<p>두 Head는 동일한 ResNet 백본을 공유합니다(자세한 내용은 <a class="" href="/ko/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/">다음 글: 이중 헤드 네트워크와 잔차 네트워크</a> 참조). 이로 인한 장점:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-매개변수-효율성">1. 매개변수 효율성<a href="#1-매개변수-효율성" class="hash-link" aria-label="1. 매개변수 효율성에 대한 직접 링크" title="1. 매개변수 효율성에 대한 직접 링크" translate="no">​</a></h4>
<p>공유 백본은 대부분의 매개변수가 두 작업에 공통 사용됨을 의미합니다. 총 매개변수 양이 줄고 과적합 위험이 낮아집니다.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-특징-공유">2. 특징 공유<a href="#2-특징-공유" class="hash-link" aria-label="2. 특징 공유에 대한 직접 링크" title="2. 특징 공유에 대한 직접 링크" translate="no">​</a></h4>
<p>&quot;어디에 둬야 하는가&quot;(Policy)와 &quot;누가 이길 것인가&quot;(Value)는 유사한 바둑판 패턴 이해가 필요합니다. 공유 백본으로 이러한 특징이 두 작업에 동시에 학습되고 활용됩니다.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-훈련-안정성">3. 훈련 안정성<a href="#3-훈련-안정성" class="hash-link" aria-label="3. 훈련 안정성에 대한 직접 링크" title="3. 훈련 안정성에 대한 직접 링크" translate="no">​</a></h4>
<p>공동 훈련으로 그래디언트 신호가 두 소스에서 오므로 더 풍부한 감독 신호를 제공하여 훈련이 더 안정적입니다.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="잔차-네트워크의-위력">잔차 네트워크의 위력<a href="#잔차-네트워크의-위력" class="hash-link" aria-label="잔차 네트워크의 위력에 대한 직접 링크" title="잔차 네트워크의 위력에 대한 직접 링크" translate="no">​</a></h3>
<p>AlphaGo Zero의 백본은 **40층 잔차 네트워크(ResNet)**를 사용하며, 원본 AlphaGo의 13층 CNN보다 훨씬 깊습니다.</p>
<p>잔차 연결(skip connections)로 심층 네트워크가 효과적으로 훈련될 수 있으며, 그래디언트 소실 문제를 피합니다. 이것은 2015년 ImageNet 대회의 획기적 기술로, AlphaGo Zero에 바둑 영역에 성공적으로 적용되었습니다.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="훈련-효율성-향상">훈련 효율성 향상<a href="#훈련-효율성-향상" class="hash-link" aria-label="훈련 효율성 향상에 대한 직접 링크" title="훈련 효율성 향상에 대한 직접 링크" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="자기-대국의-지수적-성장">자기 대국의 지수적 성장<a href="#자기-대국의-지수적-성장" class="hash-link" aria-label="자기 대국의 지수적 성장에 대한 직접 링크" title="자기 대국의 지수적 성장에 대한 직접 링크" translate="no">​</a></h3>
<p>AlphaGo Zero의 훈련 과정은 놀라운 효율성을 보여줍니다:</p>
<table><thead><tr><th>훈련 시간</th><th>ELO 평점</th><th>해당 수준</th></tr></thead><tbody><tr><td>0시간</td><td>0</td><td>무작위 착점</td></tr><tr><td>3시간</td><td>~1000</td><td>기본 규칙 발견</td></tr><tr><td>12시간</td><td>~3000</td><td>정석 발견</td></tr><tr><td>36시간</td><td>~4500</td><td>판후이 버전 초월</td></tr><tr><td>60시간</td><td>~5200</td><td>이세돌 버전 초월</td></tr><tr><td>72시간</td><td>~5400</td><td>원본 AlphaGo 초월</td></tr><tr><td>40일</td><td>~5600</td><td>최강 버전</td></tr></tbody></table>
<p><strong>3일 만에 인간 초월, 3일 만에 수개월 훈련한 이전 AI 초월</strong>—이것은 지수적 효율성 향상입니다.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="왜-이렇게-빠른가">왜 이렇게 빠른가?<a href="#왜-이렇게-빠른가" class="hash-link" aria-label="왜 이렇게 빠른가?에 대한 직접 링크" title="왜 이렇게 빠른가?에 대한 직접 링크" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-더-강한-탐색-유도">1. 더 강한 탐색 유도<a href="#1-더-강한-탐색-유도" class="hash-link" aria-label="1. 더 강한 탐색 유도에 대한 직접 링크" title="1. 더 강한 탐색 유도에 대한 직접 링크" translate="no">​</a></h4>
<p>AlphaGo Zero의 MCTS는 완전히 신경망에 의해 유도되며, 더 이상 빠른 착점 전략(rollout)을 사용하지 않습니다. 이로써 탐색이 더 효율적이고 정확해집니다.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-더-빠른-자기-대국">2. 더 빠른 자기 대국<a href="#2-더-빠른-자기-대국" class="hash-link" aria-label="2. 더 빠른 자기 대국에 대한 직접 링크" title="2. 더 빠른 자기 대국에 대한 직접 링크" translate="no">​</a></h4>
<p>하나의 네트워크만 필요하므로(두 개가 아니라), 각 자기 대국의 계산 비용이 줄어듭니다. 같은 시간에 더 많은 훈련 데이터를 생성할 수 있습니다.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-더-효과적인-학습">3. 더 효과적인 학습<a href="#3-더-효과적인-학습" class="hash-link" aria-label="3. 더 효과적인 학습에 대한 직접 링크" title="3. 더 효과적인 학습에 대한 직접 링크" translate="no">​</a></h4>
<p>이중 헤드 네트워크의 공동 훈련으로 각 대국의 정보가 더 효과적으로 활용됩니다. Policy와 Value의 그래디언트가 서로 강화하여 수렴을 가속화합니다.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="인간-학습과의-비교">인간 학습과의 비교<a href="#인간-학습과의-비교" class="hash-link" aria-label="인간 학습과의 비교에 대한 직접 링크" title="인간 학습과의 비교에 대한 직접 링크" translate="no">​</a></h3>
<p>인간 기사가 다른 수준에 도달하는 데 얼마나 걸리는가?</p>
<table><thead><tr><th>수준</th><th>인간 소요 시간</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>입문</td><td>수주</td><td>몇 분</td></tr><tr><td>아마추어 초단</td><td>수년</td><td>몇 시간</td></tr><tr><td>프로 수준</td><td>10-20년</td><td>1-2일</td></tr><tr><td>세계 챔피언</td><td>20년+ 전업 투입</td><td>3일</td></tr><tr><td>인간 초월</td><td>불가능</td><td>3일</td></tr></tbody></table>
<p>이 비교는 인간 기사를 폄하하려는 것이 아닙니다—그들은 생물학적 뉴런을 사용하고, AlphaGo Zero는 전용 설계된 TPU와 수천 와트의 전력을 사용합니다. 하지만 올바른 학습 방법이 얼마나 효율적일 수 있는지 보여줍니다.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="범용성-체스-장기">범용성: 체스, 장기<a href="#범용성-체스-장기" class="hash-link" aria-label="범용성: 체스, 장기에 대한 직접 링크" title="범용성: 체스, 장기에 대한 직접 링크" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphazero의-탄생">AlphaZero의 탄생<a href="#alphazero의-탄생" class="hash-link" aria-label="AlphaZero의 탄생에 대한 직접 링크" title="AlphaZero의 탄생에 대한 직접 링크" translate="no">​</a></h3>
<p>2017년 12월, DeepMind는 <strong>AlphaZero</strong>—AlphaGo Zero의 범용 버전을 발표했습니다. 동일한 알고리즘으로 게임 규칙만 수정하면 세 가지 보드 게임에서 세계 최고 수준에 도달합니다:</p>
<table><thead><tr><th>게임</th><th>훈련 시간</th><th>상대</th><th>전적</th></tr></thead><tbody><tr><td>바둑</td><td>8시간</td><td>AlphaGo Zero</td><td>60:40</td></tr><tr><td>체스</td><td>4시간</td><td>Stockfish 8</td><td>28승 72무 0패</td></tr><tr><td>장기</td><td>2시간</td><td>Elmo</td><td>90:8:2</td></tr></tbody></table>
<p>상대에 주목하세요:</p>
<ul>
<li class=""><strong>Stockfish</strong>는 당시 가장 강력한 체스 엔진으로, 수십 년간의 인간 지식과 최적화 사용</li>
<li class=""><strong>Elmo</strong>는 당시 가장 강력한 장기 AI</li>
</ul>
<p>AlphaZero는 몇 시간의 훈련으로 수년간 개발된 이 전용 시스템들을 초월했습니다.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="범용성의-의미">범용성의 의미<a href="#범용성의-의미" class="hash-link" aria-label="범용성의 의미에 대한 직접 링크" title="범용성의 의미에 대한 직접 링크" translate="no">​</a></h3>
<p>AlphaGo Zero / AlphaZero는 중요한 것을 증명했습니다:</p>
<blockquote>
<p><strong>동일한 학습 알고리즘이 다른 영역에서 초인 수준에 도달할 수 있다.</strong></p>
</blockquote>
<p>이것은 세 개의 다른 AI가 아니라 하나의 범용 학습 프레임워크입니다:</p>
<ol>
<li class=""><strong>자기 대국</strong>이 경험 생성</li>
<li class=""><strong>몬테카를로 트리 탐색</strong>이 가능성 탐색</li>
<li class=""><strong>신경망</strong>이 정책과 가치 함수 학습</li>
<li class=""><strong>강화 학습</strong>이 목표 함수 최적화</li>
</ol>
<p>이 프레임워크는 도메인 특정 지식에 의존하지 않으며, AI의 범용화를 향한 중요한 단계입니다.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="전통적-ai에-대한-충격">전통적 AI에 대한 충격<a href="#전통적-ai에-대한-충격" class="hash-link" aria-label="전통적 AI에 대한 충격에 대한 직접 링크" title="전통적 AI에 대한 충격에 대한 직접 링크" translate="no">​</a></h3>
<p>AlphaZero 이전에 체스와 장기의 가장 강력한 AI는 &#x27;전문가 시스템&#x27; 스타일이었습니다:</p>
<ul>
<li class=""><strong>대량의 인간 지식</strong>: 오프닝 북, 엔드게임 북, 평가 함수</li>
<li class=""><strong>수십 년 최적화</strong>: 수많은 기사와 엔지니어의 노력</li>
<li class=""><strong>극도의 전문화</strong>: Stockfish는 바둑을 둘 수 없고, Elmo는 체스를 둘 수 없음</li>
</ul>
<p>AlphaZero는 하나의 범용 알고리즘으로 몇 시간 만에 이 모든 것을 초월했습니다. 이로 인해 많은 AI 연구자들이 다시 생각하게 되었습니다:</p>
<blockquote>
<p>&#x27;범용 학습 알고리즘&#x27;에 더 많은 노력을 투입해야 하는가, 아니면 &#x27;전문가 지식 인코딩&#x27;에?</p>
</blockquote>
<p>답은 점점 명확해지고 있습니다: 기계가 스스로 학습하게 하는 것이 지식을 가르치는 것보다 더 효과적입니다.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero의-바둑-스타일">AlphaGo Zero의 바둑 스타일<a href="#alphago-zero의-바둑-스타일" class="hash-link" aria-label="AlphaGo Zero의 바둑 스타일에 대한 직접 링크" title="AlphaGo Zero의 바둑 스타일에 대한 직접 링크" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="인간을-초월한-미학">인간을 초월한 미학<a href="#인간을-초월한-미학" class="hash-link" aria-label="인간을 초월한 미학에 대한 직접 링크" title="인간을 초월한 미학에 대한 직접 링크" translate="no">​</a></h3>
<p>바둑계는 AlphaGo Zero의 수법에 대해 보편적인 평가를 내렸습니다: <strong>더 아름답다</strong>.</p>
<p>AlphaGo Lee의 수법은 때때로 &#x27;이상해&#x27; 보였습니다—37번째 수처럼 인간은 사후 분석 후에야 그 묘미를 이해할 수 있었습니다. 하지만 AlphaGo Zero의 수법은 종종 사후에 &quot;한눈에 좋은 수라는 것을 알 수 있다&quot;고 평가됩니다.</p>
<p>이것은 아마:</p>
<ol>
<li class=""><strong>더 강한 기력</strong>: Zero는 더 깊이 볼 수 있어 착점이 더 여유로움</li>
<li class=""><strong>인간 편견 없음</strong>: 전통 정석에 구속받지 않음</li>
<li class=""><strong>일관된 목표</strong>: 오직 승률만 추구, 인간 모방 안 함</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="인간-바둑-이치-재발견">인간 바둑 이치 재발견<a href="#인간-바둑-이치-재발견" class="hash-link" aria-label="인간 바둑 이치 재발견에 대한 직접 링크" title="인간 바둑 이치 재발견에 대한 직접 링크" translate="no">​</a></h3>
<p>흥미롭게도 AlphaGo Zero는 훈련 과정에서 인간이 수천 년간 축적한 바둑 지식을 &#x27;재발견&#x27;했습니다:</p>
<ul>
<li class=""><strong>정석</strong>: Zero는 많은 일반적인 정석을 스스로 발견했습니다. 이것들이 실제로 쌍방의 최적해이기 때문</li>
<li class=""><strong>포석 원칙</strong>: 귀, 변, 중앙의 중요성 순서</li>
<li class=""><strong>모양 지식</strong>: 우형과 좋은 형의 구별</li>
</ul>
<p>이것은 인간 바둑 이치의 합리성을 검증합니다—이 지식은 우연이 아니라 바둑 본질의 반영입니다.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="인간을-초월한-혁신">인간을 초월한 혁신<a href="#인간을-초월한-혁신" class="hash-link" aria-label="인간을 초월한 혁신에 대한 직접 링크" title="인간을 초월한 혁신에 대한 직접 링크" translate="no">​</a></h3>
<p>하지만 Zero는 인간이 생각하지 못한 수법도 발견했습니다:</p>
<ul>
<li class=""><strong>비전통적 오프닝</strong>: 전통적 오프닝을 기반으로 한 변화</li>
<li class=""><strong>공격적인 버림</strong>: 인간보다 국소를 포기하고 전체 이익을 얻으려 함</li>
<li class=""><strong>반직관적 모양</strong>: 표면상 &#x27;나쁜 형&#x27;이 실제로 최적해</li>
</ul>
<p>이러한 혁신이 인간의 바둑 이해를 바꾸고 있습니다. 많은 프로 기사들은 AlphaGo Zero의 기보 연구가 바둑에 대한 완전히 새로운 인식을 주었다고 말합니다.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="기술적-세부사항-요약">기술적 세부사항 요약<a href="#기술적-세부사항-요약" class="hash-link" aria-label="기술적 세부사항 요약에 대한 직접 링크" title="기술적 세부사항 요약에 대한 직접 링크" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="원본-alphago와의-완전한-비교">원본 AlphaGo와의 완전한 비교<a href="#원본-alphago와의-완전한-비교" class="hash-link" aria-label="원본 AlphaGo와의 완전한 비교에 대한 직접 링크" title="원본 AlphaGo와의 완전한 비교에 대한 직접 링크" translate="no">​</a></h3>
<table><thead><tr><th>측면</th><th>AlphaGo(원본)</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td><strong>훈련 데이터</strong></td><td>인간 기보 + 자기 대국</td><td>순수 자기 대국</td></tr><tr><td><strong>학습 방법</strong></td><td>지도 학습 + 강화 학습</td><td>순수 강화 학습</td></tr><tr><td><strong>입력 특징</strong></td><td>48개 평면</td><td>17개 평면</td></tr><tr><td><strong>네트워크 아키텍처</strong></td><td>분리된 Policy/Value</td><td>이중 헤드 ResNet</td></tr><tr><td><strong>네트워크 깊이</strong></td><td>13층</td><td>40층(또는 그 이상)</td></tr><tr><td><strong>MCTS 평가</strong></td><td>신경망 + Rollout</td><td>순수 신경망</td></tr><tr><td><strong>탐색 횟수</strong></td><td>수당 ~100,000</td><td>수당 ~1,600</td></tr><tr><td><strong>훈련 TPU</strong></td><td>50+</td><td>4</td></tr><tr><td><strong>추론 TPU</strong></td><td>48</td><td>4(확장 가능)</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="핵심-알고리즘">핵심 알고리즘<a href="#핵심-알고리즘" class="hash-link" aria-label="핵심 알고리즘에 대한 직접 링크" title="핵심 알고리즘에 대한 직접 링크" translate="no">​</a></h3>
<p>AlphaGo Zero의 훈련 루프는 매우 간결합니다:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. 자기 대국</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 현재 네트워크로 MCTS 수행</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - MCTS 탐색 확률에 따라 착점 선택</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 각 수의 (국면, MCTS 확률, 승패 결과) 기록</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. 네트워크 훈련</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 경험 풀에서 샘플링</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Policy Head: MCTS 확률과의 크로스 엔트로피 최소화</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Value Head: 실제 승패와의 평균 제곱 오차 최소화</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 두 목표 공동 최적화</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. 네트워크 업데이트</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 새 네트워크로 구 네트워크 교체(대국으로 새 네트워크가 더 강함 검증)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 1단계로 돌아감</span><br></span></code></pre></div></div>
<p>이 루프가 지속적으로 실행되며 네트워크가 계속 강해집니다. 인간 데이터 없음, 인간 지식 없음, 오직 게임 규칙과 승패 목표만.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="ai-연구에-대한-시사점">AI 연구에 대한 시사점<a href="#ai-연구에-대한-시사점" class="hash-link" aria-label="AI 연구에 대한 시사점에 대한 직접 링크" title="AI 연구에 대한 시사점에 대한 직접 링크" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="제1원리-학습">제1원리 학습<a href="#제1원리-학습" class="hash-link" aria-label="제1원리 학습에 대한 직접 링크" title="제1원리 학습에 대한 직접 링크" translate="no">​</a></h3>
<p>AlphaGo Zero는 &#x27;제1원리&#x27; 학습 방법을 보여줍니다:</p>
<blockquote>
<p>AI에게 어떻게 해야 하는지 말하지 말고, 목표가 무엇인지만 말하고 스스로 방법을 발견하게 하라.</p>
</blockquote>
<p>이것은 전통적 전문가 시스템 방법과 선명한 대조를 이룹니다. 전문가 시스템은 인간 지식을 AI에 인코딩하려 하고, AlphaGo Zero는 AI가 스스로 지식을 발견하게 합니다.</p>
<p>결과: AI가 발견한 지식이 인간 지식보다 더 완전하고 더 정확할 수 있습니다.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="자기-대국의-위력">자기 대국의 위력<a href="#자기-대국의-위력" class="hash-link" aria-label="자기 대국의 위력에 대한 직접 링크" title="자기 대국의 위력에 대한 직접 링크" translate="no">​</a></h3>
<p>AlphaGo Zero는 자기 대국이 무한한 훈련 데이터를 생성할 수 있음을 증명했으며, 이 데이터의 품질은 네트워크 향상에 따라 향상됩니다.</p>
<p>이것은 &#x27;긍정적 순환&#x27;입니다:</p>
<ul>
<li class="">더 강한 네트워크 → 더 좋은 자기 대국 데이터</li>
<li class="">더 좋은 데이터 → 더 강한 네트워크</li>
</ul>
<p>이 순환은 게임의 이론적 상한(존재한다면)에 도달할 때까지 계속 실행될 수 있습니다.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="단순화의-중요성">단순화의 중요성<a href="#단순화의-중요성" class="hash-link" aria-label="단순화의 중요성에 대한 직접 링크" title="단순화의 중요성에 대한 직접 링크" translate="no">​</a></h3>
<p>AlphaGo Zero의 성공은 &#x27;단순화&#x27;의 중요성을 증명합니다:</p>
<ul>
<li class="">입력 단순화(48 → 17)</li>
<li class="">아키텍처 단순화(이중 네트워크 → 단일 네트워크)</li>
<li class="">훈련 단순화(지도 + 강화 → 순수 강화)</li>
</ul>
<p>매번 단순화가 시스템을 더 강하게 만들었습니다. 이것은 알려줍니다: 복잡함이 좋은 것이 아니며, 가장 간단한 해결책이 종종 가장 좋습니다.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="애니메이션-대응">애니메이션 대응<a href="#애니메이션-대응" class="hash-link" aria-label="애니메이션 대응에 대한 직접 링크" title="애니메이션 대응에 대한 직접 링크" translate="no">​</a></h2>
<p>이 글에서 다루는 핵심 개념과 애니메이션 번호:</p>
<table><thead><tr><th>번호</th><th>개념</th><th>물리/수학 대응</th></tr></thead><tbody><tr><td>E7</td><td>처음부터 훈련</td><td>자기 조직화 현상</td></tr><tr><td>E5</td><td>자기 대국</td><td>고정점 수렴</td></tr><tr><td>E12</td><td>기력 성장 곡선</td><td>S형 성장</td></tr><tr><td>D12</td><td>잔차 네트워크</td><td>그래디언트 고속도로</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="더-읽을거리">더 읽을거리<a href="#더-읽을거리" class="hash-link" aria-label="더 읽을거리에 대한 직접 링크" title="더 읽을거리에 대한 직접 링크" translate="no">​</a></h2>
<ul>
<li class=""><strong>다음 글</strong>: <a class="" href="/ko/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/">이중 헤드 네트워크와 잔차 네트워크</a> — AlphaGo Zero의 신경망 아키텍처 상세</li>
<li class=""><strong>관련 글</strong>: <a class="" href="/ko/docs/for-engineers/how-it-works/alphago-explained/self-play/">자기 대국</a> — 왜 자기 대국이 초인 수준을 만들 수 있는가</li>
<li class=""><strong>기술 심화</strong>: <a class="" href="/ko/docs/for-engineers/how-it-works/alphago-explained/training-from-scratch/">처음부터의 훈련 과정</a> — Day 0-3의 상세 진화</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="참고-자료">참고 자료<a href="#참고-자료" class="hash-link" aria-label="참고 자료에 대한 직접 링크" title="참고 자료에 대한 직접 링크" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">Silver, D., et al. (2018). &quot;A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.&quot; <em>Science</em>, 362(6419), 1140-1144.</li>
<li class="">DeepMind. (2017). &quot;AlphaGo Zero: Starting from scratch.&quot; <em>DeepMind Blog</em>.</li>
<li class="">Schrittwieser, J., et al. (2020). &quot;Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.&quot; <em>Nature</em>, 588, 604-609.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/16-alphago-zero.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>페이지 편집</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="문서 페이지"><a class="pagination-nav__link pagination-nav__link--prev" href="/ko/docs/for-engineers/how-it-works/alphago-explained/puct-formula/"><div class="pagination-nav__sublabel">이전</div><div class="pagination-nav__label">PUCT 공식 상세</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ko/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/"><div class="pagination-nav__sublabel">다음</div><div class="pagination-nav__label">이중 헤드 네트워크와 잔차 네트워크</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#왜-인간-기보가-필요-없는가" class="table-of-contents__link toc-highlight">왜 인간 기보가 필요 없는가?</a><ul><li><a href="#인간-기보의-한계" class="table-of-contents__link toc-highlight">인간 기보의 한계</a></li><li><a href="#zero의-돌파구" class="table-of-contents__link toc-highlight">Zero의 돌파구</a></li></ul></li><li><a href="#원본-alphago와의-대결-1000" class="table-of-contents__link toc-highlight">원본 AlphaGo와의 대결: 100:0</a><ul><li><a href="#압도적-승리" class="table-of-contents__link toc-highlight">압도적 승리</a></li><li><a href="#더-적은-자원으로-더-강한-기력" class="table-of-contents__link toc-highlight">더 적은 자원으로 더 강한 기력</a></li><li><a href="#왜-zero가-더-강한가" class="table-of-contents__link toc-highlight">왜 Zero가 더 강한가?</a></li></ul></li><li><a href="#단순화된-입력-특징-48에서-17로" class="table-of-contents__link toc-highlight">단순화된 입력 특징: 48에서 17로</a><ul><li><a href="#원본-alphago의-48개-특징-평면" class="table-of-contents__link toc-highlight">원본 AlphaGo의 48개 특징 평면</a></li><li><a href="#alphago-zero의-17개-특징-평면" class="table-of-contents__link toc-highlight">AlphaGo Zero의 17개 특징 평면</a></li><li><a href="#왜-단순화가-좋은가" class="table-of-contents__link toc-highlight">왜 단순화가 좋은가?</a></li></ul></li><li><a href="#단일-네트워크-아키텍처" class="table-of-contents__link toc-highlight">단일 네트워크 아키텍처</a><ul><li><a href="#원본의-이중-네트워크-설계" class="table-of-contents__link toc-highlight">원본의 이중 네트워크 설계</a></li><li><a href="#zero의-이중-헤드-네트워크" class="table-of-contents__link toc-highlight">Zero의 이중 헤드 네트워크</a></li><li><a href="#잔차-네트워크의-위력" class="table-of-contents__link toc-highlight">잔차 네트워크의 위력</a></li></ul></li><li><a href="#훈련-효율성-향상" class="table-of-contents__link toc-highlight">훈련 효율성 향상</a><ul><li><a href="#자기-대국의-지수적-성장" class="table-of-contents__link toc-highlight">자기 대국의 지수적 성장</a></li><li><a href="#왜-이렇게-빠른가" class="table-of-contents__link toc-highlight">왜 이렇게 빠른가?</a></li><li><a href="#인간-학습과의-비교" class="table-of-contents__link toc-highlight">인간 학습과의 비교</a></li></ul></li><li><a href="#범용성-체스-장기" class="table-of-contents__link toc-highlight">범용성: 체스, 장기</a><ul><li><a href="#alphazero의-탄생" class="table-of-contents__link toc-highlight">AlphaZero의 탄생</a></li><li><a href="#범용성의-의미" class="table-of-contents__link toc-highlight">범용성의 의미</a></li><li><a href="#전통적-ai에-대한-충격" class="table-of-contents__link toc-highlight">전통적 AI에 대한 충격</a></li></ul></li><li><a href="#alphago-zero의-바둑-스타일" class="table-of-contents__link toc-highlight">AlphaGo Zero의 바둑 스타일</a><ul><li><a href="#인간을-초월한-미학" class="table-of-contents__link toc-highlight">인간을 초월한 미학</a></li><li><a href="#인간-바둑-이치-재발견" class="table-of-contents__link toc-highlight">인간 바둑 이치 재발견</a></li><li><a href="#인간을-초월한-혁신" class="table-of-contents__link toc-highlight">인간을 초월한 혁신</a></li></ul></li><li><a href="#기술적-세부사항-요약" class="table-of-contents__link toc-highlight">기술적 세부사항 요약</a><ul><li><a href="#원본-alphago와의-완전한-비교" class="table-of-contents__link toc-highlight">원본 AlphaGo와의 완전한 비교</a></li><li><a href="#핵심-알고리즘" class="table-of-contents__link toc-highlight">핵심 알고리즘</a></li></ul></li><li><a href="#ai-연구에-대한-시사점" class="table-of-contents__link toc-highlight">AI 연구에 대한 시사점</a><ul><li><a href="#제1원리-학습" class="table-of-contents__link toc-highlight">제1원리 학습</a></li><li><a href="#자기-대국의-위력" class="table-of-contents__link toc-highlight">자기 대국의 위력</a></li><li><a href="#단순화의-중요성" class="table-of-contents__link toc-highlight">단순화의 중요성</a></li></ul></li><li><a href="#애니메이션-대응" class="table-of-contents__link toc-highlight">애니메이션 대응</a></li><li><a href="#더-읽을거리" class="table-of-contents__link toc-highlight">더 읽을거리</a></li><li><a href="#참고-자료" class="table-of-contents__link toc-highlight">참고 자료</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>