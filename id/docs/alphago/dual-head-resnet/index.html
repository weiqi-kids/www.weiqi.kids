<!doctype html>
<html lang="id" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-alphago/dual-head-resnet" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Dual-Head Network dan Residual Network | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/id/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/id/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/id/docs/alphago/dual-head-resnet/"><meta data-rh="true" property="og:locale" content="id"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="id"><meta data-rh="true" name="docsearch:language" content="id"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Dual-Head Network dan Residual Network | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="Analisis mendalam arsitektur neural network AlphaGo Zero - Shared backbone, Policy Head, Value Head, dan 40-layer ResNet"><meta data-rh="true" property="og:description" content="Analisis mendalam arsitektur neural network AlphaGo Zero - Shared backbone, Policy Head, Value Head, dan 40-layer ResNet"><meta data-rh="true" name="keywords" content="dual-head network,residual network,ResNet,Policy Head,Value Head,deep learning,arsitektur neural network"><link data-rh="true" rel="icon" href="/id/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/id/docs/alphago/dual-head-resnet/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/dual-head-resnet/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/alphago/dual-head-resnet/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/alphago/dual-head-resnet/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/alphago/dual-head-resnet/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/alphago/dual-head-resnet/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/alphago/dual-head-resnet/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/alphago/dual-head-resnet/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/alphago/dual-head-resnet/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/alphago/dual-head-resnet/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/alphago/dual-head-resnet/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/alphago/dual-head-resnet/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/dual-head-resnet/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AlphaGo","item":"https://www.weiqi.kids/id/docs/alphago/"},{"@type":"ListItem","position":2,"name":"Dual-Head Network dan Residual Network","item":"https://www.weiqi.kids/id/docs/alphago/dual-head-resnet"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/id/assets/css/styles.f23bf74b.css">
<script src="/id/assets/js/runtime~main.4e8b45de.js" defer="defer"></script>
<script src="/id/assets/js/main.f6e13202.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/id/img/logo.svg"><div role="region" aria-label="Lewati ke konten utama"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">Lewati ke konten utama</a></div><nav aria-label="Utama" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Alihkan bilah sisi" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/id/"><div class="navbar__logo"><img src="/id/img/logo.svg" alt="Logo Asosiasi Weiqi Kids" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/id/img/logo.svg" alt="Logo Asosiasi Weiqi Kids" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">Weiqi.Kids</b></a><a class="navbar__item navbar__link" href="/id/docs/learn/">Belajar Go</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/id/docs/alphago/">AlphaGo</a><a class="navbar__item navbar__link" href="/id/docs/animations/">Studio Animasi</a><a class="navbar__item navbar__link" href="/id/docs/tech/">Dokumentasi Teknis</a><a class="navbar__item navbar__link" href="/id/docs/about/">Tentang Kami</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>Bahasa Indonesia</a><ul class="dropdown__menu"><li><a href="/docs/alphago/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/alphago/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/alphago/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/alphago/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/alphago/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/alphago/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/alphago/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/alphago/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/alphago/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/alphago/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/alphago/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="Gulir kembali ke atas" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="Bilah sisi dokumentasi" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/id/docs/intro/"><span title="Panduan Pengguna" class="linkLabel_REp1">Panduan Pengguna</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/id/docs/alphago/"><span title="AlphaGo" class="categoryLinkLabel_ezQx">AlphaGo</span></a><button aria-label="Ciutkan kategori bilah sisi &#x27;AlphaGo&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/birth-of-alphago/"><span title="Kelahiran AlphaGo" class="linkLabel_REp1">Kelahiran AlphaGo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/key-matches/"><span title="Tinjauan Pertandingan Kunci" class="linkLabel_REp1">Tinjauan Pertandingan Kunci</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/move-37/"><span title="Analisis Mendalam &quot;Langkah Ilahi&quot;" class="linkLabel_REp1">Analisis Mendalam &quot;Langkah Ilahi&quot;</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/why-go-is-hard/"><span title="Mengapa Go Sulit?" class="linkLabel_REp1">Mengapa Go Sulit?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/traditional-limits/"><span title="Batas Metode Tradisional" class="linkLabel_REp1">Batas Metode Tradisional</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/board-representation/"><span title="Representasi Status Papan" class="linkLabel_REp1">Representasi Status Papan</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/policy-network/"><span title="Penjelasan Detail Policy Network" class="linkLabel_REp1">Penjelasan Detail Policy Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/value-network/"><span title="Penjelasan Detail Value Network" class="linkLabel_REp1">Penjelasan Detail Value Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/input-features/"><span title="Desain Fitur Input" class="linkLabel_REp1">Desain Fitur Input</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/cnn-and-go/"><span title="CNN dan Permainan Go" class="linkLabel_REp1">CNN dan Permainan Go</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/supervised-learning/"><span title="Tahap Supervised Learning" class="linkLabel_REp1">Tahap Supervised Learning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/reinforcement-intro/"><span title="Pengantar Reinforcement Learning" class="linkLabel_REp1">Pengantar Reinforcement Learning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/self-play/"><span title="Self-Play" class="linkLabel_REp1">Self-Play</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/mcts-neural-combo/"><span title="Kombinasi MCTS dan Neural Network" class="linkLabel_REp1">Kombinasi MCTS dan Neural Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/puct-formula/"><span title="Penjelasan Detail Rumus PUCT" class="linkLabel_REp1">Penjelasan Detail Rumus PUCT</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/alphago-zero/"><span title="Ikhtisar AlphaGo Zero" class="linkLabel_REp1">Ikhtisar AlphaGo Zero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/id/docs/alphago/dual-head-resnet/"><span title="Dual-Head Network dan Residual Network" class="linkLabel_REp1">Dual-Head Network dan Residual Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/training-from-scratch/"><span title="Proses Pelatihan dari Nol" class="linkLabel_REp1">Proses Pelatihan dari Nol</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/distributed-systems/"><span title="Sistem Terdistribusi dan TPU" class="linkLabel_REp1">Sistem Terdistribusi dan TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/legacy-and-impact/"><span title="Warisan AlphaGo" class="linkLabel_REp1">Warisan AlphaGo</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/id/docs/learn/"><span title="學圍棋" class="categoryLinkLabel_ezQx">學圍棋</span></a><button aria-label="Perluas kategori bilah sisi &#x27;學圍棋&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/id/docs/animations/"><span title="動畫教室" class="linkLabel_REp1">動畫教室</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/id/docs/tech/"><span title="技術文件" class="categoryLinkLabel_ezQx">技術文件</span></a><button aria-label="Perluas kategori bilah sisi &#x27;技術文件&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/id/docs/about/"><span title="關於我們" class="categoryLinkLabel_ezQx">關於我們</span></a><button aria-label="Perluas kategori bilah sisi &#x27;關於我們&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="Runut navigasi"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Halaman utama" class="breadcrumbs__link" href="/id/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/id/docs/alphago/"><span>AlphaGo</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Dual-Head Network dan Residual Network</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">Pada halaman ini</button></div><div class="theme-doc-markdown markdown"><header><h1>Dual-Head Network dan Residual Network</h1></header>
<p>Salah satu inovasi arsitektur terpenting AlphaGo Zero adalah menggunakan <strong>Dual-Head Network</strong> menggantikan desain dual network AlphaGo original. Perubahan yang tampak sederhana ini membawa peningkatan performa signifikan dan proses pembelajaran yang lebih elegan.</p>
<p>Artikel ini akan menganalisis secara mendalam prinsip desain arsitektur ini, dasar matematika, dan mengapa ia sangat efektif.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="desain-dual-head-network">Desain Dual-Head Network<a href="#desain-dual-head-network" class="hash-link" aria-label="Taut langsung ke Desain Dual-Head Network" title="Taut langsung ke Desain Dual-Head Network" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="arsitektur-keseluruhan">Arsitektur Keseluruhan<a href="#arsitektur-keseluruhan" class="hash-link" aria-label="Taut langsung ke Arsitektur Keseluruhan" title="Taut langsung ke Arsitektur Keseluruhan" translate="no">​</a></h3>
<p>Neural network AlphaGo Zero bisa dibagi menjadi tiga bagian:</p>
<!-- -->
<p>Mari kita analisis setiap bagian satu per satu.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="shared-backbone">Shared Backbone<a href="#shared-backbone" class="hash-link" aria-label="Taut langsung ke Shared Backbone" title="Taut langsung ke Shared Backbone" translate="no">​</a></h3>
<p>Shared backbone adalah <strong>Residual Network (ResNet)</strong> yang dalam, bertanggung jawab mengekstrak fitur dari state papan.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="detail-arsitektur">Detail Arsitektur<a href="#detail-arsitektur" class="hash-link" aria-label="Taut langsung ke Detail Arsitektur" title="Taut langsung ke Detail Arsitektur" translate="no">​</a></h4>
<table><thead><tr><th>Komponen</th><th>Spesifikasi</th></tr></thead><tbody><tr><td>Layer input</td><td>Konvolusi 3x3, 256 channel</td></tr><tr><td>Residual block</td><td>40 (atau 20 versi ringkas)</td></tr><tr><td>Setiap residual block</td><td>2 layer konvolusi 3x3, 256 channel</td></tr><tr><td>Fungsi aktivasi</td><td>ReLU</td></tr><tr><td>Normalisasi</td><td>Batch Normalization</td></tr></tbody></table>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="representasi-matematika">Representasi Matematika<a href="#representasi-matematika" class="hash-link" aria-label="Taut langsung ke Representasi Matematika" title="Taut langsung ke Representasi Matematika" translate="no">​</a></h4>
<p>Misalkan input adalah x (dimensi 17 x 19 x 19), output shared backbone adalah:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">f(x) = ResNet_40(Conv_3x3(x))</span><br></span></code></pre></div></div>
<p>Di mana f(x) (dimensi 256 x 19 x 19) adalah representasi fitur dimensi tinggi.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="policy-head-head-strategi">Policy Head (Head Strategi)<a href="#policy-head-head-strategi" class="hash-link" aria-label="Taut langsung ke Policy Head (Head Strategi)" title="Taut langsung ke Policy Head (Head Strategi)" translate="no">​</a></h3>
<p>Policy Head bertanggung jawab memprediksi probabilitas langkah untuk setiap posisi.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="detail-arsitektur-1">Detail Arsitektur<a href="#detail-arsitektur-1" class="hash-link" aria-label="Taut langsung ke Detail Arsitektur" title="Taut langsung ke Detail Arsitektur" translate="no">​</a></h4>
<!-- -->
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="representasi-matematika-1">Representasi Matematika<a href="#representasi-matematika-1" class="hash-link" aria-label="Taut langsung ke Representasi Matematika" title="Taut langsung ke Representasi Matematika" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">π = Softmax(FC(Flatten(ReLU(BN(Conv_1x1(f(x)))))))</span><br></span></code></pre></div></div>
<p>Output π adalah vektor 362 dimensi, memenuhi semua elemen non-negatif dan jumlahnya 1.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="value-head-head-nilai">Value Head (Head Nilai)<a href="#value-head-head-nilai" class="hash-link" aria-label="Taut langsung ke Value Head (Head Nilai)" title="Taut langsung ke Value Head (Head Nilai)" translate="no">​</a></h3>
<p>Value Head bertanggung jawab memprediksi win rate posisi saat ini.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="detail-arsitektur-2">Detail Arsitektur<a href="#detail-arsitektur-2" class="hash-link" aria-label="Taut langsung ke Detail Arsitektur" title="Taut langsung ke Detail Arsitektur" translate="no">​</a></h4>
<!-- -->
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="representasi-matematika-2">Representasi Matematika<a href="#representasi-matematika-2" class="hash-link" aria-label="Taut langsung ke Representasi Matematika" title="Taut langsung ke Representasi Matematika" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">v = Tanh(FC_1(ReLU(FC_2(Flatten(ReLU(BN(Conv_1x1(f(x)))))))))</span><br></span></code></pre></div></div>
<p>Output v dalam range [-1, 1]:</p>
<ul>
<li class="">v = 1: Pemain saat ini pasti menang</li>
<li class="">v = -1: Pemain saat ini pasti kalah</li>
<li class="">v = 0: Seimbang</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="mengapa-perlu-shared-backbone">Mengapa Perlu Shared Backbone?<a href="#mengapa-perlu-shared-backbone" class="hash-link" aria-label="Taut langsung ke Mengapa Perlu Shared Backbone?" title="Taut langsung ke Mengapa Perlu Shared Backbone?" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="pemahaman-intuitif">Pemahaman Intuitif<a href="#pemahaman-intuitif" class="hash-link" aria-label="Taut langsung ke Pemahaman Intuitif" title="Taut langsung ke Pemahaman Intuitif" translate="no">​</a></h3>
<p>&quot;Langkah berikutnya harus dimainkan di mana&quot; (Policy) dan &quot;siapa yang akan menang&quot; (Value) kedua masalah ini sebenarnya membutuhkan pemahaman pola papan yang sama:</p>
<ul>
<li class=""><strong>Bentuk batu</strong>: Bentuk mana yang bagus, mana yang buruk</li>
<li class=""><strong>Pengaruh</strong>: Sisi mana lebih besar, area mana masih ada ruang</li>
<li class=""><strong>Hidup-mati</strong>: Kelompok batu mana yang sudah hidup, mana yang masih dalam ko</li>
<li class=""><strong>Pertempuran</strong>: Di mana ada serangan, bagaimana hasil lokal</li>
</ul>
<p>Jika menggunakan dua network independen, fitur-fitur ini perlu dipelajari dua kali. Shared backbone membuat fitur-fitur dasar ini hanya perlu dipelajari sekali, kedua tugas bisa menggunakannya.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="perspektif-multi-task-learning">Perspektif Multi-task Learning<a href="#perspektif-multi-task-learning" class="hash-link" aria-label="Taut langsung ke Perspektif Multi-task Learning" title="Taut langsung ke Perspektif Multi-task Learning" translate="no">​</a></h3>
<p>Dari sudut pandang machine learning, ini adalah <strong>Multi-task Learning</strong>:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L = L_policy + L_value</span><br></span></code></pre></div></div>
<p>Kedua tugas berbagi representasi dasar, ini membawa beberapa keuntungan:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-efek-regularisasi">1. Efek Regularisasi<a href="#1-efek-regularisasi" class="hash-link" aria-label="Taut langsung ke 1. Efek Regularisasi" title="Taut langsung ke 1. Efek Regularisasi" translate="no">​</a></h4>
<p>Berbagi parameter setara dengan regularisasi implisit. Jika suatu fitur hanya berguna untuk Policy tapi tidak untuk Value (atau sebaliknya), lebih sulit untuk diperbesar berlebihan.</p>
<p>Jumlah parameter efektif lebih kecil dari jumlah parameter dua network independen.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-efisiensi-data">2. Efisiensi Data<a href="#2-efisiensi-data" class="hash-link" aria-label="Taut langsung ke 2. Efisiensi Data" title="Taut langsung ke 2. Efisiensi Data" translate="no">​</a></h4>
<p>Setiap permainan secara bersamaan menghasilkan label Policy (probabilitas pencarian MCTS) dan label Value (hasil akhir menang/kalah). Shared backbone membuat kedua label digunakan untuk melatih fitur bersama, meningkatkan efisiensi pemanfaatan data.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-sinyal-gradien-yang-kaya">3. Sinyal Gradien yang Kaya<a href="#3-sinyal-gradien-yang-kaya" class="hash-link" aria-label="Taut langsung ke 3. Sinyal Gradien yang Kaya" title="Taut langsung ke 3. Sinyal Gradien yang Kaya" translate="no">​</a></h4>
<p>Gradien dari kedua tugas mengalir ke shared backbone:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">∂L/∂θ_shared = ∂L_policy/∂θ_shared + ∂L_value/∂θ_shared</span><br></span></code></pre></div></div>
<p>Ini menyediakan sinyal supervisi yang lebih kaya, membuat fitur bersama lebih robust.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="bukti-eksperimental">Bukti Eksperimental<a href="#bukti-eksperimental" class="hash-link" aria-label="Taut langsung ke Bukti Eksperimental" title="Taut langsung ke Bukti Eksperimental" translate="no">​</a></h3>
<p>Eksperimen ablasi DeepMind menunjukkan, performa dual-head network secara signifikan lebih baik dari dual network terpisah:</p>
<table><thead><tr><th>Konfigurasi</th><th>Rating ELO</th><th>Selisih Relatif</th></tr></thead><tbody><tr><td>Network Policy + Value terpisah</td><td>Baseline</td><td>-</td></tr><tr><td>Dual-head network (shared backbone)</td><td>+300 ELO</td><td>~65% selisih win rate</td></tr></tbody></table>
<p>Selisih 300 ELO berarti dual-head network memiliki sekitar 65% win rate terhadap network terpisah. Ini adalah peningkatan yang signifikan.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="prinsip-residual-network">Prinsip Residual Network<a href="#prinsip-residual-network" class="hash-link" aria-label="Taut langsung ke Prinsip Residual Network" title="Taut langsung ke Prinsip Residual Network" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="dilema-deep-network">Dilema Deep Network<a href="#dilema-deep-network" class="hash-link" aria-label="Taut langsung ke Dilema Deep Network" title="Taut langsung ke Dilema Deep Network" translate="no">​</a></h3>
<p>Sebelum ResNet ditemukan, deep neural network menghadapi paradoks:</p>
<blockquote>
<p>Secara teori, network lebih dalam seharusnya setidaknya sama baiknya dengan network dangkal (worst case, layer tambahan bisa mempelajari identity mapping). Tapi kenyataannya, network lebih dalam seringkali berkinerja lebih buruk.</p>
</blockquote>
<p>Inilah <strong>Masalah Degradasi (Degradation Problem)</strong>:</p>
<ul>
<li class="">Training error meningkat dengan kedalaman (bukan overfitting, tapi kesulitan optimisasi)</li>
<li class="">Gradien menghilang secara bertahap saat backpropagation (Vanishing Gradient)</li>
<li class="">Parameter layer dalam hampir tidak bisa diupdate secara efektif</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="desain-residual-block">Desain Residual Block<a href="#desain-residual-block" class="hash-link" aria-label="Taut langsung ke Desain Residual Block" title="Taut langsung ke Desain Residual Block" translate="no">​</a></h3>
<p>He Kaiming et al. pada 2015 mengajukan solusi simpel namun elegan: <strong>Skip Connection (Residual Connection)</strong>.</p>
<!-- -->
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="representasi-matematika-3">Representasi Matematika<a href="#representasi-matematika-3" class="hash-link" aria-label="Taut langsung ke Representasi Matematika" title="Taut langsung ke Representasi Matematika" translate="no">​</a></h4>
<p>Network tradisional: Belajar target mapping H(x)</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">y = H(x)</span><br></span></code></pre></div></div>
<p>Residual network: Belajar <strong>residual mapping</strong> F(x) = H(x) - x</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">y = F(x) + x</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="mengapa-residual-connection-efektif">Mengapa Residual Connection Efektif?<a href="#mengapa-residual-connection-efektif" class="hash-link" aria-label="Taut langsung ke Mengapa Residual Connection Efektif?" title="Taut langsung ke Mengapa Residual Connection Efektif?" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-highway-gradien">1. Highway Gradien<a href="#1-highway-gradien" class="hash-link" aria-label="Taut langsung ke 1. Highway Gradien" title="Taut langsung ke 1. Highway Gradien" translate="no">​</a></h4>
<p>Pertimbangkan gradien saat backpropagation:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">∂L/∂x = ∂L/∂y × ∂y/∂x = ∂L/∂y × (1 + ∂F(x)/∂x)</span><br></span></code></pre></div></div>
<p>Kuncinya adalah <strong>+1</strong> itu. Bahkan jika ∂F(x)/∂x sangat kecil atau nol, gradien tetap bisa langsung diteruskan kembali melalui +1.</p>
<p>Ini seperti membangun &quot;highway gradien&quot;, membiarkan gradien mengalir tanpa hambatan dari output layer kembali ke input layer.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-identity-mapping-lebih-mudah-dipelajari">2. Identity Mapping Lebih Mudah Dipelajari<a href="#2-identity-mapping-lebih-mudah-dipelajari" class="hash-link" aria-label="Taut langsung ke 2. Identity Mapping Lebih Mudah Dipelajari" title="Taut langsung ke 2. Identity Mapping Lebih Mudah Dipelajari" translate="no">​</a></h4>
<p>Jika solusi optimal mendekati identity mapping (H(x) mendekati x), maka:</p>
<ul>
<li class="">Network tradisional: Perlu belajar H(x) = x, mungkin sulit</li>
<li class="">Residual network: Hanya perlu belajar F(x) mendekati 0, relatif mudah</li>
</ul>
<p>Menginisialisasi weight ke nol atau mendekati nol, residual block secara natural menuju identity mapping.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-efek-ensemble">3. Efek Ensemble<a href="#3-efek-ensemble" class="hash-link" aria-label="Taut langsung ke 3. Efek Ensemble" title="Taut langsung ke 3. Efek Ensemble" translate="no">​</a></h4>
<p>Deep ResNet bisa dilihat sebagai <strong>ensemble implisit</strong> dari banyak shallow network. Jika ada n residual block, informasi bisa mengalir melalui 2^n jalur berbeda.</p>
<p>Efek ensemble ini meningkatkan robustness model.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="terobosan-resnet-di-imagenet">Terobosan ResNet di ImageNet<a href="#terobosan-resnet-di-imagenet" class="hash-link" aria-label="Taut langsung ke Terobosan ResNet di ImageNet" title="Taut langsung ke Terobosan ResNet di ImageNet" translate="no">​</a></h3>
<p>ResNet meraih hasil menakjubkan di kompetisi ImageNet 2015:</p>
<table><thead><tr><th>Kedalaman</th><th>Top-5 Error Rate</th></tr></thead><tbody><tr><td>VGG-19 (tanpa residual)</td><td>7.3%</td></tr><tr><td>ResNet-34</td><td>5.7%</td></tr><tr><td>ResNet-152</td><td>4.5%</td></tr><tr><td>Level manusia</td><td>~5.1%</td></tr></tbody></table>
<p>ResNet <strong>152 layer</strong> tidak hanya bisa dilatih, tapi juga jauh lebih baik dari VGG 19 layer. Ini membuktikan residual connection memang menyelesaikan masalah pelatihan deep network.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="40-layer-resnet-alphago-zero">40-Layer ResNet AlphaGo Zero<a href="#40-layer-resnet-alphago-zero" class="hash-link" aria-label="Taut langsung ke 40-Layer ResNet AlphaGo Zero" title="Taut langsung ke 40-Layer ResNet AlphaGo Zero" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="mengapa-memilih-40-layer">Mengapa Memilih 40 Layer?<a href="#mengapa-memilih-40-layer" class="hash-link" aria-label="Taut langsung ke Mengapa Memilih 40 Layer?" title="Taut langsung ke Mengapa Memilih 40 Layer?" translate="no">​</a></h3>
<p>DeepMind menguji ResNet dengan kedalaman berbeda:</p>
<table><thead><tr><th>Jumlah Residual Block</th><th>Total Layer</th><th>Rating ELO</th></tr></thead><tbody><tr><td>5</td><td>11</td><td>Baseline</td></tr><tr><td>10</td><td>21</td><td>+200</td></tr><tr><td>20</td><td>41</td><td>+400</td></tr><tr><td>40</td><td>81</td><td>+500</td></tr></tbody></table>
<p>Network lebih dalam memang lebih kuat, tapi diminishing returns. AlphaGo Zero menggunakan 20 atau 40 residual block:</p>
<ul>
<li class=""><strong>AlphaGo Zero (versi paper)</strong>: 40 residual block, 256 channel</li>
<li class=""><strong>Versi ringkas</strong>: 20 residual block, 256 channel</li>
</ul>
<p>Konfigurasi 40 layer mencapai keseimbangan yang baik antara kekuatan bermain dan biaya pelatihan.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="konfigurasi-konkret">Konfigurasi Konkret<a href="#konfigurasi-konkret" class="hash-link" aria-label="Taut langsung ke Konfigurasi Konkret" title="Taut langsung ke Konfigurasi Konkret" translate="no">​</a></h3>
<p>Konfigurasi ResNet AlphaGo Zero sebagai berikut:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Input: 17 x 19 x 19</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Conv layer: 3x3, 256 channel, BN, ReLU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Residual block x40:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ├─ Conv layer: 3x3, 256 channel, BN, ReLU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ├─ Conv layer: 3x3, 256 channel, BN</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  └─ Skip connection + ReLU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Policy Head / Value Head</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="estimasi-jumlah-parameter">Estimasi Jumlah Parameter<a href="#estimasi-jumlah-parameter" class="hash-link" aria-label="Taut langsung ke Estimasi Jumlah Parameter" title="Taut langsung ke Estimasi Jumlah Parameter" translate="no">​</a></h4>
<table><thead><tr><th>Komponen</th><th>Jumlah Parameter (aprox.)</th></tr></thead><tbody><tr><td>Input convolution</td><td>17 x 3 x 3 x 256 ≈ 39K</td></tr><tr><td>Setiap residual block</td><td>2 x 256 x 3 x 3 x 256 ≈ 1.2M</td></tr><tr><td>40 residual block</td><td>40 x 1.2M ≈ 47M</td></tr><tr><td>Policy Head</td><td>~1M</td></tr><tr><td>Value Head</td><td>~0.2M</td></tr><tr><td><strong>Total</strong></td><td><strong>~48M</strong></td></tr></tbody></table>
<p>Sekitar 48 juta parameter, neural network skala menengah menurut standar modern.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="peran-batch-normalization">Peran Batch Normalization<a href="#peran-batch-normalization" class="hash-link" aria-label="Taut langsung ke Peran Batch Normalization" title="Taut langsung ke Peran Batch Normalization" translate="no">​</a></h3>
<p>Setiap layer konvolusi diikuti <strong>Batch Normalization (BN)</strong>, ini krusial untuk stabilitas pelatihan:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-normalisasi-nilai-aktivasi">1. Normalisasi Nilai Aktivasi<a href="#1-normalisasi-nilai-aktivasi" class="hash-link" aria-label="Taut langsung ke 1. Normalisasi Nilai Aktivasi" title="Taut langsung ke 1. Normalisasi Nilai Aktivasi" translate="no">​</a></h4>
<p>BN menormalisasi nilai aktivasi setiap layer ke mean 0, variance 1:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">x_hat = (x - μ_B) / sqrt(σ_B² + ε)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">y = γ × x_hat + β</span><br></span></code></pre></div></div>
<p>Di mana γ dan β adalah parameter yang bisa dipelajari.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-mengurangi-internal-covariate-shift">2. Mengurangi Internal Covariate Shift<a href="#2-mengurangi-internal-covariate-shift" class="hash-link" aria-label="Taut langsung ke 2. Mengurangi Internal Covariate Shift" title="Taut langsung ke 2. Mengurangi Internal Covariate Shift" translate="no">​</a></h4>
<p>Dalam deep network, distribusi input setiap layer berubah seiring update parameter layer sebelumnya. BN membuat distribusi input setiap layer tetap stabil, mempercepat konvergensi pelatihan.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-efek-regularisasi">3. Efek Regularisasi<a href="#3-efek-regularisasi" class="hash-link" aria-label="Taut langsung ke 3. Efek Regularisasi" title="Taut langsung ke 3. Efek Regularisasi" translate="no">​</a></h4>
<p>BN menggunakan statistik mini-batch saat pelatihan, memperkenalkan randomness, memiliki efek regularisasi ringan.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="perbandingan-dengan-arsitektur-lain">Perbandingan dengan Arsitektur Lain<a href="#perbandingan-dengan-arsitektur-lain" class="hash-link" aria-label="Taut langsung ke Perbandingan dengan Arsitektur Lain" title="Taut langsung ke Perbandingan dengan Arsitektur Lain" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="vs-cnn-alphago-original">vs. CNN AlphaGo Original<a href="#vs-cnn-alphago-original" class="hash-link" aria-label="Taut langsung ke vs. CNN AlphaGo Original" title="Taut langsung ke vs. CNN AlphaGo Original" translate="no">​</a></h3>
<table><thead><tr><th>Fitur</th><th>AlphaGo Original</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Tipe arsitektur</td><td>CNN standar</td><td>ResNet</td></tr><tr><td>Kedalaman</td><td>13 layer</td><td>41-81 layer</td></tr><tr><td>Residual connection</td><td>Tidak ada</td><td>Ada</td></tr><tr><td>Jumlah network</td><td>2 (terpisah)</td><td>1 (shared)</td></tr><tr><td>BN</td><td>Tidak ada</td><td>Ada</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="vs-network-gaya-vgg">vs. Network Gaya VGG<a href="#vs-network-gaya-vgg" class="hash-link" aria-label="Taut langsung ke vs. Network Gaya VGG" title="Taut langsung ke vs. Network Gaya VGG" translate="no">​</a></h3>
<p>VGG adalah arsitektur runner-up ImageNet 2014, menggunakan konvolusi 3x3 bertumpuk:</p>
<table><thead><tr><th>Fitur</th><th>VGG</th><th>ResNet</th></tr></thead><tbody><tr><td>Kedalaman maksimum yang bisa dilatih</td><td>~19 layer</td><td>152+ layer</td></tr><tr><td>Aliran gradien</td><td>Berkurang layer per layer</td><td>Ada highway</td></tr><tr><td>Kesulitan pelatihan</td><td>Layer dalam sulit</td><td>Layer dalam bisa dilatih</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="vs-inception--googlenet">vs. Inception / GoogLeNet<a href="#vs-inception--googlenet" class="hash-link" aria-label="Taut langsung ke vs. Inception / GoogLeNet" title="Taut langsung ke vs. Inception / GoogLeNet" translate="no">​</a></h3>
<p>Inception menggunakan konvolusi multi-skala paralel:</p>
<table><thead><tr><th>Fitur</th><th>Inception</th><th>ResNet</th></tr></thead><tbody><tr><td>Keunggulan</td><td>Fitur multi-skala</td><td>Penumpukan dalam</td></tr><tr><td>Kompleksitas</td><td>Lebih tinggi</td><td>Simpel</td></tr><tr><td>Aplikabilitas Go</td><td>Biasa</td><td>Bagus</td></tr></tbody></table>
<p>Desain simpel ResNet lebih cocok untuk Go yang membutuhkan penalaran dalam.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="vs-transformer">vs. Transformer<a href="#vs-transformer" class="hash-link" aria-label="Taut langsung ke vs. Transformer" title="Taut langsung ke vs. Transformer" translate="no">​</a></h3>
<p>Arsitektur Transformer yang diajukan tahun 2017 meraih sukses besar di bidang NLP. Ada yang mencoba menerapkan Transformer ke Go:</p>
<table><thead><tr><th>Fitur</th><th>ResNet</th><th>Transformer</th></tr></thead><tbody><tr><td>Inductive bias</td><td>Lokalitas (konvolusi)</td><td>Attention global</td></tr><tr><td>Position encoding</td><td>Implisit (konvolusi)</td><td>Eksplisit</td></tr><tr><td>Performa Go</td><td>Bagus</td><td>Bisa tapi tidak lebih baik dari ResNet</td></tr><tr><td>Efisiensi komputasi</td><td>Lebih tinggi</td><td>Lebih rendah (O(n²))</td></tr></tbody></table>
<p>Untuk masalah seperti Go yang memiliki struktur spasial jelas, inductive bias CNN/ResNet lebih cocok.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="analisis-mendalam-pilihan-desain">Analisis Mendalam Pilihan Desain<a href="#analisis-mendalam-pilihan-desain" class="hash-link" aria-label="Taut langsung ke Analisis Mendalam Pilihan Desain" title="Taut langsung ke Analisis Mendalam Pilihan Desain" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="mengapa-menggunakan-konvolusi-3x3">Mengapa Menggunakan Konvolusi 3x3?<a href="#mengapa-menggunakan-konvolusi-3x3" class="hash-link" aria-label="Taut langsung ke Mengapa Menggunakan Konvolusi 3x3?" title="Taut langsung ke Mengapa Menggunakan Konvolusi 3x3?" translate="no">​</a></h3>
<p>AlphaGo Zero menggunakan konvolusi 3x3 secara konsisten, bukan kernel konvolusi lebih besar:</p>
<ol>
<li class=""><strong>Efisiensi parameter</strong>: Dua konvolusi 3x3 memiliki receptive field sama dengan satu 5x5, tapi jumlah parameter lebih sedikit (18 vs 25)</li>
<li class=""><strong>Network lebih dalam</strong>: Dengan jumlah parameter sama, bisa menumpuk lebih banyak layer</li>
<li class=""><strong>Lebih banyak non-linearitas</strong>: Ada ReLU antar layer, meningkatkan ekspresivitas</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="mengapa-menggunakan-256-channel">Mengapa Menggunakan 256 Channel?<a href="#mengapa-menggunakan-256-channel" class="hash-link" aria-label="Taut langsung ke Mengapa Menggunakan 256 Channel?" title="Taut langsung ke Mengapa Menggunakan 256 Channel?" translate="no">​</a></h3>
<p>256 channel adalah pilihan empiris:</p>
<ul>
<li class=""><strong>Terlalu sedikit</strong> (mis. 64): Ekspresivitas tidak cukup, tidak bisa menangkap pola kompleks</li>
<li class=""><strong>Terlalu banyak</strong> (mis. 512): Jumlah parameter berlipat ganda, biaya pelatihan meningkat drastis, tapi peningkatan kekuatan terbatas</li>
</ul>
<p>Eksperimen KataGo kemudian menunjukkan, jumlah channel bisa disesuaikan berdasarkan sumber daya pelatihan:</p>
<ul>
<li class="">Sumber daya rendah: 128 channel, 20 block</li>
<li class="">Sumber daya tinggi: 256 channel, 40 block</li>
<li class="">Sumber daya lebih tinggi: 384 channel, 60 block</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="mengapa-policy-head-menggunakan-softmax-value-head-menggunakan-tanh">Mengapa Policy Head Menggunakan Softmax, Value Head Menggunakan Tanh?<a href="#mengapa-policy-head-menggunakan-softmax-value-head-menggunakan-tanh" class="hash-link" aria-label="Taut langsung ke Mengapa Policy Head Menggunakan Softmax, Value Head Menggunakan Tanh?" title="Taut langsung ke Mengapa Policy Head Menggunakan Softmax, Value Head Menggunakan Tanh?" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="policy-head-softmax">Policy Head: Softmax<a href="#policy-head-softmax" class="hash-link" aria-label="Taut langsung ke Policy Head: Softmax" title="Taut langsung ke Policy Head: Softmax" translate="no">​</a></h4>
<p>Memainkan langkah adalah <strong>masalah klasifikasi</strong>—memilih satu dari 361 posisi (plus Pass). Output Softmax memenuhi:</p>
<ul>
<li class="">Semua probabilitas non-negatif: π_i &gt;= 0</li>
<li class="">Jumlah probabilitas = 1: Σπ_i = 1</li>
</ul>
<p>Ini konsisten dengan definisi distribusi probabilitas.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="value-head-tanh">Value Head: Tanh<a href="#value-head-tanh" class="hash-link" aria-label="Taut langsung ke Value Head: Tanh" title="Taut langsung ke Value Head: Tanh" translate="no">​</a></h4>
<p>Win rate adalah <strong>masalah regresi</strong>—memprediksi nilai kontinu. Range output Tanh adalah [-1, 1]:</p>
<ul>
<li class="">Bounded: Tidak akan menghasilkan nilai ekstrem</li>
<li class="">Simetris: Menang dan kalah ditangani secara simetris</li>
<li class="">Differentiable: Mudah untuk perhitungan gradien</li>
</ul>
<p>Menggunakan Tanh bukan output unbounded (seperti linear layer) bisa mencegah ketidakstabilan pelatihan.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="detail-pelatihan">Detail Pelatihan<a href="#detail-pelatihan" class="hash-link" aria-label="Taut langsung ke Detail Pelatihan" title="Taut langsung ke Detail Pelatihan" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="loss-function">Loss Function<a href="#loss-function" class="hash-link" aria-label="Taut langsung ke Loss Function" title="Taut langsung ke Loss Function" translate="no">​</a></h3>
<p>Total loss AlphaGo Zero adalah jumlah tiga term:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L = L_policy + L_value + L_reg</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="policy-loss">Policy Loss<a href="#policy-loss" class="hash-link" aria-label="Taut langsung ke Policy Loss" title="Taut langsung ke Policy Loss" translate="no">​</a></h4>
<p>Menggunakan <strong>cross-entropy loss</strong>, membuat output network mendekati probabilitas pencarian MCTS:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L_policy = -Σ π_MCTS(a) × log(π_net(a))</span><br></span></code></pre></div></div>
<p>Di mana:</p>
<ul>
<li class="">π_MCTS(a) adalah probabilitas pencarian MCTS untuk aksi a</li>
<li class="">π_net(a) adalah probabilitas output network</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="value-loss">Value Loss<a href="#value-loss" class="hash-link" aria-label="Taut langsung ke Value Loss" title="Taut langsung ke Value Loss" translate="no">​</a></h4>
<p>Menggunakan <strong>Mean Squared Error (MSE)</strong>, membuat output network mendekati hasil menang/kalah aktual:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L_value = (v_net - z)²</span><br></span></code></pre></div></div>
<p>Di mana:</p>
<ul>
<li class="">v_net adalah win rate prediksi network</li>
<li class="">z adalah hasil pertandingan aktual (+1 atau -1)</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="regularization-loss">Regularization Loss<a href="#regularization-loss" class="hash-link" aria-label="Taut langsung ke Regularization Loss" title="Taut langsung ke Regularization Loss" translate="no">​</a></h4>
<p>Menggunakan <strong>L2 regularization</strong> untuk mencegah overfitting:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L_reg = c × ||θ||²</span><br></span></code></pre></div></div>
<p>Di mana c adalah koefisien regularisasi, θ adalah parameter network.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="konfigurasi-optimizer">Konfigurasi Optimizer<a href="#konfigurasi-optimizer" class="hash-link" aria-label="Taut langsung ke Konfigurasi Optimizer" title="Taut langsung ke Konfigurasi Optimizer" translate="no">​</a></h3>
<table><thead><tr><th>Parameter</th><th>Nilai</th></tr></thead><tbody><tr><td>Optimizer</td><td>SGD + Momentum</td></tr><tr><td>Momentum</td><td>0.9</td></tr><tr><td>Learning rate awal</td><td>0.01</td></tr><tr><td>Decay learning rate</td><td>Setengah setiap X langkah</td></tr><tr><td>Batch Size</td><td>32 x 2048 = 64K (distributed)</td></tr><tr><td>Koefisien L2 regularization</td><td>1e-4</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="data-augmentation">Data Augmentation<a href="#data-augmentation" class="hash-link" aria-label="Taut langsung ke Data Augmentation" title="Taut langsung ke Data Augmentation" translate="no">​</a></h3>
<p>Papan Go memiliki 8 simetri (4 rotasi x 2 flip). Saat pelatihan, setiap posisi bisa menghasilkan 8 sampel pelatihan ekuivalen.</p>
<p>Ini membuat data pelatihan efektif meningkat 8 kali lipat, tanpa memerlukan self-play tambahan.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="pertimbangan-implementasi">Pertimbangan Implementasi<a href="#pertimbangan-implementasi" class="hash-link" aria-label="Taut langsung ke Pertimbangan Implementasi" title="Taut langsung ke Pertimbangan Implementasi" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="optimisasi-memori">Optimisasi Memori<a href="#optimisasi-memori" class="hash-link" aria-label="Taut langsung ke Optimisasi Memori" title="Taut langsung ke Optimisasi Memori" translate="no">​</a></h3>
<p>Pelatihan 40-layer ResNet membutuhkan banyak memori:</p>
<ul>
<li class=""><strong>Forward pass</strong>: Perlu menyimpan nilai aktivasi setiap layer (untuk backpropagation)</li>
<li class=""><strong>Backward pass</strong>: Perlu menyimpan gradien</li>
</ul>
<p>Strategi optimisasi:</p>
<ol>
<li class=""><strong>Gradient Checkpointing</strong>: Hanya simpan sebagian nilai aktivasi, hitung ulang saat dibutuhkan</li>
<li class=""><strong>Mixed Precision Training</strong>: Gunakan FP16 untuk mengurangi penggunaan memori</li>
<li class=""><strong>Distributed Training</strong>: Sebarkan batch ke beberapa GPU/TPU</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="optimisasi-inferensi">Optimisasi Inferensi<a href="#optimisasi-inferensi" class="hash-link" aria-label="Taut langsung ke Optimisasi Inferensi" title="Taut langsung ke Optimisasi Inferensi" translate="no">​</a></h3>
<p>Saat inferensi tidak perlu statistik mini-batch BN, bisa menggunakan moving average yang diakumulasi saat pelatihan:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">x_hat = (x - μ_moving) / sqrt(σ_moving² + ε)</span><br></span></code></pre></div></div>
<p>Ini membuat kecepatan inferensi lebih cepat dan hasil deterministik.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="quantization-dan-compression">Quantization dan Compression<a href="#quantization-dan-compression" class="hash-link" aria-label="Taut langsung ke Quantization dan Compression" title="Taut langsung ke Quantization dan Compression" translate="no">​</a></h3>
<p>Saat deployment bisa compress network lebih lanjut:</p>
<ul>
<li class=""><strong>Weight Quantization</strong>: FP32 → INT8, memori berkurang 4x</li>
<li class=""><strong>Pruning</strong>: Hapus koneksi weight kecil</li>
<li class=""><strong>Knowledge Distillation</strong>: Gunakan network besar untuk melatih network kecil</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="korespondensi-animasi">Korespondensi Animasi<a href="#korespondensi-animasi" class="hash-link" aria-label="Taut langsung ke Korespondensi Animasi" title="Taut langsung ke Korespondensi Animasi" translate="no">​</a></h2>
<p>Konsep inti yang dibahas dalam artikel ini dan nomor animasinya:</p>
<table><thead><tr><th>Nomor</th><th>Konsep</th><th>Korespondensi Fisika/Matematika</th></tr></thead><tbody><tr><td>E5 E3</td><td>Dual-head network</td><td>Multi-task learning</td></tr><tr><td>E5 D12</td><td>Residual connection</td><td>Highway gradien</td></tr><tr><td>E5 D8</td><td>Convolutional Neural Network</td><td>Receptive field lokal</td></tr><tr><td>E5 D10</td><td>Batch Normalization</td><td>Normalisasi distribusi</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="bacaan-lanjutan">Bacaan Lanjutan<a href="#bacaan-lanjutan" class="hash-link" aria-label="Taut langsung ke Bacaan Lanjutan" title="Taut langsung ke Bacaan Lanjutan" translate="no">​</a></h2>
<ul>
<li class=""><strong>Artikel Sebelumnya</strong>: <a class="" href="/id/docs/alphago/alphago-zero/">Ikhtisar AlphaGo Zero</a> — Mengapa tidak memerlukan catatan permainan manusia</li>
<li class=""><strong>Artikel Berikutnya</strong>: <a class="" href="/id/docs/alphago/training-from-scratch/">Proses Pelatihan dari Nol</a> — Evolusi detail Hari 0-3</li>
<li class=""><strong>Teknis Mendalam</strong>: <a class="" href="/id/docs/alphago/cnn-and-go/">CNN dan Go</a> — Mengapa CNN cocok untuk papan</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="referensi">Referensi<a href="#referensi" class="hash-link" aria-label="Taut langsung ke Referensi" title="Taut langsung ke Referensi" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">He, K., et al. (2016). &quot;Deep Residual Learning for Image Recognition.&quot; <em>CVPR 2016</em>.</li>
<li class="">Ioffe, S., &amp; Szegedy, C. (2015). &quot;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.&quot; <em>ICML 2015</em>.</li>
<li class="">Caruana, R. (1997). &quot;Multitask Learning.&quot; <em>Machine Learning</em>, 28(1), 41-75.</li>
<li class="">Veit, A., et al. (2016). &quot;Residual Networks Behave Like Ensembles of Relatively Shallow Networks.&quot; <em>NeurIPS 2016</em>.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/17-dual-head-resnet.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Sunting halaman ini</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Halaman dokumentasi"><a class="pagination-nav__link pagination-nav__link--prev" href="/id/docs/alphago/alphago-zero/"><div class="pagination-nav__sublabel">Sebelum</div><div class="pagination-nav__label">Ikhtisar AlphaGo Zero</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/id/docs/alphago/training-from-scratch/"><div class="pagination-nav__sublabel">Berikut</div><div class="pagination-nav__label">Proses Pelatihan dari Nol</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#desain-dual-head-network" class="table-of-contents__link toc-highlight">Desain Dual-Head Network</a><ul><li><a href="#arsitektur-keseluruhan" class="table-of-contents__link toc-highlight">Arsitektur Keseluruhan</a></li><li><a href="#shared-backbone" class="table-of-contents__link toc-highlight">Shared Backbone</a></li><li><a href="#policy-head-head-strategi" class="table-of-contents__link toc-highlight">Policy Head (Head Strategi)</a></li><li><a href="#value-head-head-nilai" class="table-of-contents__link toc-highlight">Value Head (Head Nilai)</a></li></ul></li><li><a href="#mengapa-perlu-shared-backbone" class="table-of-contents__link toc-highlight">Mengapa Perlu Shared Backbone?</a><ul><li><a href="#pemahaman-intuitif" class="table-of-contents__link toc-highlight">Pemahaman Intuitif</a></li><li><a href="#perspektif-multi-task-learning" class="table-of-contents__link toc-highlight">Perspektif Multi-task Learning</a></li><li><a href="#bukti-eksperimental" class="table-of-contents__link toc-highlight">Bukti Eksperimental</a></li></ul></li><li><a href="#prinsip-residual-network" class="table-of-contents__link toc-highlight">Prinsip Residual Network</a><ul><li><a href="#dilema-deep-network" class="table-of-contents__link toc-highlight">Dilema Deep Network</a></li><li><a href="#desain-residual-block" class="table-of-contents__link toc-highlight">Desain Residual Block</a></li><li><a href="#mengapa-residual-connection-efektif" class="table-of-contents__link toc-highlight">Mengapa Residual Connection Efektif?</a></li><li><a href="#terobosan-resnet-di-imagenet" class="table-of-contents__link toc-highlight">Terobosan ResNet di ImageNet</a></li></ul></li><li><a href="#40-layer-resnet-alphago-zero" class="table-of-contents__link toc-highlight">40-Layer ResNet AlphaGo Zero</a><ul><li><a href="#mengapa-memilih-40-layer" class="table-of-contents__link toc-highlight">Mengapa Memilih 40 Layer?</a></li><li><a href="#konfigurasi-konkret" class="table-of-contents__link toc-highlight">Konfigurasi Konkret</a></li><li><a href="#peran-batch-normalization" class="table-of-contents__link toc-highlight">Peran Batch Normalization</a></li></ul></li><li><a href="#perbandingan-dengan-arsitektur-lain" class="table-of-contents__link toc-highlight">Perbandingan dengan Arsitektur Lain</a><ul><li><a href="#vs-cnn-alphago-original" class="table-of-contents__link toc-highlight">vs. CNN AlphaGo Original</a></li><li><a href="#vs-network-gaya-vgg" class="table-of-contents__link toc-highlight">vs. Network Gaya VGG</a></li><li><a href="#vs-inception--googlenet" class="table-of-contents__link toc-highlight">vs. Inception / GoogLeNet</a></li><li><a href="#vs-transformer" class="table-of-contents__link toc-highlight">vs. Transformer</a></li></ul></li><li><a href="#analisis-mendalam-pilihan-desain" class="table-of-contents__link toc-highlight">Analisis Mendalam Pilihan Desain</a><ul><li><a href="#mengapa-menggunakan-konvolusi-3x3" class="table-of-contents__link toc-highlight">Mengapa Menggunakan Konvolusi 3x3?</a></li><li><a href="#mengapa-menggunakan-256-channel" class="table-of-contents__link toc-highlight">Mengapa Menggunakan 256 Channel?</a></li><li><a href="#mengapa-policy-head-menggunakan-softmax-value-head-menggunakan-tanh" class="table-of-contents__link toc-highlight">Mengapa Policy Head Menggunakan Softmax, Value Head Menggunakan Tanh?</a></li></ul></li><li><a href="#detail-pelatihan" class="table-of-contents__link toc-highlight">Detail Pelatihan</a><ul><li><a href="#loss-function" class="table-of-contents__link toc-highlight">Loss Function</a></li><li><a href="#konfigurasi-optimizer" class="table-of-contents__link toc-highlight">Konfigurasi Optimizer</a></li><li><a href="#data-augmentation" class="table-of-contents__link toc-highlight">Data Augmentation</a></li></ul></li><li><a href="#pertimbangan-implementasi" class="table-of-contents__link toc-highlight">Pertimbangan Implementasi</a><ul><li><a href="#optimisasi-memori" class="table-of-contents__link toc-highlight">Optimisasi Memori</a></li><li><a href="#optimisasi-inferensi" class="table-of-contents__link toc-highlight">Optimisasi Inferensi</a></li><li><a href="#quantization-dan-compression" class="table-of-contents__link toc-highlight">Quantization dan Compression</a></li></ul></li><li><a href="#korespondensi-animasi" class="table-of-contents__link toc-highlight">Korespondensi Animasi</a></li><li><a href="#bacaan-lanjutan" class="table-of-contents__link toc-highlight">Bacaan Lanjutan</a></li><li><a href="#referensi" class="table-of-contents__link toc-highlight">Referensi</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>