<!doctype html>
<html lang="id" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-alphago/alphago-zero" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Ikhtisar AlphaGo Zero | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/id/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/id/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/id/docs/alphago/alphago-zero/"><meta data-rh="true" property="og:locale" content="id"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="id"><meta data-rh="true" name="docsearch:language" content="id"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Ikhtisar AlphaGo Zero | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="Mulai dari nol, belajar mandiri sepenuhnya, bagaimana AlphaGo Zero melampaui semua versi sebelumnya tanpa menggunakan catatan permainan manusia"><meta data-rh="true" property="og:description" content="Mulai dari nol, belajar mandiri sepenuhnya, bagaimana AlphaGo Zero melampaui semua versi sebelumnya tanpa menggunakan catatan permainan manusia"><meta data-rh="true" name="keywords" content="AlphaGo Zero,self-play,reinforcement learning,deep learning,AI Go,unsupervised learning"><link data-rh="true" rel="icon" href="/id/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/id/docs/alphago/alphago-zero/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/alphago-zero/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/alphago/alphago-zero/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/alphago/alphago-zero/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/alphago/alphago-zero/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/alphago/alphago-zero/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/alphago/alphago-zero/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/alphago/alphago-zero/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/alphago/alphago-zero/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/alphago/alphago-zero/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/alphago/alphago-zero/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/alphago/alphago-zero/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/alphago-zero/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AlphaGo","item":"https://www.weiqi.kids/id/docs/alphago/"},{"@type":"ListItem","position":2,"name":"Ikhtisar AlphaGo Zero","item":"https://www.weiqi.kids/id/docs/alphago/alphago-zero"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/id/assets/css/styles.f23bf74b.css">
<script src="/id/assets/js/runtime~main.4e8b45de.js" defer="defer"></script>
<script src="/id/assets/js/main.f6e13202.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/id/img/logo.svg"><div role="region" aria-label="Lewati ke konten utama"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">Lewati ke konten utama</a></div><nav aria-label="Utama" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Alihkan bilah sisi" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/id/"><div class="navbar__logo"><img src="/id/img/logo.svg" alt="Logo Asosiasi Weiqi Kids" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/id/img/logo.svg" alt="Logo Asosiasi Weiqi Kids" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">Weiqi.Kids</b></a><a class="navbar__item navbar__link" href="/id/docs/learn/">Belajar Go</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/id/docs/alphago/">AlphaGo</a><a class="navbar__item navbar__link" href="/id/docs/animations/">Studio Animasi</a><a class="navbar__item navbar__link" href="/id/docs/tech/">Dokumentasi Teknis</a><a class="navbar__item navbar__link" href="/id/docs/about/">Tentang Kami</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>Bahasa Indonesia</a><ul class="dropdown__menu"><li><a href="/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/alphago/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="Gulir kembali ke atas" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="Bilah sisi dokumentasi" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/id/docs/intro/"><span title="Panduan Pengguna" class="linkLabel_REp1">Panduan Pengguna</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/id/docs/alphago/"><span title="AlphaGo" class="categoryLinkLabel_ezQx">AlphaGo</span></a><button aria-label="Ciutkan kategori bilah sisi &#x27;AlphaGo&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/birth-of-alphago/"><span title="Kelahiran AlphaGo" class="linkLabel_REp1">Kelahiran AlphaGo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/key-matches/"><span title="Tinjauan Pertandingan Kunci" class="linkLabel_REp1">Tinjauan Pertandingan Kunci</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/move-37/"><span title="Analisis Mendalam &quot;Langkah Ilahi&quot;" class="linkLabel_REp1">Analisis Mendalam &quot;Langkah Ilahi&quot;</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/why-go-is-hard/"><span title="Mengapa Go Sulit?" class="linkLabel_REp1">Mengapa Go Sulit?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/traditional-limits/"><span title="Batas Metode Tradisional" class="linkLabel_REp1">Batas Metode Tradisional</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/board-representation/"><span title="Representasi Status Papan" class="linkLabel_REp1">Representasi Status Papan</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/policy-network/"><span title="Penjelasan Detail Policy Network" class="linkLabel_REp1">Penjelasan Detail Policy Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/value-network/"><span title="Penjelasan Detail Value Network" class="linkLabel_REp1">Penjelasan Detail Value Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/input-features/"><span title="Desain Fitur Input" class="linkLabel_REp1">Desain Fitur Input</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/cnn-and-go/"><span title="CNN dan Permainan Go" class="linkLabel_REp1">CNN dan Permainan Go</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/supervised-learning/"><span title="Tahap Supervised Learning" class="linkLabel_REp1">Tahap Supervised Learning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/reinforcement-intro/"><span title="Pengantar Reinforcement Learning" class="linkLabel_REp1">Pengantar Reinforcement Learning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/self-play/"><span title="Self-Play" class="linkLabel_REp1">Self-Play</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/mcts-neural-combo/"><span title="Kombinasi MCTS dan Neural Network" class="linkLabel_REp1">Kombinasi MCTS dan Neural Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/puct-formula/"><span title="Penjelasan Detail Rumus PUCT" class="linkLabel_REp1">Penjelasan Detail Rumus PUCT</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/id/docs/alphago/alphago-zero/"><span title="Ikhtisar AlphaGo Zero" class="linkLabel_REp1">Ikhtisar AlphaGo Zero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/dual-head-resnet/"><span title="Dual-Head Network dan Residual Network" class="linkLabel_REp1">Dual-Head Network dan Residual Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/training-from-scratch/"><span title="Proses Pelatihan dari Nol" class="linkLabel_REp1">Proses Pelatihan dari Nol</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/distributed-systems/"><span title="Sistem Terdistribusi dan TPU" class="linkLabel_REp1">Sistem Terdistribusi dan TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/id/docs/alphago/legacy-and-impact/"><span title="Warisan AlphaGo" class="linkLabel_REp1">Warisan AlphaGo</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/id/docs/learn/"><span title="學圍棋" class="categoryLinkLabel_ezQx">學圍棋</span></a><button aria-label="Perluas kategori bilah sisi &#x27;學圍棋&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/id/docs/animations/"><span title="動畫教室" class="linkLabel_REp1">動畫教室</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/id/docs/tech/"><span title="技術文件" class="categoryLinkLabel_ezQx">技術文件</span></a><button aria-label="Perluas kategori bilah sisi &#x27;技術文件&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/id/docs/about/"><span title="關於我們" class="categoryLinkLabel_ezQx">關於我們</span></a><button aria-label="Perluas kategori bilah sisi &#x27;關於我們&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="Runut navigasi"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Halaman utama" class="breadcrumbs__link" href="/id/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/id/docs/alphago/"><span>AlphaGo</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Ikhtisar AlphaGo Zero</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">Pada halaman ini</button></div><div class="theme-doc-markdown markdown"><header><h1>Ikhtisar AlphaGo Zero</h1></header>
<p>Pada Oktober 2017, DeepMind mengumumkan hasil yang mengejutkan dunia AI: <strong>AlphaGo Zero</strong> tanpa menggunakan catatan permainan manusia sama sekali, mulai berlatih dari keadaan acak sepenuhnya, hanya dalam tiga hari melampaui AlphaGo original yang mengalahkan Lee Sedol, dan menang dengan skor <strong>100:0</strong>.</p>
<p>Ini bukan sekadar kemajuan dalam angka. Ini mewakili paradigma baru: <strong>AI tidak membutuhkan pengetahuan manusia, bisa menemukan segalanya dari nol</strong>.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="mengapa-tidak-memerlukan-catatan-permainan-manusia">Mengapa Tidak Memerlukan Catatan Permainan Manusia?<a href="#mengapa-tidak-memerlukan-catatan-permainan-manusia" class="hash-link" aria-label="Taut langsung ke Mengapa Tidak Memerlukan Catatan Permainan Manusia?" title="Taut langsung ke Mengapa Tidak Memerlukan Catatan Permainan Manusia?" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="keterbatasan-catatan-permainan-manusia">Keterbatasan Catatan Permainan Manusia<a href="#keterbatasan-catatan-permainan-manusia" class="hash-link" aria-label="Taut langsung ke Keterbatasan Catatan Permainan Manusia" title="Taut langsung ke Keterbatasan Catatan Permainan Manusia" translate="no">​</a></h3>
<p>Proses pelatihan AlphaGo original dibagi menjadi dua tahap:</p>
<ol>
<li class=""><strong>Supervised Learning</strong>: Melatih Policy Network dengan 30 juta catatan permainan manusia</li>
<li class=""><strong>Reinforcement Learning</strong>: Meningkatkan lebih lanjut melalui self-play</li>
</ol>
<p>Metode ini memiliki beberapa masalah fundamental:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-catatan-permainan-manusia-memiliki-batas-atas">1. Catatan Permainan Manusia Memiliki Batas Atas<a href="#1-catatan-permainan-manusia-memiliki-batas-atas" class="hash-link" aria-label="Taut langsung ke 1. Catatan Permainan Manusia Memiliki Batas Atas" title="Taut langsung ke 1. Catatan Permainan Manusia Memiliki Batas Atas" translate="no">​</a></h4>
<p>Kekuatan bermain pemain manusia terbatas, catatan permainan berisi pemahaman manusia, juga termasuk kesalahan dan bias manusia. Ketika AI belajar dari catatan permainan manusia, yang dipelajarinya adalah:</p>
<ul>
<li class="">Langkah yang menurut manusia bagus (tapi belum tentu optimal)</li>
<li class="">Pola berpikir manusia (tapi mungkin membatasi inovasi)</li>
<li class="">Kesalahan manusia (akan dipelajari sebagai sampel yang benar)</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-bottleneck-supervised-learning">2. Bottleneck Supervised Learning<a href="#2-bottleneck-supervised-learning" class="hash-link" aria-label="Taut langsung ke 2. Bottleneck Supervised Learning" title="Taut langsung ke 2. Bottleneck Supervised Learning" translate="no">​</a></h4>
<p>Tujuan supervised learning adalah &quot;meniru manusia&quot;—memprediksi langkah mana yang akan dimainkan pemain manusia. Ini berarti batas kemampuan AI dibatasi oleh kemampuan pemain manusia.</p>
<p>Seperti seorang murid yang hanya bisa meniru gurunya, tidak akan pernah bisa melampaui gurunya.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-biaya-pengumpulan-data">3. Biaya Pengumpulan Data<a href="#3-biaya-pengumpulan-data" class="hash-link" aria-label="Taut langsung ke 3. Biaya Pengumpulan Data" title="Taut langsung ke 3. Biaya Pengumpulan Data" translate="no">​</a></h4>
<p>Catatan permainan manusia berkualitas tinggi membutuhkan waktu bertahun-tahun untuk dikumpulkan, dan hanya ada di game seperti Go yang memiliki sejarah panjang. Jika ingin menerapkan AI ke bidang baru (seperti prediksi struktur protein), tidak ada &quot;catatan permainan ahli manusia&quot; yang tersedia sama sekali.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="terobosan-zero">Terobosan Zero<a href="#terobosan-zero" class="hash-link" aria-label="Taut langsung ke Terobosan Zero" title="Taut langsung ke Terobosan Zero" translate="no">​</a></h3>
<p>AlphaGo Zero sepenuhnya melewati tahap supervised learning, langsung memulai self-play dari <strong>inisialisasi acak</strong>. Ini menyelesaikan semua masalah di atas:</p>
<table><thead><tr><th>Masalah</th><th>AlphaGo Original</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Batas pengetahuan manusia</td><td>Dibatasi kualitas catatan</td><td>Tidak ada batasan ini</td></tr><tr><td>Tujuan pembelajaran</td><td>Meniru manusia</td><td>Memaksimalkan win rate</td></tr><tr><td>Kebutuhan data</td><td>30 juta catatan</td><td>0</td></tr><tr><td>Kemampuan generalisasi</td><td>Hanya terbatas Go</td><td>Bisa digeneralisasi ke bidang lain</td></tr></tbody></table>
<p>Ini adalah perubahan paradigma fundamental: dari &quot;mempelajari pengetahuan manusia&quot; ke &quot;menemukan pengetahuan dari prinsip pertama&quot;.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="perbandingan-dengan-alphago-original-1000">Perbandingan dengan AlphaGo Original: 100:0<a href="#perbandingan-dengan-alphago-original-1000" class="hash-link" aria-label="Taut langsung ke Perbandingan dengan AlphaGo Original: 100:0" title="Taut langsung ke Perbandingan dengan AlphaGo Original: 100:0" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="kemenangan-telak">Kemenangan Telak<a href="#kemenangan-telak" class="hash-link" aria-label="Taut langsung ke Kemenangan Telak" title="Taut langsung ke Kemenangan Telak" translate="no">​</a></h3>
<p>DeepMind membuat AlphaGo Zero yang sudah terlatih bertanding melawan berbagai versi AlphaGo:</p>
<table><thead><tr><th>Lawan</th><th>Rekor AlphaGo Zero</th></tr></thead><tbody><tr><td>AlphaGo Fan (versi yang mengalahkan Fan Hui)</td><td>100:0</td></tr><tr><td>AlphaGo Lee (versi yang mengalahkan Lee Sedol)</td><td>100:0</td></tr><tr><td>AlphaGo Master (versi 60 kemenangan beruntun)</td><td>89:11</td></tr></tbody></table>
<p><strong>100:0</strong>—ini berarti dalam 100 pertandingan, AlphaGo original tidak bisa menang satu pun.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="lebih-sedikit-sumber-daya-kekuatan-bermain-lebih-tinggi">Lebih Sedikit Sumber Daya, Kekuatan Bermain Lebih Tinggi<a href="#lebih-sedikit-sumber-daya-kekuatan-bermain-lebih-tinggi" class="hash-link" aria-label="Taut langsung ke Lebih Sedikit Sumber Daya, Kekuatan Bermain Lebih Tinggi" title="Taut langsung ke Lebih Sedikit Sumber Daya, Kekuatan Bermain Lebih Tinggi" translate="no">​</a></h3>
<p>Bukan hanya menang, AlphaGo Zero juga mencapai kekuatan bermain lebih tinggi dengan sumber daya lebih sedikit:</p>
<table><thead><tr><th>Metrik</th><th>AlphaGo Lee</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Waktu pelatihan</td><td>Berbulan-bulan</td><td>40 hari (3 hari melampaui AlphaGo Lee)</td></tr><tr><td>Jumlah permainan pelatihan</td><td>30 juta catatan manusia + self-play</td><td>4.9 juta self-play</td></tr><tr><td>Jumlah TPU (pelatihan)</td><td>50+</td><td>4</td></tr><tr><td>Jumlah TPU (inferensi)</td><td>48</td><td>4</td></tr><tr><td>Fitur input</td><td>48 plane</td><td>17 plane</td></tr><tr><td>Neural network</td><td>Dual network SL + RL</td><td>Single dual-head network</td></tr></tbody></table>
<p>Ini adalah peningkatan efisiensi yang menakjubkan: <strong>sumber daya berkurang 10 kali lipat atau lebih, kekuatan bermain justru meningkat signifikan</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="mengapa-zero-lebih-kuat">Mengapa Zero Lebih Kuat?<a href="#mengapa-zero-lebih-kuat" class="hash-link" aria-label="Taut langsung ke Mengapa Zero Lebih Kuat?" title="Taut langsung ke Mengapa Zero Lebih Kuat?" translate="no">​</a></h3>
<p>Alasan AlphaGo Zero lebih kuat bisa dipahami dari beberapa sudut pandang:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-pembelajaran-tanpa-bias">1. Pembelajaran Tanpa Bias<a href="#1-pembelajaran-tanpa-bias" class="hash-link" aria-label="Taut langsung ke 1. Pembelajaran Tanpa Bias" title="Taut langsung ke 1. Pembelajaran Tanpa Bias" translate="no">​</a></h4>
<p>AlphaGo original belajar dari catatan permainan manusia, mewarisi bias manusia. Misalnya, pemain manusia mungkin terlalu menekankan joseki tertentu, atau memiliki evaluasi yang salah pada posisi tertentu.</p>
<p>AlphaGo Zero tidak memiliki beban ini. Ia mulai dari kertas kosong, hanya belajar apa yang merupakan langkah bagus melalui hasil menang/kalah. Ini memungkinkannya menemukan langkah yang tidak pernah terpikirkan manusia.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-tujuan-pembelajaran-yang-konsisten">2. Tujuan Pembelajaran yang Konsisten<a href="#2-tujuan-pembelajaran-yang-konsisten" class="hash-link" aria-label="Taut langsung ke 2. Tujuan Pembelajaran yang Konsisten" title="Taut langsung ke 2. Tujuan Pembelajaran yang Konsisten" translate="no">​</a></h4>
<p>Pelatihan AlphaGo original memiliki dua tujuan berbeda:</p>
<ul>
<li class="">Supervised learning: Memaksimalkan akurasi prediksi langkah manusia</li>
<li class="">Reinforcement learning: Memaksimalkan win rate</li>
</ul>
<p>Kedua tujuan ini mungkin saling bertentangan. AlphaGo Zero hanya memiliki satu tujuan: <strong>maksimalisasi win rate</strong>. Ini membuat proses pembelajaran lebih konsisten dan efektif.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-arsitektur-yang-lebih-simpel">3. Arsitektur yang Lebih Simpel<a href="#3-arsitektur-yang-lebih-simpel" class="hash-link" aria-label="Taut langsung ke 3. Arsitektur yang Lebih Simpel" title="Taut langsung ke 3. Arsitektur yang Lebih Simpel" translate="no">​</a></h4>
<p>AlphaGo original menggunakan Policy Network dan Value Network yang terpisah. AlphaGo Zero menggunakan single dual-head network (lihat artikel berikutnya untuk detail), memungkinkan representasi fitur dibagi, meningkatkan efisiensi pembelajaran.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="fitur-input-yang-disederhanakan-dari-48-ke-17">Fitur Input yang Disederhanakan: Dari 48 ke 17<a href="#fitur-input-yang-disederhanakan-dari-48-ke-17" class="hash-link" aria-label="Taut langsung ke Fitur Input yang Disederhanakan: Dari 48 ke 17" title="Taut langsung ke Fitur Input yang Disederhanakan: Dari 48 ke 17" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="48-plane-fitur-alphago-original">48 Plane Fitur AlphaGo Original<a href="#48-plane-fitur-alphago-original" class="hash-link" aria-label="Taut langsung ke 48 Plane Fitur AlphaGo Original" title="Taut langsung ke 48 Plane Fitur AlphaGo Original" translate="no">​</a></h3>
<p>Input neural network AlphaGo original berisi 48 plane fitur 19x19, mengkodekan banyak fitur yang dirancang manusia:</p>
<table><thead><tr><th>Kategori</th><th>Jumlah Fitur</th><th>Konten</th></tr></thead><tbody><tr><td>Posisi batu</td><td>3</td><td>Batu hitam, batu putih, titik kosong</td></tr><tr><td>Jumlah liberty</td><td>8</td><td>String dengan 1-8 liberty</td></tr><tr><td>Capture</td><td>8</td><td>Bisa capture 1-8 batu</td></tr><tr><td>Ko</td><td>1</td><td>Posisi ko</td></tr><tr><td>Jarak dari tepi</td><td>4</td><td>Baris pertama sampai keempat</td></tr><tr><td>Legalitas langkah</td><td>1</td><td>Posisi mana yang bisa dimainkan</td></tr><tr><td>State historis</td><td>8</td><td>Posisi 8 langkah terakhir</td></tr><tr><td>Giliran</td><td>1</td><td>Hitam atau putih</td></tr><tr><td>Lainnya</td><td>14</td><td>Ladder, eye, dll.</td></tr></tbody></table>
<p>48 fitur ini dirancang dengan cermat oleh ahli Go, berisi banyak domain knowledge.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="17-plane-fitur-alphago-zero">17 Plane Fitur AlphaGo Zero<a href="#17-plane-fitur-alphago-zero" class="hash-link" aria-label="Taut langsung ke 17 Plane Fitur AlphaGo Zero" title="Taut langsung ke 17 Plane Fitur AlphaGo Zero" translate="no">​</a></h3>
<p>AlphaGo Zero sangat menyederhanakan input, hanya menggunakan 17 plane fitur:</p>
<table><thead><tr><th>Nomor Plane</th><th>Konten</th><th>Jumlah</th></tr></thead><tbody><tr><td>1-8</td><td>Posisi batu hitam (8 langkah terakhir)</td><td>8</td></tr><tr><td>9-16</td><td>Posisi batu putih (8 langkah terakhir)</td><td>8</td></tr><tr><td>17</td><td>Giliran saat ini (semua 1 atau semua 0)</td><td>1</td></tr></tbody></table>
<p>17 fitur ini hanya berisi:</p>
<ul>
<li class=""><strong>State papan saat ini</strong>: Setiap posisi ada batu hitam, batu putih, atau kosong</li>
<li class=""><strong>Informasi historis</strong>: State papan 8 langkah terakhir</li>
<li class=""><strong>Informasi giliran</strong>: Giliran siapa bermain</li>
</ul>
<p>Tidak ada jumlah liberty, tidak ada penilaian ladder, tidak ada jarak dari tepi—semua &quot;pengetahuan Go&quot; ini dibiarkan neural network mempelajarinya sendiri.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="mengapa-penyederhanaan-itu-bagus">Mengapa Penyederhanaan Itu Bagus?<a href="#mengapa-penyederhanaan-itu-bagus" class="hash-link" aria-label="Taut langsung ke Mengapa Penyederhanaan Itu Bagus?" title="Taut langsung ke Mengapa Penyederhanaan Itu Bagus?" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-biarkan-network-menemukan-fitur-sendiri">1. Biarkan Network Menemukan Fitur Sendiri<a href="#1-biarkan-network-menemukan-fitur-sendiri" class="hash-link" aria-label="Taut langsung ke 1. Biarkan Network Menemukan Fitur Sendiri" title="Taut langsung ke 1. Biarkan Network Menemukan Fitur Sendiri" translate="no">​</a></h4>
<p>Fitur buatan tangan yang kompleks mungkin melewatkan informasi penting, atau mengkodekan asumsi yang salah. Membiarkan neural network belajar dari data mentah, ia mungkin menemukan representasi fitur yang lebih baik.</p>
<p>Faktanya terbukti, AlphaGo Zero mempelajari semua fitur yang dirancang manusia (jumlah liberty, ladder, dll.), dan juga mempelajari beberapa pola yang tidak disadari manusia secara eksplisit.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-kemampuan-generalisasi-lebih-baik">2. Kemampuan Generalisasi Lebih Baik<a href="#2-kemampuan-generalisasi-lebih-baik" class="hash-link" aria-label="Taut langsung ke 2. Kemampuan Generalisasi Lebih Baik" title="Taut langsung ke 2. Kemampuan Generalisasi Lebih Baik" translate="no">​</a></h4>
<p>Banyak dari 48 fitur yang khusus untuk Go (seperti ladder, jarak dari tepi). 17 fitur yang disederhanakan bersifat universal—game papan apa pun bisa dikodekan dengan cara serupa.</p>
<p>Ini meletakkan dasar untuk <strong>AlphaZero</strong> (AI game universal) selanjutnya.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-mengurangi-kesalahan-buatan">3. Mengurangi Kesalahan Buatan<a href="#3-mengurangi-kesalahan-buatan" class="hash-link" aria-label="Taut langsung ke 3. Mengurangi Kesalahan Buatan" title="Taut langsung ke 3. Mengurangi Kesalahan Buatan" translate="no">​</a></h4>
<p>Fitur yang dirancang manual mungkin berisi definisi yang salah atau tidak lengkap. Menyederhanakan input menghilangkan kemungkinan masalah semacam ini.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="arsitektur-network-tunggal">Arsitektur Network Tunggal<a href="#arsitektur-network-tunggal" class="hash-link" aria-label="Taut langsung ke Arsitektur Network Tunggal" title="Taut langsung ke Arsitektur Network Tunggal" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="desain-dual-network-original">Desain Dual Network Original<a href="#desain-dual-network-original" class="hash-link" aria-label="Taut langsung ke Desain Dual Network Original" title="Taut langsung ke Desain Dual Network Original" translate="no">​</a></h3>
<p>AlphaGo original menggunakan dua neural network independen:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Policy Network:  Input → CNN → Probabilitas langkah 19x19</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Value Network:   Input → CNN → Estimasi win rate (-1 sampai 1)</span><br></span></code></pre></div></div>
<p>Kedua network ini:</p>
<ul>
<li class="">Memiliki arsitektur berbeda (jumlah layer, channel sedikit berbeda)</li>
<li class="">Dilatih secara independen (latih Policy dulu, lalu Value)</li>
<li class="">Tidak berbagi parameter apa pun</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="dual-head-network-zero">Dual-Head Network Zero<a href="#dual-head-network-zero" class="hash-link" aria-label="Taut langsung ke Dual-Head Network Zero" title="Taut langsung ke Dual-Head Network Zero" translate="no">​</a></h3>
<p>AlphaGo Zero menggunakan network tunggal, tapi dengan dua output head:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Input → ResNet shared backbone → Policy Head → Probabilitas langkah 19x19</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                               → Value Head  → Estimasi win rate</span><br></span></code></pre></div></div>
<p>Kedua Head berbagi backbone ResNet yang sama (lihat <a class="" href="/id/docs/alphago/dual-head-resnet/">artikel berikutnya: Dual-Head Network dan Residual Network</a> untuk detail), ini membawa beberapa keuntungan:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-efisiensi-parameter">1. Efisiensi Parameter<a href="#1-efisiensi-parameter" class="hash-link" aria-label="Taut langsung ke 1. Efisiensi Parameter" title="Taut langsung ke 1. Efisiensi Parameter" translate="no">​</a></h4>
<p>Shared backbone berarti sebagian besar parameter digunakan bersama oleh kedua tugas. Ini mengurangi total jumlah parameter, menurunkan risiko overfitting.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-berbagi-fitur">2. Berbagi Fitur<a href="#2-berbagi-fitur" class="hash-link" aria-label="Taut langsung ke 2. Berbagi Fitur" title="Taut langsung ke 2. Berbagi Fitur" translate="no">​</a></h4>
<p>&quot;Harus bermain di mana&quot; (Policy) dan &quot;siapa yang akan menang&quot; (Value) membutuhkan pemahaman pola papan yang serupa. Shared backbone memungkinkan fitur-fitur ini dipelajari dan dimanfaatkan oleh kedua tugas secara bersamaan.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-stabilitas-pelatihan">3. Stabilitas Pelatihan<a href="#3-stabilitas-pelatihan" class="hash-link" aria-label="Taut langsung ke 3. Stabilitas Pelatihan" title="Taut langsung ke 3. Stabilitas Pelatihan" translate="no">​</a></h4>
<p>Pelatihan joint membuat sinyal gradien berasal dari dua sumber, menyediakan sinyal supervisi yang lebih kaya, membuat pelatihan lebih stabil.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="kekuatan-residual-network">Kekuatan Residual Network<a href="#kekuatan-residual-network" class="hash-link" aria-label="Taut langsung ke Kekuatan Residual Network" title="Taut langsung ke Kekuatan Residual Network" translate="no">​</a></h3>
<p>Backbone AlphaGo Zero menggunakan <strong>40-layer Residual Network (ResNet)</strong>, jauh lebih dalam dari 13-layer CNN AlphaGo original.</p>
<p>Residual connection (skip connections) memungkinkan deep network dilatih secara efektif, menghindari masalah vanishing gradient. Ini adalah teknologi terobosan dari kompetisi ImageNet 2015, berhasil diterapkan oleh AlphaGo Zero ke bidang Go.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="peningkatan-efisiensi-pelatihan">Peningkatan Efisiensi Pelatihan<a href="#peningkatan-efisiensi-pelatihan" class="hash-link" aria-label="Taut langsung ke Peningkatan Efisiensi Pelatihan" title="Taut langsung ke Peningkatan Efisiensi Pelatihan" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="pertumbuhan-eksponensial-self-play">Pertumbuhan Eksponensial Self-Play<a href="#pertumbuhan-eksponensial-self-play" class="hash-link" aria-label="Taut langsung ke Pertumbuhan Eksponensial Self-Play" title="Taut langsung ke Pertumbuhan Eksponensial Self-Play" translate="no">​</a></h3>
<p>Proses pelatihan AlphaGo Zero menunjukkan efisiensi yang menakjubkan:</p>
<table><thead><tr><th>Waktu Pelatihan</th><th>Rating ELO</th><th>Setara dengan</th></tr></thead><tbody><tr><td>0 jam</td><td>0</td><td>Bermain acak</td></tr><tr><td>3 jam</td><td>~1000</td><td>Menemukan aturan dasar</td></tr><tr><td>12 jam</td><td>~3000</td><td>Menemukan joseki</td></tr><tr><td>36 jam</td><td>~4500</td><td>Melampaui versi Fan Hui</td></tr><tr><td>60 jam</td><td>~5200</td><td>Melampaui versi Lee Sedol</td></tr><tr><td>72 jam</td><td>~5400</td><td>Melampaui AlphaGo original</td></tr><tr><td>40 hari</td><td>~5600</td><td>Versi terkuat</td></tr></tbody></table>
<p><strong>Tiga hari melampaui manusia, tiga hari melampaui AI yang sebelumnya dilatih berbulan-bulan</strong>—ini adalah peningkatan efisiensi eksponensial.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="mengapa-secepat-ini">Mengapa Secepat Ini?<a href="#mengapa-secepat-ini" class="hash-link" aria-label="Taut langsung ke Mengapa Secepat Ini?" title="Taut langsung ke Mengapa Secepat Ini?" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-panduan-pencarian-yang-lebih-kuat">1. Panduan Pencarian yang Lebih Kuat<a href="#1-panduan-pencarian-yang-lebih-kuat" class="hash-link" aria-label="Taut langsung ke 1. Panduan Pencarian yang Lebih Kuat" title="Taut langsung ke 1. Panduan Pencarian yang Lebih Kuat" translate="no">​</a></h4>
<p>MCTS AlphaGo Zero sepenuhnya dipandu oleh neural network, tidak lagi menggunakan fast rollout policy. Ini membuat pencarian lebih efisien dan akurat.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-self-play-lebih-cepat">2. Self-Play Lebih Cepat<a href="#2-self-play-lebih-cepat" class="hash-link" aria-label="Taut langsung ke 2. Self-Play Lebih Cepat" title="Taut langsung ke 2. Self-Play Lebih Cepat" translate="no">​</a></h4>
<p>Karena hanya membutuhkan satu network (bukan dua), biaya komputasi setiap self-play berkurang. Ini berarti lebih banyak data pelatihan bisa dihasilkan dalam waktu yang sama.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-pembelajaran-lebih-efektif">3. Pembelajaran Lebih Efektif<a href="#3-pembelajaran-lebih-efektif" class="hash-link" aria-label="Taut langsung ke 3. Pembelajaran Lebih Efektif" title="Taut langsung ke 3. Pembelajaran Lebih Efektif" translate="no">​</a></h4>
<p>Pelatihan joint dual-head network membuat informasi setiap permainan dimanfaatkan lebih efektif. Gradien Policy dan Value saling memperkuat, mempercepat konvergensi.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="perbandingan-dengan-pembelajaran-manusia">Perbandingan dengan Pembelajaran Manusia<a href="#perbandingan-dengan-pembelajaran-manusia" class="hash-link" aria-label="Taut langsung ke Perbandingan dengan Pembelajaran Manusia" title="Taut langsung ke Perbandingan dengan Pembelajaran Manusia" translate="no">​</a></h3>
<p>Berapa lama waktu yang dibutuhkan pemain manusia untuk mencapai level berbeda?</p>
<table><thead><tr><th>Level</th><th>Waktu yang Dibutuhkan Manusia</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Pemula</td><td>Beberapa minggu</td><td>Beberapa menit</td></tr><tr><td>Amateur 1-dan</td><td>Beberapa tahun</td><td>Beberapa jam</td></tr><tr><td>Level profesional</td><td>10-20 tahun</td><td>1-2 hari</td></tr><tr><td>Juara dunia</td><td>20+ tahun dedikasi penuh</td><td>3 hari</td></tr><tr><td>Melampaui manusia</td><td>Tidak mungkin</td><td>3 hari</td></tr></tbody></table>
<p>Perbandingan ini bukan untuk meremehkan pemain manusia—mereka menggunakan neuron biologis, sedangkan AlphaGo Zero menggunakan TPU yang dirancang khusus dan listrik beberapa ribu watt. Tapi ini memang menunjukkan betapa efisiennya metode pembelajaran yang tepat.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="universalitas-catur-shogi">Universalitas: Catur, Shogi<a href="#universalitas-catur-shogi" class="hash-link" aria-label="Taut langsung ke Universalitas: Catur, Shogi" title="Taut langsung ke Universalitas: Catur, Shogi" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="kelahiran-alphazero">Kelahiran AlphaZero<a href="#kelahiran-alphazero" class="hash-link" aria-label="Taut langsung ke Kelahiran AlphaZero" title="Taut langsung ke Kelahiran AlphaZero" translate="no">​</a></h3>
<p>Pada Desember 2017, DeepMind mengumumkan <strong>AlphaZero</strong>—versi universal AlphaGo Zero. Algoritma yang sama, hanya perlu memodifikasi aturan permainan, bisa mencapai level dunia tertinggi di tiga permainan papan:</p>
<table><thead><tr><th>Permainan</th><th>Waktu Pelatihan</th><th>Lawan</th><th>Rekor</th></tr></thead><tbody><tr><td>Go</td><td>8 jam</td><td>AlphaGo Zero</td><td>60:40</td></tr><tr><td>Catur</td><td>4 jam</td><td>Stockfish 8</td><td>28 menang 72 seri 0 kalah</td></tr><tr><td>Shogi</td><td>2 jam</td><td>Elmo</td><td>90:8:2</td></tr></tbody></table>
<p>Perhatikan lawan-lawannya:</p>
<ul>
<li class=""><strong>Stockfish</strong> adalah engine catur terkuat saat itu, menggunakan puluhan tahun pengetahuan manusia dan optimisasi</li>
<li class=""><strong>Elmo</strong> adalah AI shogi terkuat saat itu</li>
</ul>
<p>AlphaZero dengan pelatihan beberapa jam, melampaui sistem khusus yang dikembangkan bertahun-tahun ini.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="makna-universalitas">Makna Universalitas<a href="#makna-universalitas" class="hash-link" aria-label="Taut langsung ke Makna Universalitas" title="Taut langsung ke Makna Universalitas" translate="no">​</a></h3>
<p>AlphaGo Zero / AlphaZero membuktikan satu hal penting:</p>
<blockquote>
<p><strong>Algoritma pembelajaran yang sama, bisa mencapai level superhuman di berbagai domain.</strong></p>
</blockquote>
<p>Ini bukan tiga AI berbeda, tapi satu framework pembelajaran universal:</p>
<ol>
<li class=""><strong>Self-play</strong> menghasilkan pengalaman</li>
<li class=""><strong>Monte Carlo Tree Search</strong> mengeksplorasi kemungkinan</li>
<li class=""><strong>Neural Network</strong> mempelajari fungsi policy dan value</li>
<li class=""><strong>Reinforcement Learning</strong> mengoptimalkan fungsi objektif</li>
</ol>
<p>Framework ini tidak bergantung pada pengetahuan domain-specific, ini adalah langkah penting menuju AI yang lebih universal.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="dampak-terhadap-ai-tradisional">Dampak terhadap AI Tradisional<a href="#dampak-terhadap-ai-tradisional" class="hash-link" aria-label="Taut langsung ke Dampak terhadap AI Tradisional" title="Taut langsung ke Dampak terhadap AI Tradisional" translate="no">​</a></h3>
<p>Sebelum AlphaZero, AI terkuat untuk catur dan shogi semuanya bergaya &quot;expert system&quot;:</p>
<ul>
<li class=""><strong>Banyak pengetahuan manusia</strong>: Opening book, endgame tablebase, fungsi evaluasi</li>
<li class=""><strong>Optimisasi puluhan tahun</strong>: Hasil kerja keras tak terhitung pemain dan engineer</li>
<li class=""><strong>Sangat terspesialisasi</strong>: Stockfish tidak bisa bermain Go, Elmo tidak bisa bermain catur</li>
</ul>
<p>AlphaZero dengan satu algoritma universal melampaui semua ini dalam beberapa jam. Ini membuat banyak peneliti AI memikirkan ulang:</p>
<blockquote>
<p>Haruskah kita menginvestasikan lebih banyak usaha pada &quot;algoritma pembelajaran universal&quot;, atau &quot;pengkodean pengetahuan expert&quot;?</p>
</blockquote>
<p>Jawabannya tampaknya semakin jelas: membiarkan mesin belajar sendiri, lebih efektif daripada mengajarinya pengetahuan.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="gaya-bermain-alphago-zero">Gaya Bermain AlphaGo Zero<a href="#gaya-bermain-alphago-zero" class="hash-link" aria-label="Taut langsung ke Gaya Bermain AlphaGo Zero" title="Taut langsung ke Gaya Bermain AlphaGo Zero" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="melampaui-estetika-manusia">Melampaui Estetika Manusia<a href="#melampaui-estetika-manusia" class="hash-link" aria-label="Taut langsung ke Melampaui Estetika Manusia" title="Taut langsung ke Melampaui Estetika Manusia" translate="no">​</a></h3>
<p>Komunitas Go memiliki evaluasi umum terhadap langkah-langkah AlphaGo Zero: <strong>lebih indah</strong>.</p>
<p>Langkah-langkah AlphaGo Lee kadang terlihat &quot;aneh&quot;—seperti langkah ke-37, manusia perlu analisis setelahnya baru memahami keindahannya. Tapi langkah-langkah AlphaGo Zero sering dievaluasi sebagai &quot;langsung terlihat sebagai langkah bagus&quot; setelahnya.</p>
<p>Ini mungkin karena:</p>
<ol>
<li class=""><strong>Kekuatan bermain lebih tinggi</strong>: Zero bisa melihat lebih dalam, langkah lebih tenang</li>
<li class=""><strong>Tanpa bias manusia</strong>: Tidak terikat oleh joseki tradisional</li>
<li class=""><strong>Tujuan yang konsisten</strong>: Hanya mengejar win rate, tidak meniru manusia</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="menemukan-kembali-prinsip-go-manusia">Menemukan Kembali Prinsip Go Manusia<a href="#menemukan-kembali-prinsip-go-manusia" class="hash-link" aria-label="Taut langsung ke Menemukan Kembali Prinsip Go Manusia" title="Taut langsung ke Menemukan Kembali Prinsip Go Manusia" translate="no">​</a></h3>
<p>Menariknya, AlphaGo Zero dalam proses pelatihan &quot;menemukan kembali&quot; pengetahuan Go yang diakumulasi manusia selama ribuan tahun:</p>
<ul>
<li class=""><strong>Joseki</strong>: Zero menemukan sendiri banyak joseki umum, karena ini memang solusi optimal untuk kedua belah pihak</li>
<li class=""><strong>Prinsip opening</strong>: Urutan pentingnya sudut, sisi, tengah</li>
<li class=""><strong>Pengetahuan bentuk</strong>: Perbedaan antara bentuk buruk dan bentuk bagus</li>
</ul>
<p>Ini memvalidasi rasionalitas prinsip Go manusia—pengetahuan ini bukan kebetulan, tapi refleksi dari esensi Go.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="inovasi-melampaui-manusia">Inovasi Melampaui Manusia<a href="#inovasi-melampaui-manusia" class="hash-link" aria-label="Taut langsung ke Inovasi Melampaui Manusia" title="Taut langsung ke Inovasi Melampaui Manusia" translate="no">​</a></h3>
<p>Tapi Zero juga menemukan langkah yang tidak pernah terpikirkan manusia:</p>
<ul>
<li class=""><strong>Opening non-konvensional</strong>: Variasi pada dasar opening tradisional</li>
<li class=""><strong>Korban agresif</strong>: Lebih bersedia daripada manusia untuk mengorbankan lokal demi keuntungan global</li>
<li class=""><strong>Bentuk counter-intuitif</strong>: Bentuk yang tampak &quot;buruk&quot; ternyata adalah solusi optimal</li>
</ul>
<p>Inovasi-inovasi ini sedang mengubah pemahaman manusia tentang Go. Banyak pemain profesional mengatakan, mempelajari catatan permainan AlphaGo Zero memberi mereka pemahaman baru tentang Go.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="ringkasan-detail-teknis">Ringkasan Detail Teknis<a href="#ringkasan-detail-teknis" class="hash-link" aria-label="Taut langsung ke Ringkasan Detail Teknis" title="Taut langsung ke Ringkasan Detail Teknis" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="perbandingan-lengkap-dengan-alphago-original">Perbandingan Lengkap dengan AlphaGo Original<a href="#perbandingan-lengkap-dengan-alphago-original" class="hash-link" aria-label="Taut langsung ke Perbandingan Lengkap dengan AlphaGo Original" title="Taut langsung ke Perbandingan Lengkap dengan AlphaGo Original" translate="no">​</a></h3>
<table><thead><tr><th>Aspek</th><th>AlphaGo (Original)</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td><strong>Data pelatihan</strong></td><td>Catatan manusia + self-play</td><td>Self-play murni</td></tr><tr><td><strong>Metode pembelajaran</strong></td><td>Supervised learning + Reinforcement learning</td><td>Reinforcement learning murni</td></tr><tr><td><strong>Fitur input</strong></td><td>48 plane</td><td>17 plane</td></tr><tr><td><strong>Arsitektur network</strong></td><td>Policy/Value terpisah</td><td>Dual-head ResNet</td></tr><tr><td><strong>Kedalaman network</strong></td><td>13 layer</td><td>40 layer (atau lebih)</td></tr><tr><td><strong>Evaluasi MCTS</strong></td><td>Neural network + Rollout</td><td>Neural network murni</td></tr><tr><td><strong>Jumlah pencarian</strong></td><td>~100,000 per langkah</td><td>~1,600 per langkah</td></tr><tr><td><strong>TPU pelatihan</strong></td><td>50+</td><td>4</td></tr><tr><td><strong>TPU inferensi</strong></td><td>48</td><td>4 (bisa diperluas)</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="algoritma-inti">Algoritma Inti<a href="#algoritma-inti" class="hash-link" aria-label="Taut langsung ke Algoritma Inti" title="Taut langsung ke Algoritma Inti" translate="no">​</a></h3>
<p>Loop pelatihan AlphaGo Zero sangat simpel:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. Self-play</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Lakukan MCTS dengan network saat ini</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Pilih langkah berdasarkan probabilitas pencarian MCTS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Catat setiap langkah (posisi, probabilitas MCTS, hasil menang/kalah)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Latih Network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Ambil sampel dari experience pool</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Policy Head: Minimalkan cross-entropy dengan probabilitas MCTS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Value Head: Minimalkan mean squared error dengan hasil menang/kalah aktual</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Optimalkan kedua tujuan secara joint</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Update Network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Ganti network lama dengan network baru (verifikasi network baru lebih kuat melalui pertandingan)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Kembali ke langkah 1</span><br></span></code></pre></div></div>
<p>Loop ini terus berjalan, network terus menjadi lebih kuat. Tanpa data manusia, tanpa pengetahuan manusia, hanya aturan permainan dan tujuan menang/kalah.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="implikasi-untuk-penelitian-ai">Implikasi untuk Penelitian AI<a href="#implikasi-untuk-penelitian-ai" class="hash-link" aria-label="Taut langsung ke Implikasi untuk Penelitian AI" title="Taut langsung ke Implikasi untuk Penelitian AI" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="pembelajaran-prinsip-pertama">Pembelajaran Prinsip Pertama<a href="#pembelajaran-prinsip-pertama" class="hash-link" aria-label="Taut langsung ke Pembelajaran Prinsip Pertama" title="Taut langsung ke Pembelajaran Prinsip Pertama" translate="no">​</a></h3>
<p>AlphaGo Zero mendemonstrasikan metode pembelajaran &quot;prinsip pertama&quot;:</p>
<blockquote>
<p>Jangan beritahu AI cara melakukan, hanya beritahu apa tujuannya, biarkan ia menemukan caranya sendiri.</p>
</blockquote>
<p>Ini membentuk kontras tajam dengan pendekatan expert system tradisional. Expert system mencoba mengkodekan pengetahuan manusia ke dalam AI, sedangkan AlphaGo Zero membiarkan AI menemukan pengetahuan sendiri.</p>
<p>Hasilnya: pengetahuan yang ditemukan AI mungkin lebih lengkap dan akurat daripada pengetahuan manusia.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="kekuatan-self-play">Kekuatan Self-Play<a href="#kekuatan-self-play" class="hash-link" aria-label="Taut langsung ke Kekuatan Self-Play" title="Taut langsung ke Kekuatan Self-Play" translate="no">​</a></h3>
<p>AlphaGo Zero membuktikan self-play bisa menghasilkan data pelatihan tak terbatas, dan kualitas data ini akan meningkat seiring peningkatan network.</p>
<p>Ini adalah &quot;siklus positif&quot;:</p>
<ul>
<li class="">Network lebih kuat → Data self-play lebih baik</li>
<li class="">Data lebih baik → Network lebih kuat</li>
</ul>
<p>Siklus ini bisa terus berjalan, sampai mencapai batas teori permainan (jika ada).</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="pentingnya-penyederhanaan">Pentingnya Penyederhanaan<a href="#pentingnya-penyederhanaan" class="hash-link" aria-label="Taut langsung ke Pentingnya Penyederhanaan" title="Taut langsung ke Pentingnya Penyederhanaan" translate="no">​</a></h3>
<p>Keberhasilan AlphaGo Zero membuktikan pentingnya &quot;penyederhanaan&quot;:</p>
<ul>
<li class="">Sederhanakan input (48 → 17)</li>
<li class="">Sederhanakan arsitektur (dual network → single network)</li>
<li class="">Sederhanakan pelatihan (supervised + reinforcement → reinforcement murni)</li>
</ul>
<p>Setiap penyederhanaan membuat sistem lebih powerful. Ini memberitahu kita: kompleks tidak berarti bagus, solusi paling sederhana seringkali yang terbaik.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="korespondensi-animasi">Korespondensi Animasi<a href="#korespondensi-animasi" class="hash-link" aria-label="Taut langsung ke Korespondensi Animasi" title="Taut langsung ke Korespondensi Animasi" translate="no">​</a></h2>
<p>Konsep inti yang dibahas dalam artikel ini dan nomor animasinya:</p>
<table><thead><tr><th>Nomor</th><th>Konsep</th><th>Korespondensi Fisika/Matematika</th></tr></thead><tbody><tr><td>E5 E7</td><td>Pelatihan dari nol</td><td>Fenomena self-organization</td></tr><tr><td>E5 E5</td><td>Self-play</td><td>Konvergensi fixed-point</td></tr><tr><td>E5 E12</td><td>Kurva pertumbuhan kekuatan</td><td>Pertumbuhan S-shape</td></tr><tr><td>E5 D12</td><td>Residual network</td><td>Highway gradien</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="bacaan-lanjutan">Bacaan Lanjutan<a href="#bacaan-lanjutan" class="hash-link" aria-label="Taut langsung ke Bacaan Lanjutan" title="Taut langsung ke Bacaan Lanjutan" translate="no">​</a></h2>
<ul>
<li class=""><strong>Artikel Berikutnya</strong>: <a class="" href="/id/docs/alphago/dual-head-resnet/">Dual-Head Network dan Residual Network</a> — Penjelasan detail arsitektur neural network AlphaGo Zero</li>
<li class=""><strong>Artikel Terkait</strong>: <a class="" href="/id/docs/alphago/self-play/">Self-Play</a> — Mengapa self-play bisa menghasilkan level superhuman</li>
<li class=""><strong>Teknis Mendalam</strong>: <a class="" href="/id/docs/alphago/training-from-scratch/">Proses Pelatihan dari Nol</a> — Evolusi detail Hari 0-3</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="referensi">Referensi<a href="#referensi" class="hash-link" aria-label="Taut langsung ke Referensi" title="Taut langsung ke Referensi" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">Silver, D., et al. (2018). &quot;A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.&quot; <em>Science</em>, 362(6419), 1140-1144.</li>
<li class="">DeepMind. (2017). &quot;AlphaGo Zero: Starting from scratch.&quot; <em>DeepMind Blog</em>.</li>
<li class="">Schrittwieser, J., et al. (2020). &quot;Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.&quot; <em>Nature</em>, 588, 604-609.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/16-alphago-zero.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Sunting halaman ini</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Halaman dokumentasi"><a class="pagination-nav__link pagination-nav__link--prev" href="/id/docs/alphago/puct-formula/"><div class="pagination-nav__sublabel">Sebelum</div><div class="pagination-nav__label">Penjelasan Detail Rumus PUCT</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/id/docs/alphago/dual-head-resnet/"><div class="pagination-nav__sublabel">Berikut</div><div class="pagination-nav__label">Dual-Head Network dan Residual Network</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#mengapa-tidak-memerlukan-catatan-permainan-manusia" class="table-of-contents__link toc-highlight">Mengapa Tidak Memerlukan Catatan Permainan Manusia?</a><ul><li><a href="#keterbatasan-catatan-permainan-manusia" class="table-of-contents__link toc-highlight">Keterbatasan Catatan Permainan Manusia</a></li><li><a href="#terobosan-zero" class="table-of-contents__link toc-highlight">Terobosan Zero</a></li></ul></li><li><a href="#perbandingan-dengan-alphago-original-1000" class="table-of-contents__link toc-highlight">Perbandingan dengan AlphaGo Original: 100:0</a><ul><li><a href="#kemenangan-telak" class="table-of-contents__link toc-highlight">Kemenangan Telak</a></li><li><a href="#lebih-sedikit-sumber-daya-kekuatan-bermain-lebih-tinggi" class="table-of-contents__link toc-highlight">Lebih Sedikit Sumber Daya, Kekuatan Bermain Lebih Tinggi</a></li><li><a href="#mengapa-zero-lebih-kuat" class="table-of-contents__link toc-highlight">Mengapa Zero Lebih Kuat?</a></li></ul></li><li><a href="#fitur-input-yang-disederhanakan-dari-48-ke-17" class="table-of-contents__link toc-highlight">Fitur Input yang Disederhanakan: Dari 48 ke 17</a><ul><li><a href="#48-plane-fitur-alphago-original" class="table-of-contents__link toc-highlight">48 Plane Fitur AlphaGo Original</a></li><li><a href="#17-plane-fitur-alphago-zero" class="table-of-contents__link toc-highlight">17 Plane Fitur AlphaGo Zero</a></li><li><a href="#mengapa-penyederhanaan-itu-bagus" class="table-of-contents__link toc-highlight">Mengapa Penyederhanaan Itu Bagus?</a></li></ul></li><li><a href="#arsitektur-network-tunggal" class="table-of-contents__link toc-highlight">Arsitektur Network Tunggal</a><ul><li><a href="#desain-dual-network-original" class="table-of-contents__link toc-highlight">Desain Dual Network Original</a></li><li><a href="#dual-head-network-zero" class="table-of-contents__link toc-highlight">Dual-Head Network Zero</a></li><li><a href="#kekuatan-residual-network" class="table-of-contents__link toc-highlight">Kekuatan Residual Network</a></li></ul></li><li><a href="#peningkatan-efisiensi-pelatihan" class="table-of-contents__link toc-highlight">Peningkatan Efisiensi Pelatihan</a><ul><li><a href="#pertumbuhan-eksponensial-self-play" class="table-of-contents__link toc-highlight">Pertumbuhan Eksponensial Self-Play</a></li><li><a href="#mengapa-secepat-ini" class="table-of-contents__link toc-highlight">Mengapa Secepat Ini?</a></li><li><a href="#perbandingan-dengan-pembelajaran-manusia" class="table-of-contents__link toc-highlight">Perbandingan dengan Pembelajaran Manusia</a></li></ul></li><li><a href="#universalitas-catur-shogi" class="table-of-contents__link toc-highlight">Universalitas: Catur, Shogi</a><ul><li><a href="#kelahiran-alphazero" class="table-of-contents__link toc-highlight">Kelahiran AlphaZero</a></li><li><a href="#makna-universalitas" class="table-of-contents__link toc-highlight">Makna Universalitas</a></li><li><a href="#dampak-terhadap-ai-tradisional" class="table-of-contents__link toc-highlight">Dampak terhadap AI Tradisional</a></li></ul></li><li><a href="#gaya-bermain-alphago-zero" class="table-of-contents__link toc-highlight">Gaya Bermain AlphaGo Zero</a><ul><li><a href="#melampaui-estetika-manusia" class="table-of-contents__link toc-highlight">Melampaui Estetika Manusia</a></li><li><a href="#menemukan-kembali-prinsip-go-manusia" class="table-of-contents__link toc-highlight">Menemukan Kembali Prinsip Go Manusia</a></li><li><a href="#inovasi-melampaui-manusia" class="table-of-contents__link toc-highlight">Inovasi Melampaui Manusia</a></li></ul></li><li><a href="#ringkasan-detail-teknis" class="table-of-contents__link toc-highlight">Ringkasan Detail Teknis</a><ul><li><a href="#perbandingan-lengkap-dengan-alphago-original" class="table-of-contents__link toc-highlight">Perbandingan Lengkap dengan AlphaGo Original</a></li><li><a href="#algoritma-inti" class="table-of-contents__link toc-highlight">Algoritma Inti</a></li></ul></li><li><a href="#implikasi-untuk-penelitian-ai" class="table-of-contents__link toc-highlight">Implikasi untuk Penelitian AI</a><ul><li><a href="#pembelajaran-prinsip-pertama" class="table-of-contents__link toc-highlight">Pembelajaran Prinsip Pertama</a></li><li><a href="#kekuatan-self-play" class="table-of-contents__link toc-highlight">Kekuatan Self-Play</a></li><li><a href="#pentingnya-penyederhanaan" class="table-of-contents__link toc-highlight">Pentingnya Penyederhanaan</a></li></ul></li><li><a href="#korespondensi-animasi" class="table-of-contents__link toc-highlight">Korespondensi Animasi</a></li><li><a href="#bacaan-lanjutan" class="table-of-contents__link toc-highlight">Bacaan Lanjutan</a></li><li><a href="#referensi" class="table-of-contents__link toc-highlight">Referensi</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>