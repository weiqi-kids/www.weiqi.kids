"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[897],{90301(a,e,n){n.r(e),n.d(e,{assets:()=>o,contentTitle:()=>d,default:()=>p,frontMatter:()=>l,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"alphago/explained/supervised-learning","title":"Tahap Supervised Learning","description":"Bagaimana AlphaGo belajar dari 30 juta catatan permainan manusia, mencapai akurasi prediksi 57%","source":"@site/i18n/id/docusaurus-plugin-content-docs/current/alphago/explained/11-supervised-learning.mdx","sourceDirName":"alphago/explained","slug":"/alphago/explained/supervised-learning","permalink":"/id/docs/alphago/explained/supervised-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/explained/11-supervised-learning.mdx","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12,"title":"Tahap Supervised Learning","description":"Bagaimana AlphaGo belajar dari 30 juta catatan permainan manusia, mencapai akurasi prediksi 57%"},"sidebar":"tutorialSidebar","previous":{"title":"CNN dan Permainan Go","permalink":"/id/docs/alphago/explained/cnn-and-go"},"next":{"title":"Pengantar Reinforcement Learning","permalink":"/id/docs/alphago/explained/reinforcement-intro"}}');var i=n(62615),r=n(30416),s=n(45695);const l={sidebar_position:12,title:"Tahap Supervised Learning",description:"Bagaimana AlphaGo belajar dari 30 juta catatan permainan manusia, mencapai akurasi prediksi 57%"},d="Tahap Supervised Learning",o={},c=[{value:"Mengapa Memulai dari Catatan Permainan Manusia?",id:"mengapa-memulai-dari-catatan-permainan-manusia",level:2},{value:"Titik Awal Pembelajaran",id:"titik-awal-pembelajaran",level:3},{value:"Nilai Catatan Permainan Manusia",id:"nilai-catatan-permainan-manusia",level:3},{value:"Sumber Data Pelatihan",id:"sumber-data-pelatihan",level:2},{value:"KGS Go Server",id:"kgs-go-server",level:3},{value:"Karakteristik KGS",id:"karakteristik-kgs",level:4},{value:"Mengapa Memilih KGS?",id:"mengapa-memilih-kgs",level:4},{value:"30 Juta Posisi",id:"30-juta-posisi",level:3},{value:"Format Data",id:"format-data",level:3},{value:"Pra-pemrosesan Data",id:"pra-pemrosesan-data",level:2},{value:"Parsing SGF",id:"parsing-sgf",level:3},{value:"Ekstraksi Fitur",id:"ekstraksi-fitur",level:3},{value:"Augmentasi Data",id:"augmentasi-data",level:3},{value:"Fungsi Loss",id:"fungsi-loss",level:2},{value:"Cross-Entropy Loss",id:"cross-entropy-loss",level:3},{value:"Pemahaman Intuitif",id:"pemahaman-intuitif",level:3},{value:"Perbandingan dengan MSE",id:"perbandingan-dengan-mse",level:3},{value:"Proses Pelatihan",id:"proses-pelatihan",level:2},{value:"Konfigurasi Hardware",id:"konfigurasi-hardware",level:3},{value:"Optimizer",id:"optimizer",level:3},{value:"Mengapa Menggunakan SGD daripada Adam?",id:"mengapa-menggunakan-sgd-daripada-adam",level:4},{value:"Jadwal Learning Rate",id:"jadwal-learning-rate",level:3},{value:"Loop Pelatihan",id:"loop-pelatihan",level:3},{value:"Kurva Pelatihan",id:"kurva-pelatihan",level:3},{value:"Analisis Hasil",id:"analisis-hasil",level:2},{value:"Akurasi 57%",id:"akurasi-57",level:3},{value:"Apa itu Akurasi Top-1?",id:"apa-itu-akurasi-top-1",level:4},{value:"Perbandingan dengan Program Lain",id:"perbandingan-dengan-program-lain",level:3},{value:"Evaluasi Kekuatan",id:"evaluasi-kekuatan",level:3},{value:"Akurasi vs Kekuatan",id:"akurasi-vs-kekuatan",level:3},{value:"Keterbatasan Supervised Learning",id:"keterbatasan-supervised-learning",level:2},{value:"Masalah 1: Efek Ceiling",id:"masalah-1-efek-ceiling",level:3},{value:"Masalah 2: Tidak Dapat Membedakan Langkah Bagus dan Buruk",id:"masalah-2-tidak-dapat-membedakan-langkah-bagus-dan-buruk",level:3},{value:"Masalah 3: Eksplorasi yang Tidak Memadai",id:"masalah-3-eksplorasi-yang-tidak-memadai",level:3},{value:"Solusi: Reinforcement Learning",id:"solusi-reinforcement-learning",level:3},{value:"Poin Implementasi",id:"poin-implementasi",level:2},{value:"Kode Pelatihan Lengkap",id:"kode-pelatihan-lengkap",level:3},{value:"Kode Evaluasi",id:"kode-evaluasi",level:3},{value:"Masalah Umum dan Solusi",id:"masalah-umum-dan-solusi",level:3},{value:"Korespondensi Animasi",id:"korespondensi-animasi",level:2},{value:"Bacaan Lanjutan",id:"bacaan-lanjutan",level:2},{value:"Poin Penting",id:"poin-penting",level:2},{value:"Referensi",id:"referensi",level:2}];function h(a){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...a.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"tahap-supervised-learning",children:"Tahap Supervised Learning"})}),"\n",(0,i.jsxs)(e.p,{children:['Sebelum AlphaGo dapat bermain melawan dirinya sendiri, ia perlu "melihat" sejumlah besar catatan permainan manusia terlebih dahulu. Proses ini disebut ',(0,i.jsx)(e.strong,{children:"supervised learning"}),"."]}),"\n",(0,i.jsxs)(e.p,{children:["Dengan menganalisis 30 juta posisi permainan manusia, Policy Network AlphaGo mencapai ",(0,i.jsx)(e.strong,{children:"akurasi prediksi 57%"}),"\u2014mampu menebak langkah selanjutnya pemain ahli manusia lebih dari separuh waktu."]}),"\n",(0,i.jsx)(e.p,{children:"Ini mungkin tidak terdengar mengejutkan, tetapi mengingat setiap posisi rata-rata memiliki 250 langkah legal, ini adalah pencapaian yang luar biasa."}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"mengapa-memulai-dari-catatan-permainan-manusia",children:"Mengapa Memulai dari Catatan Permainan Manusia?"}),"\n",(0,i.jsx)(e.h3,{id:"titik-awal-pembelajaran",children:"Titik Awal Pembelajaran"}),"\n",(0,i.jsx)(e.p,{children:"Bayangkan Anda ingin mengajar seseorang yang sama sekali tidak tahu Go cara bermain. Apa yang akan Anda lakukan?"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Rencana A: Eksplorasi Acak"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Biarkan dia bermain sembarangan, perlahan menemukan apa yang merupakan langkah bagus\n\u2192 Efisiensi sangat rendah, mungkin tidak pernah bisa belajar\n"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Rencana B: Lihat Bagaimana Ahli Bermain"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Biarkan dia menonton banyak pertandingan pemain profesional, meniru cara mereka bermain\n\u2192 Setelah memiliki dasar, baru eksplorasi sendiri\n"})}),"\n",(0,i.jsx)(e.p,{children:'AlphaGo memilih Rencana B. Supervised learning adalah versi matematis dari "melihat bagaimana ahli bermain".'}),"\n",(0,i.jsx)(e.h3,{id:"nilai-catatan-permainan-manusia",children:"Nilai Catatan Permainan Manusia"}),"\n",(0,i.jsx)(e.p,{children:"Manusia telah menghabiskan ribuan tahun mengembangkan teori Go. Pengetahuan ini semua dikodekan dalam catatan permainan:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Joseki pembukaan"}),": Cara pembukaan yang telah diverifikasi dalam jangka panjang"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Taktik permainan tengah"}),": Kebijaksanaan transisi serangan dan pertahanan"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Teknik yose"}),": Esensi perhitungan poin"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Visi besar"}),": Intuisi penilaian keseluruhan"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:'Supervised learning memungkinkan AlphaGo "mewarisi" kebijaksanaan manusia ini, tanpa perlu meraba-raba dari nol.'}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"sumber-data-pelatihan",children:"Sumber Data Pelatihan"}),"\n",(0,i.jsx)(e.h3,{id:"kgs-go-server",children:"KGS Go Server"}),"\n",(0,i.jsxs)(e.p,{children:["Data pelatihan AlphaGo terutama berasal dari ",(0,i.jsx)(e.strong,{children:"KGS Go Server"})," (juga dikenal sebagai Kiseido Go Server), platform Go online yang terkenal."]}),"\n",(0,i.jsx)(e.h4,{id:"karakteristik-kgs",children:"Karakteristik KGS"}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Fitur"}),(0,i.jsx)(e.th,{children:"Deskripsi"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Pengguna"}),(0,i.jsx)(e.td,{children:"Terutama pemain amatir, juga ada pemain profesional"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Rentang kekuatan"}),(0,i.jsx)(e.td,{children:"Dari pemula hingga 9 dan profesional"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Catatan permainan"}),(0,i.jsx)(e.td,{children:"Menyimpan catatan SGF lengkap"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Periode aktif"}),(0,i.jsx)(e.td,{children:"2000 hingga sekarang"})]})]})]}),"\n",(0,i.jsx)(e.h4,{id:"mengapa-memilih-kgs",children:"Mengapa Memilih KGS?"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Volume data besar"}),": Jutaan catatan permainan"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Format seragam"}),": Format SGF mudah diurai"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Memiliki label kekuatan"}),": Setiap pengguna memiliki peringkat"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Keragaman"}),": Pemain dengan gaya berbeda"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"30-juta-posisi",children:"30 Juta Posisi"}),"\n",(0,i.jsxs)(e.p,{children:["Dari catatan permainan KGS, DeepMind mengekstrak sekitar ",(0,i.jsx)(e.strong,{children:"30 juta posisi"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Data mentah:\n- Sekitar 160.000 catatan permainan\n- Setiap permainan sekitar 200 langkah\n- Total ~32 juta posisi\n\nPenyaringan data:\n- Filter permainan peringkat rendah\n- Filter posisi menyerah di tengah permainan\n- Akhirnya sekitar 30 juta posisi berkualitas tinggi\n"})}),"\n",(0,i.jsx)(e.h3,{id:"format-data",children:"Format Data"}),"\n",(0,i.jsx)(e.p,{children:"Setiap sampel pelatihan berisi:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'{\n    "board_state": [[0, 1, 2, ...], ...],  # Papan 19\xd719\n    "features": [...],                      # 48 bidang fitur\n    "next_move": 123,                       # Posisi yang dimainkan manusia (0-360)\n    "game_result": 1,                       # 1=hitam menang, -1=putih menang\n    "player_rank": "5d",                    # Peringkat pemain yang memainkan langkah ini\n}\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"pra-pemrosesan-data",children:"Pra-pemrosesan Data"}),"\n",(0,i.jsx)(e.h3,{id:"parsing-sgf",children:"Parsing SGF"}),"\n",(0,i.jsx)(e.p,{children:"SGF (Smart Game Format) adalah format standar untuk catatan permainan Go:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"(;GM[1]FF[4]CA[UTF-8]AP[CGoban:3]ST[2]\nRU[Japanese]SZ[19]KM[6.50]\nPW[White]PB[Black]\n;B[pd];W[dd];B[pq];W[dp];B[qk];W[nc]...\n)\n"})}),"\n",(0,i.jsx)(e.p,{children:"Perlu mengurai:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Ukuran papan (SZ[19])"}),"\n",(0,i.jsx)(e.li,{children:"Setiap langkah (B[pd], W[dd]...)"}),"\n",(0,i.jsx)(e.li,{children:"Hasil permainan (RE[B+2.5])"}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"def parse_sgf(sgf_string):\n    \"\"\"Mengurai catatan permainan SGF\"\"\"\n    moves = []\n    # Ekstrak semua langkah\n    pattern = r';([BW])\\[([a-s]{2})\\]'\n    for match in re.finditer(pattern, sgf_string):\n        color = match.group(1)  # 'B' or 'W'\n        coord = match.group(2)  # 'pd', 'dd', etc.\n\n        # Konversi koordinat\n        x = ord(coord[0]) - ord('a')\n        y = ord(coord[1]) - ord('a')\n\n        moves.append((color, x, y))\n\n    return moves\n"})}),"\n",(0,i.jsx)(e.h3,{id:"ekstraksi-fitur",children:"Ekstraksi Fitur"}),"\n",(0,i.jsxs)(e.p,{children:["Untuk setiap posisi, ekstrak 48 bidang fitur (lihat detail di ",(0,i.jsx)(e.a,{href:"../input-features",children:"Desain Fitur Input"}),"):"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def extract_features(board, history, current_player):\n    """Ekstrak 48 bidang fitur"""\n    features = np.zeros((48, 19, 19))\n\n    # Posisi batu\n    features[0] = (board == 1)  # Batu hitam\n    features[1] = (board == 2)  # Batu putih\n    features[2] = (board == 0)  # Titik kosong\n\n    # Riwayat\n    for i, hist in enumerate(history[:8]):\n        features[3+i] = (hist == 1)\n        features[11+i] = (hist == 2)\n\n    # Kebebasan, atari, tangkapan tangga, dll...\n    # (implementasi detail dihilangkan)\n\n    return features\n'})}),"\n",(0,i.jsx)(e.h3,{id:"augmentasi-data",children:"Augmentasi Data"}),"\n",(0,i.jsxs)(e.p,{children:["Papan Go memiliki ",(0,i.jsx)(e.strong,{children:"8-fold symmetry"})," (4 rotasi x 2 refleksi). Setiap sampel asli dapat menjadi 8:"]}),"\n",(0,i.jsx)(e.mermaid,{value:'flowchart LR\n    A["Asli"] --\x3e B["Rotasi 90\xb0"] --\x3e C["Rotasi 180\xb0"] --\x3e D["Rotasi 270\xb0"]'}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.em,{children:"Masing-masing di-flip horizontal lagi, mendapatkan 8 sampel pelatihan yang setara"})}),"\n",(0,i.jsx)(e.p,{children:"Ini meningkatkan data pelatihan efektif 8 kali lipat, sekaligus memastikan model mempelajari pola yang tidak bergantung pada arah tertentu."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def augment(state, action):\n    """Augmentasi 8-fold symmetry"""\n    augmented = []\n\n    for rotation in [0, 1, 2, 3]:  # 0, 90, 180, 270 derajat\n        rotated_state = np.rot90(state, rotation, axes=(1, 2))\n        rotated_action = rotate_action(action, rotation)\n        augmented.append((rotated_state, rotated_action))\n\n        # Flip horizontal\n        flipped_state = np.flip(rotated_state, axis=2)\n        flipped_action = flip_action(rotated_action)\n        augmented.append((flipped_state, flipped_action))\n\n    return augmented\n'})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"fungsi-loss",children:"Fungsi Loss"}),"\n",(0,i.jsx)(e.h3,{id:"cross-entropy-loss",children:"Cross-Entropy Loss"}),"\n",(0,i.jsxs)(e.p,{children:["Supervised learning menggunakan ",(0,i.jsx)(e.strong,{children:"Cross-Entropy Loss"})," untuk melatih Policy Network:"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"L(\u03b8) = -\u03a3 log p_\u03b8(a | s)\n"})}),"\n",(0,i.jsx)(e.p,{children:"Di mana:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.code,{children:"s"}),": Status papan"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.code,{children:"a"}),": Posisi yang sebenarnya dimainkan manusia (label)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.code,{children:"p_\u03b8(a | s)"}),": Probabilitas model memprediksi posisi tersebut"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"pemahaman-intuitif",children:"Pemahaman Intuitif"}),"\n",(0,i.jsx)(e.p,{children:'Cross-entropy loss mengukur "perbedaan antara prediksi model dan label":'}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Skenario"}),(0,i.jsx)(e.th,{children:"Prediksi Model"}),(0,i.jsx)(e.th,{children:"Loss"}),(0,i.jsx)(e.th,{children:"Deskripsi"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Prediksi sempurna"}),(0,i.jsx)(e.td,{children:"probabilitas a = 1.0"}),(0,i.jsx)(e.td,{children:"0"}),(0,i.jsx)(e.td,{children:"Terbaik"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Yakin dan benar"}),(0,i.jsx)(e.td,{children:"probabilitas a = 0.9"}),(0,i.jsx)(e.td,{children:"0.1"}),(0,i.jsx)(e.td,{children:"Sangat bagus"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Tidak yakin tapi benar"}),(0,i.jsx)(e.td,{children:"probabilitas a = 0.5"}),(0,i.jsx)(e.td,{children:"0.7"}),(0,i.jsx)(e.td,{children:"Cukup"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Prediksi salah"}),(0,i.jsx)(e.td,{children:"probabilitas a = 0.1"}),(0,i.jsx)(e.td,{children:"2.3"}),(0,i.jsx)(e.td,{children:"Sangat buruk"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Sepenuhnya salah"}),(0,i.jsx)(e.td,{children:"probabilitas a = 0.01"}),(0,i.jsx)(e.td,{children:"4.6"}),(0,i.jsx)(e.td,{children:"Terburuk"})]})]})]}),"\n",(0,i.jsx)(e.p,{children:"Fungsi loss mendorong model untuk meningkatkan probabilitas posisi yang benar."}),"\n",(0,i.jsx)(e.h3,{id:"perbandingan-dengan-mse",children:"Perbandingan dengan MSE"}),"\n",(0,i.jsx)(e.p,{children:"Mengapa tidak menggunakan Mean Squared Error (MSE)?"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# MSE:\nloss_mse = (prediction - target)^2\n\n# Cross-Entropy:\nloss_ce = -log(prediction[target])\n"})}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Karakteristik"}),(0,i.jsx)(e.th,{children:"MSE"}),(0,i.jsx)(e.th,{children:"Cross-Entropy"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Tipe target"}),(0,i.jsx)(e.td,{children:"Regresi (nilai kontinu)"}),(0,i.jsx)(e.td,{children:"Klasifikasi (distribusi probabilitas)"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Perilaku gradien"}),(0,i.jsx)(e.td,{children:"Error lebih besar, gradien lebih besar"}),(0,i.jsx)(e.td,{children:"Ketika yakin tapi salah, gradien lebih besar"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Skenario cocok"}),(0,i.jsx)(e.td,{children:"Value Network"}),(0,i.jsx)(e.td,{children:"Policy Network"})]})]})]}),"\n",(0,i.jsx)(e.p,{children:"Policy Network menghasilkan distribusi probabilitas dari 361 kelas, cross-entropy adalah pilihan alami."}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"proses-pelatihan",children:"Proses Pelatihan"}),"\n",(0,i.jsx)(e.h3,{id:"konfigurasi-hardware",children:"Konfigurasi Hardware"}),"\n",(0,i.jsx)(e.p,{children:"DeepMind menggunakan sumber daya komputasi yang besar:"}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Sumber Daya"}),(0,i.jsx)(e.th,{children:"Jumlah"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"GPU"}),(0,i.jsx)(e.td,{children:"50"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Waktu pelatihan"}),(0,i.jsx)(e.td,{children:"Sekitar 3 minggu"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Ukuran batch"}),(0,i.jsx)(e.td,{children:"16"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Total langkah pelatihan"}),(0,i.jsx)(e.td,{children:"~340M"})]})]})]}),"\n",(0,i.jsx)(e.h3,{id:"optimizer",children:"Optimizer"}),"\n",(0,i.jsxs)(e.p,{children:["Menggunakan ",(0,i.jsx)(e.strong,{children:"Stochastic Gradient Descent (SGD) + Momentum"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"optimizer = torch.optim.SGD(\n    model.parameters(),\n    lr=0.003,         # Learning rate awal\n    momentum=0.9,     # Koefisien momentum\n    weight_decay=1e-4 # Regularisasi L2\n)\n"})}),"\n",(0,i.jsx)(e.h4,{id:"mengapa-menggunakan-sgd-daripada-adam",children:"Mengapa Menggunakan SGD daripada Adam?"}),"\n",(0,i.jsx)(e.p,{children:"Pada 2016, SGD + momentum masih merupakan pilihan utama untuk tugas gambar. Sebenarnya, penelitian selanjutnya (termasuk KataGo) menemukan bahwa optimizer tipe Adam mungkin lebih baik."}),"\n",(0,i.jsx)(e.h3,{id:"jadwal-learning-rate",children:"Jadwal Learning Rate"}),"\n",(0,i.jsx)(e.p,{children:"Learning rate secara bertahap menurun selama proses pelatihan:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=80_000_000,  # Setiap 80M langkah\n    gamma=0.1              # Learning rate dikalikan 0.1\n)\n"})}),"\n",(0,i.jsx)(e.mermaid,{value:'flowchart LR\n    A["0.003<br/>(0-80M langkah)"] --\x3e B["0.0003<br/>(80M-160M langkah)"] --\x3e C["0.00003<br/>(160M-240M langkah)"]'}),"\n",(0,i.jsx)(e.h3,{id:"loop-pelatihan",children:"Loop Pelatihan"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"def train_epoch(model, dataloader, optimizer):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    for batch in dataloader:\n        states, actions = batch\n\n        # Forward pass\n        policy = model(states)  # (batch, 361)\n\n        # Hitung loss\n        loss = F.cross_entropy(policy, actions)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Statistik\n        total_loss += loss.item()\n        predictions = policy.argmax(dim=1)\n        correct += (predictions == actions).sum().item()\n        total += actions.size(0)\n\n    accuracy = correct / total\n    avg_loss = total_loss / len(dataloader)\n\n    return avg_loss, accuracy\n"})}),"\n",(0,i.jsx)(e.h3,{id:"kurva-pelatihan",children:"Kurva Pelatihan"}),"\n",(0,i.jsx)(e.p,{children:"Proses pelatihan tipikal:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Akurasi\n60% |                    ......**********\n    |              ......*\n50% |        ......*\n    |    ....*\n40% |  ..*\n    |..*\n30% |*\n    +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Langkah pelatihan\n    0       100M     200M     300M     340M\n"})}),"\n",(0,i.jsx)(e.p,{children:"Loss dan akurasi akan meningkat dengan cepat, kemudian cenderung stabil."}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"analisis-hasil",children:"Analisis Hasil"}),"\n",(0,i.jsx)(e.h3,{id:"akurasi-57",children:"Akurasi 57%"}),"\n",(0,i.jsxs)(e.p,{children:["Setelah pelatihan lengkap, Policy Network mencapai ",(0,i.jsx)(e.strong,{children:"akurasi top-1 57.0%"}),"."]}),"\n",(0,i.jsx)(e.h4,{id:"apa-itu-akurasi-top-1",children:"Apa itu Akurasi Top-1?"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Prediksi: Model menghasilkan 361 probabilitas\nTop-1: Posisi dengan probabilitas tertinggi\nAkurasi: Proporsi posisi ini sama dengan posisi yang sebenarnya dimainkan manusia\n"})}),"\n",(0,i.jsx)(e.p,{children:"57% berarti: Model memiliki lebih dari separuh kemungkinan untuk menebak langkah selanjutnya pemain ahli manusia."}),"\n",(0,i.jsx)(e.h3,{id:"perbandingan-dengan-program-lain",children:"Perbandingan dengan Program Lain"}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Program"}),(0,i.jsx)(e.th,{children:"Akurasi Top-1"}),(0,i.jsx)(e.th,{children:"Deskripsi"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Pilihan acak"}),(0,i.jsx)(e.td,{children:"0.4%"}),(0,i.jsx)(e.td,{children:"Baseline"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Fitur tradisional + model linear"}),(0,i.jsx)(e.td,{children:"~24%"}),(0,i.jsx)(e.td,{children:"Level 2008"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"CNN dangkal"}),(0,i.jsx)(e.td,{children:"~44%"}),(0,i.jsx)(e.td,{children:"Level 2014"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"AlphaGo Policy Network"})}),(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"57%"})}),(0,i.jsx)(e.td,{children:"Terobosan 2016"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"AlphaGo Zero"}),(0,i.jsx)(e.td,{children:"~60%"}),(0,i.jsx)(e.td,{children:"2017"})]})]})]}),"\n",(0,i.jsx)(e.p,{children:"CNN dalam DeepMind meningkatkan 13 poin persentase dibandingkan metode terbaik sebelumnya."}),"\n",(0,i.jsx)(e.h3,{id:"evaluasi-kekuatan",children:"Evaluasi Kekuatan"}),"\n",(0,i.jsx)(e.p,{children:"Kekuatan bermain hanya menggunakan Policy Network (tanpa pencarian):"}),"\n",(0,i.jsx)(s.$W,{mode:"training",width:600,height:350}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Konfigurasi"}),(0,i.jsx)(e.th,{children:"Rating Elo"}),(0,i.jsx)(e.th,{children:"Perkiraan Level"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Terkuat tradisional (Pachi)"}),(0,i.jsx)(e.td,{children:"~2500"}),(0,i.jsx)(e.td,{children:"Amatir 4-5 dan"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"SL Policy Network"}),(0,i.jsx)(e.td,{children:"~2800"}),(0,i.jsx)(e.td,{children:"Amatir 6-7 dan"})]})]})]}),"\n",(0,i.jsx)(e.p,{children:"Supervised learning murni sudah mencapai level amatir tinggi, ini adalah terobosan besar pada 2016."}),"\n",(0,i.jsx)(e.h3,{id:"akurasi-vs-kekuatan",children:"Akurasi vs Kekuatan"}),"\n",(0,i.jsx)(e.p,{children:"Menariknya, akurasi dan kekuatan bukanlah hubungan linear:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Akurasi:  44% \u2192 57% (peningkatan 13%)\nElo:     ~2500 \u2192 ~2800 (peningkatan ~300)\n\nRasio peningkatan akurasi: 13% / 44% \u2248 30%\nRasio peningkatan Elo: 300 / 2500 \u2248 12%\n"})}),"\n",(0,i.jsx)(e.p,{children:"Peningkatan kecil dalam akurasi dapat membawa peningkatan kekuatan yang signifikan, karena:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Pilihan yang benar di posisi kritis lebih penting"}),"\n",(0,i.jsx)(e.li,{children:"Menghindari kesalahan jelas lebih penting daripada membuat banyak langkah bagus"}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"keterbatasan-supervised-learning",children:"Keterbatasan Supervised Learning"}),"\n",(0,i.jsx)(e.h3,{id:"masalah-1-efek-ceiling",children:"Masalah 1: Efek Ceiling"}),"\n",(0,i.jsx)(e.p,{children:'Supervised learning hanya dapat mencapai "level manusia", tidak dapat melampauinya:'}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Tujuan SL Policy: Meniru manusia\n          \u2193\nJika manusia memiliki kebiasaan yang salah\n          \u2193\nSL Policy juga akan mempelajari kesalahan ini\n"})}),"\n",(0,i.jsx)(e.p,{children:'Misalnya, jika pemain dalam data pelatihan jarang memainkan langkah non-tradisional seperti "Langkah 37", SL Policy juga tidak akan mempelajarinya.'}),"\n",(0,i.jsx)(e.h3,{id:"masalah-2-tidak-dapat-membedakan-langkah-bagus-dan-buruk",children:"Masalah 2: Tidak Dapat Membedakan Langkah Bagus dan Buruk"}),"\n",(0,i.jsx)(e.p,{children:'Supervised learning hanya melihat "apa yang dimainkan manusia", tidak peduli apakah langkah itu bagus atau tidak:'}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Posisi A: Manusia memainkan K10 (sebenarnya langkah buruk)\nPosisi B: Manusia memainkan Q4 (langkah bagus)\n\nSL Policy memperlakukan keduanya sama, keduanya harus dipelajari\n"})}),"\n",(0,i.jsx)(e.p,{children:"Data pelatihan mencakup permainan pemain amatir, yang mengandung banyak kesalahan. SL Policy akan mempelajari kesalahan ini."}),"\n",(0,i.jsx)(e.h3,{id:"masalah-3-eksplorasi-yang-tidak-memadai",children:"Masalah 3: Eksplorasi yang Tidak Memadai"}),"\n",(0,i.jsx)(e.p,{children:"SL Policy hanya mempelajari langkah yang sudah diketahui manusia:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Set langkah manusia: {A, B, C, D, E}\n           \u2193\nSL Policy hanya akan memilih di antara langkah-langkah ini\n           \u2193\nMungkin ada langkah yang lebih baik F, tapi tidak pernah ditemukan\n"})}),"\n",(0,i.jsx)(e.p,{children:"Ini adalah keterbatasan fundamental supervised learning: hanya dapat mempelajari apa yang ada dalam data pelatihan."}),"\n",(0,i.jsx)(e.h3,{id:"solusi-reinforcement-learning",children:"Solusi: Reinforcement Learning"}),"\n",(0,i.jsxs)(e.p,{children:["Untuk melampaui manusia, AlphaGo melakukan ",(0,i.jsx)(e.strong,{children:"reinforcement learning"})," setelah supervised learning:"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"SL Policy (level manusia)\n      \u2193 Self-play\nRL Policy (melampaui manusia)\n"})}),"\n",(0,i.jsxs)(e.p,{children:["Lihat detail di ",(0,i.jsx)(e.a,{href:"../reinforcement-intro",children:"Pengantar Reinforcement Learning"})," dan ",(0,i.jsx)(e.a,{href:"../self-play",children:"Self-Play"}),"."]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"poin-implementasi",children:"Poin Implementasi"}),"\n",(0,i.jsx)(e.h3,{id:"kode-pelatihan-lengkap",children:"Kode Pelatihan Lengkap"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\nclass GoDataset(Dataset):\n    def __init__(self, data_path):\n        # Muat data yang telah diproses\n        self.states = np.load(f"{data_path}/states.npy")\n        self.actions = np.load(f"{data_path}/actions.npy")\n\n    def __len__(self):\n        return len(self.states)\n\n    def __getitem__(self, idx):\n        state = torch.FloatTensor(self.states[idx])\n        action = torch.LongTensor([self.actions[idx]])[0]\n        return state, action\n\ndef train_policy_network():\n    # Model\n    model = PolicyNetwork(input_channels=48, num_filters=192, num_layers=12)\n    model = model.cuda()\n\n    # Data\n    dataset = GoDataset("data/kgs")\n    dataloader = DataLoader(\n        dataset, batch_size=16, shuffle=True, num_workers=4\n    )\n\n    # Optimizer\n    optimizer = optim.SGD(\n        model.parameters(),\n        lr=0.003,\n        momentum=0.9,\n        weight_decay=1e-4\n    )\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=80_000_000, gamma=0.1)\n\n    # Loop pelatihan\n    best_accuracy = 0\n\n    for epoch in range(100):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        for states, actions in dataloader:\n            states = states.cuda()\n            actions = actions.cuda()\n\n            # Forward pass\n            policy = model(states)\n            loss = nn.functional.cross_entropy(policy, actions)\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            # Statistik\n            total_loss += loss.item()\n            predictions = policy.argmax(dim=1)\n            correct += (predictions == actions).sum().item()\n            total += actions.size(0)\n\n        accuracy = correct / total\n        print(f"Epoch {epoch}: Loss={total_loss/len(dataloader):.4f}, Acc={accuracy:.4f}")\n\n        # Simpan model terbaik\n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n            torch.save(model.state_dict(), "best_policy.pth")\n\n    print(f"Best accuracy: {best_accuracy:.4f}")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"kode-evaluasi",children:"Kode Evaluasi"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def evaluate_policy(model, test_dataloader):\n    model.eval()\n\n    correct_top1 = 0\n    correct_top5 = 0\n    total = 0\n\n    with torch.no_grad():\n        for states, actions in test_dataloader:\n            states = states.cuda()\n            actions = actions.cuda()\n\n            policy = model(states)\n\n            # Akurasi Top-1\n            top1_pred = policy.argmax(dim=1)\n            correct_top1 += (top1_pred == actions).sum().item()\n\n            # Akurasi Top-5\n            top5_pred = policy.topk(5, dim=1)[1]\n            for i, action in enumerate(actions):\n                if action in top5_pred[i]:\n                    correct_top5 += 1\n\n            total += actions.size(0)\n\n    print(f"Top-1 Accuracy: {correct_top1/total:.4f}")\n    print(f"Top-5 Accuracy: {correct_top5/total:.4f}")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"masalah-umum-dan-solusi",children:"Masalah Umum dan Solusi"}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Masalah"}),(0,i.jsx)(e.th,{children:"Gejala"}),(0,i.jsx)(e.th,{children:"Solusi"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Overfitting"}),(0,i.jsx)(e.td,{children:"Akurasi pelatihan tinggi, akurasi pengujian rendah"}),(0,i.jsx)(e.td,{children:"Tambah augmentasi data, Dropout"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Pelatihan tidak stabil"}),(0,i.jsx)(e.td,{children:"Loss berfluktuasi drastis"}),(0,i.jsx)(e.td,{children:"Kurangi learning rate, tingkatkan ukuran batch"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Konvergensi lambat"}),(0,i.jsx)(e.td,{children:"Akurasi stagnan"}),(0,i.jsx)(e.td,{children:"Sesuaikan learning rate, periksa data"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Memori tidak cukup"}),(0,i.jsx)(e.td,{children:"Error OOM"}),(0,i.jsx)(e.td,{children:"Kurangi ukuran batch, gunakan mixed precision"})]})]})]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"korespondensi-animasi",children:"Korespondensi Animasi"}),"\n",(0,i.jsx)(e.p,{children:"Konsep inti dalam artikel ini dan nomor animasi terkait:"}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Nomor"}),(0,i.jsx)(e.th,{children:"Konsep"}),(0,i.jsx)(e.th,{children:"Korespondensi Fisika/Matematika"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"D3"}),(0,i.jsx)(e.td,{children:"Supervised learning"}),(0,i.jsx)(e.td,{children:"Maximum likelihood estimation"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"D5"}),(0,i.jsx)(e.td,{children:"Cross-entropy loss"}),(0,i.jsx)(e.td,{children:"KL divergence"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"D6"}),(0,i.jsx)(e.td,{children:"Gradient descent"}),(0,i.jsx)(e.td,{children:"Optimisasi"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"A6"}),(0,i.jsx)(e.td,{children:"Pra-pemrosesan data"}),(0,i.jsx)(e.td,{children:"Standardisasi"})]})]})]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"bacaan-lanjutan",children:"Bacaan Lanjutan"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Artikel sebelumnya"}),": ",(0,i.jsx)(e.a,{href:"../cnn-and-go",children:"CNN dan Permainan Go"})," \u2014 Bagaimana convolutional neural network memproses papan"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Artikel berikutnya"}),": ",(0,i.jsx)(e.a,{href:"../reinforcement-intro",children:"Pengantar Reinforcement Learning"})," \u2014 Kunci untuk melampaui manusia"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Topik terkait"}),": ",(0,i.jsx)(e.a,{href:"../policy-network",children:"Detail Policy Network"})," \u2014 Detail arsitektur jaringan"]}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"poin-penting",children:"Poin Penting"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Catatan KGS adalah sumber data pelatihan"}),": Sekitar 30 juta posisi berkualitas tinggi"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cross-entropy loss mendorong pembelajaran"}),": Membuat model meningkatkan probabilitas posisi yang benar"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Akurasi 57% adalah terobosan besar"}),": Melampaui metode terbaik sebelumnya sebesar 13 poin persentase"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Augmentasi 8-fold symmetry"}),": Secara efektif meningkatkan data pelatihan"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Supervised learning memiliki ceiling"}),": Tidak dapat melampaui level data pelatihan"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:'Supervised learning adalah "titik awal" AlphaGo\u2014ia mewarisi kebijaksanaan Go ribuan tahun manusia, meletakkan dasar untuk reinforcement learning selanjutnya.'}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"referensi",children:"Referensi"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:['Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." ',(0,i.jsx)(e.em,{children:"Nature"}),", 529, 484-489."]}),"\n",(0,i.jsxs)(e.li,{children:['Maddison, C. J., et al. (2014). "Move Evaluation in Go Using Deep Convolutional Neural Networks." ',(0,i.jsx)(e.em,{children:"arXiv:1412.6564"}),"."]}),"\n",(0,i.jsxs)(e.li,{children:['Clark, C., & Storkey, A. (2015). "Training Deep Convolutional Neural Networks to Play Go." ',(0,i.jsx)(e.em,{children:"ICML"}),"."]}),"\n",(0,i.jsxs)(e.li,{children:["KGS Game Archives: ",(0,i.jsx)(e.a,{href:"https://www.gokgs.com/archives.jsp",children:"https://www.gokgs.com/archives.jsp"})]}),"\n"]})]})}function p(a={}){const{wrapper:e}={...(0,r.R)(),...a.components};return e?(0,i.jsx)(e,{...a,children:(0,i.jsx)(h,{...a})}):h(a)}},42948(a,e,n){n.d(e,{A:()=>r});n(59471);var t=n(61785),i=n(62615);function r({children:a,fallback:e}){return(0,t.A)()?(0,i.jsx)(i.Fragment,{children:a?.()}):e??null}},45695(a,e,n){n.d(e,{$W:()=>S,tO:()=>d,u8:()=>j,dW:()=>m});var t=n(59471),i=n(90989),r=n(62615);const s=19,l=[[3,3],[3,9],[3,15],[9,3],[9,9],[9,15],[15,3],[15,9],[15,15]];function d({size:a=400,stones:e=[],highlights:n=[],labels:d=[],onCellClick:o=null,showCoordinates:c=!0}){const h=(0,t.useRef)(null),p=c?30:15,u=a-2*p,m=u/18;return(0,t.useEffect)(()=>{if(!h.current)return;const a=i.Ltv(h.current);a.selectAll("*").remove();const t=a.append("g").attr("transform",`translate(${p}, ${p})`);t.append("rect").attr("x",-m/2).attr("y",-m/2).attr("width",u+m).attr("height",u+m).attr("fill","#dcb35c").attr("rx",4);const r=t.append("g").attr("class","grid");for(let e=0;e<s;e++)r.append("line").attr("class","grid-line").attr("x1",0).attr("y1",e*m).attr("x2",18*m).attr("y2",e*m);for(let e=0;e<s;e++)r.append("line").attr("class","grid-line").attr("x1",e*m).attr("y1",0).attr("x2",e*m).attr("y2",18*m);const x=t.append("g").attr("class","star-points");if(l.forEach(([a,e])=>{x.append("circle").attr("class","star-point").attr("cx",a*m).attr("cy",e*m).attr("r",m/8)}),n.length>0){const a=t.append("g").attr("class","highlights");n.forEach(({x:e,y:n,intensity:t})=>{a.append("rect").attr("class","heatmap-cell").attr("x",e*m-m/2).attr("y",n*m-m/2).attr("width",m).attr("height",m).attr("fill",i.Q3(t)).attr("opacity",.7*t)})}const g=t.append("g").attr("class","stones");if(e.forEach(({x:a,y:e,color:n})=>{const t="black"===n?"stone-black":"stone-white";g.append("circle").attr("cx",a*m+2).attr("cy",e*m+2).attr("r",.45*m).attr("fill","rgba(0,0,0,0.2)"),g.append("circle").attr("class",t).attr("cx",a*m).attr("cy",e*m).attr("r",.45*m)}),d.length>0){const a=t.append("g").attr("class","labels");d.forEach(({x:n,y:t,text:i})=>{const r=e.find(a=>a.x===n&&a.y===t),s="black"===r?.color?"#fff":"#000";a.append("text").attr("x",n*m).attr("y",t*m).attr("dy","0.35em").attr("text-anchor","middle").attr("fill",s).attr("font-size",.5*m).attr("font-weight","bold").text(i)})}if(c){const e=a.append("g").attr("class","coordinates"),n="ABCDEFGHJKLMNOPQRST";for(let a=0;a<s;a++)e.append("text").attr("x",p+a*m).attr("y",p/2).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(n[a]);for(let a=0;a<s;a++)e.append("text").attr("x",p/2).attr("y",p+a*m).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(s-a)}o&&t.append("g").attr("class","click-targets").selectAll("rect").data(i.y17(361)).enter().append("rect").attr("x",a=>a%s*m-m/2).attr("y",a=>Math.floor(a/s)*m-m/2).attr("width",m).attr("height",m).attr("fill","transparent").attr("cursor","pointer").on("click",(a,e)=>{const n=e%s,t=Math.floor(e/s);o({x:n,y:t})})},[a,e,n,d,c,o,m,p,u]),(0,r.jsx)("div",{className:"go-board-container",children:(0,r.jsx)("svg",{ref:h,width:a,height:a,className:"go-board"})})}var o=n(42948);const c=19,h={empty:function(){const a=[];for(let e=0;e<c;e++)for(let n=0;n<c;n++)a.push({x:n,y:e,prob:1/361});return a}(),corner:function(){const a=[],e=[[3,3],[3,15],[15,3],[15,15]],n=[[2,4],[4,2],[2,14],[4,16],[14,2],[16,4],[14,16],[16,14]];for(let t=0;t<c;t++)for(let i=0;i<c;i++){let r=.001;e.some(([a,e])=>a===i&&e===t)?r=.15:n.some(([a,e])=>a===i&&e===t)?r=.05:0!==i&&18!==i&&0!==t&&18!==t||(r=5e-4),a.push({x:i,y:t,prob:r})}return p(a)}(),move37:function(){const a=[],e={x:9,y:4},n=[[3,2],[15,2],[10,10],[8,6]];for(let t=0;t<c;t++)for(let i=0;i<c;i++){let r=.001;i===e.x&&t===e.y?r=.08:n.some(([a,e])=>a===i&&e===t)?r=.12:i>=5&&i<=13&&t>=5&&t<=13&&(r=.005+.01*Math.random()),a.push({x:i,y:t,prob:r})}return p(a)}()};function p(a){const e=a.reduce((a,e)=>a+e.prob,0);return a.map(a=>({...a,prob:a.prob/e}))}function u({initialPosition:a="corner",stones:e=[],highlightMoves:n=[],size:s=450,showTopN:l=5,interactive:d=!0}){const o=(0,t.useRef)(null),p=(0,t.useRef)(null),[u,m]=(0,t.useState)(h[a]||h.corner),[x,g]=(0,t.useState)(null),j=35,k=s-70,f=k/18;(0,t.useEffect)(()=>{if(!o.current)return;const a=i.Ltv(o.current);a.selectAll("*").remove();const n=a.append("g").attr("transform","translate(35, 35)");n.append("rect").attr("x",-f/2).attr("y",-f/2).attr("width",k+f).attr("height",k+f).attr("fill","#dcb35c").attr("rx",4);const t=Math.max(...u.map(a=>a.prob)),r=i.exT(i.oKI).domain([0,t]);n.append("g").attr("class","heatmap").selectAll("rect").data(u).enter().append("rect").attr("class","heatmap-cell").attr("x",a=>a.x*f-f/2).attr("y",a=>a.y*f-f/2).attr("width",f).attr("height",f).attr("fill",a=>r(a.prob)).attr("opacity",a=>.3+a.prob/t*.6).attr("cursor",d?"pointer":"default").on("mouseover",function(a,e){if(!d)return;i.Ltv(this).attr("stroke","#333").attr("stroke-width",2);i.Ltv(p.current).style("display","block").style("left",`${a.pageX+10}px`).style("top",a.pageY-10+"px").html(`\u4f4d\u7f6e: ${String.fromCharCode(65+e.x)}${19-e.y}<br>\u6a5f\u7387: ${(100*e.prob).toFixed(2)}%`)}).on("mouseout",function(){i.Ltv(this).attr("stroke","none"),i.Ltv(p.current).style("display","none")}).on("click",function(a,e){d&&g(e)});const s=n.append("g").attr("class","grid");for(let e=0;e<c;e++)s.append("line").attr("class","grid-line").attr("x1",0).attr("y1",e*f).attr("x2",18*f).attr("y2",e*f).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5),s.append("line").attr("class","grid-line").attr("x1",e*f).attr("y1",0).attr("x2",e*f).attr("y2",18*f).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5);const h=n.append("g").attr("class","stones");e.forEach(({x:a,y:e,color:n})=>{h.append("circle").attr("cx",a*f).attr("cy",e*f).attr("r",.45*f).attr("fill","black"===n?"#1a1a1a":"#f5f5f5").attr("stroke","black"===n?"#000":"#333").attr("stroke-width",1)});const m=[...u].sort((a,e)=>e.prob-a.prob).slice(0,l),x=n.append("g").attr("class","top-labels");m.forEach((a,n)=>{e.some(e=>e.x===a.x&&e.y===a.y)||(x.append("circle").attr("cx",a.x*f).attr("cy",a.y*f).attr("r",.3*f).attr("fill","rgba(255,255,255,0.8)").attr("stroke","#e74c3c").attr("stroke-width",2),x.append("text").attr("x",a.x*f).attr("y",a.y*f).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#e74c3c").attr("font-size",.4*f).attr("font-weight","bold").text(n+1))});const b=a.append("g").attr("class","coordinates");for(let e=0;e<c;e++)b.append("text").attr("x",j+e*f).attr("y",17.5).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text("ABCDEFGHJKLMNOPQRST"[e]),b.append("text").attr("x",17.5).attr("y",j+e*f).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(c-e)},[u,e,l,d,f,j,k]);const b=a=>{m(h[a]||h.corner)};return(0,r.jsxs)("div",{children:[d&&(0,r.jsxs)("div",{className:"d3-controls",children:[(0,r.jsx)("button",{className:"empty"===a?"active":"",onClick:()=>b("empty"),children:"\u5747\u52fb\u5206\u5e03"}),(0,r.jsx)("button",{className:"corner"===a?"active":"",onClick:()=>b("corner"),children:"\u958b\u5c40\u661f\u4f4d"}),(0,r.jsx)("button",{className:"move37"===a?"active":"",onClick:()=>b("move37"),children:"\u7b2c 37 \u624b"})]}),(0,r.jsx)("div",{className:"go-board-container",children:(0,r.jsx)("svg",{ref:o,width:s,height:s,className:"go-board"})}),(0,r.jsx)("div",{ref:p,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),x&&(0,r.jsx)("div",{className:"d3-legend",children:(0,r.jsxs)("div",{className:"d3-legend-item",children:["\u5df2\u9078\u64c7: ",String.fromCharCode(65+x.x),19-x.y,"\u2014 \u6a5f\u7387: ",(100*x.prob).toFixed(2),"%"]})}),(0,r.jsxs)("div",{className:"d3-legend",children:[(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#ffffb2"}}),"\u4f4e\u6a5f\u7387"]}),(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#fd8d3c"}}),"\u4e2d\u6a5f\u7387"]}),(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#bd0026"}}),"\u9ad8\u6a5f\u7387"]})]})]})}function m(a){return(0,r.jsx)(o.A,{fallback:(0,r.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,r.jsx)(u,{...a})})}const x={name:"Root",visits:1600,value:.55,prior:1,children:[{name:"D4",visits:800,value:.62,prior:.35,selected:!0,children:[{name:"Q16",visits:400,value:.58,prior:.3},{name:"R4",visits:300,value:.65,prior:.25,selected:!0},{name:"C16",visits:100,value:.55,prior:.2}]},{name:"Q4",visits:500,value:.52,prior:.3,children:[{name:"D16",visits:300,value:.5,prior:.28},{name:"Q16",visits:200,value:.54,prior:.22}]},{name:"D16",visits:200,value:.48,prior:.2},{name:"Q16",visits:100,value:.45,prior:.15}]};function g({data:a=x,width:e=700,height:n=450,showPUCT:s=!0,cPuct:l=1.5,interactive:d=!0}){const o=(0,t.useRef)(null),c=(0,t.useRef)(null),[h,p]=(0,t.useState)(null),[u,m]=(0,t.useState)(l),g=40,j=40,k=e-j-40,f=n-g-40;return(0,t.useEffect)(()=>{if(!o.current)return;const t=i.Ltv(o.current);t.selectAll("*").remove();const r=i.B22().size([k,f-50]),l=i.Sk5(a);r(l);const h=t.append("g").attr("transform",`translate(${j}, ${g})`);h.append("g").attr("class","links").selectAll("path").data(l.links()).enter().append("path").attr("class",a=>"link "+(a.target.data.selected?"selected":"")).attr("fill","none").attr("stroke",a=>a.target.data.selected?"#4a90d9":"#999").attr("stroke-width",a=>a.target.data.selected?3:1.5).attr("d",i.vu().x(a=>a.x).y(a=>a.y));const m=h.append("g").attr("class","nodes").selectAll("g").data(l.descendants()).enter().append("g").attr("class","node").attr("transform",a=>`translate(${a.x}, ${a.y})`).attr("cursor",d?"pointer":"default").on("mouseover",function(a,e){if(!d)return;i.Ltv(this).select("circle").transition().duration(200).attr("r",30);const n=e.parent?e.parent.data.visits:e.data.visits,t=((a,e)=>{if(!e)return 0;const n=a.value,t=a.prior,i=a.visits;return n+u*t*Math.sqrt(e)/(1+i)})(e.data,n);i.Ltv(c.current).style("display","block").style("left",`${a.pageX+15}px`).style("top",a.pageY-10+"px").html(`\n            <strong>${e.data.name}</strong><br>\n            \u8a2a\u554f\u6b21\u6578 (N): ${e.data.visits}<br>\n            \u5e73\u5747\u50f9\u503c (Q): ${e.data.value.toFixed(3)}<br>\n            \u5148\u9a57\u6a5f\u7387 (P): ${(100*e.data.prior).toFixed(1)}%<br>\n            ${s?`PUCT \u5206\u6578: ${t.toFixed(3)}`:""}\n          `)}).on("mouseout",function(){i.Ltv(this).select("circle").transition().duration(200).attr("r",25),i.Ltv(c.current).style("display","none")}).on("click",function(a,e){d&&p(e.data)});m.append("circle").attr("r",25).attr("fill",a=>a.data.selected?"#4a90d9":"#fff").attr("stroke",e=>{if(e.data.selected)return"#2c5282";const n=e.data.visits/a.visits;return i.dM(.3+.5*n)}).attr("stroke-width",a=>a.data.selected?3:2),m.append("text").attr("dy",-5).attr("text-anchor","middle").attr("fill",a=>a.data.selected?"#fff":"#333").attr("font-size",11).attr("font-weight","bold").text(a=>a.data.name),m.append("text").attr("dy",10).attr("text-anchor","middle").attr("fill",a=>a.data.selected?"#fff":"#666").attr("font-size",9).text(a=>`N=${a.data.visits}`),t.append("text").attr("x",e/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("MCTS \u641c\u7d22\u6a39"),s&&t.append("text").attr("x",e/2).attr("y",n-10).attr("text-anchor","middle").attr("font-size",11).attr("fill","#666").text("\u85cd\u8272\u8def\u5f91\uff1aPUCT \u9078\u64c7\u7684\u6700\u4f73\u8def\u5f91")},[a,e,n,s,u,d,k,f]),(0,r.jsxs)("div",{children:[s&&d&&(0,r.jsx)("div",{className:"d3-controls",children:(0,r.jsxs)("div",{className:"d3-slider",children:[(0,r.jsxs)("label",{children:["c_puct: ",u.toFixed(1)]}),(0,r.jsx)("input",{type:"range",min:"0.5",max:"3",step:"0.1",value:u,onChange:a=>m(parseFloat(a.target.value))})]})}),(0,r.jsx)("div",{className:"mcts-tree-container",children:(0,r.jsx)("svg",{ref:o,width:e,height:n,className:"mcts-tree"})}),(0,r.jsx)("div",{ref:c,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),h&&(0,r.jsxs)("div",{className:"d3-legend",style:{background:"#f5f5f5",padding:"1rem",borderRadius:"4px"},children:[(0,r.jsxs)("strong",{children:["\u5df2\u9078\u64c7\u7bc0\u9ede: ",h.name]}),(0,r.jsxs)("div",{children:["\u8a2a\u554f\u6b21\u6578: ",h.visits]}),(0,r.jsxs)("div",{children:["\u5e73\u5747\u50f9\u503c: ",h.value.toFixed(3)]}),(0,r.jsxs)("div",{children:["\u5148\u9a57\u6a5f\u7387: ",(100*h.prior).toFixed(1),"%"]})]}),(0,r.jsxs)("div",{className:"d3-legend",children:[(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"\u9078\u4e2d\u8def\u5f91"]}),(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#fff",border:"2px solid #999"}}),"\u5176\u4ed6\u7bc0\u9ede"]}),(0,r.jsx)("div",{className:"d3-legend-item",children:(0,r.jsx)("span",{style:{fontSize:"12px"},children:"\u7bc0\u9ede\u5927\u5c0f \u221d \u8a2a\u554f\u6b21\u6578"})})]})]})}function j(a){return(0,r.jsx)(o.A,{fallback:(0,r.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,r.jsx)(g,{...a})})}const k=[{hours:0,elo:0,label:"\u96a8\u6a5f"},{hours:3,elo:1e3,label:"\u767c\u73fe\u898f\u5247"},{hours:6,elo:2e3},{hours:12,elo:3e3,label:"\u767c\u73fe\u5b9a\u5f0f"},{hours:24,elo:4e3},{hours:36,elo:4500,label:"\u8d85\u8d8a Fan Hui"},{hours:48,elo:5e3},{hours:60,elo:5200,label:"\u8d85\u8d8a Lee Sedol"},{hours:72,elo:5400,label:"\u8d85\u8d8a\u539f\u7248 AlphaGo"}],f=[{elo:2700,label:"\u696d\u9918\u5f37\u8c6a"},{elo:3500,label:"Fan Hui (\u8077\u696d\u4e8c\u6bb5)"},{elo:4500,label:"Lee Sedol (\u4e16\u754c\u51a0\u8ecd)"},{elo:5e3,label:"\u539f\u7248 AlphaGo"}],b=[{epochs:0,elo:0},{epochs:10,elo:1500},{epochs:20,elo:2500},{epochs:30,elo:3e3},{epochs:40,elo:3200},{epochs:50,elo:3300}],v=[{games:0,elo:3300},{games:1e3,elo:3800},{games:5e3,elo:4200},{games:1e4,elo:4500},{games:5e4,elo:4800},{games:1e5,elo:5e3}];function y({mode:a="zero",width:e=600,height:n=400,animated:s=!0,showMilestones:l=!0}){const d=(0,t.useRef)(null),[o,c]=(0,t.useState)(a),h=40,p=70,u=e-p-100,m=n-h-60;return(0,t.useEffect)(()=>{if(!d.current)return;const a=i.Ltv(d.current);let n,t,r;a.selectAll("*").remove(),"zero"===o?(n=k,t="\u8a13\u7df4\u6642\u9593\uff08\u5c0f\u6642\uff09",r=[0,80]):"sl"===o?(n=b,t="\u8a13\u7df4\u8f2a\u6578\uff08Epochs\uff09",r=[0,60]):(n=v,t="\u81ea\u6211\u5c0d\u5f08\u5c40\u6578",r=[0,12e4]);const c="selfplay"===o?i.ZEH().domain([1,r[1]]).range([0,u]):i.m4Y().domain(r).range([0,u]),x=i.m4Y().domain([0,6e3]).range([m,0]),g=a.append("g").attr("transform",`translate(${p}, ${h})`);if(g.append("g").attr("class","grid").selectAll(".grid-line-y").data(x.ticks(6)).enter().append("line").attr("class","grid-line-y").attr("x1",0).attr("x2",u).attr("y1",a=>x(a)).attr("y2",a=>x(a)).attr("stroke","#ddd").attr("stroke-dasharray","3,3"),l&&"zero"===o){const a=g.append("g").attr("class","human-levels");f.forEach(e=>{a.append("line").attr("x1",0).attr("x2",u).attr("y1",x(e.elo)).attr("y2",x(e.elo)).attr("stroke","#e74c3c").attr("stroke-dasharray","5,5").attr("opacity",.6),a.append("text").attr("x",u+5).attr("y",x(e.elo)).attr("dy","0.35em").attr("fill","#e74c3c").attr("font-size",10).text(e.label)})}const j=i.n8j().x(a=>c("zero"===o?a.hours:"sl"===o?a.epochs:Math.max(1,a.games))).y(a=>x(a.elo)).curve(i.nVG),y=i.Wcw().x(a=>c("zero"===o?a.hours:"sl"===o?a.epochs:Math.max(1,a.games))).y0(m).y1(a=>x(a.elo)).curve(i.nVG);g.append("path").datum(n).attr("class","area").attr("fill","#4a90d9").attr("opacity",.1).attr("d",y);const S=g.append("path").datum(n).attr("class","line").attr("fill","none").attr("stroke","#4a90d9").attr("stroke-width",3).attr("d",j);if(s){const a=S.node().getTotalLength();S.attr("stroke-dasharray",`${a} ${a}`).attr("stroke-dashoffset",a).transition().duration(2e3).ease(i.yfw).attr("stroke-dashoffset",0)}if(l&&"zero"===o){const a=n.filter(a=>a.label),e=g.append("g").attr("class","milestones");e.selectAll("circle").data(a).enter().append("circle").attr("cx",a=>c(a.hours)).attr("cy",a=>x(a.elo)).attr("r",6).attr("fill","#e74c3c").attr("stroke","#fff").attr("stroke-width",2),e.selectAll("text").data(a).enter().append("text").attr("x",a=>c(a.hours)).attr("y",a=>x(a.elo)-15).attr("text-anchor","middle").attr("fill","#333").attr("font-size",10).text(a=>a.label)}const P="selfplay"===o?i.l78(c).ticks(5,"~s"):i.l78(c);g.append("g").attr("class","x-axis").attr("transform",`translate(0, ${m})`).call(P),g.append("text").attr("class","axis-label").attr("x",u/2).attr("y",m+45).attr("text-anchor","middle").attr("fill","#666").text(t),g.append("g").attr("class","y-axis").call(i.V4s(x).ticks(6)),g.append("text").attr("class","axis-label").attr("transform","rotate(-90)").attr("x",-m/2).attr("y",-50).attr("text-anchor","middle").attr("fill","#666").text("ELO \u8a55\u5206"),a.append("text").attr("x",e/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("zero"===o?"AlphaGo Zero \u8a13\u7df4\u66f2\u7dda":"sl"===o?"\u76e3\u7763\u5b78\u7fd2\u68cb\u529b\u6210\u9577":"\u81ea\u6211\u5c0d\u5f08\u68cb\u529b\u6210\u9577")},[o,e,n,s,l,u,m]),(0,r.jsxs)("div",{children:[(0,r.jsxs)("div",{className:"d3-controls",children:[(0,r.jsx)("button",{className:"zero"===o?"active":"",onClick:()=>c("zero"),children:"AlphaGo Zero"}),(0,r.jsx)("button",{className:"sl"===o?"active":"",onClick:()=>c("sl"),children:"\u76e3\u7763\u5b78\u7fd2"}),(0,r.jsx)("button",{className:"selfplay"===o?"active":"",onClick:()=>c("selfplay"),children:"\u81ea\u6211\u5c0d\u5f08"})]}),(0,r.jsx)("div",{className:"elo-chart-container",children:(0,r.jsx)("svg",{ref:d,width:e,height:n,className:"elo-chart"})}),(0,r.jsxs)("div",{className:"d3-legend",children:[(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"ELO \u8a55\u5206"]}),l&&"zero"===o&&(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c"}}),"\u91cc\u7a0b\u7891"]}),(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c",opacity:.3}}),"\u4eba\u985e\u6c34\u5e73\u53c3\u8003\u7dda"]})]})]})]})}function S(a){return(0,r.jsx)(o.A,{fallback:(0,r.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,r.jsx)(y,{...a})})}},30416(a,e,n){n.d(e,{R:()=>s,x:()=>l});var t=n(59471);const i={},r=t.createContext(i);function s(a){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof a?a(e):{...e,...a}},[e,a])}function l(a){let e;return e=a.disableParentContext?"function"==typeof a.components?a.components(i):a.components||i:s(a.components),t.createElement(r.Provider,{value:e},a.children)}}}]);