"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[3427],{22705(n,e,a){a.r(e),a.d(e,{assets:()=>d,contentTitle:()=>t,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"alphago/value-network","title":"Penjelasan Detail Value Network","description":"Memahami secara mendalam arsitektur value network AlphaGo, tantangan training, dan peran kunci dalam MCTS","source":"@site/i18n/id/docusaurus-plugin-content-docs/current/alphago/08-value-network.mdx","sourceDirName":"alphago","slug":"/alphago/value-network","permalink":"/id/docs/alphago/value-network","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/08-value-network.mdx","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9,"title":"Penjelasan Detail Value Network","description":"Memahami secara mendalam arsitektur value network AlphaGo, tantangan training, dan peran kunci dalam MCTS"},"sidebar":"tutorialSidebar","previous":{"title":"Penjelasan Detail Policy Network","permalink":"/id/docs/alphago/policy-network"},"next":{"title":"Desain Fitur Input","permalink":"/id/docs/alphago/input-features"}}');var s=a(62615),l=a(30416);const r={sidebar_position:9,title:"Penjelasan Detail Value Network",description:"Memahami secara mendalam arsitektur value network AlphaGo, tantangan training, dan peran kunci dalam MCTS"},t="Penjelasan Detail Value Network",d={},h=[{value:"Apa itu Value Network?",id:"apa-itu-value-network",level:2},{value:"Fungsi Inti",id:"fungsi-inti",level:3},{value:"Makna Output",id:"makna-output",level:3},{value:"Mengapa Memerlukan Nilai Tunggal?",id:"mengapa-memerlukan-nilai-tunggal",level:3},{value:"Membandingkan Pilihan Berbeda",id:"membandingkan-pilihan-berbeda",level:4},{value:"Menggantikan Banyak Simulasi",id:"menggantikan-banyak-simulasi",level:4},{value:"Arsitektur Jaringan",id:"arsitektur-jaringan",level:2},{value:"Kesamaan dengan Policy Network",id:"kesamaan-dengan-policy-network",level:3},{value:"Layer Input",id:"layer-input",level:3},{value:"Layer Konvolusi",id:"layer-konvolusi",level:3},{value:"Perbedaan Layer Output",id:"perbedaan-layer-output",level:3},{value:"Output Policy Network",id:"output-policy-network",level:4},{value:"Output Value Network",id:"output-value-network",level:4},{value:"Fungsi Aktivasi Tanh",id:"fungsi-aktivasi-tanh",level:3},{value:"Mengapa Tanh Bukan Sigmoid?",id:"mengapa-tanh-bukan-sigmoid",level:4},{value:"Diagram Arsitektur Lengkap",id:"diagram-arsitektur-lengkap",level:3},{value:"Jumlah Parameter",id:"jumlah-parameter",level:3},{value:"Tantangan Training",id:"tantangan-training",level:2},{value:"Masalah Overfitting",id:"masalah-overfitting",level:3},{value:"Apa itu Overfitting?",id:"apa-itu-overfitting",level:4},{value:"Mengapa Value Network Mudah Overfitting?",id:"mengapa-value-network-mudah-overfitting",level:4},{value:"Solusi: Data Self-Play",id:"solusi-data-self-play",level:3},{value:"Mengapa Ini Dapat Menyelesaikan Overfitting?",id:"mengapa-ini-dapat-menyelesaikan-overfitting",level:4},{value:"Pembuatan Data Training",id:"pembuatan-data-training",level:3},{value:"Tujuan dan Metode Training",id:"tujuan-dan-metode-training",level:2},{value:"Loss Mean Squared Error",id:"loss-mean-squared-error",level:3},{value:"Mengapa MSE Bukan Cross-Entropy?",id:"mengapa-mse-bukan-cross-entropy",level:4},{value:"Proses Training",id:"proses-training",level:3},{value:"Analisis Akurasi",id:"analisis-akurasi",level:2},{value:"Perbandingan dengan Simulasi Acak",id:"perbandingan-dengan-simulasi-acak",level:3},{value:"Akurasi di Berbagai Tahap",id:"akurasi-di-berbagai-tahap",level:3},{value:"Distribusi Output",id:"distribusi-output",level:3},{value:"Posisi Tidak Pasti",id:"posisi-tidak-pasti",level:3},{value:"Peran dalam MCTS",id:"peran-dalam-mcts",level:2},{value:"Evaluasi Leaf Node",id:"evaluasi-leaf-node",level:3},{value:"Mengapa Menggabungkan?",id:"mengapa-menggabungkan",level:4},{value:"Penyederhanaan AlphaGo Zero",id:"penyederhanaan-alphago-zero",level:3},{value:"Backpropagation Update",id:"backpropagation-update",level:3},{value:"Analisis Visualisasi",id:"analisis-visualisasi",level:2},{value:"Permukaan Nilai",id:"permukaan-nilai",level:3},{value:"Evolusi Selama Proses Training",id:"evolusi-selama-proses-training",level:3},{value:"Identifikasi Posisi Sulit",id:"identifikasi-posisi-sulit",level:3},{value:"Poin Implementasi",id:"poin-implementasi",level:2},{value:"Implementasi PyTorch",id:"implementasi-pytorch",level:3},{value:"Loop Training",id:"loop-training",level:3},{value:"Teknik Menghindari Overfitting",id:"teknik-menghindari-overfitting",level:3},{value:"Kolaborasi dengan Policy Network",id:"kolaborasi-dengan-policy-network",level:2},{value:"Hubungan Komplementer",id:"hubungan-komplementer",level:3},{value:"Jaringan Dual-Head yang Bersatu",id:"jaringan-dual-head-yang-bersatu",level:3},{value:"Korespondensi Animasi",id:"korespondensi-animasi",level:2},{value:"Bacaan Lanjutan",id:"bacaan-lanjutan",level:2},{value:"Poin Kunci",id:"poin-kunci",level:2},{value:"Referensi",id:"referensi",level:2}];function o(n){const e={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"penjelasan-detail-value-network",children:"Penjelasan Detail Value Network"})}),"\n",(0,s.jsx)(e.p,{children:'Jika Policy Network memberitahu AlphaGo "langkah berikutnya harus di mana", maka Value Network menjawab pertanyaan yang lebih fundamental:'}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'"Permainan ini, apakah saya akan menang?"'})}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"apa-itu-value-network",children:"Apa itu Value Network?"}),"\n",(0,s.jsx)(e.h3,{id:"fungsi-inti",children:"Fungsi Inti"}),"\n",(0,s.jsx)(e.p,{children:"Value Network adalah deep convolutional neural network, tugasnya adalah:"}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Diberikan state papan saat ini, memprediksi win rate akhir"})}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Dinyatakan secara matematis:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"v = f_\u03b8(s)\n"})}),"\n",(0,s.jsx)(e.p,{children:"Di mana:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.code,{children:"s"}),": State papan saat ini"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.code,{children:"f_\u03b8"}),": Value Network (\u03b8 adalah parameter jaringan)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.code,{children:"v"}),": Nilai antara -1 sampai +1"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"makna-output",children:"Makna Output"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Nilai Output"}),(0,s.jsx)(e.th,{children:"Makna"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"+1"}),(0,s.jsx)(e.td,{children:"Pemain saat ini pasti menang"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"+0.5"}),(0,s.jsx)(e.td,{children:"Pemain saat ini sekitar 75% win rate"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"0"}),(0,s.jsx)(e.td,{children:"Win rate kedua pemain sama"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"-0.5"}),(0,s.jsx)(e.td,{children:"Pemain saat ini sekitar 25% win rate"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"-1"}),(0,s.jsx)(e.td,{children:"Pemain saat ini pasti kalah"})]})]})]}),"\n",(0,s.jsx)(e.h3,{id:"mengapa-memerlukan-nilai-tunggal",children:"Mengapa Memerlukan Nilai Tunggal?"}),"\n",(0,s.jsx)(e.h4,{id:"membandingkan-pilihan-berbeda",children:"Membandingkan Pilihan Berbeda"}),"\n",(0,s.jsx)(e.p,{children:"Saat bermain, kita sering perlu memilih di antara beberapa opsi. Value Network membuat perbandingan ini menjadi sederhana:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"Nilai posisi Opsi A: 0.3\nNilai posisi Opsi B: 0.5\nNilai posisi Opsi C: 0.2\n\n\u2192 Pilih B (nilai tertinggi)\n"})}),"\n",(0,s.jsx)(e.p,{children:'Tanpa nilai tunggal, bagaimana kita membandingkan "menangkap satu grup lawan" dan "mengepung area besar" mana yang lebih baik?'}),"\n",(0,s.jsx)(e.h4,{id:"menggantikan-banyak-simulasi",children:"Menggantikan Banyak Simulasi"}),"\n",(0,s.jsxs)(e.p,{children:["Dalam Monte Carlo tree search tradisional, mengevaluasi satu posisi memerlukan ",(0,s.jsx)(e.strong,{children:"simulasi acak (rollout)"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Mulai dari posisi saat ini"}),"\n",(0,s.jsx)(e.li,{children:"Kedua pemain bermain secara acak sampai akhir permainan"}),"\n",(0,s.jsx)(e.li,{children:"Catat menang/kalah"}),"\n",(0,s.jsx)(e.li,{children:"Ulangi ribuan kali, hitung win rate"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:["Ini sangat lambat. Value Network dapat memberikan evaluasi dengan ",(0,s.jsx)(e.strong,{children:"satu forward propagation"}),", kecepatannya jauh lebih tinggi."]}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Metode"}),(0,s.jsx)(e.th,{children:"Waktu Evaluasi"}),(0,s.jsx)(e.th,{children:"Presisi"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"1000 simulasi acak"}),(0,s.jsx)(e.td,{children:"~2000 milidetik"}),(0,s.jsx)(e.td,{children:"Lebih rendah"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"15000 simulasi acak"}),(0,s.jsx)(e.td,{children:"~30000 milidetik"}),(0,s.jsx)(e.td,{children:"Menengah"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Value Network"}),(0,s.jsx)(e.td,{children:"~3 milidetik"}),(0,s.jsx)(e.td,{children:"Tinggi (setara 15000 simulasi)"})]})]})]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"arsitektur-jaringan",children:"Arsitektur Jaringan"}),"\n",(0,s.jsx)(e.h3,{id:"kesamaan-dengan-policy-network",children:"Kesamaan dengan Policy Network"}),"\n",(0,s.jsx)(e.p,{children:"Arsitektur Value Network sangat mirip dengan Policy Network, keduanya adalah deep convolutional neural network:"}),"\n",(0,s.jsx)(e.mermaid,{value:'flowchart LR\n    A["Layer Input<br/>19\xd719\xd748"] --\x3e B["Layer Konvolusi \xd712<br/>19\xd719\xd7192"] --\x3e C["Layer Fully Connected<br/>256 dimensi"] --\x3e D["Output<br/>Nilai tunggal"]'}),"\n",(0,s.jsx)(e.h3,{id:"layer-input",children:"Layer Input"}),"\n",(0,s.jsxs)(e.p,{children:["Sama dengan Policy Network, input adalah tensor fitur ",(0,s.jsx)(e.strong,{children:"19\xd719\xd749"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"19\xd719"}),": Ukuran papan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"49"}),": 48 feature plane + 1 plane menunjukkan giliran siapa"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"1 plane tambahan ini penting: Value Network perlu tahu giliran siapa, karena nilai posisi yang sama berlawanan untuk hitam dan putih."}),"\n",(0,s.jsx)(e.h3,{id:"layer-konvolusi",children:"Layer Konvolusi"}),"\n",(0,s.jsx)(e.p,{children:"Sama dengan Policy Network:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"12 layer konvolusi"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"192 filter"})}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Kernel 3\xd73"})," (layer pertama 5\xd75)"]}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Fungsi aktivasi ReLU"})}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"perbedaan-layer-output",children:"Perbedaan Layer Output"}),"\n",(0,s.jsx)(e.p,{children:"Ini adalah perbedaan kunci antara Value Network dan Policy Network:"}),"\n",(0,s.jsx)(e.h4,{id:"output-policy-network",children:"Output Policy Network"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"19\xd719\xd7192 \u2192 Konvolusi 1\xd71 \u2192 19\xd719\xd71 \u2192 Flatten \u2192 361 dimensi \u2192 Softmax \u2192 Distribusi probabilitas\n"})}),"\n",(0,s.jsx)(e.h4,{id:"output-value-network",children:"Output Value Network"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"19\xd719\xd7192 \u2192 Konvolusi 1\xd71 \u2192 19\xd719\xd71 \u2192 Flatten \u2192 361 dimensi \u2192 Fully Connected 256 \u2192 ReLU \u2192 Fully Connected 1 \u2192 Tanh \u2192 Nilai tunggal\n"})}),"\n",(0,s.jsx)(e.h3,{id:"fungsi-aktivasi-tanh",children:"Fungsi Aktivasi Tanh"}),"\n",(0,s.jsxs)(e.p,{children:["Layer terakhir Value Network menggunakan fungsi ",(0,s.jsx)(e.strong,{children:"Tanh"})," (hyperbolic tangent):"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n"})}),"\n",(0,s.jsxs)(e.p,{children:["Rentang output Tanh adalah ",(0,s.jsx)(e.strong,{children:"(-1, +1)"}),", tepat sesuai dengan menang/kalah."]}),"\n",(0,s.jsx)(e.h4,{id:"mengapa-tanh-bukan-sigmoid",children:"Mengapa Tanh Bukan Sigmoid?"}),"\n",(0,s.jsx)(e.p,{children:"Rentang output Sigmoid adalah (0, 1), juga bisa mewakili win rate. Tapi Tanh memiliki beberapa keuntungan:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simetri"}),": Berpusat di 0, output bisa positif atau negatif"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Gradien lebih baik"}),": Gradien mendekati 1 di sekitar 0"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Semantik jelas"}),": Nilai positif menang, nilai negatif kalah, nol seri"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"diagram-arsitektur-lengkap",children:"Diagram Arsitektur Lengkap"}),"\n",(0,s.jsx)(e.mermaid,{value:'flowchart TB\n    A["Input: 19\xd719\xd749"] --\x3e B["Conv 5\xd75, 192 filter"]\n    B --\x3e C["ReLU"]\n    C --\x3e D["Conv 3\xd73, 192 filter (\xd711)"]\n    D --\x3e E["ReLU"]\n    E --\x3e F["Conv 1\xd71, 1 filter"]\n    F --\x3e G["Flatten (361 dimensi)"]\n    G --\x3e H["Fully Connected (256 dimensi)"]\n    H --\x3e I["ReLU"]\n    I --\x3e J["Fully Connected (1 dimensi)"]\n    J --\x3e K["Tanh"]\n    K --\x3e L["Output: [-1, +1]"]'}),"\n",(0,s.jsx)(e.h3,{id:"jumlah-parameter",children:"Jumlah Parameter"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Layer"}),(0,s.jsx)(e.th,{children:"Perhitungan"}),(0,s.jsx)(e.th,{children:"Jumlah Parameter"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Layer konvolusi"}),(0,s.jsx)(e.td,{children:"Sama dengan Policy Network"}),(0,s.jsx)(e.td,{children:"~3.9M"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Fully connected 1"}),(0,s.jsx)(e.td,{children:"361\xd7256 + 256"}),(0,s.jsx)(e.td,{children:"92,672"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Fully connected 2"}),(0,s.jsx)(e.td,{children:"256\xd71 + 1"}),(0,s.jsx)(e.td,{children:"257"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Total"})}),(0,s.jsx)(e.td,{}),(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"~4.0M"})})]})]})]}),"\n",(0,s.jsx)(e.p,{children:"Sekitar 4 juta parameter, sedikit lebih banyak dari Policy Network."}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"tantangan-training",children:"Tantangan Training"}),"\n",(0,s.jsx)(e.h3,{id:"masalah-overfitting",children:"Masalah Overfitting"}),"\n",(0,s.jsxs)(e.p,{children:["Training Value Network jauh lebih sulit dari Policy Network. Masalah utamanya adalah ",(0,s.jsx)(e.strong,{children:"overfitting"}),"."]}),"\n",(0,s.jsx)(e.h4,{id:"apa-itu-overfitting",children:"Apa itu Overfitting?"}),"\n",(0,s.jsx)(e.p,{children:'Overfitting mengacu pada model yang "menghafal" data training, bukan belajar generalisasi. Manifestasinya:'}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Performa sangat baik di training set"}),"\n",(0,s.jsx)(e.li,{children:"Performa sangat buruk di test set"}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"mengapa-value-network-mudah-overfitting",children:"Mengapa Value Network Mudah Overfitting?"}),"\n",(0,s.jsx)(e.p,{children:"Pertimbangkan data dari satu permainan:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"Posisi 1 \u2192 Posisi 2 \u2192 Posisi 3 \u2192 ... \u2192 Posisi 200 \u2192 Hasil: Hitam menang\n"})}),"\n",(0,s.jsx)(e.p,{children:"Jika langsung menggunakan data ini untuk training:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"200 posisi ini memiliki korelasi yang kuat"}),"\n",(0,s.jsx)(e.li,{children:"Mereka berasal dari permainan yang sama, memiliki hasil yang sama"}),"\n",(0,s.jsx)(e.li,{children:'Model mungkin belajar "mengenali" permainan ini, bukan memahami posisi'}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"DeepMind menemukan: jika menggunakan catatan permainan manusia yang sama untuk melatih Policy dan Value Network, Value Network akan mengalami overfitting parah."}),"\n",(0,s.jsx)(e.h3,{id:"solusi-data-self-play",children:"Solusi: Data Self-Play"}),"\n",(0,s.jsxs)(e.p,{children:["Solusi DeepMind adalah menggunakan ",(0,s.jsx)(e.strong,{children:"self-play"})," untuk menghasilkan data training baru:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"1. Gunakan RL Policy Network yang sudah dilatih untuk self-play\n2. Ambil hanya satu posisi dari setiap permainan (menghindari korelasi)\n3. Label posisi ini adalah hasil akhir permainan tersebut\n4. Hasilkan 30 juta sampel seperti ini\n"})}),"\n",(0,s.jsx)(e.h4,{id:"mengapa-ini-dapat-menyelesaikan-overfitting",children:"Mengapa Ini Dapat Menyelesaikan Overfitting?"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Jumlah data besar"}),": 30 juta posisi independen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Tanpa korelasi"}),": Hanya mengambil satu posisi per permainan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Distribusi berbeda"}),": Distribusi posisi self-play berbeda dari catatan permainan manusia"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"pembuatan-data-training",children:"Pembuatan Data Training"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Pseudocode\ntraining_data = []\n\nfor game_id in range(30_000_000):\n    # Self-play satu permainan\n    states, result = self_play(rl_policy_network)\n\n    # Pilih satu posisi secara acak\n    random_index = random.randint(0, len(states) - 1)\n    state = states[random_index]\n\n    # Catat posisi dan hasil\n    training_data.append((state, result))\n"})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"tujuan-dan-metode-training",children:"Tujuan dan Metode Training"}),"\n",(0,s.jsx)(e.h3,{id:"loss-mean-squared-error",children:"Loss Mean Squared Error"}),"\n",(0,s.jsxs)(e.p,{children:["Value Network menggunakan ",(0,s.jsx)(e.strong,{children:"Mean Squared Error (MSE)"})," sebagai fungsi loss:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"L(\u03b8) = (1/n) \xd7 \u03a3 (v_\u03b8(s) - z)\xb2\n"})}),"\n",(0,s.jsx)(e.p,{children:"Di mana:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.code,{children:"v_\u03b8(s)"}),": Nilai yang diprediksi model"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.code,{children:"z"}),": Hasil aktual (+1 atau -1)"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"mengapa-mse-bukan-cross-entropy",children:"Mengapa MSE Bukan Cross-Entropy?"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Cross-entropy"})," cocok untuk masalah klasifikasi (label diskrit)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"MSE"})," cocok untuk masalah regresi (nilai kontinu)"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Meskipun hasil hanya +1 atau -1, prediksi model adalah nilai kontinu (angka apapun antara -1 dan +1). MSE membuat model belajar memprediksi nilai mendekati +1 atau -1."}),"\n",(0,s.jsx)(e.h3,{id:"proses-training",children:"Proses Training"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Pseudocode\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        states, outcomes = batch\n\n        # Forward propagation\n        values = network(states)  # (batch, 1)\n\n        # Menghitung loss (MSE)\n        loss = mse_loss(values, outcomes)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n"})}),"\n",(0,s.jsx)(e.p,{children:"Detail training:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Optimizer"}),": SGD with momentum"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Learning rate"}),": 0.003"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Batch size"}),": 32"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Waktu training"}),": Sekitar 1 minggu (50 GPU)"]}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"analisis-akurasi",children:"Analisis Akurasi"}),"\n",(0,s.jsx)(e.h3,{id:"perbandingan-dengan-simulasi-acak",children:"Perbandingan dengan Simulasi Acak"}),"\n",(0,s.jsx)(e.p,{children:"DeepMind melakukan perbandingan detail dalam papernya:"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Metode Evaluasi"}),(0,s.jsx)(e.th,{children:"Error Prediksi"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"1000 simulasi acak"}),(0,s.jsx)(e.td,{children:"Lebih tinggi"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"15000 simulasi acak"}),(0,s.jsx)(e.td,{children:"Menengah"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Value Network"}),(0,s.jsx)(e.td,{children:"Setara 15000 simulasi"})]})]})]}),"\n",(0,s.jsx)(e.p,{children:"Ini berarti satu evaluasi Value Network \u2248 15000 simulasi acak, tapi sekitar 1000 kali lebih cepat."}),"\n",(0,s.jsx)(e.h3,{id:"akurasi-di-berbagai-tahap",children:"Akurasi di Berbagai Tahap"}),"\n",(0,s.jsx)(e.p,{children:"Akurasi Value Network bergantung pada progres permainan:"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Tahap"}),(0,s.jsx)(e.th,{children:"Sisa Langkah"}),(0,s.jsx)(e.th,{children:"Kesulitan Prediksi"}),(0,s.jsx)(e.th,{children:"Akurasi"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Pembukaan"}),(0,s.jsx)(e.td,{children:"~300"}),(0,s.jsx)(e.td,{children:"Sangat sulit"}),(0,s.jsx)(e.td,{children:"Lebih rendah"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Pertengahan"}),(0,s.jsx)(e.td,{children:"~150"}),(0,s.jsx)(e.td,{children:"Sulit"}),(0,s.jsx)(e.td,{children:"Menengah"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Yose"}),(0,s.jsx)(e.td,{children:"~50"}),(0,s.jsx)(e.td,{children:"Lebih mudah"}),(0,s.jsx)(e.td,{children:"Lebih tinggi"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Akhir"}),(0,s.jsx)(e.td,{children:"~10"}),(0,s.jsx)(e.td,{children:"Mudah"}),(0,s.jsx)(e.td,{children:"Sangat tinggi"})]})]})]}),"\n",(0,s.jsx)(e.p,{children:"Ini secara intuitif masuk akal: semakin dekat ke akhir permainan, semakin pasti hasilnya."}),"\n",(0,s.jsx)(e.h3,{id:"distribusi-output",children:"Distribusi Output"}),"\n",(0,s.jsx)(e.p,{children:"Distribusi output Value Network yang terlatih dengan baik:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"        Frekuensi\n          |\n          |    *\n          |   * *\n          |  *   *\n          | *     *\n          |*       *\n          +----+----+---- Nilai Output\n         -1    0   +1\n\nSebagian besar output terkonsentrasi di sekitar -1 dan +1\n(karena sebagian besar posisi memiliki kecenderungan menang/kalah yang jelas)\n"})}),"\n",(0,s.jsx)(e.h3,{id:"posisi-tidak-pasti",children:"Posisi Tidak Pasti"}),"\n",(0,s.jsx)(e.p,{children:"Ketika output Value Network mendekati 0, menunjukkan posisi sangat kompleks, menang/kalah tidak pasti. Posisi ini biasanya:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Dalam pertarungan besar"}),"\n",(0,s.jsx)(e.li,{children:"Kedua pihak seimbang"}),"\n",(0,s.jsx)(e.li,{children:"Ada beberapa kemungkinan variasi"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Dalam MCTS, node ini akan mendapat lebih banyak sumber daya pencarian (karena ketidakpastian tinggi)."}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"peran-dalam-mcts",children:"Peran dalam MCTS"}),"\n",(0,s.jsx)(e.h3,{id:"evaluasi-leaf-node",children:"Evaluasi Leaf Node"}),"\n",(0,s.jsxs)(e.p,{children:["Value Network memainkan peran kunci dalam tahap ",(0,s.jsx)(e.strong,{children:"Evaluation"})," MCTS:"]}),"\n",(0,s.jsx)(e.mermaid,{value:'flowchart TB\n    Root["Root Node<br/>(posisi saat ini)"]\n    Root --\x3e A\n    Root --\x3e B\n    A --\x3e A1\n    A --\x3e A2\n    B --\x3e B1\n    B --\x3e B2\n    A1 --\x3e E1["Eval"]\n    A2 --\x3e E2["Eval"]\n    B1 --\x3e E3["Eval"]\n    B2 --\x3e E4["Eval"]'}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.em,{children:"A1, A2, B1, B2 adalah leaf node"})}),"\n",(0,s.jsx)(e.p,{children:"Ketika MCTS mencapai leaf node, perlu mengevaluasi nilai posisi ini. Ada dua metode:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulasi acak (Rollout)"}),": Dari leaf node bermain acak sampai akhir permainan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Evaluasi Value Network"}),": Langsung menggunakan neural network untuk memprediksi"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"AlphaGo menggabungkan keduanya:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"V(leaf) = (1-\u03bb) \xd7 V_network(leaf) + \u03bb \xd7 V_rollout(leaf)\n"})}),"\n",(0,s.jsx)(e.p,{children:"Di mana \u03bb = 0.5, yaitu masing-masing setengah bobot."}),"\n",(0,s.jsx)(e.h4,{id:"mengapa-menggabungkan",children:"Mengapa Menggabungkan?"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Value Network"})," lebih akurat, tapi mungkin memiliki bias sistematis"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulasi acak"})," kurang akurat, tapi memberikan estimasi independen"]}),"\n",(0,s.jsx)(e.li,{children:"Menggabungkan keduanya dapat saling melengkapi"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"penyederhanaan-alphago-zero",children:"Penyederhanaan AlphaGo Zero"}),"\n",(0,s.jsx)(e.p,{children:"AlphaGo Zero kemudian sepenuhnya meninggalkan simulasi acak:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"V(leaf) = V_network(leaf)\n"})}),"\n",(0,s.jsx)(e.p,{children:'Ini sangat menyederhanakan sistem, sekaligus kekuatan bermain lebih kuat. Ini membuktikan Value Network cukup andal, tidak perlu "asuransi" simulasi acak.'}),"\n",(0,s.jsx)(e.h3,{id:"backpropagation-update",children:"Backpropagation Update"}),"\n",(0,s.jsx)(e.p,{children:"Setelah mengevaluasi leaf node, nilai ini akan dibackpropagate sepanjang jalur:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"v3 = V(leaf) = 0.6\n      \u2191\nQ value A2 diperbarui\n      \u2191\nQ value A diperbarui\n      \u2191\nStatistik root node diperbarui\n"})}),"\n",(0,s.jsx)(e.p,{children:"Setiap node mempertahankan Q value yang merupakan rata-rata semua evaluasi leaf node yang melewatinya:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"Q(s, a) = (1/N(s,a)) \xd7 \u03a3 V(leaf)\n"})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"analisis-visualisasi",children:"Analisis Visualisasi"}),"\n",(0,s.jsx)(e.h3,{id:"permukaan-nilai",children:"Permukaan Nilai"}),"\n",(0,s.jsx)(e.p,{children:'Bayangkan papan 3\xd73 yang disederhanakan. Value Network mempelajari "permukaan nilai":'}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{style:{textAlign:"center"},children:"Posisi Hitam / Posisi Putih"}),(0,s.jsx)(e.th,{style:{textAlign:"center"},children:"1"}),(0,s.jsx)(e.th,{style:{textAlign:"center"},children:"2"}),(0,s.jsx)(e.th,{style:{textAlign:"center"},children:"3"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{style:{textAlign:"center"},children:(0,s.jsx)(e.strong,{children:"1"})}),(0,s.jsx)(e.td,{style:{textAlign:"center"},children:"+0.3"}),(0,s.jsx)(e.td,{style:{textAlign:"center"},children:"-0.1"}),(0,s.jsx)(e.td,{style:{textAlign:"center"},children:"+0.2"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{style:{textAlign:"center"},children:(0,s.jsx)(e.strong,{children:"2"})}),(0,s.jsx)(e.td,{style:{textAlign:"center"},children:"-0.2"}),(0,s.jsx)(e.td,{style:{textAlign:"center"},children:"+0.5"}),(0,s.jsx)(e.td,{style:{textAlign:"center"},children:"-0.3"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{style:{textAlign:"center"},children:(0,s.jsx)(e.strong,{children:"3"})}),(0,s.jsx)(e.td,{style:{textAlign:"center"},children:"+0.1"}),(0,s.jsx)(e.td,{style:{textAlign:"center"},children:"-0.2"}),(0,s.jsx)(e.td,{style:{textAlign:"center"},children:"+0.4"})]})]})]}),"\n",(0,s.jsx)(e.p,{children:"Permukaan ini memberitahu kita nilai setiap kombinasi posisi. Nilai positif menguntungkan hitam, nilai negatif menguntungkan putih."}),"\n",(0,s.jsx)(e.h3,{id:"evolusi-selama-proses-training",children:"Evolusi Selama Proses Training"}),"\n",(0,s.jsx)(e.p,{children:"Seiring training berlangsung, prediksi Value Network secara bertahap menjadi lebih akurat:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"       Error Prediksi\n          |\n     1.0  |*\n          | *\n     0.5  |  *\n          |   *\n     0.1  |    * * * * *\n          +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Langkah Training\n          0   100K  500K  1M\n"})}),"\n",(0,s.jsx)(e.p,{children:"Error akan turun dengan cepat, kemudian stabil."}),"\n",(0,s.jsx)(e.h3,{id:"identifikasi-posisi-sulit",children:"Identifikasi Posisi Sulit"}),"\n",(0,s.jsx)(e.p,{children:"Value Network dapat membantu mengidentifikasi posisi sulit:"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Output"}),(0,s.jsx)(e.th,{children:"Makna"}),(0,s.jsx)(e.th,{children:"Strategi Respons"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Mendekati +1"}),(0,s.jsx)(e.td,{children:"Sangat unggul"}),(0,s.jsx)(e.td,{children:"Bermain solid"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Mendekati -1"}),(0,s.jsx)(e.td,{children:"Sangat tertinggal"}),(0,s.jsx)(e.td,{children:"Cari peluang comeback"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Mendekati 0"}),(0,s.jsx)(e.td,{children:"Posisi kompleks"}),(0,s.jsx)(e.td,{children:"Perlu kalkulasi mendalam"})]})]})]}),"\n",(0,s.jsx)(e.p,{children:"AlphaGo akan menginvestasikan lebih banyak waktu berpikir di posisi mendekati 0."}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"poin-implementasi",children:"Poin Implementasi"}),"\n",(0,s.jsx)(e.h3,{id:"implementasi-pytorch",children:"Implementasi PyTorch"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ValueNetwork(nn.Module):\n    def __init__(self, input_channels=49, num_filters=192, num_layers=12):\n        super().__init__()\n\n        # Layer konvolusi pertama (5\xd75)\n        self.conv1 = nn.Conv2d(input_channels, num_filters,\n                               kernel_size=5, padding=2)\n\n        # Layer konvolusi tengah (3\xd73)\xd711\n        self.conv_layers = nn.ModuleList([\n            nn.Conv2d(num_filters, num_filters,\n                     kernel_size=3, padding=1)\n            for _ in range(num_layers - 1)\n        ])\n\n        # Layer konvolusi output\n        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)\n\n        # Layer fully connected\n        self.fc1 = nn.Linear(361, 256)\n        self.fc2 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        # x: (batch, 49, 19, 19)\n\n        # Layer konvolusi\n        x = F.relu(self.conv1(x))\n        for conv in self.conv_layers:\n            x = F.relu(conv(x))\n        x = self.conv_out(x)\n\n        # Flatten\n        x = x.view(x.size(0), -1)  # (batch, 361)\n\n        # Layer fully connected\n        x = F.relu(self.fc1(x))\n        x = torch.tanh(self.fc2(x))\n\n        return x.squeeze(-1)  # (batch,)\n"})}),"\n",(0,s.jsx)(e.h3,{id:"loop-training",children:"Loop Training"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'def train_value_network(model, optimizer, states, outcomes):\n    """\n    states: (batch, 49, 19, 19) - Fitur papan\n    outcomes: (batch,) - Hasil permainan (+1 atau -1)\n    """\n    # Forward propagation\n    values = model(states)  # (batch,)\n\n    # Loss MSE\n    loss = F.mse_loss(values, outcomes)\n\n    # Backpropagation\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Menghitung akurasi (prediksi menang/kalah yang benar)\n    predictions = (values > 0).float() * 2 - 1  # Konversi ke +1/-1\n    accuracy = (predictions == outcomes).float().mean()\n\n    return loss.item(), accuracy.item()\n'})}),"\n",(0,s.jsx)(e.h3,{id:"teknik-menghindari-overfitting",children:"Teknik Menghindari Overfitting"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# 1. Augmentasi data (8 simetri)\ndef augment(state, outcome):\n    augmented = []\n    for rotation in [0, 90, 180, 270]:\n        s = rotate(state, rotation)\n        augmented.append((s, outcome))\n        augmented.append((flip(s), outcome))\n    return augmented\n\n# 2. Dropout\nclass ValueNetworkWithDropout(ValueNetwork):\n    def __init__(self, *args, dropout_rate=0.5, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        # ... layer konvolusi ...\n        x = self.dropout(x)  # Dropout sebelum layer fully connected\n        # ... layer fully connected ...\n\n# 3. Early Stopping\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(max_epochs):\n    train_loss = train_one_epoch()\n    val_loss = evaluate()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        save_model()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping!\")\n            break\n"})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"kolaborasi-dengan-policy-network",children:"Kolaborasi dengan Policy Network"}),"\n",(0,s.jsx)(e.h3,{id:"hubungan-komplementer",children:"Hubungan Komplementer"}),"\n",(0,s.jsx)(e.p,{children:"Policy Network dan Value Network saling melengkapi di AlphaGo:"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Jaringan"}),(0,s.jsx)(e.th,{children:"Pertanyaan yang Dijawab"}),(0,s.jsx)(e.th,{children:"Output"}),(0,s.jsx)(e.th,{children:"Peran MCTS"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Policy"}),(0,s.jsx)(e.td,{children:"Langkah berikutnya di mana?"}),(0,s.jsx)(e.td,{children:"Distribusi probabilitas"}),(0,s.jsx)(e.td,{children:"Membimbing arah pencarian"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Value"}),(0,s.jsx)(e.td,{children:"Permainan ini akan menang?"}),(0,s.jsx)(e.td,{children:"Nilai tunggal"}),(0,s.jsx)(e.td,{children:"Mengevaluasi leaf node"})]})]})]}),"\n",(0,s.jsx)(e.h3,{id:"jaringan-dual-head-yang-bersatu",children:"Jaringan Dual-Head yang Bersatu"}),"\n",(0,s.jsxs)(e.p,{children:["Di AlphaGo Zero, dua jaringan ini digabungkan menjadi satu ",(0,s.jsx)(e.strong,{children:"jaringan dual-head"}),":"]}),"\n",(0,s.jsx)(e.mermaid,{value:'flowchart TB\n    Shared["Layer ekstraksi fitur bersama"]\n    PH["Policy Head"]\n    VH["Value Head"]\n    PO["361 probabilitas"]\n    VO["Nilai tunggal"]\n\n    Shared --\x3e PH\n    Shared --\x3e VH\n    PH --\x3e PO\n    VH --\x3e VO'}),"\n",(0,s.jsx)(e.p,{children:"Keuntungan desain ini:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameter sharing"}),": Mengurangi komputasi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Feature sharing"}),": Policy dan Value menggunakan fitur yang sama"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Training lebih stabil"}),": Dua objektif saling meregularisasi"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:["Lihat ",(0,s.jsx)(e.a,{href:"../dual-head-resnet",children:"Dual-Head Network dan Residual Network"})," untuk detail."]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"korespondensi-animasi",children:"Korespondensi Animasi"}),"\n",(0,s.jsx)(e.p,{children:"Konsep inti yang dibahas dalam artikel ini dan nomor animasi:"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Nomor"}),(0,s.jsx)(e.th,{children:"Konsep"}),(0,s.jsx)(e.th,{children:"Korespondensi Fisika/Matematika"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"E2"}),(0,s.jsx)(e.td,{children:"Value Network"}),(0,s.jsx)(e.td,{children:"Permukaan potensial"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"D4"}),(0,s.jsx)(e.td,{children:"Fungsi nilai"}),(0,s.jsx)(e.td,{children:"Expected reward"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"C6"}),(0,s.jsx)(e.td,{children:"Evaluasi leaf node"}),(0,s.jsx)(e.td,{children:"Function approximation"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"H3"}),(0,s.jsx)(e.td,{children:"Temporal difference"}),(0,s.jsx)(e.td,{children:"Bootstrap learning"})]})]})]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"bacaan-lanjutan",children:"Bacaan Lanjutan"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Artikel sebelumnya"}),": ",(0,s.jsx)(e.a,{href:"../policy-network",children:"Penjelasan Detail Policy Network"})," \u2014 Bagaimana policy network memilih langkah"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Artikel berikutnya"}),": ",(0,s.jsx)(e.a,{href:"../input-features",children:"Desain Fitur Input"})," \u2014 Penjelasan detail 48 feature plane"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Topik lanjutan"}),": ",(0,s.jsx)(e.a,{href:"../mcts-neural-combo",children:"Kombinasi MCTS dan Neural Network"})," \u2014 Alur pencarian lengkap"]}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"poin-kunci",children:"Poin Kunci"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Value Network memprediksi win rate"}),": Output nilai tunggal antara -1 dan +1"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Output Tanh"}),": Memastikan output dalam rentang yang benar"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Loss MSE"}),": Mendekati nilai prediksi ke hasil aktual"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Tantangan overfitting"}),": Memerlukan data self-play untuk menghindari"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Menggantikan simulasi acak"}),": Satu evaluasi \u2248 15000 simulasi"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:'Value Network adalah "kemampuan penilaian" AlphaGo \u2014 memungkinkan AI mengevaluasi baik/buruknya posisi apapun, tanpa perlu menghabiskan semua kemungkinan.'}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"referensi",children:"Referensi"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:['Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." ',(0,s.jsx)(e.em,{children:"Nature"}),", 529, 484-489."]}),"\n",(0,s.jsxs)(e.li,{children:['Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." ',(0,s.jsx)(e.em,{children:"Nature"}),", 551, 354-359."]}),"\n",(0,s.jsxs)(e.li,{children:["Sutton, R. S., & Barto, A. G. (2018). ",(0,s.jsx)(e.em,{children:"Reinforcement Learning: An Introduction"}),". MIT Press."]}),"\n",(0,s.jsxs)(e.li,{children:['Tesauro, G. (1995). "Temporal difference learning and TD-Gammon." ',(0,s.jsx)(e.em,{children:"Communications of the ACM"}),", 38(3), 58-68."]}),"\n"]})]})}function u(n={}){const{wrapper:e}={...(0,l.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(o,{...n})}):o(n)}},30416(n,e,a){a.d(e,{R:()=>r,x:()=>t});var i=a(59471);const s={},l=i.createContext(s);function r(n){const e=i.useContext(l);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:r(n.components),i.createElement(l.Provider,{value:e},n.children)}}}]);