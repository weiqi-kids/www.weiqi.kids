"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[6410],{96759(a,n,e){e.r(n),e.d(n,{assets:()=>d,contentTitle:()=>t,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"alphago/alphago-zero","title":"Ikhtisar AlphaGo Zero","description":"Mulai dari nol, belajar mandiri sepenuhnya, bagaimana AlphaGo Zero melampaui semua versi sebelumnya tanpa menggunakan catatan permainan manusia","source":"@site/i18n/id/docusaurus-plugin-content-docs/current/alphago/16-alphago-zero.mdx","sourceDirName":"alphago","slug":"/alphago/alphago-zero","permalink":"/id/docs/alphago/alphago-zero","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/16-alphago-zero.mdx","tags":[],"version":"current","sidebarPosition":17,"frontMatter":{"sidebar_position":17,"title":"Ikhtisar AlphaGo Zero","description":"Mulai dari nol, belajar mandiri sepenuhnya, bagaimana AlphaGo Zero melampaui semua versi sebelumnya tanpa menggunakan catatan permainan manusia","keywords":["AlphaGo Zero","self-play","reinforcement learning","deep learning","AI Go","unsupervised learning"]},"sidebar":"tutorialSidebar","previous":{"title":"Penjelasan Detail Rumus PUCT","permalink":"/id/docs/alphago/puct-formula"},"next":{"title":"Dual-Head Network dan Residual Network","permalink":"/id/docs/alphago/dual-head-resnet"}}');var r=e(62615),l=e(30416);const s={sidebar_position:17,title:"Ikhtisar AlphaGo Zero",description:"Mulai dari nol, belajar mandiri sepenuhnya, bagaimana AlphaGo Zero melampaui semua versi sebelumnya tanpa menggunakan catatan permainan manusia",keywords:["AlphaGo Zero","self-play","reinforcement learning","deep learning","AI Go","unsupervised learning"]},t="Ikhtisar AlphaGo Zero",d={},h=[{value:"Mengapa Tidak Memerlukan Catatan Permainan Manusia?",id:"mengapa-tidak-memerlukan-catatan-permainan-manusia",level:2},{value:"Keterbatasan Catatan Permainan Manusia",id:"keterbatasan-catatan-permainan-manusia",level:3},{value:"1. Catatan Permainan Manusia Memiliki Batas Atas",id:"1-catatan-permainan-manusia-memiliki-batas-atas",level:4},{value:"2. Bottleneck Supervised Learning",id:"2-bottleneck-supervised-learning",level:4},{value:"3. Biaya Pengumpulan Data",id:"3-biaya-pengumpulan-data",level:4},{value:"Terobosan Zero",id:"terobosan-zero",level:3},{value:"Perbandingan dengan AlphaGo Original: 100:0",id:"perbandingan-dengan-alphago-original-1000",level:2},{value:"Kemenangan Telak",id:"kemenangan-telak",level:3},{value:"Lebih Sedikit Sumber Daya, Kekuatan Bermain Lebih Tinggi",id:"lebih-sedikit-sumber-daya-kekuatan-bermain-lebih-tinggi",level:3},{value:"Mengapa Zero Lebih Kuat?",id:"mengapa-zero-lebih-kuat",level:3},{value:"1. Pembelajaran Tanpa Bias",id:"1-pembelajaran-tanpa-bias",level:4},{value:"2. Tujuan Pembelajaran yang Konsisten",id:"2-tujuan-pembelajaran-yang-konsisten",level:4},{value:"3. Arsitektur yang Lebih Simpel",id:"3-arsitektur-yang-lebih-simpel",level:4},{value:"Fitur Input yang Disederhanakan: Dari 48 ke 17",id:"fitur-input-yang-disederhanakan-dari-48-ke-17",level:2},{value:"48 Plane Fitur AlphaGo Original",id:"48-plane-fitur-alphago-original",level:3},{value:"17 Plane Fitur AlphaGo Zero",id:"17-plane-fitur-alphago-zero",level:3},{value:"Mengapa Penyederhanaan Itu Bagus?",id:"mengapa-penyederhanaan-itu-bagus",level:3},{value:"1. Biarkan Network Menemukan Fitur Sendiri",id:"1-biarkan-network-menemukan-fitur-sendiri",level:4},{value:"2. Kemampuan Generalisasi Lebih Baik",id:"2-kemampuan-generalisasi-lebih-baik",level:4},{value:"3. Mengurangi Kesalahan Buatan",id:"3-mengurangi-kesalahan-buatan",level:4},{value:"Arsitektur Network Tunggal",id:"arsitektur-network-tunggal",level:2},{value:"Desain Dual Network Original",id:"desain-dual-network-original",level:3},{value:"Dual-Head Network Zero",id:"dual-head-network-zero",level:3},{value:"1. Efisiensi Parameter",id:"1-efisiensi-parameter",level:4},{value:"2. Berbagi Fitur",id:"2-berbagi-fitur",level:4},{value:"3. Stabilitas Pelatihan",id:"3-stabilitas-pelatihan",level:4},{value:"Kekuatan Residual Network",id:"kekuatan-residual-network",level:3},{value:"Peningkatan Efisiensi Pelatihan",id:"peningkatan-efisiensi-pelatihan",level:2},{value:"Pertumbuhan Eksponensial Self-Play",id:"pertumbuhan-eksponensial-self-play",level:3},{value:"Mengapa Secepat Ini?",id:"mengapa-secepat-ini",level:3},{value:"1. Panduan Pencarian yang Lebih Kuat",id:"1-panduan-pencarian-yang-lebih-kuat",level:4},{value:"2. Self-Play Lebih Cepat",id:"2-self-play-lebih-cepat",level:4},{value:"3. Pembelajaran Lebih Efektif",id:"3-pembelajaran-lebih-efektif",level:4},{value:"Perbandingan dengan Pembelajaran Manusia",id:"perbandingan-dengan-pembelajaran-manusia",level:3},{value:"Universalitas: Catur, Shogi",id:"universalitas-catur-shogi",level:2},{value:"Kelahiran AlphaZero",id:"kelahiran-alphazero",level:3},{value:"Makna Universalitas",id:"makna-universalitas",level:3},{value:"Dampak terhadap AI Tradisional",id:"dampak-terhadap-ai-tradisional",level:3},{value:"Gaya Bermain AlphaGo Zero",id:"gaya-bermain-alphago-zero",level:2},{value:"Melampaui Estetika Manusia",id:"melampaui-estetika-manusia",level:3},{value:"Menemukan Kembali Prinsip Go Manusia",id:"menemukan-kembali-prinsip-go-manusia",level:3},{value:"Inovasi Melampaui Manusia",id:"inovasi-melampaui-manusia",level:3},{value:"Ringkasan Detail Teknis",id:"ringkasan-detail-teknis",level:2},{value:"Perbandingan Lengkap dengan AlphaGo Original",id:"perbandingan-lengkap-dengan-alphago-original",level:3},{value:"Algoritma Inti",id:"algoritma-inti",level:3},{value:"Implikasi untuk Penelitian AI",id:"implikasi-untuk-penelitian-ai",level:2},{value:"Pembelajaran Prinsip Pertama",id:"pembelajaran-prinsip-pertama",level:3},{value:"Kekuatan Self-Play",id:"kekuatan-self-play",level:3},{value:"Pentingnya Penyederhanaan",id:"pentingnya-penyederhanaan",level:3},{value:"Korespondensi Animasi",id:"korespondensi-animasi",level:2},{value:"Bacaan Lanjutan",id:"bacaan-lanjutan",level:2},{value:"Referensi",id:"referensi",level:2}];function u(a){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...a.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"ikhtisar-alphago-zero",children:"Ikhtisar AlphaGo Zero"})}),"\n",(0,r.jsxs)(n.p,{children:["Pada Oktober 2017, DeepMind mengumumkan hasil yang mengejutkan dunia AI: ",(0,r.jsx)(n.strong,{children:"AlphaGo Zero"})," tanpa menggunakan catatan permainan manusia sama sekali, mulai berlatih dari keadaan acak sepenuhnya, hanya dalam tiga hari melampaui AlphaGo original yang mengalahkan Lee Sedol, dan menang dengan skor ",(0,r.jsx)(n.strong,{children:"100:0"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["Ini bukan sekadar kemajuan dalam angka. Ini mewakili paradigma baru: ",(0,r.jsx)(n.strong,{children:"AI tidak membutuhkan pengetahuan manusia, bisa menemukan segalanya dari nol"}),"."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"mengapa-tidak-memerlukan-catatan-permainan-manusia",children:"Mengapa Tidak Memerlukan Catatan Permainan Manusia?"}),"\n",(0,r.jsx)(n.h3,{id:"keterbatasan-catatan-permainan-manusia",children:"Keterbatasan Catatan Permainan Manusia"}),"\n",(0,r.jsx)(n.p,{children:"Proses pelatihan AlphaGo original dibagi menjadi dua tahap:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Supervised Learning"}),": Melatih Policy Network dengan 30 juta catatan permainan manusia"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reinforcement Learning"}),": Meningkatkan lebih lanjut melalui self-play"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Metode ini memiliki beberapa masalah fundamental:"}),"\n",(0,r.jsx)(n.h4,{id:"1-catatan-permainan-manusia-memiliki-batas-atas",children:"1. Catatan Permainan Manusia Memiliki Batas Atas"}),"\n",(0,r.jsx)(n.p,{children:"Kekuatan bermain pemain manusia terbatas, catatan permainan berisi pemahaman manusia, juga termasuk kesalahan dan bias manusia. Ketika AI belajar dari catatan permainan manusia, yang dipelajarinya adalah:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Langkah yang menurut manusia bagus (tapi belum tentu optimal)"}),"\n",(0,r.jsx)(n.li,{children:"Pola berpikir manusia (tapi mungkin membatasi inovasi)"}),"\n",(0,r.jsx)(n.li,{children:"Kesalahan manusia (akan dipelajari sebagai sampel yang benar)"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"2-bottleneck-supervised-learning",children:"2. Bottleneck Supervised Learning"}),"\n",(0,r.jsx)(n.p,{children:'Tujuan supervised learning adalah "meniru manusia"\u2014memprediksi langkah mana yang akan dimainkan pemain manusia. Ini berarti batas kemampuan AI dibatasi oleh kemampuan pemain manusia.'}),"\n",(0,r.jsx)(n.p,{children:"Seperti seorang murid yang hanya bisa meniru gurunya, tidak akan pernah bisa melampaui gurunya."}),"\n",(0,r.jsx)(n.h4,{id:"3-biaya-pengumpulan-data",children:"3. Biaya Pengumpulan Data"}),"\n",(0,r.jsx)(n.p,{children:'Catatan permainan manusia berkualitas tinggi membutuhkan waktu bertahun-tahun untuk dikumpulkan, dan hanya ada di game seperti Go yang memiliki sejarah panjang. Jika ingin menerapkan AI ke bidang baru (seperti prediksi struktur protein), tidak ada "catatan permainan ahli manusia" yang tersedia sama sekali.'}),"\n",(0,r.jsx)(n.h3,{id:"terobosan-zero",children:"Terobosan Zero"}),"\n",(0,r.jsxs)(n.p,{children:["AlphaGo Zero sepenuhnya melewati tahap supervised learning, langsung memulai self-play dari ",(0,r.jsx)(n.strong,{children:"inisialisasi acak"}),". Ini menyelesaikan semua masalah di atas:"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Masalah"}),(0,r.jsx)(n.th,{children:"AlphaGo Original"}),(0,r.jsx)(n.th,{children:"AlphaGo Zero"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Batas pengetahuan manusia"}),(0,r.jsx)(n.td,{children:"Dibatasi kualitas catatan"}),(0,r.jsx)(n.td,{children:"Tidak ada batasan ini"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Tujuan pembelajaran"}),(0,r.jsx)(n.td,{children:"Meniru manusia"}),(0,r.jsx)(n.td,{children:"Memaksimalkan win rate"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Kebutuhan data"}),(0,r.jsx)(n.td,{children:"30 juta catatan"}),(0,r.jsx)(n.td,{children:"0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Kemampuan generalisasi"}),(0,r.jsx)(n.td,{children:"Hanya terbatas Go"}),(0,r.jsx)(n.td,{children:"Bisa digeneralisasi ke bidang lain"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:'Ini adalah perubahan paradigma fundamental: dari "mempelajari pengetahuan manusia" ke "menemukan pengetahuan dari prinsip pertama".'}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"perbandingan-dengan-alphago-original-1000",children:"Perbandingan dengan AlphaGo Original: 100:0"}),"\n",(0,r.jsx)(n.h3,{id:"kemenangan-telak",children:"Kemenangan Telak"}),"\n",(0,r.jsx)(n.p,{children:"DeepMind membuat AlphaGo Zero yang sudah terlatih bertanding melawan berbagai versi AlphaGo:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Lawan"}),(0,r.jsx)(n.th,{children:"Rekor AlphaGo Zero"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaGo Fan (versi yang mengalahkan Fan Hui)"}),(0,r.jsx)(n.td,{children:"100:0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaGo Lee (versi yang mengalahkan Lee Sedol)"}),(0,r.jsx)(n.td,{children:"100:0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaGo Master (versi 60 kemenangan beruntun)"}),(0,r.jsx)(n.td,{children:"89:11"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"100:0"}),"\u2014ini berarti dalam 100 pertandingan, AlphaGo original tidak bisa menang satu pun."]}),"\n",(0,r.jsx)(n.h3,{id:"lebih-sedikit-sumber-daya-kekuatan-bermain-lebih-tinggi",children:"Lebih Sedikit Sumber Daya, Kekuatan Bermain Lebih Tinggi"}),"\n",(0,r.jsx)(n.p,{children:"Bukan hanya menang, AlphaGo Zero juga mencapai kekuatan bermain lebih tinggi dengan sumber daya lebih sedikit:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Metrik"}),(0,r.jsx)(n.th,{children:"AlphaGo Lee"}),(0,r.jsx)(n.th,{children:"AlphaGo Zero"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Waktu pelatihan"}),(0,r.jsx)(n.td,{children:"Berbulan-bulan"}),(0,r.jsx)(n.td,{children:"40 hari (3 hari melampaui AlphaGo Lee)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Jumlah permainan pelatihan"}),(0,r.jsx)(n.td,{children:"30 juta catatan manusia + self-play"}),(0,r.jsx)(n.td,{children:"4.9 juta self-play"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Jumlah TPU (pelatihan)"}),(0,r.jsx)(n.td,{children:"50+"}),(0,r.jsx)(n.td,{children:"4"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Jumlah TPU (inferensi)"}),(0,r.jsx)(n.td,{children:"48"}),(0,r.jsx)(n.td,{children:"4"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Fitur input"}),(0,r.jsx)(n.td,{children:"48 plane"}),(0,r.jsx)(n.td,{children:"17 plane"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Neural network"}),(0,r.jsx)(n.td,{children:"Dual network SL + RL"}),(0,r.jsx)(n.td,{children:"Single dual-head network"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:["Ini adalah peningkatan efisiensi yang menakjubkan: ",(0,r.jsx)(n.strong,{children:"sumber daya berkurang 10 kali lipat atau lebih, kekuatan bermain justru meningkat signifikan"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"mengapa-zero-lebih-kuat",children:"Mengapa Zero Lebih Kuat?"}),"\n",(0,r.jsx)(n.p,{children:"Alasan AlphaGo Zero lebih kuat bisa dipahami dari beberapa sudut pandang:"}),"\n",(0,r.jsx)(n.h4,{id:"1-pembelajaran-tanpa-bias",children:"1. Pembelajaran Tanpa Bias"}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo original belajar dari catatan permainan manusia, mewarisi bias manusia. Misalnya, pemain manusia mungkin terlalu menekankan joseki tertentu, atau memiliki evaluasi yang salah pada posisi tertentu."}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo Zero tidak memiliki beban ini. Ia mulai dari kertas kosong, hanya belajar apa yang merupakan langkah bagus melalui hasil menang/kalah. Ini memungkinkannya menemukan langkah yang tidak pernah terpikirkan manusia."}),"\n",(0,r.jsx)(n.h4,{id:"2-tujuan-pembelajaran-yang-konsisten",children:"2. Tujuan Pembelajaran yang Konsisten"}),"\n",(0,r.jsx)(n.p,{children:"Pelatihan AlphaGo original memiliki dua tujuan berbeda:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Supervised learning: Memaksimalkan akurasi prediksi langkah manusia"}),"\n",(0,r.jsx)(n.li,{children:"Reinforcement learning: Memaksimalkan win rate"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Kedua tujuan ini mungkin saling bertentangan. AlphaGo Zero hanya memiliki satu tujuan: ",(0,r.jsx)(n.strong,{children:"maksimalisasi win rate"}),". Ini membuat proses pembelajaran lebih konsisten dan efektif."]}),"\n",(0,r.jsx)(n.h4,{id:"3-arsitektur-yang-lebih-simpel",children:"3. Arsitektur yang Lebih Simpel"}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo original menggunakan Policy Network dan Value Network yang terpisah. AlphaGo Zero menggunakan single dual-head network (lihat artikel berikutnya untuk detail), memungkinkan representasi fitur dibagi, meningkatkan efisiensi pembelajaran."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"fitur-input-yang-disederhanakan-dari-48-ke-17",children:"Fitur Input yang Disederhanakan: Dari 48 ke 17"}),"\n",(0,r.jsx)(n.h3,{id:"48-plane-fitur-alphago-original",children:"48 Plane Fitur AlphaGo Original"}),"\n",(0,r.jsx)(n.p,{children:"Input neural network AlphaGo original berisi 48 plane fitur 19x19, mengkodekan banyak fitur yang dirancang manusia:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Kategori"}),(0,r.jsx)(n.th,{children:"Jumlah Fitur"}),(0,r.jsx)(n.th,{children:"Konten"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Posisi batu"}),(0,r.jsx)(n.td,{children:"3"}),(0,r.jsx)(n.td,{children:"Batu hitam, batu putih, titik kosong"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Jumlah liberty"}),(0,r.jsx)(n.td,{children:"8"}),(0,r.jsx)(n.td,{children:"String dengan 1-8 liberty"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Capture"}),(0,r.jsx)(n.td,{children:"8"}),(0,r.jsx)(n.td,{children:"Bisa capture 1-8 batu"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Ko"}),(0,r.jsx)(n.td,{children:"1"}),(0,r.jsx)(n.td,{children:"Posisi ko"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Jarak dari tepi"}),(0,r.jsx)(n.td,{children:"4"}),(0,r.jsx)(n.td,{children:"Baris pertama sampai keempat"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Legalitas langkah"}),(0,r.jsx)(n.td,{children:"1"}),(0,r.jsx)(n.td,{children:"Posisi mana yang bisa dimainkan"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"State historis"}),(0,r.jsx)(n.td,{children:"8"}),(0,r.jsx)(n.td,{children:"Posisi 8 langkah terakhir"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Giliran"}),(0,r.jsx)(n.td,{children:"1"}),(0,r.jsx)(n.td,{children:"Hitam atau putih"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Lainnya"}),(0,r.jsx)(n.td,{children:"14"}),(0,r.jsx)(n.td,{children:"Ladder, eye, dll."})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"48 fitur ini dirancang dengan cermat oleh ahli Go, berisi banyak domain knowledge."}),"\n",(0,r.jsx)(n.h3,{id:"17-plane-fitur-alphago-zero",children:"17 Plane Fitur AlphaGo Zero"}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo Zero sangat menyederhanakan input, hanya menggunakan 17 plane fitur:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Nomor Plane"}),(0,r.jsx)(n.th,{children:"Konten"}),(0,r.jsx)(n.th,{children:"Jumlah"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1-8"}),(0,r.jsx)(n.td,{children:"Posisi batu hitam (8 langkah terakhir)"}),(0,r.jsx)(n.td,{children:"8"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"9-16"}),(0,r.jsx)(n.td,{children:"Posisi batu putih (8 langkah terakhir)"}),(0,r.jsx)(n.td,{children:"8"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"17"}),(0,r.jsx)(n.td,{children:"Giliran saat ini (semua 1 atau semua 0)"}),(0,r.jsx)(n.td,{children:"1"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"17 fitur ini hanya berisi:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"State papan saat ini"}),": Setiap posisi ada batu hitam, batu putih, atau kosong"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Informasi historis"}),": State papan 8 langkah terakhir"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Informasi giliran"}),": Giliran siapa bermain"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:'Tidak ada jumlah liberty, tidak ada penilaian ladder, tidak ada jarak dari tepi\u2014semua "pengetahuan Go" ini dibiarkan neural network mempelajarinya sendiri.'}),"\n",(0,r.jsx)(n.h3,{id:"mengapa-penyederhanaan-itu-bagus",children:"Mengapa Penyederhanaan Itu Bagus?"}),"\n",(0,r.jsx)(n.h4,{id:"1-biarkan-network-menemukan-fitur-sendiri",children:"1. Biarkan Network Menemukan Fitur Sendiri"}),"\n",(0,r.jsx)(n.p,{children:"Fitur buatan tangan yang kompleks mungkin melewatkan informasi penting, atau mengkodekan asumsi yang salah. Membiarkan neural network belajar dari data mentah, ia mungkin menemukan representasi fitur yang lebih baik."}),"\n",(0,r.jsx)(n.p,{children:"Faktanya terbukti, AlphaGo Zero mempelajari semua fitur yang dirancang manusia (jumlah liberty, ladder, dll.), dan juga mempelajari beberapa pola yang tidak disadari manusia secara eksplisit."}),"\n",(0,r.jsx)(n.h4,{id:"2-kemampuan-generalisasi-lebih-baik",children:"2. Kemampuan Generalisasi Lebih Baik"}),"\n",(0,r.jsx)(n.p,{children:"Banyak dari 48 fitur yang khusus untuk Go (seperti ladder, jarak dari tepi). 17 fitur yang disederhanakan bersifat universal\u2014game papan apa pun bisa dikodekan dengan cara serupa."}),"\n",(0,r.jsxs)(n.p,{children:["Ini meletakkan dasar untuk ",(0,r.jsx)(n.strong,{children:"AlphaZero"})," (AI game universal) selanjutnya."]}),"\n",(0,r.jsx)(n.h4,{id:"3-mengurangi-kesalahan-buatan",children:"3. Mengurangi Kesalahan Buatan"}),"\n",(0,r.jsx)(n.p,{children:"Fitur yang dirancang manual mungkin berisi definisi yang salah atau tidak lengkap. Menyederhanakan input menghilangkan kemungkinan masalah semacam ini."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"arsitektur-network-tunggal",children:"Arsitektur Network Tunggal"}),"\n",(0,r.jsx)(n.h3,{id:"desain-dual-network-original",children:"Desain Dual Network Original"}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo original menggunakan dua neural network independen:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Policy Network:  Input \u2192 CNN \u2192 Probabilitas langkah 19x19\nValue Network:   Input \u2192 CNN \u2192 Estimasi win rate (-1 sampai 1)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Kedua network ini:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Memiliki arsitektur berbeda (jumlah layer, channel sedikit berbeda)"}),"\n",(0,r.jsx)(n.li,{children:"Dilatih secara independen (latih Policy dulu, lalu Value)"}),"\n",(0,r.jsx)(n.li,{children:"Tidak berbagi parameter apa pun"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"dual-head-network-zero",children:"Dual-Head Network Zero"}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo Zero menggunakan network tunggal, tapi dengan dua output head:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Input \u2192 ResNet shared backbone \u2192 Policy Head \u2192 Probabilitas langkah 19x19\n                               \u2192 Value Head  \u2192 Estimasi win rate\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Kedua Head berbagi backbone ResNet yang sama (lihat ",(0,r.jsx)(n.a,{href:"../dual-head-resnet",children:"artikel berikutnya: Dual-Head Network dan Residual Network"})," untuk detail), ini membawa beberapa keuntungan:"]}),"\n",(0,r.jsx)(n.h4,{id:"1-efisiensi-parameter",children:"1. Efisiensi Parameter"}),"\n",(0,r.jsx)(n.p,{children:"Shared backbone berarti sebagian besar parameter digunakan bersama oleh kedua tugas. Ini mengurangi total jumlah parameter, menurunkan risiko overfitting."}),"\n",(0,r.jsx)(n.h4,{id:"2-berbagi-fitur",children:"2. Berbagi Fitur"}),"\n",(0,r.jsx)(n.p,{children:'"Harus bermain di mana" (Policy) dan "siapa yang akan menang" (Value) membutuhkan pemahaman pola papan yang serupa. Shared backbone memungkinkan fitur-fitur ini dipelajari dan dimanfaatkan oleh kedua tugas secara bersamaan.'}),"\n",(0,r.jsx)(n.h4,{id:"3-stabilitas-pelatihan",children:"3. Stabilitas Pelatihan"}),"\n",(0,r.jsx)(n.p,{children:"Pelatihan joint membuat sinyal gradien berasal dari dua sumber, menyediakan sinyal supervisi yang lebih kaya, membuat pelatihan lebih stabil."}),"\n",(0,r.jsx)(n.h3,{id:"kekuatan-residual-network",children:"Kekuatan Residual Network"}),"\n",(0,r.jsxs)(n.p,{children:["Backbone AlphaGo Zero menggunakan ",(0,r.jsx)(n.strong,{children:"40-layer Residual Network (ResNet)"}),", jauh lebih dalam dari 13-layer CNN AlphaGo original."]}),"\n",(0,r.jsx)(n.p,{children:"Residual connection (skip connections) memungkinkan deep network dilatih secara efektif, menghindari masalah vanishing gradient. Ini adalah teknologi terobosan dari kompetisi ImageNet 2015, berhasil diterapkan oleh AlphaGo Zero ke bidang Go."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"peningkatan-efisiensi-pelatihan",children:"Peningkatan Efisiensi Pelatihan"}),"\n",(0,r.jsx)(n.h3,{id:"pertumbuhan-eksponensial-self-play",children:"Pertumbuhan Eksponensial Self-Play"}),"\n",(0,r.jsx)(n.p,{children:"Proses pelatihan AlphaGo Zero menunjukkan efisiensi yang menakjubkan:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Waktu Pelatihan"}),(0,r.jsx)(n.th,{children:"Rating ELO"}),(0,r.jsx)(n.th,{children:"Setara dengan"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"0 jam"}),(0,r.jsx)(n.td,{children:"0"}),(0,r.jsx)(n.td,{children:"Bermain acak"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"3 jam"}),(0,r.jsx)(n.td,{children:"~1000"}),(0,r.jsx)(n.td,{children:"Menemukan aturan dasar"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"12 jam"}),(0,r.jsx)(n.td,{children:"~3000"}),(0,r.jsx)(n.td,{children:"Menemukan joseki"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"36 jam"}),(0,r.jsx)(n.td,{children:"~4500"}),(0,r.jsx)(n.td,{children:"Melampaui versi Fan Hui"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"60 jam"}),(0,r.jsx)(n.td,{children:"~5200"}),(0,r.jsx)(n.td,{children:"Melampaui versi Lee Sedol"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"72 jam"}),(0,r.jsx)(n.td,{children:"~5400"}),(0,r.jsx)(n.td,{children:"Melampaui AlphaGo original"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"40 hari"}),(0,r.jsx)(n.td,{children:"~5600"}),(0,r.jsx)(n.td,{children:"Versi terkuat"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tiga hari melampaui manusia, tiga hari melampaui AI yang sebelumnya dilatih berbulan-bulan"}),"\u2014ini adalah peningkatan efisiensi eksponensial."]}),"\n",(0,r.jsx)(n.h3,{id:"mengapa-secepat-ini",children:"Mengapa Secepat Ini?"}),"\n",(0,r.jsx)(n.h4,{id:"1-panduan-pencarian-yang-lebih-kuat",children:"1. Panduan Pencarian yang Lebih Kuat"}),"\n",(0,r.jsx)(n.p,{children:"MCTS AlphaGo Zero sepenuhnya dipandu oleh neural network, tidak lagi menggunakan fast rollout policy. Ini membuat pencarian lebih efisien dan akurat."}),"\n",(0,r.jsx)(n.h4,{id:"2-self-play-lebih-cepat",children:"2. Self-Play Lebih Cepat"}),"\n",(0,r.jsx)(n.p,{children:"Karena hanya membutuhkan satu network (bukan dua), biaya komputasi setiap self-play berkurang. Ini berarti lebih banyak data pelatihan bisa dihasilkan dalam waktu yang sama."}),"\n",(0,r.jsx)(n.h4,{id:"3-pembelajaran-lebih-efektif",children:"3. Pembelajaran Lebih Efektif"}),"\n",(0,r.jsx)(n.p,{children:"Pelatihan joint dual-head network membuat informasi setiap permainan dimanfaatkan lebih efektif. Gradien Policy dan Value saling memperkuat, mempercepat konvergensi."}),"\n",(0,r.jsx)(n.h3,{id:"perbandingan-dengan-pembelajaran-manusia",children:"Perbandingan dengan Pembelajaran Manusia"}),"\n",(0,r.jsx)(n.p,{children:"Berapa lama waktu yang dibutuhkan pemain manusia untuk mencapai level berbeda?"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Level"}),(0,r.jsx)(n.th,{children:"Waktu yang Dibutuhkan Manusia"}),(0,r.jsx)(n.th,{children:"AlphaGo Zero"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Pemula"}),(0,r.jsx)(n.td,{children:"Beberapa minggu"}),(0,r.jsx)(n.td,{children:"Beberapa menit"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Amateur 1-dan"}),(0,r.jsx)(n.td,{children:"Beberapa tahun"}),(0,r.jsx)(n.td,{children:"Beberapa jam"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Level profesional"}),(0,r.jsx)(n.td,{children:"10-20 tahun"}),(0,r.jsx)(n.td,{children:"1-2 hari"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Juara dunia"}),(0,r.jsx)(n.td,{children:"20+ tahun dedikasi penuh"}),(0,r.jsx)(n.td,{children:"3 hari"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Melampaui manusia"}),(0,r.jsx)(n.td,{children:"Tidak mungkin"}),(0,r.jsx)(n.td,{children:"3 hari"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Perbandingan ini bukan untuk meremehkan pemain manusia\u2014mereka menggunakan neuron biologis, sedangkan AlphaGo Zero menggunakan TPU yang dirancang khusus dan listrik beberapa ribu watt. Tapi ini memang menunjukkan betapa efisiennya metode pembelajaran yang tepat."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"universalitas-catur-shogi",children:"Universalitas: Catur, Shogi"}),"\n",(0,r.jsx)(n.h3,{id:"kelahiran-alphazero",children:"Kelahiran AlphaZero"}),"\n",(0,r.jsxs)(n.p,{children:["Pada Desember 2017, DeepMind mengumumkan ",(0,r.jsx)(n.strong,{children:"AlphaZero"}),"\u2014versi universal AlphaGo Zero. Algoritma yang sama, hanya perlu memodifikasi aturan permainan, bisa mencapai level dunia tertinggi di tiga permainan papan:"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Permainan"}),(0,r.jsx)(n.th,{children:"Waktu Pelatihan"}),(0,r.jsx)(n.th,{children:"Lawan"}),(0,r.jsx)(n.th,{children:"Rekor"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Go"}),(0,r.jsx)(n.td,{children:"8 jam"}),(0,r.jsx)(n.td,{children:"AlphaGo Zero"}),(0,r.jsx)(n.td,{children:"60:40"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Catur"}),(0,r.jsx)(n.td,{children:"4 jam"}),(0,r.jsx)(n.td,{children:"Stockfish 8"}),(0,r.jsx)(n.td,{children:"28 menang 72 seri 0 kalah"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Shogi"}),(0,r.jsx)(n.td,{children:"2 jam"}),(0,r.jsx)(n.td,{children:"Elmo"}),(0,r.jsx)(n.td,{children:"90:8:2"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Perhatikan lawan-lawannya:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stockfish"})," adalah engine catur terkuat saat itu, menggunakan puluhan tahun pengetahuan manusia dan optimisasi"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Elmo"})," adalah AI shogi terkuat saat itu"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"AlphaZero dengan pelatihan beberapa jam, melampaui sistem khusus yang dikembangkan bertahun-tahun ini."}),"\n",(0,r.jsx)(n.h3,{id:"makna-universalitas",children:"Makna Universalitas"}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo Zero / AlphaZero membuktikan satu hal penting:"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Algoritma pembelajaran yang sama, bisa mencapai level superhuman di berbagai domain."})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Ini bukan tiga AI berbeda, tapi satu framework pembelajaran universal:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Self-play"})," menghasilkan pengalaman"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Monte Carlo Tree Search"})," mengeksplorasi kemungkinan"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Neural Network"})," mempelajari fungsi policy dan value"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reinforcement Learning"})," mengoptimalkan fungsi objektif"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Framework ini tidak bergantung pada pengetahuan domain-specific, ini adalah langkah penting menuju AI yang lebih universal."}),"\n",(0,r.jsx)(n.h3,{id:"dampak-terhadap-ai-tradisional",children:"Dampak terhadap AI Tradisional"}),"\n",(0,r.jsx)(n.p,{children:'Sebelum AlphaZero, AI terkuat untuk catur dan shogi semuanya bergaya "expert system":'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Banyak pengetahuan manusia"}),": Opening book, endgame tablebase, fungsi evaluasi"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Optimisasi puluhan tahun"}),": Hasil kerja keras tak terhitung pemain dan engineer"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sangat terspesialisasi"}),": Stockfish tidak bisa bermain Go, Elmo tidak bisa bermain catur"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"AlphaZero dengan satu algoritma universal melampaui semua ini dalam beberapa jam. Ini membuat banyak peneliti AI memikirkan ulang:"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:'Haruskah kita menginvestasikan lebih banyak usaha pada "algoritma pembelajaran universal", atau "pengkodean pengetahuan expert"?'}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Jawabannya tampaknya semakin jelas: membiarkan mesin belajar sendiri, lebih efektif daripada mengajarinya pengetahuan."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"gaya-bermain-alphago-zero",children:"Gaya Bermain AlphaGo Zero"}),"\n",(0,r.jsx)(n.h3,{id:"melampaui-estetika-manusia",children:"Melampaui Estetika Manusia"}),"\n",(0,r.jsxs)(n.p,{children:["Komunitas Go memiliki evaluasi umum terhadap langkah-langkah AlphaGo Zero: ",(0,r.jsx)(n.strong,{children:"lebih indah"}),"."]}),"\n",(0,r.jsx)(n.p,{children:'Langkah-langkah AlphaGo Lee kadang terlihat "aneh"\u2014seperti langkah ke-37, manusia perlu analisis setelahnya baru memahami keindahannya. Tapi langkah-langkah AlphaGo Zero sering dievaluasi sebagai "langsung terlihat sebagai langkah bagus" setelahnya.'}),"\n",(0,r.jsx)(n.p,{children:"Ini mungkin karena:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kekuatan bermain lebih tinggi"}),": Zero bisa melihat lebih dalam, langkah lebih tenang"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tanpa bias manusia"}),": Tidak terikat oleh joseki tradisional"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tujuan yang konsisten"}),": Hanya mengejar win rate, tidak meniru manusia"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"menemukan-kembali-prinsip-go-manusia",children:"Menemukan Kembali Prinsip Go Manusia"}),"\n",(0,r.jsx)(n.p,{children:'Menariknya, AlphaGo Zero dalam proses pelatihan "menemukan kembali" pengetahuan Go yang diakumulasi manusia selama ribuan tahun:'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Joseki"}),": Zero menemukan sendiri banyak joseki umum, karena ini memang solusi optimal untuk kedua belah pihak"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Prinsip opening"}),": Urutan pentingnya sudut, sisi, tengah"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pengetahuan bentuk"}),": Perbedaan antara bentuk buruk dan bentuk bagus"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Ini memvalidasi rasionalitas prinsip Go manusia\u2014pengetahuan ini bukan kebetulan, tapi refleksi dari esensi Go."}),"\n",(0,r.jsx)(n.h3,{id:"inovasi-melampaui-manusia",children:"Inovasi Melampaui Manusia"}),"\n",(0,r.jsx)(n.p,{children:"Tapi Zero juga menemukan langkah yang tidak pernah terpikirkan manusia:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Opening non-konvensional"}),": Variasi pada dasar opening tradisional"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Korban agresif"}),": Lebih bersedia daripada manusia untuk mengorbankan lokal demi keuntungan global"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Bentuk counter-intuitif"}),': Bentuk yang tampak "buruk" ternyata adalah solusi optimal']}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Inovasi-inovasi ini sedang mengubah pemahaman manusia tentang Go. Banyak pemain profesional mengatakan, mempelajari catatan permainan AlphaGo Zero memberi mereka pemahaman baru tentang Go."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"ringkasan-detail-teknis",children:"Ringkasan Detail Teknis"}),"\n",(0,r.jsx)(n.h3,{id:"perbandingan-lengkap-dengan-alphago-original",children:"Perbandingan Lengkap dengan AlphaGo Original"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspek"}),(0,r.jsx)(n.th,{children:"AlphaGo (Original)"}),(0,r.jsx)(n.th,{children:"AlphaGo Zero"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Data pelatihan"})}),(0,r.jsx)(n.td,{children:"Catatan manusia + self-play"}),(0,r.jsx)(n.td,{children:"Self-play murni"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Metode pembelajaran"})}),(0,r.jsx)(n.td,{children:"Supervised learning + Reinforcement learning"}),(0,r.jsx)(n.td,{children:"Reinforcement learning murni"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Fitur input"})}),(0,r.jsx)(n.td,{children:"48 plane"}),(0,r.jsx)(n.td,{children:"17 plane"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Arsitektur network"})}),(0,r.jsx)(n.td,{children:"Policy/Value terpisah"}),(0,r.jsx)(n.td,{children:"Dual-head ResNet"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Kedalaman network"})}),(0,r.jsx)(n.td,{children:"13 layer"}),(0,r.jsx)(n.td,{children:"40 layer (atau lebih)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Evaluasi MCTS"})}),(0,r.jsx)(n.td,{children:"Neural network + Rollout"}),(0,r.jsx)(n.td,{children:"Neural network murni"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Jumlah pencarian"})}),(0,r.jsx)(n.td,{children:"~100,000 per langkah"}),(0,r.jsx)(n.td,{children:"~1,600 per langkah"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"TPU pelatihan"})}),(0,r.jsx)(n.td,{children:"50+"}),(0,r.jsx)(n.td,{children:"4"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"TPU inferensi"})}),(0,r.jsx)(n.td,{children:"48"}),(0,r.jsx)(n.td,{children:"4 (bisa diperluas)"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"algoritma-inti",children:"Algoritma Inti"}),"\n",(0,r.jsx)(n.p,{children:"Loop pelatihan AlphaGo Zero sangat simpel:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"1. Self-play\n   - Lakukan MCTS dengan network saat ini\n   - Pilih langkah berdasarkan probabilitas pencarian MCTS\n   - Catat setiap langkah (posisi, probabilitas MCTS, hasil menang/kalah)\n\n2. Latih Network\n   - Ambil sampel dari experience pool\n   - Policy Head: Minimalkan cross-entropy dengan probabilitas MCTS\n   - Value Head: Minimalkan mean squared error dengan hasil menang/kalah aktual\n   - Optimalkan kedua tujuan secara joint\n\n3. Update Network\n   - Ganti network lama dengan network baru (verifikasi network baru lebih kuat melalui pertandingan)\n   - Kembali ke langkah 1\n"})}),"\n",(0,r.jsx)(n.p,{children:"Loop ini terus berjalan, network terus menjadi lebih kuat. Tanpa data manusia, tanpa pengetahuan manusia, hanya aturan permainan dan tujuan menang/kalah."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"implikasi-untuk-penelitian-ai",children:"Implikasi untuk Penelitian AI"}),"\n",(0,r.jsx)(n.h3,{id:"pembelajaran-prinsip-pertama",children:"Pembelajaran Prinsip Pertama"}),"\n",(0,r.jsx)(n.p,{children:'AlphaGo Zero mendemonstrasikan metode pembelajaran "prinsip pertama":'}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"Jangan beritahu AI cara melakukan, hanya beritahu apa tujuannya, biarkan ia menemukan caranya sendiri."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Ini membentuk kontras tajam dengan pendekatan expert system tradisional. Expert system mencoba mengkodekan pengetahuan manusia ke dalam AI, sedangkan AlphaGo Zero membiarkan AI menemukan pengetahuan sendiri."}),"\n",(0,r.jsx)(n.p,{children:"Hasilnya: pengetahuan yang ditemukan AI mungkin lebih lengkap dan akurat daripada pengetahuan manusia."}),"\n",(0,r.jsx)(n.h3,{id:"kekuatan-self-play",children:"Kekuatan Self-Play"}),"\n",(0,r.jsx)(n.p,{children:"AlphaGo Zero membuktikan self-play bisa menghasilkan data pelatihan tak terbatas, dan kualitas data ini akan meningkat seiring peningkatan network."}),"\n",(0,r.jsx)(n.p,{children:'Ini adalah "siklus positif":'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Network lebih kuat \u2192 Data self-play lebih baik"}),"\n",(0,r.jsx)(n.li,{children:"Data lebih baik \u2192 Network lebih kuat"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Siklus ini bisa terus berjalan, sampai mencapai batas teori permainan (jika ada)."}),"\n",(0,r.jsx)(n.h3,{id:"pentingnya-penyederhanaan",children:"Pentingnya Penyederhanaan"}),"\n",(0,r.jsx)(n.p,{children:'Keberhasilan AlphaGo Zero membuktikan pentingnya "penyederhanaan":'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Sederhanakan input (48 \u2192 17)"}),"\n",(0,r.jsx)(n.li,{children:"Sederhanakan arsitektur (dual network \u2192 single network)"}),"\n",(0,r.jsx)(n.li,{children:"Sederhanakan pelatihan (supervised + reinforcement \u2192 reinforcement murni)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Setiap penyederhanaan membuat sistem lebih powerful. Ini memberitahu kita: kompleks tidak berarti bagus, solusi paling sederhana seringkali yang terbaik."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"korespondensi-animasi",children:"Korespondensi Animasi"}),"\n",(0,r.jsx)(n.p,{children:"Konsep inti yang dibahas dalam artikel ini dan nomor animasinya:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Nomor"}),(0,r.jsx)(n.th,{children:"Konsep"}),(0,r.jsx)(n.th,{children:"Korespondensi Fisika/Matematika"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"E5 E7"}),(0,r.jsx)(n.td,{children:"Pelatihan dari nol"}),(0,r.jsx)(n.td,{children:"Fenomena self-organization"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"E5 E5"}),(0,r.jsx)(n.td,{children:"Self-play"}),(0,r.jsx)(n.td,{children:"Konvergensi fixed-point"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"E5 E12"}),(0,r.jsx)(n.td,{children:"Kurva pertumbuhan kekuatan"}),(0,r.jsx)(n.td,{children:"Pertumbuhan S-shape"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"E5 D12"}),(0,r.jsx)(n.td,{children:"Residual network"}),(0,r.jsx)(n.td,{children:"Highway gradien"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"bacaan-lanjutan",children:"Bacaan Lanjutan"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Artikel Berikutnya"}),": ",(0,r.jsx)(n.a,{href:"../dual-head-resnet",children:"Dual-Head Network dan Residual Network"})," \u2014 Penjelasan detail arsitektur neural network AlphaGo Zero"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Artikel Terkait"}),": ",(0,r.jsx)(n.a,{href:"../self-play",children:"Self-Play"})," \u2014 Mengapa self-play bisa menghasilkan level superhuman"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Teknis Mendalam"}),": ",(0,r.jsx)(n.a,{href:"../training-from-scratch",children:"Proses Pelatihan dari Nol"})," \u2014 Evolusi detail Hari 0-3"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"referensi",children:"Referensi"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:['Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." ',(0,r.jsx)(n.em,{children:"Nature"}),", 550, 354-359."]}),"\n",(0,r.jsxs)(n.li,{children:['Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." ',(0,r.jsx)(n.em,{children:"Science"}),", 362(6419), 1140-1144."]}),"\n",(0,r.jsxs)(n.li,{children:['DeepMind. (2017). "AlphaGo Zero: Starting from scratch." ',(0,r.jsx)(n.em,{children:"DeepMind Blog"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:['Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." ',(0,r.jsx)(n.em,{children:"Nature"}),", 588, 604-609."]}),"\n"]})]})}function m(a={}){const{wrapper:n}={...(0,l.R)(),...a.components};return n?(0,r.jsx)(n,{...a,children:(0,r.jsx)(u,{...a})}):u(a)}},30416(a,n,e){e.d(n,{R:()=>s,x:()=>t});var i=e(59471);const r={},l=i.createContext(r);function s(a){const n=i.useContext(l);return i.useMemo(function(){return"function"==typeof a?a(n):{...n,...a}},[n,a])}function t(a){let n;return n=a.disableParentContext?"function"==typeof a.components?a.components(r):a.components||r:s(a.components),i.createElement(l.Provider,{value:n},a.children)}}}]);