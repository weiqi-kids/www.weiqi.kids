"use strict";(self.webpackChunktemp_docusaurus=self.webpackChunktemp_docusaurus||[]).push([[4792],{2737:(a,e,n)=>{n.r(e),n.d(e,{assets:()=>d,contentTitle:()=>t,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"for-engineers/background-info/alphago","title":"Pembahasan Makalah AlphaGo","description":"Artikel ini menganalisis secara mendalam makalah klasik yang diterbitkan DeepMind di Nature \\"Mastering the game of Go with deep neural networks and tree search\\", serta makalah lanjutan AlphaGo Zero dan AlphaZero.","source":"@site/i18n/id/docusaurus-plugin-content-docs/current/for-engineers/background-info/alphago.md","sourceDirName":"for-engineers/background-info","slug":"/for-engineers/background-info/alphago","permalink":"/id/docs/for-engineers/background-info/alphago","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/background-info/alphago.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Pembahasan Makalah AlphaGo"},"sidebar":"tutorialSidebar","previous":{"title":"Latar Belakang Pengetahuan","permalink":"/id/docs/for-engineers/background-info/"},"next":{"title":"Pembahasan Makalah KataGo","permalink":"/id/docs/for-engineers/background-info/katago-paper"}}');var r=n(3420),l=n(5521);const s={sidebar_position:1,title:"Pembahasan Makalah AlphaGo"},t="Pembahasan Makalah AlphaGo",d={},h=[{value:"Signifikansi Sejarah AlphaGo",id:"signifikansi-sejarah-alphago",level:2},{value:"Peristiwa Tonggak",id:"peristiwa-tonggak",level:3},{value:"Arsitektur Teknis Inti",id:"arsitektur-teknis-inti",level:2},{value:"Policy Network (Jaringan Strategi)",id:"policy-network-jaringan-strategi",level:3},{value:"Arsitektur Jaringan",id:"arsitektur-jaringan",level:4},{value:"Fitur Input",id:"fitur-input",level:4},{value:"Metode Pelatihan",id:"metode-pelatihan",level:4},{value:"Value Network (Jaringan Nilai)",id:"value-network-jaringan-nilai",level:3},{value:"Arsitektur Jaringan",id:"arsitektur-jaringan-1",level:4},{value:"Metode Pelatihan",id:"metode-pelatihan-1",level:4},{value:"Monte Carlo Tree Search (MCTS)",id:"monte-carlo-tree-search-mcts",level:2},{value:"Empat Langkah MCTS",id:"empat-langkah-mcts",level:3},{value:"Formula Seleksi (PUCT)",id:"formula-seleksi-puct",level:3},{value:"Penjelasan Detail Proses Pencarian",id:"penjelasan-detail-proses-pencarian",level:3},{value:"Rollout (Bermain Cepat)",id:"rollout-bermain-cepat",level:3},{value:"Metode Pelatihan Self-play",id:"metode-pelatihan-self-play",level:2},{value:"Siklus Pelatihan",id:"siklus-pelatihan",level:3},{value:"Mengapa Self-play Efektif?",id:"mengapa-self-play-efektif",level:3},{value:"Perbaikan AlphaGo Zero",id:"perbaikan-alphago-zero",level:2},{value:"Perbedaan Utama",id:"perbedaan-utama",level:3},{value:"Penyederhanaan Arsitektur",id:"penyederhanaan-arsitektur",level:3},{value:"Fitur Input yang Disederhanakan",id:"fitur-input-yang-disederhanakan",level:3},{value:"Perbaikan Pelatihan",id:"perbaikan-pelatihan",level:3},{value:"Generalisasi AlphaZero",id:"generalisasi-alphazero",level:2},{value:"Fitur Utama",id:"fitur-utama",level:3},{value:"Perbedaan dengan AlphaGo Zero",id:"perbedaan-dengan-alphago-zero",level:3},{value:"Poin Penting Implementasi",id:"poin-penting-implementasi",level:2},{value:"Sumber Daya Komputasi",id:"sumber-daya-komputasi",level:3},{value:"Hyperparameter Utama",id:"hyperparameter-utama",level:3},{value:"Masalah Umum",id:"masalah-umum",level:3},{value:"Bacaan Lanjutan",id:"bacaan-lanjutan",level:2}];function o(a){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...a.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"pembahasan-makalah-alphago",children:"Pembahasan Makalah AlphaGo"})}),"\n",(0,r.jsx)(e.p,{children:'Artikel ini menganalisis secara mendalam makalah klasik yang diterbitkan DeepMind di Nature "Mastering the game of Go with deep neural networks and tree search", serta makalah lanjutan AlphaGo Zero dan AlphaZero.'}),"\n",(0,r.jsx)(e.h2,{id:"signifikansi-sejarah-alphago",children:"Signifikansi Sejarah AlphaGo"}),"\n",(0,r.jsx)(e.p,{children:'Go telah lama dianggap sebagai tantangan "Cawan Suci" kecerdasan buatan. Berbeda dengan catur internasional, ruang pencarian Go sangat besar:'}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Permainan"}),(0,r.jsx)(e.th,{children:"Rata-rata Branching Factor"}),(0,r.jsx)(e.th,{children:"Rata-rata Panjang Permainan"}),(0,r.jsx)(e.th,{children:"State Space"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Catur Internasional"}),(0,r.jsx)(e.td,{children:"~35"}),(0,r.jsx)(e.td,{children:"~80"}),(0,r.jsx)(e.td,{children:"~10^47"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Go"}),(0,r.jsx)(e.td,{children:"~250"}),(0,r.jsx)(e.td,{children:"~150"}),(0,r.jsx)(e.td,{children:"~10^170"})]})]})]}),"\n",(0,r.jsx)(e.p,{children:"Metode brute-force search tradisional sama sekali tidak dapat diterapkan pada Go. AlphaGo mengalahkan Lee Sedol pada 2016 membuktikan kekuatan kombinasi deep learning dan reinforcement learning."}),"\n",(0,r.jsx)(e.h3,{id:"peristiwa-tonggak",children:"Peristiwa Tonggak"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Oktober 2015"}),": AlphaGo Fan mengalahkan juara Eropa Fan Hui 5:0 (profesional 2-dan)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Maret 2016"}),": AlphaGo Lee mengalahkan juara dunia Lee Sedol 4:1 (profesional 9-dan)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Mei 2017"}),": AlphaGo Master mengalahkan peringkat satu dunia Ke Jie 3:0"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Oktober 2017"}),": AlphaGo Zero dipublikasikan, pelatihan self-play murni, melampaui semua versi sebelumnya"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"arsitektur-teknis-inti",children:"Arsitektur Teknis Inti"}),"\n",(0,r.jsx)(e.p,{children:"Inovasi inti AlphaGo adalah menggabungkan tiga teknologi kunci:"}),"\n",(0,r.jsx)(e.mermaid,{value:'graph TD\n    subgraph AlphaGo["Arsitektur AlphaGo"]\n        PolicyNet["Policy Network<br/>(Strategi)"]\n        ValueNet["Value Network<br/>(Evaluasi)"]\n        MCTS["MCTS<br/>(Monte Carlo Tree Search)"]\n\n        PolicyNet --\x3e MCTS\n        ValueNet --\x3e MCTS\n    end'}),"\n",(0,r.jsx)(e.h3,{id:"policy-network-jaringan-strategi",children:"Policy Network (Jaringan Strategi)"}),"\n",(0,r.jsx)(e.p,{children:"Policy Network bertanggung jawab memprediksi probabilitas bermain di setiap posisi, digunakan untuk memandu arah pencarian."}),"\n",(0,r.jsx)(e.h4,{id:"arsitektur-jaringan",children:"Arsitektur Jaringan"}),"\n",(0,r.jsx)(e.mermaid,{value:'graph TD\n    Input["Layer Input: 19\xd719\xd748 feature planes"]\n    Conv1["Layer Konvolusi 1: 5\xd75 kernel, 192 filter"]\n    Conv2_12["Layer Konvolusi 2-12: 3\xd73 kernel, 192 filter"]\n    Output["Layer Output: Distribusi probabilitas 19\xd719 (softmax)"]\n\n    Input --\x3e Conv1\n    Conv1 --\x3e Conv2_12\n    Conv2_12 --\x3e Output'}),"\n",(0,r.jsx)(e.h4,{id:"fitur-input",children:"Fitur Input"}),"\n",(0,r.jsx)(e.p,{children:"AlphaGo menggunakan 48 feature planes sebagai input:"}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Fitur"}),(0,r.jsx)(e.th,{children:"Jumlah Plane"}),(0,r.jsx)(e.th,{children:"Deskripsi"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Warna batu"}),(0,r.jsx)(e.td,{children:"3"}),(0,r.jsx)(e.td,{children:"Batu hitam, batu putih, titik kosong"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Jumlah liberti"}),(0,r.jsx)(e.td,{children:"8"}),(0,r.jsx)(e.td,{children:"1 liberti, 2 liberti, ..., 8 liberti atau lebih"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Liberti setelah penangkapan"}),(0,r.jsx)(e.td,{children:"8"}),(0,r.jsx)(e.td,{children:"Berapa liberti setelah menangkap"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Jumlah tangkapan"}),(0,r.jsx)(e.td,{children:"8"}),(0,r.jsx)(e.td,{children:"Berapa batu yang dapat ditangkap di posisi itu"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Ko"}),(0,r.jsx)(e.td,{children:"1"}),(0,r.jsx)(e.td,{children:"Apakah posisi ko"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Legalitas bermain"}),(0,r.jsx)(e.td,{children:"1"}),(0,r.jsx)(e.td,{children:"Apakah posisi itu dapat dimainkan"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Posisi 1-8 langkah terakhir"}),(0,r.jsx)(e.td,{children:"8"}),(0,r.jsx)(e.td,{children:"Posisi beberapa langkah sebelumnya"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Giliran siapa"}),(0,r.jsx)(e.td,{children:"1"}),(0,r.jsx)(e.td,{children:"Saat ini giliran hitam atau putih"})]})]})]}),"\n",(0,r.jsx)(e.h4,{id:"metode-pelatihan",children:"Metode Pelatihan"}),"\n",(0,r.jsx)(e.p,{children:"Pelatihan Policy Network dibagi dua tahap:"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Tahap Pertama: Supervised Learning (SL Policy Network)"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Menggunakan 30 juta permainan dari server Go KGS"}),"\n",(0,r.jsx)(e.li,{children:"Tujuan: Memprediksi langkah berikutnya pemain manusia"}),"\n",(0,r.jsx)(e.li,{children:"Mencapai akurasi prediksi 57%"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Tahap Kedua: Reinforcement Learning (RL Policy Network)"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Mulai dari SL Policy Network"}),"\n",(0,r.jsx)(e.li,{children:"Self-play melawan versi sebelumnya"}),"\n",(0,r.jsx)(e.li,{children:"Optimisasi menggunakan algoritma REINFORCE"}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"# Simplified Policy Gradient update\n# reward: +1 menang, -1 kalah\nloss = -log(policy[action]) * reward\n"})}),"\n",(0,r.jsx)(e.h3,{id:"value-network-jaringan-nilai",children:"Value Network (Jaringan Nilai)"}),"\n",(0,r.jsx)(e.p,{children:"Value Network mengevaluasi tingkat kemenangan posisi saat ini, digunakan untuk mengurangi kedalaman pencarian."}),"\n",(0,r.jsx)(e.h4,{id:"arsitektur-jaringan-1",children:"Arsitektur Jaringan"}),"\n",(0,r.jsx)(e.mermaid,{value:'graph TD\n    Input["Layer Input: 19\xd719\xd748 feature planes<br/>(sama dengan Policy Network)"]\n    Conv["Layer Konvolusi 1-12: Mirip Policy Network"]\n    FC["Layer Fully Connected: 256 neuron"]\n    Output["Layer Output: 1 neuron<br/>(tanh, range [-1, 1])"]\n\n    Input --\x3e Conv\n    Conv --\x3e FC\n    FC --\x3e Output'}),"\n",(0,r.jsx)(e.h4,{id:"metode-pelatihan-1",children:"Metode Pelatihan"}),"\n",(0,r.jsx)(e.p,{children:"Value Network dilatih menggunakan 30 juta posisi dari self-play RL Policy Network:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Sampel acak satu posisi dari setiap permainan"}),"\n",(0,r.jsx)(e.li,{children:"Gunakan hasil akhir menang-kalah sebagai label"}),"\n",(0,r.jsx)(e.li,{children:"Gunakan fungsi loss MSE"}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"# Pelatihan Value Network\nvalue_prediction = value_network(position)\nloss = (value_prediction - game_outcome) ** 2\n"})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mengapa hanya mengambil satu sampel per permainan?"})}),"\n",(0,r.jsx)(e.p,{children:"Jika mengambil banyak sampel, posisi berdekatan dari permainan yang sama akan sangat berkorelasi, menyebabkan overfitting. Sampling acak memastikan keragaman data pelatihan."}),"\n",(0,r.jsx)(e.h2,{id:"monte-carlo-tree-search-mcts",children:"Monte Carlo Tree Search (MCTS)"}),"\n",(0,r.jsx)(e.p,{children:"MCTS adalah inti pengambilan keputusan AlphaGo, menggabungkan neural network untuk pencarian langkah terbaik yang efisien."}),"\n",(0,r.jsx)(e.h3,{id:"empat-langkah-mcts",children:"Empat Langkah MCTS"}),"\n",(0,r.jsx)(e.mermaid,{value:'graph LR\n    subgraph Step1["(1) Selection"]\n        S1["Pilih<br/>Jalur Terbaik"]\n    end\n    subgraph Step2["(2) Expansion"]\n        S2["Perluas<br/>Node Baru"]\n    end\n    subgraph Step3["(3) Evaluation"]\n        S3["Neural Network<br/>Evaluasi"]\n    end\n    subgraph Step4["(4) Backpropagation"]\n        S4["Kirim Kembali<br/>Update"]\n    end\n\n    Step1 --\x3e Step2\n    Step2 --\x3e Step3\n    Step3 --\x3e Step4'}),"\n",(0,r.jsx)(e.h3,{id:"formula-seleksi-puct",children:"Formula Seleksi (PUCT)"}),"\n",(0,r.jsx)(e.p,{children:"AlphaGo menggunakan formula PUCT (Predictor + UCT) untuk memilih cabang yang akan dieksplorasi:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"a = argmax[Q(s,a) + u(s,a)]\n\nu(s,a) = c_puct * P(s,a) * sqrt(N(s)) / (1 + N(s,a))\n"})}),"\n",(0,r.jsx)(e.p,{children:"Di mana:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Q(s,a)"}),": Nilai rata-rata aksi a (exploitation)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"P(s,a)"}),": Probabilitas prior yang diprediksi Policy Network"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"N(s)"}),": Jumlah kunjungan parent node"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"N(s,a)"}),": Jumlah kunjungan aksi tersebut"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"c_puct"}),": Konstanta eksplorasi, menyeimbangkan exploration dan exploitation"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"penjelasan-detail-proses-pencarian",children:"Penjelasan Detail Proses Pencarian"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Selection"}),": Dari root node, gunakan formula PUCT untuk memilih aksi, sampai mencapai leaf node"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Expansion"}),": Perluas child node baru di leaf node, gunakan Policy Network untuk menginisialisasi probabilitas prior"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Evaluation"}),": Gabungkan evaluasi Value Network dan simulasi bermain cepat (Rollout) untuk mengevaluasi nilai"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Backpropagation"}),": Kirim nilai evaluasi kembali melalui jalur, perbarui nilai Q dan N"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"rollout-bermain-cepat",children:"Rollout (Bermain Cepat)"}),"\n",(0,r.jsx)(e.p,{children:"AlphaGo (bukan versi Zero) juga menggunakan jaringan strategi kecil dan cepat untuk simulasi:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"Leaf node \u2192 Bermain acak cepat sampai akhir permainan \u2192 Hitung menang-kalah\n"})}),"\n",(0,r.jsx)(e.p,{children:"Nilai evaluasi akhir menggabungkan Value Network dan Rollout:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"V = \u03bb * v_network + (1-\u03bb) * v_rollout\n"})}),"\n",(0,r.jsx)(e.p,{children:"AlphaGo menggunakan \u03bb = 0.5, memberikan bobot yang sama untuk keduanya."}),"\n",(0,r.jsx)(e.h2,{id:"metode-pelatihan-self-play",children:"Metode Pelatihan Self-play"}),"\n",(0,r.jsx)(e.p,{children:"Self-play adalah strategi pelatihan inti AlphaGo, memungkinkan AI terus meningkat melalui bermain melawan dirinya sendiri."}),"\n",(0,r.jsx)(e.h3,{id:"siklus-pelatihan",children:"Siklus Pelatihan"}),"\n",(0,r.jsx)(e.mermaid,{value:'graph LR\n    subgraph SelfPlay["Siklus Pelatihan Self-play"]\n        CurrentModel["Model Saat ini"]\n        Play["Self-play"]\n        GenerateData["Hasilkan Data"]\n        DataPool["Data Pool"]\n        Train["Latih"]\n        NewModel["Model Baru"]\n\n        CurrentModel --\x3e Play\n        Play --\x3e GenerateData\n        GenerateData --\x3e DataPool\n        DataPool --\x3e Train\n        Train --\x3e NewModel\n        NewModel --\x3e CurrentModel\n    end'}),"\n",(0,r.jsx)(e.h3,{id:"mengapa-self-play-efektif",children:"Mengapa Self-play Efektif?"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Data tak terbatas"}),": Tidak dibatasi oleh jumlah catatan permainan manusia"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Kesulitan adaptif"}),": Kekuatan lawan meningkat seiring dengan diri sendiri"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Eksplorasi inovasi"}),": Tidak dibatasi pola pikir manusia yang ada"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Tujuan jelas"}),": Langsung mengoptimalkan tingkat kemenangan, bukan meniru manusia"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"perbaikan-alphago-zero",children:"Perbaikan AlphaGo Zero"}),"\n",(0,r.jsx)(e.p,{children:"AlphaGo Zero yang dipublikasikan tahun 2017 membawa perbaikan revolusioner:"}),"\n",(0,r.jsx)(e.h3,{id:"perbedaan-utama",children:"Perbedaan Utama"}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Fitur"}),(0,r.jsx)(e.th,{children:"AlphaGo"}),(0,r.jsx)(e.th,{children:"AlphaGo Zero"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Pelatihan awal"}),(0,r.jsx)(e.td,{children:"Supervised learning catatan manusia"}),(0,r.jsx)(e.td,{children:"Sepenuhnya dari nol"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Arsitektur jaringan"}),(0,r.jsx)(e.td,{children:"Policy/Value terpisah"}),(0,r.jsx)(e.td,{children:"Jaringan tunggal dua kepala"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Struktur jaringan"}),(0,r.jsx)(e.td,{children:"CNN biasa"}),(0,r.jsx)(e.td,{children:"ResNet"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Feature engineering"}),(0,r.jsx)(e.td,{children:"48 fitur handcrafted"}),(0,r.jsx)(e.td,{children:"17 fitur sederhana"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Rollout"}),(0,r.jsx)(e.td,{children:"Diperlukan"}),(0,r.jsx)(e.td,{children:"Tidak diperlukan"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Waktu pelatihan"}),(0,r.jsx)(e.td,{children:"Berbulan-bulan"}),(0,r.jsx)(e.td,{children:"3 hari melampaui manusia"})]})]})]}),"\n",(0,r.jsx)(e.h3,{id:"penyederhanaan-arsitektur",children:"Penyederhanaan Arsitektur"}),"\n",(0,r.jsx)(e.mermaid,{value:'graph TD\n    subgraph AlphaGoZero["Jaringan Dua Kepala AlphaGo Zero"]\n        Input["Input: 19\xd719\xd717 (fitur sederhana)"]\n        ResNet["ResNet Backbone<br/>(40 residual blocks)"]\n        PolicyHead["Policy Head<br/>(19\xd719+1)"]\n        ValueHead["Value Head<br/>([-1,1])"]\n\n        Input --\x3e ResNet\n        ResNet --\x3e PolicyHead\n        ResNet --\x3e ValueHead\n    end'}),"\n",(0,r.jsx)(e.h3,{id:"fitur-input-yang-disederhanakan",children:"Fitur Input yang Disederhanakan"}),"\n",(0,r.jsx)(e.p,{children:"AlphaGo Zero hanya menggunakan 17 feature planes:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"8 planes: Posisi batu diri sendiri untuk 8 langkah terakhir"}),"\n",(0,r.jsx)(e.li,{children:"8 planes: Posisi batu lawan untuk 8 langkah terakhir"}),"\n",(0,r.jsx)(e.li,{children:"1 plane: Giliran siapa saat ini (semua 0 atau semua 1)"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"perbaikan-pelatihan",children:"Perbaikan Pelatihan"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Self-play murni"}),": Tidak menggunakan data manusia sama sekali"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Langsung gunakan probabilitas MCTS sebagai target pelatihan"}),": Bukan menang-kalah biner"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Tanpa Rollout"}),": Sepenuhnya bergantung pada Value Network"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Pelatihan jaringan tunggal"}),": Policy dan Value berbagi parameter, saling memperkuat"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"generalisasi-alphazero",children:"Generalisasi AlphaZero"}),"\n",(0,r.jsx)(e.p,{children:"AlphaZero yang dipublikasikan akhir 2017 menerapkan arsitektur yang sama pada Go, catur internasional, dan shogi Jepang:"}),"\n",(0,r.jsx)(e.h3,{id:"fitur-utama",children:"Fitur Utama"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Nol pengetahuan domain"}),": Tidak menggunakan pengetahuan domain spesifik selain aturan permainan"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Arsitektur terpadu"}),": Satu set algoritma yang sama berlaku untuk berbagai jenis catur"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Pelatihan lebih cepat"}),":","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Go: 8 jam melampaui AlphaGo Lee"}),"\n",(0,r.jsx)(e.li,{children:"Catur internasional: 4 jam melampaui Stockfish"}),"\n",(0,r.jsx)(e.li,{children:"Shogi: 2 jam melampaui Elmo"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"perbedaan-dengan-alphago-zero",children:"Perbedaan dengan AlphaGo Zero"}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Fitur"}),(0,r.jsx)(e.th,{children:"AlphaGo Zero"}),(0,r.jsx)(e.th,{children:"AlphaZero"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Target permainan"}),(0,r.jsx)(e.td,{children:"Hanya Go"}),(0,r.jsx)(e.td,{children:"Go, catur internasional, shogi"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Pemanfaatan simetri"}),(0,r.jsx)(e.td,{children:"Memanfaatkan 8-fold simetri Go"}),(0,r.jsx)(e.td,{children:"Tidak mengasumsikan simetri"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Tuning hyperparameter"}),(0,r.jsx)(e.td,{children:"Dioptimalkan untuk Go"}),(0,r.jsx)(e.td,{children:"Pengaturan universal"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Metode pelatihan"}),(0,r.jsx)(e.td,{children:"Self-play model terbaik"}),(0,r.jsx)(e.td,{children:"Self-play model terbaru"})]})]})]}),"\n",(0,r.jsx)(e.h2,{id:"poin-penting-implementasi",children:"Poin Penting Implementasi"}),"\n",(0,r.jsx)(e.p,{children:"Jika Anda ingin mengimplementasikan sistem serupa, berikut pertimbangan utama:"}),"\n",(0,r.jsx)(e.h3,{id:"sumber-daya-komputasi",children:"Sumber Daya Komputasi"}),"\n",(0,r.jsx)(e.p,{children:"Pelatihan AlphaGo memerlukan sumber daya komputasi yang besar:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"AlphaGo Lee"}),": 176 GPU + 48 TPU"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"AlphaGo Zero"}),": 4 TPU (pelatihan) + 1 TPU (self-play)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"AlphaZero"}),": 5000 TPU (pelatihan)"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"hyperparameter-utama",children:"Hyperparameter Utama"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"# Terkait MCTS\nnum_simulations = 800     # Jumlah simulasi pencarian per langkah\nc_puct = 1.5              # Konstanta eksplorasi\ntemperature = 1.0         # Parameter suhu untuk memilih aksi\n\n# Terkait pelatihan\nbatch_size = 2048\nlearning_rate = 0.01      # Dengan decay\nl2_regularization = 1e-4\n"})}),"\n",(0,r.jsx)(e.h3,{id:"masalah-umum",children:"Masalah Umum"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Pelatihan tidak stabil"}),": Gunakan learning rate lebih kecil, tingkatkan batch size"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Overfitting"}),": Pastikan keragaman data pelatihan, gunakan regularisasi"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Efisiensi pencarian"}),": Optimalkan batch inference GPU, paralelisasi MCTS"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"bacaan-lanjutan",children:"Bacaan Lanjutan"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://www.nature.com/articles/nature16961",children:"Makalah asli: Mastering the game of Go with deep neural networks and tree search"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://www.nature.com/articles/nature24270",children:"Makalah AlphaGo Zero: Mastering the game of Go without human knowledge"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://www.science.org/doi/10.1126/science.aar6404",children:"Makalah AlphaZero: A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"})}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:["Setelah memahami teknologi AlphaGo, selanjutnya mari kita lihat ",(0,r.jsx)(e.a,{href:"/id/docs/for-engineers/background-info/katago-paper",children:"bagaimana KataGo melakukan perbaikan berdasarkan ini"}),"."]})]})}function u(a={}){const{wrapper:e}={...(0,l.R)(),...a.components};return e?(0,r.jsx)(e,{...a,children:(0,r.jsx)(o,{...a})}):o(a)}},5521:(a,e,n)=>{n.d(e,{R:()=>s,x:()=>t});var i=n(6672);const r={},l=i.createContext(r);function s(a){const e=i.useContext(l);return i.useMemo((function(){return"function"==typeof a?a(e):{...e,...a}}),[e,a])}function t(a){let e;return e=a.disableParentContext?"function"==typeof a.components?a.components(r):a.components||r:s(a.components),i.createElement(l.Provider,{value:e},a.children)}}}]);