"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[441],{74443(a,e,n){n.r(e),n.d(e,{assets:()=>o,contentTitle:()=>d,default:()=>p,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"alphago/explained/policy-network","title":"Penjelasan Detail Policy Network","description":"Memahami secara mendalam arsitektur, metode training, dan aplikasi praktis policy network AlphaGo, dari 13 layer konvolusi hingga output Softmax","source":"@site/i18n/id/docusaurus-plugin-content-docs/current/alphago/explained/07-policy-network.mdx","sourceDirName":"alphago/explained","slug":"/alphago/explained/policy-network","permalink":"/id/docs/alphago/explained/policy-network","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/explained/07-policy-network.mdx","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"title":"Penjelasan Detail Policy Network","description":"Memahami secara mendalam arsitektur, metode training, dan aplikasi praktis policy network AlphaGo, dari 13 layer konvolusi hingga output Softmax"},"sidebar":"tutorialSidebar","previous":{"title":"Representasi Status Papan","permalink":"/id/docs/alphago/explained/board-representation"},"next":{"title":"Penjelasan Detail Value Network","permalink":"/id/docs/alphago/explained/value-network"}}');var t=n(62615),r=n(30416),s=n(45695);const l={sidebar_position:8,title:"Penjelasan Detail Policy Network",description:"Memahami secara mendalam arsitektur, metode training, dan aplikasi praktis policy network AlphaGo, dari 13 layer konvolusi hingga output Softmax"},d="Penjelasan Detail Policy Network",o={},c=[{value:"Apa itu Policy Network?",id:"apa-itu-policy-network",level:2},{value:"Fungsi Inti",id:"fungsi-inti",level:3},{value:"Pemahaman Intuitif",id:"pemahaman-intuitif",level:3},{value:"Mengapa Memerlukan Policy Network?",id:"mengapa-memerlukan-policy-network",level:3},{value:"Arsitektur Jaringan",id:"arsitektur-jaringan",level:2},{value:"Struktur Keseluruhan",id:"struktur-keseluruhan",level:3},{value:"Layer Input",id:"layer-input",level:3},{value:"Layer Konvolusi",id:"layer-konvolusi",level:3},{value:"Mengapa 192 Filter?",id:"mengapa-192-filter",level:4},{value:"Mengapa Kernel 3\xd73?",id:"mengapa-kernel-33",level:4},{value:"Mengapa Layer Pertama Menggunakan 5\xd75?",id:"mengapa-layer-pertama-menggunakan-55",level:4},{value:"Fungsi Aktivasi ReLU",id:"fungsi-aktivasi-relu",level:3},{value:"Layer Output",id:"layer-output",level:3},{value:"Konvolusi 1\xd71",id:"konvolusi-11",level:4},{value:"Output Softmax",id:"output-softmax",level:4},{value:"Jumlah Parameter",id:"jumlah-parameter",level:3},{value:"Tujuan dan Metode Training",id:"tujuan-dan-metode-training",level:2},{value:"Data Training",id:"data-training",level:3},{value:"Fungsi Loss Cross-Entropy",id:"fungsi-loss-cross-entropy",level:3},{value:"Pemahaman Intuitif",id:"pemahaman-intuitif-1",level:4},{value:"Proses Training",id:"proses-training",level:3},{value:"Augmentasi Data",id:"augmentasi-data",level:3},{value:"Hasil Training",id:"hasil-training",level:2},{value:"Akurasi 57%",id:"akurasi-57",level:3},{value:"Apakah Akurasi Ini Tinggi?",id:"apakah-akurasi-ini-tinggi",level:4},{value:"Peningkatan Kekuatan Bermain",id:"peningkatan-kekuatan-bermain",level:3},{value:"Mengapa Hanya 57%?",id:"mengapa-hanya-57",level:3},{value:"1. Beberapa Langkah Bagus",id:"1-beberapa-langkah-bagus",level:4},{value:"2. Perbedaan Gaya",id:"2-perbedaan-gaya",level:4},{value:"3. Manusia Juga Membuat Kesalahan",id:"3-manusia-juga-membuat-kesalahan",level:4},{value:"Peran dalam MCTS",id:"peran-dalam-mcts",level:2},{value:"1. Membimbing Arah Pencarian",id:"1-membimbing-arah-pencarian",level:3},{value:"2. Prior untuk Node yang Diperluas",id:"2-prior-untuk-node-yang-diperluas",level:3},{value:"Versi Ringan vs Versi Lengkap",id:"versi-ringan-vs-versi-lengkap",level:2},{value:"Versi Lengkap (SL Policy Network)",id:"versi-lengkap-sl-policy-network",level:3},{value:"Versi Ringan (Rollout Policy Network)",id:"versi-ringan-rollout-policy-network",level:3},{value:"Mengapa Memerlukan Versi Ringan?",id:"mengapa-memerlukan-versi-ringan",level:3},{value:"Fitur Versi Ringan",id:"fitur-versi-ringan",level:3},{value:"Perbaikan AlphaGo Zero",id:"perbaikan-alphago-zero",level:3},{value:"Fine-tuning Reinforcement Learning (RL Policy Network)",id:"fine-tuning-reinforcement-learning-rl-policy-network",level:2},{value:"Keterbatasan Supervised Learning",id:"keterbatasan-supervised-learning",level:3},{value:"Penguatan Self-Play",id:"penguatan-self-play",level:3},{value:"Algoritma REINFORCE",id:"algoritma-reinforce",level:3},{value:"Hasil",id:"hasil",level:3},{value:"Dari &quot;Meniru&quot; ke &quot;Berinovasi&quot;",id:"dari-meniru-ke-berinovasi",level:3},{value:"Analisis Visualisasi",id:"analisis-visualisasi",level:2},{value:"Distribusi Probabilitas di Posisi Berbeda",id:"distribusi-probabilitas-di-posisi-berbeda",level:3},{value:"Pembukaan (Tahap Fuseki)",id:"pembukaan-tahap-fuseki",level:4},{value:"Posisi Pertarungan",id:"posisi-pertarungan",level:4},{value:"Tahap Yose",id:"tahap-yose",level:4},{value:"Apa yang Dipelajari Hidden Layer?",id:"apa-yang-dipelajari-hidden-layer",level:3},{value:"Poin Implementasi",id:"poin-implementasi",level:2},{value:"Implementasi PyTorch",id:"implementasi-pytorch",level:3},{value:"Loop Training",id:"loop-training",level:3},{value:"Catatan Saat Inferensi",id:"catatan-saat-inferensi",level:3},{value:"Korespondensi Animasi",id:"korespondensi-animasi",level:2},{value:"Bacaan Lanjutan",id:"bacaan-lanjutan",level:2},{value:"Poin Kunci",id:"poin-kunci",level:2},{value:"Referensi",id:"referensi",level:2}];function h(a){const e={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...a.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"penjelasan-detail-policy-network",children:"Penjelasan Detail Policy Network"})}),"\n",(0,t.jsx)(e.p,{children:"Dalam posisi apapun di Go, rata-rata ada 250 langkah legal. Jika komputer memilih secara acak, tidak akan pernah bisa bermain dengan baik."}),"\n",(0,t.jsx)(e.p,{children:'Terobosan AlphaGo adalah: ia belajar "melihat papan sekali dan tahu posisi mana yang layak dipertimbangkan".'}),"\n",(0,t.jsxs)(e.p,{children:["Kemampuan ini berasal dari ",(0,t.jsx)(e.strong,{children:"Policy Network (Jaringan Kebijakan)"}),"."]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"apa-itu-policy-network",children:"Apa itu Policy Network?"}),"\n",(0,t.jsx)(e.h3,{id:"fungsi-inti",children:"Fungsi Inti"}),"\n",(0,t.jsx)(e.p,{children:"Policy Network adalah deep convolutional neural network, tugasnya adalah:"}),"\n",(0,t.jsxs)(e.blockquote,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Diberikan state papan saat ini, menghasilkan probabilitas langkah untuk setiap posisi"})}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"Dinyatakan secara matematis:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"p = f_\u03b8(s)\n"})}),"\n",(0,t.jsx)(e.p,{children:"Di mana:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"s"}),": State papan saat ini (papan 19\xd719 + fitur lainnya)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"f_\u03b8"}),": Policy Network (\u03b8 adalah parameter jaringan)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"p"}),": Distribusi probabilitas 361 posisi (termasuk pass)"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"pemahaman-intuitif",children:"Pemahaman Intuitif"}),"\n",(0,t.jsx)(e.p,{children:'Bayangkan Anda adalah pemain Go profesional. Ketika Anda melihat suatu posisi, otak Anda secara otomatis "menyalakan" beberapa posisi penting \u2014 ini adalah titik-titik yang intuisi Anda anggap layak dipertimbangkan.'}),"\n",(0,t.jsx)(e.p,{children:"Policy Network mensimulasikan proses ini."}),"\n",(0,t.jsx)(s.dW,{initialPosition:"corner",size:400}),"\n",(0,t.jsx)(e.p,{children:"Heatmap di atas menunjukkan output Policy Network. Warna yang lebih terang menunjukkan posisi yang model anggap lebih layak dimainkan."}),"\n",(0,t.jsx)(e.h3,{id:"mengapa-memerlukan-policy-network",children:"Mengapa Memerlukan Policy Network?"}),"\n",(0,t.jsx)(e.p,{children:"Ruang pencarian Go terlalu besar. Jika mencari semua kemungkinan langkah tanpa penyaringan:"}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Strategi"}),(0,t.jsx)(e.th,{children:"Langkah yang dipertimbangkan per giliran"}),(0,t.jsx)(e.th,{children:"Node untuk pencarian 10 langkah"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Pertimbangkan semua"}),(0,t.jsx)(e.td,{children:"361"}),(0,t.jsx)(e.td,{children:"361^10 \u2248 10^25"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Penyaringan Policy Network"}),(0,t.jsx)(e.td,{children:"~20"}),(0,t.jsx)(e.td,{children:"20^10 \u2248 10^13"})]})]})]}),"\n",(0,t.jsxs)(e.p,{children:["Policy Network mengurangi ruang pencarian sebesar ",(0,t.jsx)(e.strong,{children:"10^12 kali"})," (satu triliun kali)."]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"arsitektur-jaringan",children:"Arsitektur Jaringan"}),"\n",(0,t.jsx)(e.h3,{id:"struktur-keseluruhan",children:"Struktur Keseluruhan"}),"\n",(0,t.jsx)(e.p,{children:"Policy Network AlphaGo menggunakan arsitektur deep convolutional neural network (CNN):"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Layer Input \u2192 Layer Konvolusi \xd712 \u2192 Layer Konvolusi Output \u2192 Softmax\n   \u2193         \u2193            \u2193           \u2193\n19\xd719\xd748   19\xd719\xd7192   19\xd719\xd71     362 probabilitas\n"})}),"\n",(0,t.jsx)(e.h3,{id:"layer-input",children:"Layer Input"}),"\n",(0,t.jsxs)(e.p,{children:["Input adalah tensor fitur ",(0,t.jsx)(e.strong,{children:"19\xd719\xd748"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"19\xd719"}),": Ukuran papan"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"48"}),": 48 feature plane (lihat ",(0,t.jsx)(e.a,{href:"../input-features",children:"Desain Fitur Input"}),")"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"48 plane ini mencakup:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Posisi batu hitam, posisi batu putih"}),"\n",(0,t.jsx)(e.li,{children:"Riwayat 8 langkah terakhir"}),"\n",(0,t.jsx)(e.li,{children:"Fitur liberty, atari, ladder, dll."}),"\n",(0,t.jsx)(e.li,{children:"Legalitas (posisi mana yang bisa dimainkan)"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"layer-konvolusi",children:"Layer Konvolusi"}),"\n",(0,t.jsxs)(e.p,{children:["Jaringan berisi ",(0,t.jsx)(e.strong,{children:"12 layer konvolusi"}),", konfigurasi setiap layer:"]}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Parameter"}),(0,t.jsx)(e.th,{children:"Nilai"}),(0,t.jsx)(e.th,{children:"Deskripsi"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Jumlah filter"}),(0,t.jsx)(e.td,{children:"192"}),(0,t.jsx)(e.td,{children:"Setiap layer menghasilkan 192 feature map"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Ukuran kernel"}),(0,t.jsx)(e.td,{children:"3\xd73 (layer pertama 5\xd75)"}),(0,t.jsx)(e.td,{children:"Melihat area 3\xd73 setiap kali"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Mode padding"}),(0,t.jsx)(e.td,{children:"same"}),(0,t.jsx)(e.td,{children:"Mempertahankan ukuran 19\xd719"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Fungsi aktivasi"}),(0,t.jsx)(e.td,{children:"ReLU"}),(0,t.jsx)(e.td,{children:"max(0, x)"})]})]})]}),"\n",(0,t.jsx)(e.h4,{id:"mengapa-192-filter",children:"Mengapa 192 Filter?"}),"\n",(0,t.jsx)(e.p,{children:"Ini adalah nilai empiris. Terlalu sedikit akan membatasi kapasitas model, terlalu banyak akan meningkatkan beban komputasi dan risiko overfitting. Tim DeepMind menentukan 192 adalah titik keseimbangan yang baik melalui eksperimen."}),"\n",(0,t.jsx)(e.h4,{id:"mengapa-kernel-33",children:"Mengapa Kernel 3\xd73?"}),"\n",(0,t.jsx)(e.p,{children:"3\xd73 adalah ukuran paling umum dalam CNN, alasannya:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Cukup untuk menangkap pola lokal"}),": Eye, koneksi, pemotongan di Go semuanya dalam rentang 3\xd73"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Efisien secara komputasi"}),": Dibandingkan kernel besar, 3\xd73 memiliki lebih sedikit parameter"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dapat ditumpuk"}),": Multiple layer konvolusi 3\xd73 dapat mencapai receptive field besar"]}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"mengapa-layer-pertama-menggunakan-55",children:"Mengapa Layer Pertama Menggunakan 5\xd75?"}),"\n",(0,t.jsx)(e.p,{children:"Layer pertama menggunakan kernel 5\xd75 yang lebih besar untuk menangkap pola rentang sedikit lebih besar (seperti knight's move, jump) di layer input. Ini adalah pilihan desain, AlphaGo Zero kemudian menyatukan menggunakan 3\xd73."}),"\n",(0,t.jsx)(e.h3,{id:"fungsi-aktivasi-relu",children:"Fungsi Aktivasi ReLU"}),"\n",(0,t.jsx)(e.p,{children:"Setiap layer konvolusi diikuti fungsi aktivasi ReLU (Rectified Linear Unit):"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"ReLU(x) = max(0, x)\n"})}),"\n",(0,t.jsx)(e.p,{children:"Mengapa menggunakan ReLU?"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Komputasi sederhana"}),": Hanya mengambil maksimum, jauh lebih cepat dari sigmoid"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Mengurangi vanishing gradient"}),": Gradien di area positif selalu 1"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Aktivasi sparse"}),": Nilai negatif menjadi nol, menghasilkan representasi sparse"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"layer-output",children:"Layer Output"}),"\n",(0,t.jsx)(e.p,{children:"Layer terakhir adalah layer konvolusi khusus:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"19\xd719\xd7192 \u2192 Konvolusi(1\xd71, 1 filter) \u2192 19\xd719\xd71 \u2192 Flatten \u2192 Vektor 362 dimensi \u2192 Softmax\n"})}),"\n",(0,t.jsx)(e.h4,{id:"konvolusi-11",children:"Konvolusi 1\xd71"}),"\n",(0,t.jsx)(e.p,{children:"Layer output menggunakan konvolusi 1\xd71, mengompres 192 channel menjadi 1. Ini setara dengan kombinasi linear fitur 192 dimensi di setiap posisi."}),"\n",(0,t.jsx)(e.h4,{id:"output-softmax",children:"Output Softmax"}),"\n",(0,t.jsx)(e.p,{children:"Vektor 362 dimensi (361 posisi papan + 1 pass) melewati fungsi Softmax:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Softmax(z_i) = exp(z_i) / \u03a3_j exp(z_j)\n"})}),"\n",(0,t.jsx)(e.p,{children:"Softmax memastikan output adalah distribusi probabilitas yang valid:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Semua nilai antara 0 dan 1"}),"\n",(0,t.jsx)(e.li,{children:"Jumlah semua nilai adalah 1"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"jumlah-parameter",children:"Jumlah Parameter"}),"\n",(0,t.jsx)(e.p,{children:"Mari kita hitung total parameter jaringan:"}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Layer"}),(0,t.jsx)(e.th,{children:"Perhitungan"}),(0,t.jsx)(e.th,{children:"Jumlah Parameter"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Layer konvolusi pertama"}),(0,t.jsx)(e.td,{children:"5\xd75\xd748\xd7192 + 192"}),(0,t.jsx)(e.td,{children:"230,592"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Layer konvolusi tengah \xd711"}),(0,t.jsx)(e.td,{children:"(3\xd73\xd7192\xd7192 + 192) \xd7 11"}),(0,t.jsx)(e.td,{children:"3,633,792"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Layer konvolusi output"}),(0,t.jsx)(e.td,{children:"1\xd71\xd7192\xd71 + 1"}),(0,t.jsx)(e.td,{children:"193"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Total"})}),(0,t.jsx)(e.td,{}),(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"~3.9M"})})]})]})]}),"\n",(0,t.jsxs)(e.p,{children:["Sekitar ",(0,t.jsx)(e.strong,{children:"3,9 juta parameter"}),", yang merupakan jaringan kecil menurut standar saat ini."]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"tujuan-dan-metode-training",children:"Tujuan dan Metode Training"}),"\n",(0,t.jsx)(e.h3,{id:"data-training",children:"Data Training"}),"\n",(0,t.jsxs)(e.p,{children:["Policy Network menggunakan ",(0,t.jsx)(e.strong,{children:"supervised learning"}),", belajar dari catatan permainan manusia."]}),"\n",(0,t.jsx)(e.p,{children:"Sumber data:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"KGS Go Server"}),": Permainan pemain amatir dan profesional"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sekitar 30 juta posisi"}),": Diambil sampel dari 160.000 permainan"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Label"}),": Langkah manusia berikutnya yang sesuai dengan setiap posisi"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"fungsi-loss-cross-entropy",children:"Fungsi Loss Cross-Entropy"}),"\n",(0,t.jsx)(e.p,{children:"Tujuan training adalah memaksimalkan probabilitas memprediksi langkah manusia. Menggunakan fungsi loss cross-entropy:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"L(\u03b8) = -\u03a3 log p_\u03b8(a | s)\n"})}),"\n",(0,t.jsx)(e.p,{children:"Di mana:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"s"}),": State papan"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"a"}),": Posisi aktual yang dimainkan manusia"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"p_\u03b8(a | s)"}),": Probabilitas model memprediksi posisi tersebut"]}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"pemahaman-intuitif-1",children:"Pemahaman Intuitif"}),"\n",(0,t.jsx)(e.p,{children:"Loss cross-entropy memiliki makna sederhana:"}),"\n",(0,t.jsxs)(e.blockquote,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Semakin tinggi probabilitas model memprediksi posisi yang benar, semakin rendah loss-nya"})}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"Jika manusia bermain di K10, dan probabilitas model untuk K10 adalah:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"0.9 \u2192 Loss = -log(0.9) \u2248 0.1 (sangat rendah, bagus)"}),"\n",(0,t.jsx)(e.li,{children:"0.1 \u2192 Loss = -log(0.1) \u2248 2.3 (sangat tinggi, buruk)"}),"\n",(0,t.jsx)(e.li,{children:"0.01 \u2192 Loss = -log(0.01) \u2248 4.6 (sangat tinggi, sangat buruk)"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"proses-training",children:"Proses Training"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# Pseudocode\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        states, actions = batch\n\n        # Forward propagation\n        policy = network(states)  # Vektor probabilitas 361 dimensi\n\n        # Menghitung loss (cross-entropy)\n        loss = cross_entropy(policy, actions)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n"})}),"\n",(0,t.jsx)(e.p,{children:"Detail training:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Optimizer"}),": SGD with momentum"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Learning rate"}),": Awal 0.003, menurun secara bertahap"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Batch size"}),": 16"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Waktu training"}),": Sekitar 3 minggu (50 GPU)"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"augmentasi-data",children:"Augmentasi Data"}),"\n",(0,t.jsx)(e.p,{children:"Papan Go memiliki 8 simetri (4 rotasi \xd7 2 pencerminan). Setiap sampel training dapat ditransformasi menjadi 8 sampel ekivalen:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Asli \u2192 Rotasi 90\xb0 \u2192 Rotasi 180\xb0 \u2192 Rotasi 270\xb0\n  \u2193       \u2193         \u2193          \u2193\nFlip horizontal \u2192 ...\n"})}),"\n",(0,t.jsx)(e.p,{children:"Ini meningkatkan data training efektif 8 kali lipat, dan memastikan pola yang dipelajari model tidak bergantung pada arah."}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"hasil-training",children:"Hasil Training"}),"\n",(0,t.jsx)(e.h3,{id:"akurasi-57",children:"Akurasi 57%"}),"\n",(0,t.jsxs)(e.p,{children:["Setelah training, Policy Network mencapai ",(0,t.jsx)(e.strong,{children:"akurasi top-1 57%"}),"."]}),"\n",(0,t.jsx)(e.p,{children:"Ini berarti: diberikan posisi apapun, model memiliki 57% peluang untuk memprediksi langkah yang benar-benar dimainkan ahli manusia."}),"\n",(0,t.jsx)(e.h4,{id:"apakah-akurasi-ini-tinggi",children:"Apakah Akurasi Ini Tinggi?"}),"\n",(0,t.jsx)(e.p,{children:"Mengingat setiap posisi rata-rata memiliki 250 langkah legal, akurasi menebak acak hanya 0,4%."}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Metode"}),(0,t.jsx)(e.th,{children:"Akurasi Top-1"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Tebakan acak"}),(0,t.jsx)(e.td,{children:"0.4%"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Program Go komputer terkuat sebelumnya"}),(0,t.jsx)(e.td,{children:"~44%"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Policy Network AlphaGo"}),(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"57%"})})]})]})]}),"\n",(0,t.jsx)(e.p,{children:"Peningkatan 13 poin persentase, terlihat tidak banyak, tapi sangat signifikan."}),"\n",(0,t.jsx)(e.h3,{id:"peningkatan-kekuatan-bermain",children:"Peningkatan Kekuatan Bermain"}),"\n",(0,t.jsx)(e.p,{children:"Murni menggunakan Policy Network (tanpa pencarian) untuk bermain, kekuatan apa yang bisa dicapai?"}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Konfigurasi"}),(0,t.jsx)(e.th,{children:"Rating Elo"}),(0,t.jsx)(e.th,{children:"Perkiraan Level"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Program terkuat sebelumnya (Pachi)"}),(0,t.jsx)(e.td,{children:"2,500"}),(0,t.jsx)(e.td,{children:"Amatir 4-5 dan"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Policy Network saja"}),(0,t.jsx)(e.td,{children:"2,800"}),(0,t.jsx)(e.td,{children:"Amatir 6-7 dan"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"+ MCTS 1600 simulasi"}),(0,t.jsx)(e.td,{children:"3,200+"}),(0,t.jsx)(e.td,{children:"Level profesional"})]})]})]}),"\n",(0,t.jsx)(e.p,{children:"Policy Network saja sudah level amatir tinggi, dengan MCTS naik ke level profesional."}),"\n",(0,t.jsx)(e.h3,{id:"mengapa-hanya-57",children:"Mengapa Hanya 57%?"}),"\n",(0,t.jsx)(e.p,{children:"Catatan permainan manusia memiliki karakteristik berikut yang membatasi akurasi:"}),"\n",(0,t.jsx)(e.h4,{id:"1-beberapa-langkah-bagus",children:"1. Beberapa Langkah Bagus"}),"\n",(0,t.jsx)(e.p,{children:'Banyak posisi memiliki beberapa langkah yang semuanya bagus. Misalnya "approach" dan "defend corner" mungkin keduanya pilihan yang benar. Jika model memilih langkah bagus lainnya, dihitung sebagai "salah".'}),"\n",(0,t.jsx)(e.h4,{id:"2-perbedaan-gaya",children:"2. Perbedaan Gaya"}),"\n",(0,t.jsx)(e.p,{children:'Pemain yang berbeda memiliki gaya berbeda. Pemain agresif dan pemain solid mungkin bermain berbeda di posisi yang sama. Model mempelajari gaya "rata-rata".'}),"\n",(0,t.jsx)(e.h4,{id:"3-manusia-juga-membuat-kesalahan",children:"3. Manusia Juga Membuat Kesalahan"}),"\n",(0,t.jsx)(e.p,{children:'Data KGS mencakup permainan pemain amatir, pilihan mereka belum tentu optimal. Model mempelajari beberapa "kesalahan" adalah normal.'}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"peran-dalam-mcts",children:"Peran dalam MCTS"}),"\n",(0,t.jsx)(e.p,{children:"Policy Network memainkan dua peran kunci dalam MCTS AlphaGo:"}),"\n",(0,t.jsx)(e.h3,{id:"1-membimbing-arah-pencarian",children:"1. Membimbing Arah Pencarian"}),"\n",(0,t.jsxs)(e.p,{children:["Dalam tahap ",(0,t.jsx)(e.strong,{children:"Selection"})," MCTS, output Policy Network digunakan untuk menghitung UCB (Upper Confidence Bound):"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"UCB(s, a) = Q(s, a) + c_puct \xd7 P(s, a) \xd7 \u221a(N(s)) / (1 + N(s, a))\n"})}),"\n",(0,t.jsxs)(e.p,{children:["Di mana ",(0,t.jsx)(e.code,{children:"P(s, a)"})," adalah probabilitas yang diberikan Policy Network."]}),"\n",(0,t.jsx)(e.p,{children:"Ini berarti:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.strong,{children:"Langkah dengan probabilitas tinggi dieksplorasi lebih dulu"})}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Langkah dengan probabilitas rendah juga punya kesempatan dieksplorasi"})," (karena ada term eksplorasi)"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"2-prior-untuk-node-yang-diperluas",children:"2. Prior untuk Node yang Diperluas"}),"\n",(0,t.jsxs)(e.p,{children:["Ketika MCTS memperluas node baru, Policy Network menyediakan ",(0,t.jsx)(e.strong,{children:"prior probability"})," untuk semua node anak."]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Memperluas node s:\n  for each action a:\n    child = Node()\n    child.prior = policy_network(s)[a]  # Prior probability\n    child.value = 0\n    child.visits = 0\n"})}),"\n",(0,t.jsx)(e.p,{children:'Prior probability ini membuat MCTS "tahu" node anak mana yang lebih layak dieksplorasi, bahkan ketika belum dikunjungi.'}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"versi-ringan-vs-versi-lengkap",children:"Versi Ringan vs Versi Lengkap"}),"\n",(0,t.jsx)(e.p,{children:"AlphaGo sebenarnya memiliki dua Policy Network:"}),"\n",(0,t.jsx)(e.h3,{id:"versi-lengkap-sl-policy-network",children:"Versi Lengkap (SL Policy Network)"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Arsitektur"}),": CNN 13 layer, 192 filter"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Akurasi"}),": 57%"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Waktu inferensi"}),": Sekitar 3 milidetik/posisi"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Kegunaan"}),": Selection dan Expansion dalam MCTS"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"versi-ringan-rollout-policy-network",children:"Versi Ringan (Rollout Policy Network)"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Arsitektur"}),": Model linear + fitur manual"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Akurasi"}),": 24%"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Waktu inferensi"}),": Sekitar 2 mikrodetik/posisi (1500 kali lebih cepat)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Kegunaan"}),": Simulasi cepat (rollout)"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"mengapa-memerlukan-versi-ringan",children:"Mengapa Memerlukan Versi Ringan?"}),"\n",(0,t.jsxs)(e.p,{children:["Dalam tahap ",(0,t.jsx)(e.strong,{children:"Simulation"})," MCTS, perlu bermain dari node saat ini sampai akhir permainan, mungkin memerlukan 100+ langkah. Jika setiap langkah menggunakan Policy Network versi lengkap, terlalu lambat."]}),"\n",(0,t.jsx)(e.p,{children:"Versi ringan meskipun akurasi hanya 24%, tapi 1500 kali lebih cepat. Dalam rollout, kecepatan lebih penting dari presisi."}),"\n",(0,t.jsx)(e.h3,{id:"fitur-versi-ringan",children:"Fitur Versi Ringan"}),"\n",(0,t.jsx)(e.p,{children:"Versi ringan menggunakan fitur yang dirancang secara manual, termasuk:"}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Tipe Fitur"}),(0,t.jsx)(e.th,{children:"Contoh"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Pola lokal"}),(0,t.jsx)(e.td,{children:"Konfigurasi batu di area 3\xd73"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Fitur global"}),(0,t.jsx)(e.td,{children:"Apakah di sudut/tepi, titik besar"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Fitur taktis"}),(0,t.jsx)(e.td,{children:"Atari, ladder, respons"})]})]})]}),"\n",(0,t.jsx)(e.p,{children:"Fitur ini dimasukkan ke model linear (tanpa hidden layer), perhitungan sangat cepat."}),"\n",(0,t.jsx)(e.h3,{id:"perbaikan-alphago-zero",children:"Perbaikan AlphaGo Zero"}),"\n",(0,t.jsx)(e.p,{children:"AlphaGo Zero kemudian sepenuhnya meninggalkan versi ringan dan rollout. Ia langsung menggunakan Value Network untuk mengevaluasi leaf node, tidak perlu simulasi cepat. Ini adalah penyederhanaan besar."}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"fine-tuning-reinforcement-learning-rl-policy-network",children:"Fine-tuning Reinforcement Learning (RL Policy Network)"}),"\n",(0,t.jsx)(e.h3,{id:"keterbatasan-supervised-learning",children:"Keterbatasan Supervised Learning"}),"\n",(0,t.jsx)(e.p,{children:"Policy Network yang dilatih dengan supervised learning memiliki masalah fundamental:"}),"\n",(0,t.jsxs)(e.blockquote,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:'Ia belajar "meniru manusia", bukan "menang"'})}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"Ini berarti ia akan mempelajari kebiasaan buruk manusia, dan juga tampil buruk di posisi yang manusia belum pernah temui."}),"\n",(0,t.jsx)(e.h3,{id:"penguatan-self-play",children:"Penguatan Self-Play"}),"\n",(0,t.jsxs)(e.p,{children:["Solusi DeepMind adalah menggunakan metode ",(0,t.jsx)(e.strong,{children:"Policy Gradient"})," untuk reinforcement learning:"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"1. Biarkan Policy Network self-play\n2. Catat semua langkah setiap permainan\n3. Sesuaikan parameter berdasarkan menang/kalah:\n   - Menang \u2192 Tingkatkan probabilitas langkah-langkah ini\n   - Kalah \u2192 Kurangi probabilitas langkah-langkah ini\n"})}),"\n",(0,t.jsx)(e.h3,{id:"algoritma-reinforce",children:"Algoritma REINFORCE"}),"\n",(0,t.jsx)(e.p,{children:"Secara khusus menggunakan algoritma REINFORCE:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"\u2207J(\u03b8) = E[\u03a3_t \u2207log \u03c0_\u03b8(a_t | s_t) \xd7 z]\n"})}),"\n",(0,t.jsx)(e.p,{children:"Di mana:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"z"}),": Hasil permainan (+1 menang, -1 kalah)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"\u03c0_\u03b8(a_t | s_t)"}),": Probabilitas memilih aksi ",(0,t.jsx)(e.code,{children:"a_t"})," di state ",(0,t.jsx)(e.code,{children:"s_t"})]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"hasil",children:"Hasil"}),"\n",(0,t.jsx)(e.p,{children:"Setelah sekitar 1 hari training self-play (1,28 juta permainan), RL Policy Network:"}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Metrik"}),(0,t.jsx)(e.th,{children:"SL Policy"}),(0,t.jsx)(e.th,{children:"RL Policy"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Win rate melawan SL Policy"}),(0,t.jsx)(e.td,{children:"50%"}),(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"80%"})})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Peningkatan Elo"}),(0,t.jsx)(e.td,{children:"-"}),(0,t.jsx)(e.td,{children:"+100"})]})]})]}),"\n",(0,t.jsx)(e.p,{children:"Akurasi mungkin sedikit turun (karena tidak lagi sepenuhnya meniru manusia), tapi win rate aktual meningkat signifikan."}),"\n",(0,t.jsx)(e.h3,{id:"dari-meniru-ke-berinovasi",children:'Dari "Meniru" ke "Berinovasi"'}),"\n",(0,t.jsx)(e.p,{children:"Reinforcement learning memungkinkan Policy Network mempelajari beberapa langkah yang manusia tidak pernah pikirkan. Langkah-langkah ini tidak pernah muncul dalam data training, tapi efektif."}),"\n",(0,t.jsx)(e.p,{children:'Inilah mengapa AlphaGo bisa memainkan "langkah dewa" \u2014 tidak dibatasi oleh pengalaman manusia.'}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"analisis-visualisasi",children:"Analisis Visualisasi"}),"\n",(0,t.jsx)(e.h3,{id:"distribusi-probabilitas-di-posisi-berbeda",children:"Distribusi Probabilitas di Posisi Berbeda"}),"\n",(0,t.jsx)(e.p,{children:"Mari kita lihat output Policy Network di posisi berbeda:"}),"\n",(0,t.jsx)(e.h4,{id:"pembukaan-tahap-fuseki",children:"Pembukaan (Tahap Fuseki)"}),"\n",(0,t.jsx)(s.dW,{initialPosition:"opening",size:400}),"\n",(0,t.jsx)(e.p,{children:"Saat pembukaan, probabilitas terutama terkonsentrasi di:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Sudut (mengambil sudut)"}),"\n",(0,t.jsx)(e.li,{children:"Tepi (approach, defend corner)"}),"\n",(0,t.jsx)(e.li,{children:'Posisi "titik besar"'}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"Ini sesuai dengan prinsip dasar Go: Sudut emas, tepi perak, tengah rumput."}),"\n",(0,t.jsx)(e.h4,{id:"posisi-pertarungan",children:"Posisi Pertarungan"}),"\n",(0,t.jsx)(s.dW,{initialPosition:"fighting",size:400}),"\n",(0,t.jsx)(e.p,{children:"Saat bertarung, probabilitas terkonsentrasi di:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Titik pemotongan kunci"}),"\n",(0,t.jsx)(e.li,{children:"Atari, respons"}),"\n",(0,t.jsx)(e.li,{children:"Membuat eye, menghancurkan eye"}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"Ini menunjukkan model mempelajari taktik lokal."}),"\n",(0,t.jsx)(e.h4,{id:"tahap-yose",children:"Tahap Yose"}),"\n",(0,t.jsx)(s.dW,{initialPosition:"endgame",size:400}),"\n",(0,t.jsx)(e.p,{children:"Saat yose, probabilitas tersebar di berbagai titik yose, memerlukan perhitungan poin yang presisi."}),"\n",(0,t.jsx)(e.h3,{id:"apa-yang-dipelajari-hidden-layer",children:"Apa yang Dipelajari Hidden Layer?"}),"\n",(0,t.jsx)(e.p,{children:'Dengan memvisualisasikan output layer konvolusi, kita bisa melihat "fitur" yang dipelajari model:'}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Layer rendah"}),": Bentuk dasar (eye, titik pemotongan)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Layer tengah"}),": Pola taktis (atari, ladder)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Layer tinggi"}),": Konsep global (influence, tebal/tipis)"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"Ini sangat mirip dengan struktur hierarkis bagaimana manusia memahami Go."}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"poin-implementasi",children:"Poin Implementasi"}),"\n",(0,t.jsx)(e.h3,{id:"implementasi-pytorch",children:"Implementasi PyTorch"}),"\n",(0,t.jsx)(e.p,{children:"Berikut implementasi Policy Network yang disederhanakan:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PolicyNetwork(nn.Module):\n    def __init__(self, input_channels=48, num_filters=192, num_layers=12):\n        super().__init__()\n\n        # Layer konvolusi pertama (5\xd75)\n        self.conv1 = nn.Conv2d(input_channels, num_filters,\n                               kernel_size=5, padding=2)\n\n        # Layer konvolusi tengah (3\xd73)\xd711\n        self.conv_layers = nn.ModuleList([\n            nn.Conv2d(num_filters, num_filters,\n                     kernel_size=3, padding=1)\n            for _ in range(num_layers - 1)\n        ])\n\n        # Layer konvolusi output (1\xd71)\n        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)\n\n    def forward(self, x):\n        # x: (batch, 48, 19, 19)\n\n        # Layer pertama\n        x = F.relu(self.conv1(x))\n\n        # Layer tengah\n        for conv in self.conv_layers:\n            x = F.relu(conv(x))\n\n        # Layer output\n        x = self.conv_out(x)  # (batch, 1, 19, 19)\n\n        # Flatten + Softmax\n        x = x.view(x.size(0), -1)  # (batch, 361)\n        x = F.softmax(x, dim=1)\n\n        return x\n"})}),"\n",(0,t.jsx)(e.h3,{id:"loop-training",children:"Loop Training"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def train_step(model, optimizer, states, actions):\n    """\n    states: (batch, 48, 19, 19) - Fitur papan\n    actions: (batch,) - Posisi yang dimainkan manusia (0-360)\n    """\n    # Forward propagation\n    policy = model(states)  # (batch, 361)\n\n    # Loss cross-entropy\n    loss = F.cross_entropy(\n        torch.log(policy + 1e-8),  # Mencegah log(0)\n        actions\n    )\n\n    # Backpropagation\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Menghitung akurasi\n    predictions = policy.argmax(dim=1)\n    accuracy = (predictions == actions).float().mean()\n\n    return loss.item(), accuracy.item()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"catatan-saat-inferensi",children:"Catatan Saat Inferensi"}),"\n",(0,t.jsx)(e.p,{children:"Dalam permainan sebenarnya, perlu memperhatikan:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Filter langkah ilegal"}),": Set probabilitas posisi ilegal ke 0, lalu renormalisasi"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Pengaturan temperatur"}),': Dapat menggunakan parameter temperatur untuk mengontrol "ketajaman" distribusi probabilitas']}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Batch inference"}),": Dapat memproses batch beberapa posisi dalam MCTS"]}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def get_move_probabilities(model, state, legal_moves, temperature=1.0):\n    """Mendapatkan distribusi probabilitas langkah legal"""\n    policy = model(state)  # (361,)\n\n    # Hanya menyimpan langkah legal\n    mask = torch.zeros(361)\n    mask[legal_moves] = 1\n    policy = policy * mask\n\n    # Pengaturan temperatur\n    if temperature != 1.0:\n        policy = policy ** (1 / temperature)\n\n    # Renormalisasi\n    policy = policy / policy.sum()\n\n    return policy\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"korespondensi-animasi",children:"Korespondensi Animasi"}),"\n",(0,t.jsx)(e.p,{children:"Konsep inti yang dibahas dalam artikel ini dan nomor animasi:"}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Nomor"}),(0,t.jsx)(e.th,{children:"Konsep"}),(0,t.jsx)(e.th,{children:"Korespondensi Fisika/Matematika"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"E1"}),(0,t.jsx)(e.td,{children:"Policy Network"}),(0,t.jsx)(e.td,{children:"Medan probabilitas"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"D9"}),(0,t.jsx)(e.td,{children:"Ekstraksi fitur CNN"}),(0,t.jsx)(e.td,{children:"Respons filter"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"D3"}),(0,t.jsx)(e.td,{children:"Supervised learning"}),(0,t.jsx)(e.td,{children:"Maximum likelihood estimation"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"H4"}),(0,t.jsx)(e.td,{children:"Policy gradient"}),(0,t.jsx)(e.td,{children:"Optimisasi stokastik"})]})]})]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"bacaan-lanjutan",children:"Bacaan Lanjutan"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Artikel berikutnya"}),": ",(0,t.jsx)(e.a,{href:"../value-network",children:"Penjelasan Detail Value Network"})," \u2014 Bagaimana AlphaGo mengevaluasi posisi"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Topik terkait"}),": ",(0,t.jsx)(e.a,{href:"../input-features",children:"Desain Fitur Input"})," \u2014 Penjelasan detail 48 feature plane"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Mendalam ke prinsip"}),": ",(0,t.jsx)(e.a,{href:"../cnn-and-go",children:"Kombinasi CNN dan Go"})," \u2014 Mengapa CNN cocok untuk papan"]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"poin-kunci",children:"Poin Kunci"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Policy Network adalah generator distribusi probabilitas"}),": Input papan, output probabilitas 361 posisi"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"13 layer CNN + Softmax"}),": Konvolusi dalam mengekstrak fitur, Softmax menghasilkan probabilitas"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Akurasi 57%"}),": Jauh melebihi program Go komputer sebelumnya"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dua versi"}),": Versi lengkap untuk keputusan MCTS, versi ringan untuk simulasi cepat"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Fine-tuning reinforcement learning"}),': Dari "meniru manusia" berevolusi ke "mengejar kemenangan"']}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:'Policy Network adalah "intuisi" AlphaGo \u2014 memungkinkan AI dengan cepat mengidentifikasi langkah yang layak dipertimbangkan, seperti manusia.'}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"referensi",children:"Referensi"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:['Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." ',(0,t.jsx)(e.em,{children:"Nature"}),", 529, 484-489."]}),"\n",(0,t.jsxs)(e.li,{children:['Maddison, C. J., et al. (2014). "Move Evaluation in Go Using Deep Convolutional Neural Networks." ',(0,t.jsx)(e.em,{children:"arXiv:1412.6564"}),"."]}),"\n",(0,t.jsxs)(e.li,{children:["Sutton, R. S., & Barto, A. G. (2018). ",(0,t.jsx)(e.em,{children:"Reinforcement Learning: An Introduction"}),". MIT Press."]}),"\n",(0,t.jsxs)(e.li,{children:['LeCun, Y., Bengio, Y., & Hinton, G. (2015). "Deep learning." ',(0,t.jsx)(e.em,{children:"Nature"}),", 521, 436-444."]}),"\n"]})]})}function p(a={}){const{wrapper:e}={...(0,r.R)(),...a.components};return e?(0,t.jsx)(e,{...a,children:(0,t.jsx)(h,{...a})}):h(a)}},42948(a,e,n){n.d(e,{A:()=>r});n(59471);var i=n(61785),t=n(62615);function r({children:a,fallback:e}){return(0,i.A)()?(0,t.jsx)(t.Fragment,{children:a?.()}):e??null}},45695(a,e,n){n.d(e,{$W:()=>P,tO:()=>d,u8:()=>k,dW:()=>m});var i=n(59471),t=n(90989),r=n(62615);const s=19,l=[[3,3],[3,9],[3,15],[9,3],[9,9],[9,15],[15,3],[15,9],[15,15]];function d({size:a=400,stones:e=[],highlights:n=[],labels:d=[],onCellClick:o=null,showCoordinates:c=!0}){const h=(0,i.useRef)(null),p=c?30:15,u=a-2*p,m=u/18;return(0,i.useEffect)(()=>{if(!h.current)return;const a=t.Ltv(h.current);a.selectAll("*").remove();const i=a.append("g").attr("transform",`translate(${p}, ${p})`);i.append("rect").attr("x",-m/2).attr("y",-m/2).attr("width",u+m).attr("height",u+m).attr("fill","#dcb35c").attr("rx",4);const r=i.append("g").attr("class","grid");for(let e=0;e<s;e++)r.append("line").attr("class","grid-line").attr("x1",0).attr("y1",e*m).attr("x2",18*m).attr("y2",e*m);for(let e=0;e<s;e++)r.append("line").attr("class","grid-line").attr("x1",e*m).attr("y1",0).attr("x2",e*m).attr("y2",18*m);const x=i.append("g").attr("class","star-points");if(l.forEach(([a,e])=>{x.append("circle").attr("class","star-point").attr("cx",a*m).attr("cy",e*m).attr("r",m/8)}),n.length>0){const a=i.append("g").attr("class","highlights");n.forEach(({x:e,y:n,intensity:i})=>{a.append("rect").attr("class","heatmap-cell").attr("x",e*m-m/2).attr("y",n*m-m/2).attr("width",m).attr("height",m).attr("fill",t.Q3(i)).attr("opacity",.7*i)})}const g=i.append("g").attr("class","stones");if(e.forEach(({x:a,y:e,color:n})=>{const i="black"===n?"stone-black":"stone-white";g.append("circle").attr("cx",a*m+2).attr("cy",e*m+2).attr("r",.45*m).attr("fill","rgba(0,0,0,0.2)"),g.append("circle").attr("class",i).attr("cx",a*m).attr("cy",e*m).attr("r",.45*m)}),d.length>0){const a=i.append("g").attr("class","labels");d.forEach(({x:n,y:i,text:t})=>{const r=e.find(a=>a.x===n&&a.y===i),s="black"===r?.color?"#fff":"#000";a.append("text").attr("x",n*m).attr("y",i*m).attr("dy","0.35em").attr("text-anchor","middle").attr("fill",s).attr("font-size",.5*m).attr("font-weight","bold").text(t)})}if(c){const e=a.append("g").attr("class","coordinates"),n="ABCDEFGHJKLMNOPQRST";for(let a=0;a<s;a++)e.append("text").attr("x",p+a*m).attr("y",p/2).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(n[a]);for(let a=0;a<s;a++)e.append("text").attr("x",p/2).attr("y",p+a*m).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(s-a)}o&&i.append("g").attr("class","click-targets").selectAll("rect").data(t.y17(361)).enter().append("rect").attr("x",a=>a%s*m-m/2).attr("y",a=>Math.floor(a/s)*m-m/2).attr("width",m).attr("height",m).attr("fill","transparent").attr("cursor","pointer").on("click",(a,e)=>{const n=e%s,i=Math.floor(e/s);o({x:n,y:i})})},[a,e,n,d,c,o,m,p,u]),(0,r.jsx)("div",{className:"go-board-container",children:(0,r.jsx)("svg",{ref:h,width:a,height:a,className:"go-board"})})}var o=n(42948);const c=19,h={empty:function(){const a=[];for(let e=0;e<c;e++)for(let n=0;n<c;n++)a.push({x:n,y:e,prob:1/361});return a}(),corner:function(){const a=[],e=[[3,3],[3,15],[15,3],[15,15]],n=[[2,4],[4,2],[2,14],[4,16],[14,2],[16,4],[14,16],[16,14]];for(let i=0;i<c;i++)for(let t=0;t<c;t++){let r=.001;e.some(([a,e])=>a===t&&e===i)?r=.15:n.some(([a,e])=>a===t&&e===i)?r=.05:0!==t&&18!==t&&0!==i&&18!==i||(r=5e-4),a.push({x:t,y:i,prob:r})}return p(a)}(),move37:function(){const a=[],e={x:9,y:4},n=[[3,2],[15,2],[10,10],[8,6]];for(let i=0;i<c;i++)for(let t=0;t<c;t++){let r=.001;t===e.x&&i===e.y?r=.08:n.some(([a,e])=>a===t&&e===i)?r=.12:t>=5&&t<=13&&i>=5&&i<=13&&(r=.005+.01*Math.random()),a.push({x:t,y:i,prob:r})}return p(a)}()};function p(a){const e=a.reduce((a,e)=>a+e.prob,0);return a.map(a=>({...a,prob:a.prob/e}))}function u({initialPosition:a="corner",stones:e=[],highlightMoves:n=[],size:s=450,showTopN:l=5,interactive:d=!0}){const o=(0,i.useRef)(null),p=(0,i.useRef)(null),[u,m]=(0,i.useState)(h[a]||h.corner),[x,g]=(0,i.useState)(null),k=35,j=s-70,y=j/18;(0,i.useEffect)(()=>{if(!o.current)return;const a=t.Ltv(o.current);a.selectAll("*").remove();const n=a.append("g").attr("transform","translate(35, 35)");n.append("rect").attr("x",-y/2).attr("y",-y/2).attr("width",j+y).attr("height",j+y).attr("fill","#dcb35c").attr("rx",4);const i=Math.max(...u.map(a=>a.prob)),r=t.exT(t.oKI).domain([0,i]);n.append("g").attr("class","heatmap").selectAll("rect").data(u).enter().append("rect").attr("class","heatmap-cell").attr("x",a=>a.x*y-y/2).attr("y",a=>a.y*y-y/2).attr("width",y).attr("height",y).attr("fill",a=>r(a.prob)).attr("opacity",a=>.3+a.prob/i*.6).attr("cursor",d?"pointer":"default").on("mouseover",function(a,e){if(!d)return;t.Ltv(this).attr("stroke","#333").attr("stroke-width",2);t.Ltv(p.current).style("display","block").style("left",`${a.pageX+10}px`).style("top",a.pageY-10+"px").html(`\u4f4d\u7f6e: ${String.fromCharCode(65+e.x)}${19-e.y}<br>\u6a5f\u7387: ${(100*e.prob).toFixed(2)}%`)}).on("mouseout",function(){t.Ltv(this).attr("stroke","none"),t.Ltv(p.current).style("display","none")}).on("click",function(a,e){d&&g(e)});const s=n.append("g").attr("class","grid");for(let e=0;e<c;e++)s.append("line").attr("class","grid-line").attr("x1",0).attr("y1",e*y).attr("x2",18*y).attr("y2",e*y).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5),s.append("line").attr("class","grid-line").attr("x1",e*y).attr("y1",0).attr("x2",e*y).attr("y2",18*y).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5);const h=n.append("g").attr("class","stones");e.forEach(({x:a,y:e,color:n})=>{h.append("circle").attr("cx",a*y).attr("cy",e*y).attr("r",.45*y).attr("fill","black"===n?"#1a1a1a":"#f5f5f5").attr("stroke","black"===n?"#000":"#333").attr("stroke-width",1)});const m=[...u].sort((a,e)=>e.prob-a.prob).slice(0,l),x=n.append("g").attr("class","top-labels");m.forEach((a,n)=>{e.some(e=>e.x===a.x&&e.y===a.y)||(x.append("circle").attr("cx",a.x*y).attr("cy",a.y*y).attr("r",.3*y).attr("fill","rgba(255,255,255,0.8)").attr("stroke","#e74c3c").attr("stroke-width",2),x.append("text").attr("x",a.x*y).attr("y",a.y*y).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#e74c3c").attr("font-size",.4*y).attr("font-weight","bold").text(n+1))});const b=a.append("g").attr("class","coordinates");for(let e=0;e<c;e++)b.append("text").attr("x",k+e*y).attr("y",17.5).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text("ABCDEFGHJKLMNOPQRST"[e]),b.append("text").attr("x",17.5).attr("y",k+e*y).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(c-e)},[u,e,l,d,y,k,j]);const b=a=>{m(h[a]||h.corner)};return(0,r.jsxs)("div",{children:[d&&(0,r.jsxs)("div",{className:"d3-controls",children:[(0,r.jsx)("button",{className:"empty"===a?"active":"",onClick:()=>b("empty"),children:"\u5747\u52fb\u5206\u5e03"}),(0,r.jsx)("button",{className:"corner"===a?"active":"",onClick:()=>b("corner"),children:"\u958b\u5c40\u661f\u4f4d"}),(0,r.jsx)("button",{className:"move37"===a?"active":"",onClick:()=>b("move37"),children:"\u7b2c 37 \u624b"})]}),(0,r.jsx)("div",{className:"go-board-container",children:(0,r.jsx)("svg",{ref:o,width:s,height:s,className:"go-board"})}),(0,r.jsx)("div",{ref:p,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),x&&(0,r.jsx)("div",{className:"d3-legend",children:(0,r.jsxs)("div",{className:"d3-legend-item",children:["\u5df2\u9078\u64c7: ",String.fromCharCode(65+x.x),19-x.y,"\u2014 \u6a5f\u7387: ",(100*x.prob).toFixed(2),"%"]})}),(0,r.jsxs)("div",{className:"d3-legend",children:[(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#ffffb2"}}),"\u4f4e\u6a5f\u7387"]}),(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#fd8d3c"}}),"\u4e2d\u6a5f\u7387"]}),(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#bd0026"}}),"\u9ad8\u6a5f\u7387"]})]})]})}function m(a){return(0,r.jsx)(o.A,{fallback:(0,r.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,r.jsx)(u,{...a})})}const x={name:"Root",visits:1600,value:.55,prior:1,children:[{name:"D4",visits:800,value:.62,prior:.35,selected:!0,children:[{name:"Q16",visits:400,value:.58,prior:.3},{name:"R4",visits:300,value:.65,prior:.25,selected:!0},{name:"C16",visits:100,value:.55,prior:.2}]},{name:"Q4",visits:500,value:.52,prior:.3,children:[{name:"D16",visits:300,value:.5,prior:.28},{name:"Q16",visits:200,value:.54,prior:.22}]},{name:"D16",visits:200,value:.48,prior:.2},{name:"Q16",visits:100,value:.45,prior:.15}]};function g({data:a=x,width:e=700,height:n=450,showPUCT:s=!0,cPuct:l=1.5,interactive:d=!0}){const o=(0,i.useRef)(null),c=(0,i.useRef)(null),[h,p]=(0,i.useState)(null),[u,m]=(0,i.useState)(l),g=40,k=40,j=e-k-40,y=n-g-40;return(0,i.useEffect)(()=>{if(!o.current)return;const i=t.Ltv(o.current);i.selectAll("*").remove();const r=t.B22().size([j,y-50]),l=t.Sk5(a);r(l);const h=i.append("g").attr("transform",`translate(${k}, ${g})`);h.append("g").attr("class","links").selectAll("path").data(l.links()).enter().append("path").attr("class",a=>"link "+(a.target.data.selected?"selected":"")).attr("fill","none").attr("stroke",a=>a.target.data.selected?"#4a90d9":"#999").attr("stroke-width",a=>a.target.data.selected?3:1.5).attr("d",t.vu().x(a=>a.x).y(a=>a.y));const m=h.append("g").attr("class","nodes").selectAll("g").data(l.descendants()).enter().append("g").attr("class","node").attr("transform",a=>`translate(${a.x}, ${a.y})`).attr("cursor",d?"pointer":"default").on("mouseover",function(a,e){if(!d)return;t.Ltv(this).select("circle").transition().duration(200).attr("r",30);const n=e.parent?e.parent.data.visits:e.data.visits,i=((a,e)=>{if(!e)return 0;const n=a.value,i=a.prior,t=a.visits;return n+u*i*Math.sqrt(e)/(1+t)})(e.data,n);t.Ltv(c.current).style("display","block").style("left",`${a.pageX+15}px`).style("top",a.pageY-10+"px").html(`\n            <strong>${e.data.name}</strong><br>\n            \u8a2a\u554f\u6b21\u6578 (N): ${e.data.visits}<br>\n            \u5e73\u5747\u50f9\u503c (Q): ${e.data.value.toFixed(3)}<br>\n            \u5148\u9a57\u6a5f\u7387 (P): ${(100*e.data.prior).toFixed(1)}%<br>\n            ${s?`PUCT \u5206\u6578: ${i.toFixed(3)}`:""}\n          `)}).on("mouseout",function(){t.Ltv(this).select("circle").transition().duration(200).attr("r",25),t.Ltv(c.current).style("display","none")}).on("click",function(a,e){d&&p(e.data)});m.append("circle").attr("r",25).attr("fill",a=>a.data.selected?"#4a90d9":"#fff").attr("stroke",e=>{if(e.data.selected)return"#2c5282";const n=e.data.visits/a.visits;return t.dM(.3+.5*n)}).attr("stroke-width",a=>a.data.selected?3:2),m.append("text").attr("dy",-5).attr("text-anchor","middle").attr("fill",a=>a.data.selected?"#fff":"#333").attr("font-size",11).attr("font-weight","bold").text(a=>a.data.name),m.append("text").attr("dy",10).attr("text-anchor","middle").attr("fill",a=>a.data.selected?"#fff":"#666").attr("font-size",9).text(a=>`N=${a.data.visits}`),i.append("text").attr("x",e/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("MCTS \u641c\u7d22\u6a39"),s&&i.append("text").attr("x",e/2).attr("y",n-10).attr("text-anchor","middle").attr("font-size",11).attr("fill","#666").text("\u85cd\u8272\u8def\u5f91\uff1aPUCT \u9078\u64c7\u7684\u6700\u4f73\u8def\u5f91")},[a,e,n,s,u,d,j,y]),(0,r.jsxs)("div",{children:[s&&d&&(0,r.jsx)("div",{className:"d3-controls",children:(0,r.jsxs)("div",{className:"d3-slider",children:[(0,r.jsxs)("label",{children:["c_puct: ",u.toFixed(1)]}),(0,r.jsx)("input",{type:"range",min:"0.5",max:"3",step:"0.1",value:u,onChange:a=>m(parseFloat(a.target.value))})]})}),(0,r.jsx)("div",{className:"mcts-tree-container",children:(0,r.jsx)("svg",{ref:o,width:e,height:n,className:"mcts-tree"})}),(0,r.jsx)("div",{ref:c,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),h&&(0,r.jsxs)("div",{className:"d3-legend",style:{background:"#f5f5f5",padding:"1rem",borderRadius:"4px"},children:[(0,r.jsxs)("strong",{children:["\u5df2\u9078\u64c7\u7bc0\u9ede: ",h.name]}),(0,r.jsxs)("div",{children:["\u8a2a\u554f\u6b21\u6578: ",h.visits]}),(0,r.jsxs)("div",{children:["\u5e73\u5747\u50f9\u503c: ",h.value.toFixed(3)]}),(0,r.jsxs)("div",{children:["\u5148\u9a57\u6a5f\u7387: ",(100*h.prior).toFixed(1),"%"]})]}),(0,r.jsxs)("div",{className:"d3-legend",children:[(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"\u9078\u4e2d\u8def\u5f91"]}),(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#fff",border:"2px solid #999"}}),"\u5176\u4ed6\u7bc0\u9ede"]}),(0,r.jsx)("div",{className:"d3-legend-item",children:(0,r.jsx)("span",{style:{fontSize:"12px"},children:"\u7bc0\u9ede\u5927\u5c0f \u221d \u8a2a\u554f\u6b21\u6578"})})]})]})}function k(a){return(0,r.jsx)(o.A,{fallback:(0,r.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,r.jsx)(g,{...a})})}const j=[{hours:0,elo:0,label:"\u96a8\u6a5f"},{hours:3,elo:1e3,label:"\u767c\u73fe\u898f\u5247"},{hours:6,elo:2e3},{hours:12,elo:3e3,label:"\u767c\u73fe\u5b9a\u5f0f"},{hours:24,elo:4e3},{hours:36,elo:4500,label:"\u8d85\u8d8a Fan Hui"},{hours:48,elo:5e3},{hours:60,elo:5200,label:"\u8d85\u8d8a Lee Sedol"},{hours:72,elo:5400,label:"\u8d85\u8d8a\u539f\u7248 AlphaGo"}],y=[{elo:2700,label:"\u696d\u9918\u5f37\u8c6a"},{elo:3500,label:"Fan Hui (\u8077\u696d\u4e8c\u6bb5)"},{elo:4500,label:"Lee Sedol (\u4e16\u754c\u51a0\u8ecd)"},{elo:5e3,label:"\u539f\u7248 AlphaGo"}],b=[{epochs:0,elo:0},{epochs:10,elo:1500},{epochs:20,elo:2500},{epochs:30,elo:3e3},{epochs:40,elo:3200},{epochs:50,elo:3300}],v=[{games:0,elo:3300},{games:1e3,elo:3800},{games:5e3,elo:4200},{games:1e4,elo:4500},{games:5e4,elo:4800},{games:1e5,elo:5e3}];function f({mode:a="zero",width:e=600,height:n=400,animated:s=!0,showMilestones:l=!0}){const d=(0,i.useRef)(null),[o,c]=(0,i.useState)(a),h=40,p=70,u=e-p-100,m=n-h-60;return(0,i.useEffect)(()=>{if(!d.current)return;const a=t.Ltv(d.current);let n,i,r;a.selectAll("*").remove(),"zero"===o?(n=j,i="\u8a13\u7df4\u6642\u9593\uff08\u5c0f\u6642\uff09",r=[0,80]):"sl"===o?(n=b,i="\u8a13\u7df4\u8f2a\u6578\uff08Epochs\uff09",r=[0,60]):(n=v,i="\u81ea\u6211\u5c0d\u5f08\u5c40\u6578",r=[0,12e4]);const c="selfplay"===o?t.ZEH().domain([1,r[1]]).range([0,u]):t.m4Y().domain(r).range([0,u]),x=t.m4Y().domain([0,6e3]).range([m,0]),g=a.append("g").attr("transform",`translate(${p}, ${h})`);if(g.append("g").attr("class","grid").selectAll(".grid-line-y").data(x.ticks(6)).enter().append("line").attr("class","grid-line-y").attr("x1",0).attr("x2",u).attr("y1",a=>x(a)).attr("y2",a=>x(a)).attr("stroke","#ddd").attr("stroke-dasharray","3,3"),l&&"zero"===o){const a=g.append("g").attr("class","human-levels");y.forEach(e=>{a.append("line").attr("x1",0).attr("x2",u).attr("y1",x(e.elo)).attr("y2",x(e.elo)).attr("stroke","#e74c3c").attr("stroke-dasharray","5,5").attr("opacity",.6),a.append("text").attr("x",u+5).attr("y",x(e.elo)).attr("dy","0.35em").attr("fill","#e74c3c").attr("font-size",10).text(e.label)})}const k=t.n8j().x(a=>c("zero"===o?a.hours:"sl"===o?a.epochs:Math.max(1,a.games))).y(a=>x(a.elo)).curve(t.nVG),f=t.Wcw().x(a=>c("zero"===o?a.hours:"sl"===o?a.epochs:Math.max(1,a.games))).y0(m).y1(a=>x(a.elo)).curve(t.nVG);g.append("path").datum(n).attr("class","area").attr("fill","#4a90d9").attr("opacity",.1).attr("d",f);const P=g.append("path").datum(n).attr("class","line").attr("fill","none").attr("stroke","#4a90d9").attr("stroke-width",3).attr("d",k);if(s){const a=P.node().getTotalLength();P.attr("stroke-dasharray",`${a} ${a}`).attr("stroke-dashoffset",a).transition().duration(2e3).ease(t.yfw).attr("stroke-dashoffset",0)}if(l&&"zero"===o){const a=n.filter(a=>a.label),e=g.append("g").attr("class","milestones");e.selectAll("circle").data(a).enter().append("circle").attr("cx",a=>c(a.hours)).attr("cy",a=>x(a.elo)).attr("r",6).attr("fill","#e74c3c").attr("stroke","#fff").attr("stroke-width",2),e.selectAll("text").data(a).enter().append("text").attr("x",a=>c(a.hours)).attr("y",a=>x(a.elo)-15).attr("text-anchor","middle").attr("fill","#333").attr("font-size",10).text(a=>a.label)}const N="selfplay"===o?t.l78(c).ticks(5,"~s"):t.l78(c);g.append("g").attr("class","x-axis").attr("transform",`translate(0, ${m})`).call(N),g.append("text").attr("class","axis-label").attr("x",u/2).attr("y",m+45).attr("text-anchor","middle").attr("fill","#666").text(i),g.append("g").attr("class","y-axis").call(t.V4s(x).ticks(6)),g.append("text").attr("class","axis-label").attr("transform","rotate(-90)").attr("x",-m/2).attr("y",-50).attr("text-anchor","middle").attr("fill","#666").text("ELO \u8a55\u5206"),a.append("text").attr("x",e/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("zero"===o?"AlphaGo Zero \u8a13\u7df4\u66f2\u7dda":"sl"===o?"\u76e3\u7763\u5b78\u7fd2\u68cb\u529b\u6210\u9577":"\u81ea\u6211\u5c0d\u5f08\u68cb\u529b\u6210\u9577")},[o,e,n,s,l,u,m]),(0,r.jsxs)("div",{children:[(0,r.jsxs)("div",{className:"d3-controls",children:[(0,r.jsx)("button",{className:"zero"===o?"active":"",onClick:()=>c("zero"),children:"AlphaGo Zero"}),(0,r.jsx)("button",{className:"sl"===o?"active":"",onClick:()=>c("sl"),children:"\u76e3\u7763\u5b78\u7fd2"}),(0,r.jsx)("button",{className:"selfplay"===o?"active":"",onClick:()=>c("selfplay"),children:"\u81ea\u6211\u5c0d\u5f08"})]}),(0,r.jsx)("div",{className:"elo-chart-container",children:(0,r.jsx)("svg",{ref:d,width:e,height:n,className:"elo-chart"})}),(0,r.jsxs)("div",{className:"d3-legend",children:[(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"ELO \u8a55\u5206"]}),l&&"zero"===o&&(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c"}}),"\u91cc\u7a0b\u7891"]}),(0,r.jsxs)("div",{className:"d3-legend-item",children:[(0,r.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c",opacity:.3}}),"\u4eba\u985e\u6c34\u5e73\u53c3\u8003\u7dda"]})]})]})]})}function P(a){return(0,r.jsx)(o.A,{fallback:(0,r.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,r.jsx)(f,{...a})})}},30416(a,e,n){n.d(e,{R:()=>s,x:()=>l});var i=n(59471);const t={},r=i.createContext(t);function s(a){const e=i.useContext(r);return i.useMemo(function(){return"function"==typeof a?a(e):{...e,...a}},[e,a])}function l(a){let e;return e=a.disableParentContext?"function"==typeof a.components?a.components(t):a.components||t:s(a.components),i.createElement(r.Provider,{value:e},a.children)}}}]);