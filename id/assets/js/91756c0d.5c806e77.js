"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[4628],{10379(e,n,a){a.r(n),a.d(n,{assets:()=>d,contentTitle:()=>t,default:()=>c,frontMatter:()=>s,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"for-engineers/deep-dive/papers","title":"Panduan Paper Kunci","description":"Analisis poin penting paper milestone AI Go seperti AlphaGo, AlphaZero, KataGo","source":"@site/i18n/id/docusaurus-plugin-content-docs/current/for-engineers/deep-dive/papers.md","sourceDirName":"for-engineers/deep-dive","slug":"/for-engineers/deep-dive/papers","permalink":"/id/docs/for-engineers/deep-dive/papers","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/deep-dive/papers.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11,"title":"Panduan Paper Kunci","description":"Analisis poin penting paper milestone AI Go seperti AlphaGo, AlphaZero, KataGo"},"sidebar":"tutorialSidebar","previous":{"title":"Aturan Kustom dan Varian","permalink":"/id/docs/for-engineers/deep-dive/custom-rules"},"next":{"title":"Membangun AI Go dari Nol","permalink":"/id/docs/for-engineers/deep-dive/build-from-scratch"}}');var r=a(62615),l=a(30416);const s={sidebar_position:11,title:"Panduan Paper Kunci",description:"Analisis poin penting paper milestone AI Go seperti AlphaGo, AlphaZero, KataGo"},t="Panduan Paper Kunci",d={},h=[{value:"Gambaran Paper",id:"gambaran-paper",level:2},{value:"Timeline",id:"timeline",level:3},{value:"Rekomendasi Membaca",id:"rekomendasi-membaca",level:3},{value:"1. Kelahiran MCTS (2006)",id:"1-kelahiran-mcts-2006",level:2},{value:"Informasi Paper",id:"informasi-paper",level:3},{value:"Kontribusi Inti",id:"kontribusi-inti",level:3},{value:"Konsep Kunci",id:"konsep-kunci",level:3},{value:"Formula UCB1",id:"formula-ucb1",level:4},{value:"Empat Langkah MCTS",id:"empat-langkah-mcts",level:4},{value:"Dampak",id:"dampak",level:3},{value:"2. AlphaGo (2016)",id:"2-alphago-2016",level:2},{value:"Informasi Paper",id:"informasi-paper-1",level:3},{value:"Kontribusi Inti",id:"kontribusi-inti-1",level:3},{value:"Arsitektur Sistem",id:"arsitektur-sistem",level:3},{value:"Poin Teknis",id:"poin-teknis",level:3},{value:"1. Supervised Learning Policy Network",id:"1-supervised-learning-policy-network",level:4},{value:"2. Perbaikan Reinforcement Learning",id:"2-perbaikan-reinforcement-learning",level:4},{value:"3. Pelatihan Value Network",id:"3-pelatihan-value-network",level:4},{value:"4. Integrasi MCTS",id:"4-integrasi-mcts",level:4},{value:"Data Kunci",id:"data-kunci",level:3},{value:"3. AlphaGo Zero (2017)",id:"3-alphago-zero-2017",level:2},{value:"Informasi Paper",id:"informasi-paper-2",level:3},{value:"Kontribusi Inti",id:"kontribusi-inti-2",level:3},{value:"Perbedaan dengan AlphaGo",id:"perbedaan-dengan-alphago",level:3},{value:"Inovasi Kunci",id:"inovasi-kunci",level:3},{value:"1. Single Dual-Head Network",id:"1-single-dual-head-network",level:4},{value:"2. Fitur Input yang Disederhanakan",id:"2-fitur-input-yang-disederhanakan",level:4},{value:"3. Evaluasi Pure Value Network",id:"3-evaluasi-pure-value-network",level:4},{value:"4. Alur Pelatihan",id:"4-alur-pelatihan",level:4},{value:"Kurva Pembelajaran",id:"kurva-pembelajaran",level:3},{value:"4. AlphaZero (2017)",id:"4-alphazero-2017",level:2},{value:"Informasi Paper",id:"informasi-paper-3",level:3},{value:"Kontribusi Inti",id:"kontribusi-inti-3",level:3},{value:"Arsitektur Umum",id:"arsitektur-umum",level:3},{value:"Adaptasi Lintas Game",id:"adaptasi-lintas-game",level:3},{value:"Perbaikan MCTS",id:"perbaikan-mcts",level:3},{value:"Formula PUCT",id:"formula-puct",level:4},{value:"Noise Eksplorasi",id:"noise-eksplorasi",level:4},{value:"5. KataGo (2019)",id:"5-katago-2019",level:2},{value:"Informasi Paper",id:"informasi-paper-4",level:3},{value:"Kontribusi Inti",id:"kontribusi-inti-4",level:3},{value:"Inovasi Kunci",id:"inovasi-kunci-1",level:3},{value:"1. Target Pelatihan Tambahan",id:"1-target-pelatihan-tambahan",level:4},{value:"2. Fitur Global",id:"2-fitur-global",level:4},{value:"3. Randomisasi Playout Cap",id:"3-randomisasi-playout-cap",level:4},{value:"4. Ukuran Papan Progresif",id:"4-ukuran-papan-progresif",level:4},{value:"Perbandingan Efisiensi",id:"perbandingan-efisiensi",level:3},{value:"6. Paper Lanjutan",id:"6-paper-lanjutan",level:2},{value:"MuZero (2020)",id:"muzero-2020",level:3},{value:"EfficientZero (2021)",id:"efficientzero-2021",level:3},{value:"Gumbel AlphaZero (2022)",id:"gumbel-alphazero-2022",level:3},{value:"Saran Membaca Paper",id:"saran-membaca-paper",level:2},{value:"Urutan untuk Pemula",id:"urutan-untuk-pemula",level:3},{value:"Urutan Lanjutan",id:"urutan-lanjutan",level:3},{value:"Tips Membaca",id:"tips-membaca",level:3},{value:"Link Sumber Daya",id:"link-sumber-daya",level:2},{value:"PDF Paper",id:"pdf-paper",level:3},{value:"Implementasi Open Source",id:"implementasi-open-source",level:3},{value:"Bacaan Lanjutan",id:"bacaan-lanjutan",level:2}];function o(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"panduan-paper-kunci",children:"Panduan Paper Kunci"})}),"\n",(0,r.jsx)(n.p,{children:"Artikel ini merangkum paper paling penting dalam sejarah pengembangan AI Go, menyediakan ringkasan dan poin teknis untuk pemahaman cepat."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"gambaran-paper",children:"Gambaran Paper"}),"\n",(0,r.jsx)(n.h3,{id:"timeline",children:"Timeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"2006  Coulom - MCTS pertama kali diterapkan pada Go\n2016  Silver et al. - AlphaGo (Nature)\n2017  Silver et al. - AlphaGo Zero (Nature)\n2017  Silver et al. - AlphaZero\n2019  Wu - KataGo\n2020+ Berbagai perbaikan dan aplikasi\n"})}),"\n",(0,r.jsx)(n.h3,{id:"rekomendasi-membaca",children:"Rekomendasi Membaca"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Tujuan"}),(0,r.jsx)(n.th,{children:"Paper yang Disarankan"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Memahami dasar"}),(0,r.jsx)(n.td,{children:"AlphaGo (2016)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Memahami self-play"}),(0,r.jsx)(n.td,{children:"AlphaGo Zero (2017)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Memahami metode umum"}),(0,r.jsx)(n.td,{children:"AlphaZero (2017)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Referensi implementasi"}),(0,r.jsx)(n.td,{children:"KataGo (2019)"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"1-kelahiran-mcts-2006",children:"1. Kelahiran MCTS (2006)"}),"\n",(0,r.jsx)(n.h3,{id:"informasi-paper",children:"Informasi Paper"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Judul: Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search\nPenulis: R\xe9mi Coulom\nDipresentasikan: Computers and Games 2006\n"})}),"\n",(0,r.jsx)(n.h3,{id:"kontribusi-inti",children:"Kontribusi Inti"}),"\n",(0,r.jsx)(n.p,{children:"Pertama kali menerapkan metode Monte Carlo secara sistematis pada Go:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Sebelumnya: Simulasi acak murni, tanpa struktur pohon\nSesudahnya: Membangun pohon pencarian + Seleksi UCB + Statistik backprop\n"})}),"\n",(0,r.jsx)(n.h3,{id:"konsep-kunci",children:"Konsep Kunci"}),"\n",(0,r.jsx)(n.h4,{id:"formula-ucb1",children:"Formula UCB1"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Skor Seleksi = Rata-rata winrate + C \xd7 \u221a(ln(N) / n)\n\nDi mana:\n- N: Jumlah kunjungan node induk\n- n: Jumlah kunjungan node anak\n- C: Konstanta eksplorasi\n"})}),"\n",(0,r.jsx)(n.h4,{id:"empat-langkah-mcts",children:"Empat Langkah MCTS"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"1. Selection: Pilih node menggunakan UCB\n2. Expansion: Ekspansi node baru\n3. Simulation: Simulasi acak sampai akhir permainan\n4. Backpropagation: Backprop menang/kalah\n"})}),"\n",(0,r.jsx)(n.h3,{id:"dampak",children:"Dampak"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Membuat AI Go mencapai level dan amatir"}),"\n",(0,r.jsx)(n.li,{children:"Menjadi dasar untuk semua AI Go selanjutnya"}),"\n",(0,r.jsx)(n.li,{children:"Konsep UCB mempengaruhi pengembangan PUCT"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"2-alphago-2016",children:"2. AlphaGo (2016)"}),"\n",(0,r.jsx)(n.h3,{id:"informasi-paper-1",children:"Informasi Paper"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Judul: Mastering the game of Go with deep neural networks and tree search\nPenulis: Silver, D., Huang, A., Maddison, C.J., et al.\nDipublikasikan: Nature, 2016\nDOI: 10.1038/nature16961\n"})}),"\n",(0,r.jsx)(n.h3,{id:"kontribusi-inti-1",children:"Kontribusi Inti"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Pertama kali menggabungkan deep learning dengan MCTS"}),", mengalahkan juara dunia manusia."]}),"\n",(0,r.jsx)(n.h3,{id:"arsitektur-sistem",children:"Arsitektur Sistem"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart TB\n    subgraph SL["Policy Network (SL)"]\n        SL1["Input: Status papan (48 feature plane)"]\n        SL2["Arsitektur: 13 layer CNN"]\n        SL3["Output: Probabilitas 361 posisi"]\n        SL4["Pelatihan: 30 juta rekaman manusia"]\n    end\n\n    subgraph RL["Policy Network (RL)"]\n        RL1["Diinisialisasi dari SL Policy"]\n        RL2["Reinforcement learning self-play"]\n    end\n\n    subgraph VN["Value Network"]\n        VN1["Input: Status papan"]\n        VN2["Output: Nilai winrate tunggal"]\n        VN3["Pelatihan: Posisi dari self-play"]\n    end\n\n    subgraph MCTS["MCTS"]\n        M1["Gunakan Policy Network untuk panduan"]\n        M2["Gunakan Value Network + Rollout untuk evaluasi"]\n    end\n\n    SL --\x3e RL\n    RL --\x3e VN\n    SL --\x3e MCTS\n    VN --\x3e MCTS'}),"\n",(0,r.jsx)(n.h3,{id:"poin-teknis",children:"Poin Teknis"}),"\n",(0,r.jsx)(n.h4,{id:"1-supervised-learning-policy-network",children:"1. Supervised Learning Policy Network"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Fitur input (48 plane)\n- Posisi batu sendiri\n- Posisi batu lawan\n- Jumlah liberty\n- Status setelah penangkapan\n- Posisi langkah legal\n- Posisi beberapa langkah terakhir\n...\n"})}),"\n",(0,r.jsx)(n.h4,{id:"2-perbaikan-reinforcement-learning",children:"2. Perbaikan Reinforcement Learning"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"SL Policy \u2192 Self-play \u2192 RL Policy\n\nRL Policy sekitar 80% lebih kuat dari SL Policy\n"})}),"\n",(0,r.jsx)(n.h4,{id:"3-pelatihan-value-network",children:"3. Pelatihan Value Network"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Kunci mencegah overfitting:\n- Hanya ambil satu posisi dari setiap permainan\n- Hindari posisi serupa muncul berulang\n"})}),"\n",(0,r.jsx)(n.h4,{id:"4-integrasi-mcts",children:"4. Integrasi MCTS"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Evaluasi leaf node = 0.5 \xd7 Value Network + 0.5 \xd7 Rollout\n\nRollout menggunakan Policy Network cepat (akurasi lebih rendah tapi lebih cepat)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"data-kunci",children:"Data Kunci"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Item"}),(0,r.jsx)(n.th,{children:"Nilai"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Akurasi SL Policy"}),(0,r.jsx)(n.td,{children:"57%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Winrate RL Policy vs SL Policy"}),(0,r.jsx)(n.td,{children:"80%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"GPU Pelatihan"}),(0,r.jsx)(n.td,{children:"176"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"GPU Pertandingan"}),(0,r.jsx)(n.td,{children:"48 TPU"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"3-alphago-zero-2017",children:"3. AlphaGo Zero (2017)"}),"\n",(0,r.jsx)(n.h3,{id:"informasi-paper-2",children:"Informasi Paper"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Judul: Mastering the game of Go without human knowledge\nPenulis: Silver, D., Schrittwieser, J., Simonyan, K., et al.\nDipublikasikan: Nature, 2017\nDOI: 10.1038/nature24270\n"})}),"\n",(0,r.jsx)(n.h3,{id:"kontribusi-inti-2",children:"Kontribusi Inti"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tidak memerlukan rekaman manusia sama sekali"}),", belajar sendiri dari nol."]}),"\n",(0,r.jsx)(n.h3,{id:"perbedaan-dengan-alphago",children:"Perbedaan dengan AlphaGo"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspek"}),(0,r.jsx)(n.th,{children:"AlphaGo"}),(0,r.jsx)(n.th,{children:"AlphaGo Zero"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Rekaman manusia"}),(0,r.jsx)(n.td,{children:"Diperlukan"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Tidak diperlukan"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Jumlah network"}),(0,r.jsx)(n.td,{children:"4"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"1 dual-head"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Fitur input"}),(0,r.jsx)(n.td,{children:"48 plane"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"17 plane"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Rollout"}),(0,r.jsx)(n.td,{children:"Digunakan"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Tidak digunakan"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Residual network"}),(0,r.jsx)(n.td,{children:"Tidak"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Ya"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Waktu pelatihan"}),(0,r.jsx)(n.td,{children:"Berbulan-bulan"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"3 hari"})})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"inovasi-kunci",children:"Inovasi Kunci"}),"\n",(0,r.jsx)(n.h4,{id:"1-single-dual-head-network",children:"1. Single Dual-Head Network"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart TB\n    Input["Input (17 plane)"]\n    Tower["Residual Tower<br/>(19 atau 39 layer)"]\n    Policy["Policy<br/>(361)"]\n    Value["Value<br/>(1)"]\n\n    Input --\x3e Tower\n    Tower --\x3e Policy\n    Tower --\x3e Value'}),"\n",(0,r.jsx)(n.h4,{id:"2-fitur-input-yang-disederhanakan",children:"2. Fitur Input yang Disederhanakan"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Hanya 17 feature plane\nfeatures = [\n    current_player_stones,      # Batu sendiri\n    opponent_stones,            # Batu lawan\n    history_1_player,           # Status historis 1\n    history_1_opponent,\n    ...                         # Status historis 2-7\n    color_to_play               # Giliran siapa\n]\n"})}),"\n",(0,r.jsx)(n.h4,{id:"3-evaluasi-pure-value-network",children:"3. Evaluasi Pure Value Network"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Tidak lagi menggunakan Rollout\nEvaluasi leaf node = Output Value Network\n\nLebih ringkas, lebih cepat\n"})}),"\n",(0,r.jsx)(n.h4,{id:"4-alur-pelatihan",children:"4. Alur Pelatihan"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart TB\n    Init["Inisialisasi network acak"]\n    SelfPlay["Self-play menghasilkan<br/>rekaman permainan"]\n    Train["Latih neural network<br/>- Policy: Minimisasi cross entropy<br/>- Value: Minimisasi MSE"]\n    Eval["Evaluasi network baru<br/>Jika lebih kuat ganti"]\n\n    Init --\x3e SelfPlay\n    SelfPlay --\x3e Train\n    Train --\x3e Eval\n    Eval --\x3e SelfPlay'}),"\n",(0,r.jsx)(n.h3,{id:"kurva-pembelajaran",children:"Kurva Pembelajaran"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Waktu Pelatihan    Elo\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n3 jam              Pemula\n24 jam             Melampaui AlphaGo Lee\n72 jam             Melampaui AlphaGo Master\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"4-alphazero-2017",children:"4. AlphaZero (2017)"}),"\n",(0,r.jsx)(n.h3,{id:"informasi-paper-3",children:"Informasi Paper"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Judul: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\nPenulis: Silver, D., Hubert, T., Schrittwieser, J., et al.\nDipublikasikan: arXiv:1712.01815 (kemudian dipublikasikan di Science, 2018)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"kontribusi-inti-3",children:"Kontribusi Inti"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Generalisasi"}),": Algoritma yang sama diterapkan pada Go, catur, dan shogi."]}),"\n",(0,r.jsx)(n.h3,{id:"arsitektur-umum",children:"Arsitektur Umum"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Encoding Input (game-specific) \u2192 Residual Network (umum) \u2192 Dual-head Output (umum)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"adaptasi-lintas-game",children:"Adaptasi Lintas Game"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Game"}),(0,r.jsx)(n.th,{children:"Plane Input"}),(0,r.jsx)(n.th,{children:"Ruang Aksi"}),(0,r.jsx)(n.th,{children:"Waktu Pelatihan"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Go"}),(0,r.jsx)(n.td,{children:"17"}),(0,r.jsx)(n.td,{children:"362"}),(0,r.jsx)(n.td,{children:"40 hari"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Catur"}),(0,r.jsx)(n.td,{children:"119"}),(0,r.jsx)(n.td,{children:"4672"}),(0,r.jsx)(n.td,{children:"9 jam"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Shogi"}),(0,r.jsx)(n.td,{children:"362"}),(0,r.jsx)(n.td,{children:"11259"}),(0,r.jsx)(n.td,{children:"12 jam"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"perbaikan-mcts",children:"Perbaikan MCTS"}),"\n",(0,r.jsx)(n.h4,{id:"formula-puct",children:"Formula PUCT"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Skor Seleksi = Q(s,a) + c(s) \xd7 P(s,a) \xd7 \u221aN(s) / (1 + N(s,a))\n\nc(s) = log((1 + N(s) + c_base) / c_base) + c_init\n"})}),"\n",(0,r.jsx)(n.h4,{id:"noise-eksplorasi",children:"Noise Eksplorasi"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Tambahkan noise Dirichlet di root node\nP(s,a) = (1 - \u03b5) \xd7 p_a + \u03b5 \xd7 \u03b7_a\n\n\u03b7 ~ Dir(\u03b1)\n\u03b1 = 0.03 (Go), 0.3 (catur), 0.15 (shogi)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"5-katago-2019",children:"5. KataGo (2019)"}),"\n",(0,r.jsx)(n.h3,{id:"informasi-paper-4",children:"Informasi Paper"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Judul: Accelerating Self-Play Learning in Go\nPenulis: David J. Wu\nDipublikasikan: arXiv:1902.10565\n"})}),"\n",(0,r.jsx)(n.h3,{id:"kontribusi-inti-4",children:"Kontribusi Inti"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Peningkatan efisiensi 50x"}),", memungkinkan pengembang individu melatih AI Go yang kuat."]}),"\n",(0,r.jsx)(n.h3,{id:"inovasi-kunci-1",children:"Inovasi Kunci"}),"\n",(0,r.jsx)(n.h4,{id:"1-target-pelatihan-tambahan",children:"1. Target Pelatihan Tambahan"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Total Loss = Policy Loss + Value Loss +\n             Score Loss + Ownership Loss + ...\n\nTarget tambahan membuat network konvergen lebih cepat\n"})}),"\n",(0,r.jsx)(n.h4,{id:"2-fitur-global",children:"2. Fitur Global"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Layer global pooling\nglobal_features = global_avg_pool(conv_features)\n# Gabungkan dengan fitur lokal\ncombined = concat(conv_features, broadcast(global_features))\n"})}),"\n",(0,r.jsx)(n.h4,{id:"3-randomisasi-playout-cap",children:"3. Randomisasi Playout Cap"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Tradisional: Setiap pencarian N kali tetap\nKataGo: N diambil sampel acak dari distribusi tertentu\n\nMembuat network belajar tampil baik di berbagai kedalaman pencarian\n"})}),"\n",(0,r.jsx)(n.h4,{id:"4-ukuran-papan-progresif",children:"4. Ukuran Papan Progresif"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if training_step < 1000000:\n    board_size = random.choice([9, 13, 19])\nelse:\n    board_size = 19\n"})}),"\n",(0,r.jsx)(n.h3,{id:"perbandingan-efisiensi",children:"Perbandingan Efisiensi"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Metrik"}),(0,r.jsx)(n.th,{children:"AlphaZero"}),(0,r.jsx)(n.th,{children:"KataGo"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"GPU-hari untuk mencapai level superhuman"}),(0,r.jsx)(n.td,{children:"5000"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"100"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Peningkatan efisiensi"}),(0,r.jsx)(n.td,{children:"Baseline"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"50x"})})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"6-paper-lanjutan",children:"6. Paper Lanjutan"}),"\n",(0,r.jsx)(n.h3,{id:"muzero-2020",children:"MuZero (2020)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Judul: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\nKontribusi: Mempelajari model dinamika lingkungan, tidak memerlukan aturan game\n"})}),"\n",(0,r.jsx)(n.h3,{id:"efficientzero-2021",children:"EfficientZero (2021)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Judul: Mastering Atari Games with Limited Data\nKontribusi: Peningkatan efisiensi sampel yang signifikan\n"})}),"\n",(0,r.jsx)(n.h3,{id:"gumbel-alphazero-2022",children:"Gumbel AlphaZero (2022)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Judul: Policy Improvement by Planning with Gumbel\nKontribusi: Metode policy improvement yang ditingkatkan\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"saran-membaca-paper",children:"Saran Membaca Paper"}),"\n",(0,r.jsx)(n.h3,{id:"urutan-untuk-pemula",children:"Urutan untuk Pemula"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"1. AlphaGo (2016) - Memahami arsitektur dasar\n2. AlphaGo Zero (2017) - Memahami self-play\n3. KataGo (2019) - Memahami detail implementasi\n"})}),"\n",(0,r.jsx)(n.h3,{id:"urutan-lanjutan",children:"Urutan Lanjutan"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"4. AlphaZero (2017) - Generalisasi\n5. MuZero (2020) - Mempelajari model dunia\n6. Paper MCTS asli - Memahami dasar\n"})}),"\n",(0,r.jsx)(n.h3,{id:"tips-membaca",children:"Tips Membaca"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Baca abstrak dan kesimpulan dulu"}),": Pahami cepat kontribusi inti"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lihat gambar dan tabel"}),": Pahami arsitektur keseluruhan"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Baca bagian metode"}),": Pahami detail teknis"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lihat lampiran"}),": Temukan detail implementasi dan hyperparameter"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"link-sumber-daya",children:"Link Sumber Daya"}),"\n",(0,r.jsx)(n.h3,{id:"pdf-paper",children:"PDF Paper"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Paper"}),(0,r.jsx)(n.th,{children:"Link"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaGo"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://www.nature.com/articles/nature16961",children:"Nature"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaGo Zero"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://www.nature.com/articles/nature24270",children:"Nature"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"AlphaZero"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://www.science.org/doi/10.1126/science.aar6404",children:"Science"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"KataGo"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1902.10565",children:"arXiv"})})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"implementasi-open-source",children:"Implementasi Open Source"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Proyek"}),(0,r.jsx)(n.th,{children:"Link"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"KataGo"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/lightvector/KataGo",children:"GitHub"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Leela Zero"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/leela-zero/leela-zero",children:"GitHub"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MiniGo"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/tensorflow/minigo",children:"GitHub"})})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"bacaan-lanjutan",children:"Bacaan Lanjutan"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"../neural-network",children:"Detail Arsitektur Neural Network"})," \u2014 Memahami mendalam desain network"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"../mcts-implementation",children:"Detail Implementasi MCTS"})," \u2014 Implementasi algoritma pencarian"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"../training",children:"Analisis Mekanisme Pelatihan KataGo"})," \u2014 Detail alur pelatihan"]}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(o,{...e})}):o(e)}},30416(e,n,a){a.d(n,{R:()=>s,x:()=>t});var i=a(59471);const r={},l=i.createContext(r);function s(e){const n=i.useContext(l);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(l.Provider,{value:n},e.children)}}}]);