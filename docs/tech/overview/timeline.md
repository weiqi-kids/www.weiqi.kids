---
sidebar_position: 2
title: 發展時間軸
description: 圍棋 AI 從 2006 年到 2025 年的關鍵發展里程碑
---

# 圍棋 AI 發展時間軸

圍棋曾被視為人工智慧最困難的挑戰之一，因其巨大的搜索空間（約 10^170 種可能局面）。這個時間軸記錄了圍棋 AI 從「不可能」到「超越人類」的歷程。

## 前 AI 時代（1968-2005）

### 1968 - 第一個圍棋程式

Zobrist 開發了第一個圍棋程式，但只能下出「亂下」的水平。

### 1970-2005 - 傳統方法的極限

- 基於規則的專家系統
- Minimax + Alpha-Beta 剪枝
- 最強程式僅達業餘初段水平

**為什麼圍棋這麼難？**

| 遊戲 | 平均分支因子 | 狀態空間 |
|------|-------------|---------|
| 井字棋 | 4 | 10^3 |
| 西洋棋 | 35 | 10^47 |
| 圍棋 | 250 | 10^170 |

---

## MCTS 時代（2006-2015）

### 2006 - 蒙地卡羅樹搜索的突破

Rémi Coulom 將蒙地卡羅方法應用於圍棋，開發了 Crazy Stone。

**核心創新**：
- 不需要評估函數
- 用隨機模擬估計勝率
- MCTS 四步驟：Select → Expand → Simulate → Backprop

**相關動畫概念**：
- 🎬 C1：隨機取樣 ↔ 蒙地卡羅積分
- 🎬 C5：MCTS 四步驟 ↔ 樹的遍歷

### 2008 - MoGo 首次擊敗職業棋手

MoGo 在讓九子的情況下擊敗職業九段，證明 MCTS 的潛力。

### 2012 - Zen 達到業餘六段

日本的 Zen 程式達到業餘六段水平，是當時最強的圍棋 AI。

---

## AlphaGo 時代（2015-2017）

### 2015 年 10 月 - AlphaGo 擊敗樊麾

Google DeepMind 的 AlphaGo 以 5:0 擊敗歐洲冠軍樊麾（職業二段）。

**核心創新**：
- Policy Network：預測下一步的機率分布
- Value Network：評估當前局面的勝率
- 結合 MCTS：用神經網路取代隨機模擬

**相關動畫概念**：
- 🎬 E1：策略網路 ↔ 策略分布
- 🎬 E2：價值網路 ↔ 價值函數

### 2016 年 3 月 - AlphaGo 擊敗李世乭

AlphaGo 以 4:1 擊敗世界冠軍李世乭，震驚全球。

**第 37 手「神之一手」**：
- AlphaGo 在第二盤下出人類從未想過的肩衝
- 專家評論「這不是人類會下的棋」
- 最終證明是致勝關鍵

### 2017 年 5 月 - AlphaGo 擊敗柯潔

AlphaGo 以 3:0 擊敗當時世界排名第一的柯潔，確立 AI 已超越人類頂尖水平。

### 2017 年 10 月 - AlphaGo Zero 發表

DeepMind 發表 AlphaGo Zero，完全不需要人類棋譜，從零開始自我學習。

**驚人成果**：
- 3 天超越擊敗李世乭的 AlphaGo
- 21 天超越 AlphaGo Master
- 40 天成為史上最強

**核心創新**：
- 單一網路，雙頭輸出（Policy + Value）
- 純自我對弈，不需人類棋譜
- 更簡潔的架構，更強的棋力

**相關動畫概念**：
- 🎬 E5：自我對弈 ↔ 不動點收斂
- 🎬 E7：從零開始 ↔ 自組織

### 2017 年 12 月 - AlphaZero 發表

AlphaZero 將同樣的架構應用於西洋棋和將棋，均達到超越人類的水平。

---

## 開源時代（2018-2019）

### 2018 - Leela Zero 達到超人水平

比利時開發者 Gian-Carlo Pascutto 發起 Leela Zero 專案，社群分散式訓練。

**意義**：
- 證明 AlphaZero 方法可被復現
- 開源讓所有人都能使用頂級圍棋 AI

### 2019 - KataGo 發表

David Wu 發表 KataGo，以更少資源達到更強棋力。

**核心突破**：
- 訓練效率提升 50 倍
- 30 GPU × 19 天 = 超越 Leela Zero
- 完全開源（MIT 授權）

**相關動畫概念**：
- 🎬 E3：雙頭網路 ↔ 多任務學習
- 🎬 F5：計算資源縮放 ↔ 縮放定律

---

## 產業應用時代（2020-至今）

### 2020 - KataGo 分散式訓練

KataGo Training 專案啟動，全球志願者貢獻算力進行分散式訓練。

### 2021-2022 - AI 融入職業訓練

- 韓國國家隊：使用 KataGo 和 ELF OpenGo 訓練
- 中國國家隊：使用騰訊絕藝
- 日本棋院：逐步接受 AI 輔助

### 2023-2025 - 人機共生時代

- AI 覆盤成為標準訓練流程
- 新定式不斷被 AI 發現
- 人類棋手學習 AI 的下法風格
- 「AI 流」成為主流下法

---

## 技術演進總覽

```
1968-2005  傳統方法（規則、搜索）→ 業餘初段
    │
    ↓
2006-2015  MCTS（隨機模擬）→ 業餘六段
    │
    ↓
2015-2017  AlphaGo（深度學習 + MCTS）→ 超越人類
    │
    ↓
2017-2019  AlphaGo Zero/KataGo（純自我對弈）→ 效率提升
    │
    ↓
2020-至今  產業應用（訓練、分析、教學）→ 人機共生
```

## 關鍵數據對比

| 系統 | 年份 | 訓練資源 | 棋力 |
|------|------|---------|------|
| 傳統程式 | 2005 | N/A | 業餘初段 |
| Zen | 2012 | N/A | 業餘六段 |
| AlphaGo (Lee) | 2016 | 數千 TPU | 超越世界冠軍 |
| AlphaGo Zero | 2017 | 4 TPU × 3 天 | 100:0 擊敗 AlphaGo |
| KataGo | 2019 | 30 GPU × 19 天 | 超越 Leela Zero |
| KataGo | 2024 | 分散式訓練 | 持續進化中 |

## 延伸閱讀

- [生態全景圖](../landscape) — 各種 AI、工具、平台的比較
- [一篇文章搞懂圍棋 AI](../../how-it-works/) — 深入了解技術原理
- [KataGo 的關鍵創新](../../how-it-works/katago-innovations) — 50 倍效率提升的秘密
