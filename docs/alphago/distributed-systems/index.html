<!doctype html>
<html lang="zh-tw" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-alphago/distributed-systems" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">分散式系統與 TPU | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/docs/alphago/distributed-systems/"><meta data-rh="true" property="og:locale" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="zh-tw"><meta data-rh="true" name="docsearch:language" content="zh-tw"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="分散式系統與 TPU | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="深入解析 AlphaGo 的分散式訓練架構、TPU 加速與大規模並行 MCTS"><meta data-rh="true" property="og:description" content="深入解析 AlphaGo 的分散式訓練架構、TPU 加速與大規模並行 MCTS"><meta data-rh="true" name="keywords" content="分散式系統,TPU,並行計算,MCTS,虛擬損失,深度學習,硬體加速"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/docs/alphago/distributed-systems/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/distributed-systems/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/alphago/distributed-systems/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/alphago/distributed-systems/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/alphago/distributed-systems/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/alphago/distributed-systems/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/alphago/distributed-systems/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/alphago/distributed-systems/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/alphago/distributed-systems/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/alphago/distributed-systems/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/alphago/distributed-systems/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/alphago/distributed-systems/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/distributed-systems/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AlphaGo","item":"https://www.weiqi.kids/docs/alphago/"},{"@type":"ListItem","position":2,"name":"分散式系統與 TPU","item":"https://www.weiqi.kids/docs/alphago/distributed-systems"}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"WebPage","@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/distributed-systems/#webpage","url":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/distributed-systems/","name":"分散式系統與 TPU","description":"深入解析 AlphaGo 的分散式訓練架構、TPU 加速與大規模並行 MCTS","inLanguage":"zh-TW","isPartOf":{"@id":"https://www.weiqi.kids#website"},"primaryImageOfPage":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/social-card.png"},"datePublished":"2024-01-15","dateModified":"2024-02-22","speakable":{"@type":"SpeakableSpecification","cssSelector":[".article-summary",".speakable-content",".key-takeaway",".key-answer",".expert-quote",".actionable-steps li",".faq-answer-content"]}},{"@type":"Article","@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/distributed-systems/#article","mainEntityOfPage":{"@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/distributed-systems/#webpage","significantLink":["https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/"]},"headline":"分散式系統與 TPU","description":"深入解析 AlphaGo 的分散式訓練架構、TPU 加速與大規模並行 MCTS","image":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/social-card.png","width":1200,"height":630},"author":{"@id":"https://www.weiqi.kids/docs/about/#person"},"publisher":{"@id":"https://www.weiqi.kids#organization"},"datePublished":"2024-01-15","dateModified":"2024-02-22","articleSection":"AlphaGo 完整解析","keywords":"分散式系統, TPU, 並行計算, MCTS, 虛擬損失, 深度學習, 硬體加速, 批次推理","wordCount":4600,"inLanguage":"zh-TW","isAccessibleForFree":true,"isPartOf":{"@type":"WebSite","@id":"https://www.weiqi.kids#website"}},{"@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首頁","item":"https://www.weiqi.kids"},{"@type":"ListItem","position":2,"name":"給工程師","item":"https://www.weiqi.kids/docs/for-engineers/"},{"@type":"ListItem","position":3,"name":"技術原理","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/"},{"@type":"ListItem","position":4,"name":"AlphaGo 完整解析","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/"}]},{"@type":"Person","@id":"https://www.weiqi.kids/docs/about/#person","name":"好棋寶寶協會編輯團隊","url":"https://www.weiqi.kids/docs/about/","worksFor":{"@id":"https://www.weiqi.kids#organization"},"description":"專注於圍棋 AI 研究與教育推廣的技術團隊","knowsAbout":["圍棋 AI","AlphaGo","KataGo","機器學習","深度學習"],"hasCredential":[{"@type":"EducationalOccupationalCredential","name":"AI 圍棋研究專家","credentialCategory":"技術認證"}]},{"@type":"ImageObject","@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/distributed-systems/#primaryimage","url":"https://www.weiqi.kids/img/social-card.png","width":1200,"height":630,"caption":"分散式系統與 TPU - 好棋寶寶協會","representativeOfPage":true,"license":"https://creativecommons.org/licenses/by-nc-sa/4.0/","creditText":"台灣好棋寶寶協會"}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"FAQPage","mainEntity":[{"@type":"Question","name":"什麼是虛擬損失（Virtual Loss），它如何讓 MCTS 並行化？","acceptedAnswer":{"@type":"Answer","text":"虛擬損失是一種技術，當一個執行緒正在探索某個節點時，臨時降低該節點的價值，讓其他執行緒選擇不同路徑。這樣可以同時收集多個待評估的葉節點，進行批次神經網路推理，大幅提升效率。探索完成後虛擬損失會被真實值覆蓋，不影響最終結果。"}},{"@type":"Question","name":"為什麼 AlphaGo 需要 TPU 而不是 GPU？","acceptedAnswer":{"@type":"Answer","text":"圍棋 AI 的計算瓶頸是神經網路推理（佔 95%），每步 MCTS 需要執行 1600 次推理。TPU 是 Google 專為矩陣運算設計的加速器，其 MXU 單元特別擅長這類計算，功耗更低、延遲更少。從 AlphaGo Lee 的 48 TPU 到 AlphaGo Zero 的 4 TPU，效率提升超過 10 倍。"}},{"@type":"Question","name":"AlphaGo Zero 的完整訓練成本大約是多少？","acceptedAnswer":{"@type":"Answer","text":"以 Google Cloud 定價估算，每天約 $3,700（4 TPU v2 Pod + 高記憶體 VM + 儲存空間），完整 40 天訓練約 $150,000。但這只是雲端租用成本，DeepMind 作為 Google 子公司可能有內部折扣。對比之下，培養一位職業棋手需要 10-15 年和數百萬的總成本。"}}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.f23bf74b.css">
<script src="/assets/js/runtime~main.1011f191.js" defer="defer"></script>
<script src="/assets/js/main.5e37ffeb.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.svg"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="好棋寶寶協會標誌" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/img/logo.svg" alt="好棋寶寶協會標誌" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">好棋寶寶</b></a><a class="navbar__item navbar__link" href="/docs/learn/">學圍棋</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/alphago/">AlphaGo</a><a class="navbar__item navbar__link" href="/docs/animations/">動畫教室</a><a class="navbar__item navbar__link" href="/docs/tech/">技術文件</a><a class="navbar__item navbar__link" href="/docs/about/">關於我們</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>繁體中文</a><ul class="dropdown__menu"><li><a href="/docs/alphago/distributed-systems/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/alphago/distributed-systems/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/alphago/distributed-systems/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/alphago/distributed-systems/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/alphago/distributed-systems/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/alphago/distributed-systems/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/alphago/distributed-systems/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/alphago/distributed-systems/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/alphago/distributed-systems/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/alphago/distributed-systems/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/alphago/distributed-systems/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro/"><span title="使用指南" class="linkLabel_REp1">使用指南</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/docs/alphago/"><span title="AlphaGo" class="categoryLinkLabel_ezQx">AlphaGo</span></a><button aria-label="收起側邊欄分類 &#x27;AlphaGo&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/birth-of-alphago/"><span title="AlphaGo 的誕生" class="linkLabel_REp1">AlphaGo 的誕生</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/key-matches/"><span title="關鍵對局回顧" class="linkLabel_REp1">關鍵對局回顧</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/move-37/"><span title="「神之一手」深度分析" class="linkLabel_REp1">「神之一手」深度分析</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/why-go-is-hard/"><span title="圍棋為什麼難？" class="linkLabel_REp1">圍棋為什麼難？</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/traditional-limits/"><span title="傳統方法的極限" class="linkLabel_REp1">傳統方法的極限</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/board-representation/"><span title="棋盤狀態表示" class="linkLabel_REp1">棋盤狀態表示</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/policy-network/"><span title="Policy Network 詳解" class="linkLabel_REp1">Policy Network 詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/value-network/"><span title="Value Network 詳解" class="linkLabel_REp1">Value Network 詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/input-features/"><span title="輸入特徵設計" class="linkLabel_REp1">輸入特徵設計</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/cnn-and-go/"><span title="CNN 與圍棋的結合" class="linkLabel_REp1">CNN 與圍棋的結合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/supervised-learning/"><span title="監督學習階段" class="linkLabel_REp1">監督學習階段</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/reinforcement-intro/"><span title="強化學習入門" class="linkLabel_REp1">強化學習入門</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/self-play/"><span title="自我對弈" class="linkLabel_REp1">自我對弈</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/mcts-neural-combo/"><span title="MCTS 與神經網路的結合" class="linkLabel_REp1">MCTS 與神經網路的結合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/puct-formula/"><span title="PUCT 公式詳解" class="linkLabel_REp1">PUCT 公式詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/alphago-zero/"><span title="AlphaGo Zero 概述" class="linkLabel_REp1">AlphaGo Zero 概述</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/dual-head-resnet/"><span title="雙頭網路與殘差網路" class="linkLabel_REp1">雙頭網路與殘差網路</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/training-from-scratch/"><span title="從零訓練的過程" class="linkLabel_REp1">從零訓練的過程</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/alphago/distributed-systems/"><span title="分散式系統與 TPU" class="linkLabel_REp1">分散式系統與 TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/legacy-and-impact/"><span title="AlphaGo 的遺產" class="linkLabel_REp1">AlphaGo 的遺產</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/learn/"><span title="學圍棋" class="categoryLinkLabel_ezQx">學圍棋</span></a><button aria-label="展開側邊欄分類 &#x27;學圍棋&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/animations/"><span title="動畫教室" class="linkLabel_REp1">動畫教室</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/tech/"><span title="技術文件" class="categoryLinkLabel_ezQx">技術文件</span></a><button aria-label="展開側邊欄分類 &#x27;技術文件&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/about/"><span title="關於我們" class="categoryLinkLabel_ezQx">關於我們</span></a><button aria-label="展開側邊欄分類 &#x27;關於我們&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="頁面路徑"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/alphago/"><span>AlphaGo</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">分散式系統與 TPU</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">本頁導覽</button></div><div class="theme-doc-markdown markdown">
<header><h1>分散式系統與 TPU</h1></header>
<p>AlphaGo 的成功不僅是演算法的勝利，也是工程的勝利。要在合理時間內訓練出超越人類的圍棋 AI，需要精心設計的分散式系統和專用硬體的支援。</p>
<p>本文將深入解析 AlphaGo 背後的系統架構，包括訓練流程、推理架構、並行 MCTS，以及 TPU 的關鍵角色。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="訓練架構總覽">訓練架構總覽<a href="#訓練架構總覽" class="hash-link" aria-label="訓練架構總覽的直接連結" title="訓練架構總覽的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="原版-alphago-的訓練架構">原版 AlphaGo 的訓練架構<a href="#原版-alphago-的訓練架構" class="hash-link" aria-label="原版 AlphaGo 的訓練架構的直接連結" title="原版 AlphaGo 的訓練架構的直接連結" translate="no">​</a></h3>
<p>原版 AlphaGo（擊敗李世乭的版本）的訓練分為多個階段，每個階段使用不同的資源配置：</p>
<!-- -->
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero-的訓練架構">AlphaGo Zero 的訓練架構<a href="#alphago-zero-的訓練架構" class="hash-link" aria-label="AlphaGo Zero 的訓練架構的直接連結" title="AlphaGo Zero 的訓練架構的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 大幅簡化了訓練流程，使用單一的端到端訓練循環：</p>
<!-- -->
<p>這個架構的優勢：</p>
<ol>
<li class=""><strong>持續學習</strong>：Self-play 和 Training 同時進行，不需要等待</li>
<li class=""><strong>資源效率</strong>：所有資源都在做有用的工作</li>
<li class=""><strong>快速迭代</strong>：網路更新後立即用於產生新資料</li>
</ol>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="自我對弈工作站self-play-workers">自我對弈工作站（Self-play Workers）<a href="#自我對弈工作站self-play-workers" class="hash-link" aria-label="自我對弈工作站（Self-play Workers）的直接連結" title="自我對弈工作站（Self-play Workers）的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="任務分配">任務分配<a href="#任務分配" class="hash-link" aria-label="任務分配的直接連結" title="任務分配的直接連結" translate="no">​</a></h3>
<p>Self-play Workers 負責用當前最強的網路進行自我對弈，產生訓練資料。</p>
<table><thead><tr><th>配置</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Worker 數量</td><td>數十個</td></tr><tr><td>每個 Worker</td><td>1-4 TPU</td></tr><tr><td>每局 MCTS</td><td>1600 次模擬</td></tr><tr><td>每天產生</td><td>~100,000 局</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="工作流程">工作流程<a href="#工作流程" class="hash-link" aria-label="工作流程的直接連結" title="工作流程的直接連結" translate="no">​</a></h3>
<p>每個 Self-play Worker 的工作流程：</p>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">while</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 1. 下載最新的網路權重</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    network </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> download_latest_checkpoint</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 2. 進行多局自我對弈</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> game </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">batch_size</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        positions </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        board </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> EmptyBoard</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">while</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">not</span><span class="token plain"> board</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">is_terminal</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token comment" style="color:#999988;font-style:italic"># 執行 MCTS</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            mcts </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> MCTS</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">network</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> board</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> mcts</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">search</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_simulations</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1600</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token comment" style="color:#999988;font-style:italic"># 選擇落子</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            action </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> sample</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">policy</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token comment" style="color:#999988;font-style:italic"># 記錄</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            positions</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">append</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">board</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">state</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> policy</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token comment" style="color:#999988;font-style:italic"># 落子</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            board </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> board</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">play</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">action</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 3. 獲取勝負結果</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        result </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> board</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_result</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 4. 上傳資料</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        upload_to_replay_buffer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">positions</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> result</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="負載平衡">負載平衡<a href="#負載平衡" class="hash-link" aria-label="負載平衡的直接連結" title="負載平衡的直接連結" translate="no">​</a></h3>
<p>多個 Worker 需要負載平衡：</p>
<ul>
<li class=""><strong>網路同步</strong>：所有 Worker 使用相同版本的網路</li>
<li class=""><strong>資料平衡</strong>：確保不同 Worker 的資料都被使用</li>
<li class=""><strong>容錯處理</strong>：單個 Worker 失敗不影響整體訓練</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="訓練工作站training-workers">訓練工作站（Training Workers）<a href="#訓練工作站training-workers" class="hash-link" aria-label="訓練工作站（Training Workers）的直接連結" title="訓練工作站（Training Workers）的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="任務分配-1">任務分配<a href="#任務分配-1" class="hash-link" aria-label="任務分配的直接連結" title="任務分配的直接連結" translate="no">​</a></h3>
<p>Training Workers 負責從 Replay Buffer 取樣資料，訓練神經網路。</p>
<table><thead><tr><th>配置</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Worker 數量</td><td>1-4</td></tr><tr><td>每個 Worker</td><td>4 TPU</td></tr><tr><td>Batch Size</td><td>2048（每個 TPU 512）</td></tr><tr><td>訓練步數</td><td>每天數萬步</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="分散式訓練">分散式訓練<a href="#分散式訓練" class="hash-link" aria-label="分散式訓練的直接連結" title="分散式訓練的直接連結" translate="no">​</a></h3>
<p>大規模訓練使用<strong>資料並行（Data Parallelism）</strong>：</p>
<!-- -->
<p>每個 TPU 處理不同的 mini-batch，計算出本地梯度，然後聚合更新全局參數。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="同步-vs-非同步更新">同步 vs. 非同步更新<a href="#同步-vs-非同步更新" class="hash-link" aria-label="同步 vs. 非同步更新的直接連結" title="同步 vs. 非同步更新的直接連結" translate="no">​</a></h3>
<table><thead><tr><th>更新方式</th><th>優點</th><th>缺點</th></tr></thead><tbody><tr><td>同步</td><td>穩定、可重現</td><td>Worker 需要等待最慢的</td></tr><tr><td>非同步</td><td>吞吐量高</td><td>梯度可能過時</td></tr></tbody></table>
<p>AlphaGo Zero 使用<strong>同步更新</strong>，確保訓練的穩定性。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="tpu-的角色">TPU 的角色<a href="#tpu-的角色" class="hash-link" aria-label="TPU 的角色的直接連結" title="TPU 的角色的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="什麼是-tpu">什麼是 TPU？<a href="#什麼是-tpu" class="hash-link" aria-label="什麼是 TPU？的直接連結" title="什麼是 TPU？的直接連結" translate="no">​</a></h3>
<p><strong>TPU（Tensor Processing Unit）</strong> 是 Google 專門為深度學習設計的加速器：</p>
<table><thead><tr><th>特性</th><th>TPU</th><th>GPU</th><th>CPU</th></tr></thead><tbody><tr><td>設計目標</td><td>矩陣運算</td><td>通用並行</td><td>通用計算</td></tr><tr><td>精度</td><td>FP16/BF16 優化</td><td>FP32/FP16</td><td>FP64/FP32</td></tr><tr><td>功耗</td><td>相對低</td><td>較高</td><td>最高</td></tr><tr><td>延遲</td><td>低</td><td>中等</td><td>高</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="tpu-的架構">TPU 的架構<a href="#tpu-的架構" class="hash-link" aria-label="TPU 的架構的直接連結" title="TPU 的架構的直接連結" translate="no">​</a></h3>
<p>TPU 的核心是 <strong>MXU（Matrix Multiply Unit）</strong>：</p>
<!-- -->
<p>MXU 每個週期可以執行 16K 次乘加運算，這對於神經網路的矩陣乘法至關重要。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="為什麼-alphago-需要-tpu">為什麼 AlphaGo 需要 TPU？<a href="#為什麼-alphago-需要-tpu" class="hash-link" aria-label="為什麼 AlphaGo 需要 TPU？的直接連結" title="為什麼 AlphaGo 需要 TPU？的直接連結" translate="no">​</a></h3>
<p>圍棋 AI 的計算瓶頸在於<strong>神經網路推理</strong>：</p>
<table><thead><tr><th>操作</th><th>佔比</th></tr></thead><tbody><tr><td>神經網路前向傳播</td><td>~95%</td></tr><tr><td>MCTS 樹操作</td><td>~4%</td></tr><tr><td>其他</td><td>~1%</td></tr></tbody></table>
<p>每一步 MCTS 需要執行 1600 次神經網路推理。TPU 的高吞吐量讓這成為可能。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="tpu-使用的演進">TPU 使用的演進<a href="#tpu-使用的演進" class="hash-link" aria-label="TPU 使用的演進的直接連結" title="TPU 使用的演進的直接連結" translate="no">​</a></h3>
<table><thead><tr><th>版本</th><th>訓練 TPU</th><th>推理 TPU</th></tr></thead><tbody><tr><td>AlphaGo Lee</td><td>50 GPU</td><td>48 TPU（v1）</td></tr><tr><td>AlphaGo Master</td><td>4 TPU（v2）</td><td>4 TPU（v2）</td></tr><tr><td>AlphaGo Zero</td><td>4 TPU（v2）</td><td>4 TPU（v2）（可擴展）</td></tr></tbody></table>
<p>AlphaGo Zero 使用的 TPU 數量大幅減少，這要歸功於更高效的架構和更新的 TPU 版本。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="並行-mcts-與虛擬損失">並行 MCTS 與虛擬損失<a href="#並行-mcts-與虛擬損失" class="hash-link" aria-label="並行 MCTS 與虛擬損失的直接連結" title="並行 MCTS 與虛擬損失的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="並行化的挑戰">並行化的挑戰<a href="#並行化的挑戰" class="hash-link" aria-label="並行化的挑戰的直接連結" title="並行化的挑戰的直接連結" translate="no">​</a></h3>
<p>MCTS 的標準實現是<strong>串行</strong>的：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">for i in range(num_simulations):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    1. Selection：從根向下選擇</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    2. Expansion：擴展葉節點</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    3. Evaluation：神經網路評估</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    4. Backup：回傳更新</span><br></span></code></pre></div></div>
<p>但神經網路評估是 GPU/TPU 友好的<strong>批次操作</strong>。如何讓多個模擬同時進行？</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="葉節點並行leaf-parallelization">葉節點並行（Leaf Parallelization）<a href="#葉節點並行leaf-parallelization" class="hash-link" aria-label="葉節點並行（Leaf Parallelization）的直接連結" title="葉節點並行（Leaf Parallelization）的直接連結" translate="no">​</a></h3>
<p>最簡單的並行方式：同時執行多個完整的模擬，最後合併結果。</p>
<!-- -->
<p>問題：每個模擬都從根開始，會重複探索相同的路徑。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="虛擬損失virtual-loss">虛擬損失（Virtual Loss）<a href="#虛擬損失virtual-loss" class="hash-link" aria-label="虛擬損失（Virtual Loss）的直接連結" title="虛擬損失（Virtual Loss）的直接連結" translate="no">​</a></h3>
<p>DeepMind 採用了<strong>虛擬損失</strong>技術來實現樹並行（Tree Parallelization）。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="基本概念">基本概念<a href="#基本概念" class="hash-link" aria-label="基本概念的直接連結" title="基本概念的直接連結" translate="no">​</a></h4>
<p>當一個執行緒正在探索某個節點時，臨時降低該節點的價值，讓其他執行緒選擇其他路徑。</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">正常的 UCB：Q(s,a) + c * P(s,a) * sqrt(N(s)) / (1 + N(s,a))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">加入虛擬損失後：</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Q(s,a) * N(s,a) - v * n_virtual) / (N(s,a) + n_virtual) + c * P(s,a) * sqrt(N(s)) / (1 + N(s,a) + n_virtual)</span><br></span></code></pre></div></div>
<p>其中：</p>
<ul>
<li class=""><code>n_virtual</code> 是正在探索該節點的執行緒數</li>
<li class=""><code>v</code> 是虛擬損失的值（通常為 1 或勝率對應值）</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="運作流程">運作流程<a href="#運作流程" class="hash-link" aria-label="運作流程的直接連結" title="運作流程的直接連結" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">時間 T1：</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Thread 1 選擇路徑 A → B → C</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  節點 C 獲得虛擬損失 -1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">時間 T2：</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Thread 2 選擇路徑 A → B → D（因為 C 被「懲罰」了）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  節點 D 獲得虛擬損失 -1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">時間 T3：</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Thread 1 完成評估，更新 C 的實際值，移除虛擬損失</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Thread 3 現在可能選擇 C（如果實際值夠好）</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="虛擬損失的效果">虛擬損失的效果<a href="#虛擬損失的效果" class="hash-link" aria-label="虛擬損失的效果的直接連結" title="虛擬損失的效果的直接連結" translate="no">​</a></h4>
<table><thead><tr><th>方面</th><th>效果</th></tr></thead><tbody><tr><td>探索多樣性</td><td>強制探索不同路徑</td></tr><tr><td>批次效率</td><td>可以同時評估多個葉節點</td></tr><tr><td>收斂性</td><td>虛擬損失最終被真實值覆蓋，不影響收斂</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="批次神經網路評估">批次神經網路評估<a href="#批次神經網路評估" class="hash-link" aria-label="批次神經網路評估的直接連結" title="批次神經網路評估的直接連結" translate="no">​</a></h3>
<p>透過虛擬損失，可以收集多個待評估的葉節點，進行<strong>批次推理</strong>：</p>
<!-- -->
<p>TPU 的批次推理效率遠高於逐個推理，這讓並行 MCTS 成為可能。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="推理架構">推理架構<a href="#推理架構" class="hash-link" aria-label="推理架構的直接連結" title="推理架構的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="比賽時的配置">比賽時的配置<a href="#比賽時的配置" class="hash-link" aria-label="比賽時的配置的直接連結" title="比賽時的配置的直接連結" translate="no">​</a></h3>
<p>AlphaGo 在正式比賽中的推理架構：</p>
<table><thead><tr><th>版本</th><th>硬體配置</th></tr></thead><tbody><tr><td>AlphaGo Fan</td><td>176 GPU</td></tr><tr><td>AlphaGo Lee</td><td>48 TPU + 多台伺服器</td></tr><tr><td>AlphaGo Master</td><td>4 TPU</td></tr><tr><td>AlphaGo Zero</td><td>4 TPU（可擴展）</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="分散式推理流程">分散式推理流程<a href="#分散式推理流程" class="hash-link" aria-label="分散式推理流程的直接連結" title="分散式推理流程的直接連結" translate="no">​</a></h3>
<p>比賽時的推理流程（以 AlphaGo Lee 為例）：</p>
<!-- -->
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="思考時間管理">思考時間管理<a href="#思考時間管理" class="hash-link" aria-label="思考時間管理的直接連結" title="思考時間管理的直接連結" translate="no">​</a></h3>
<p>AlphaGo 的時間管理策略：</p>
<table><thead><tr><th>局面</th><th>思考時間</th><th>MCTS 次數</th></tr></thead><tbody><tr><td>開局（有定式）</td><td>較短</td><td>~10,000</td></tr><tr><td>中盤（複雜）</td><td>較長</td><td>~100,000</td></tr><tr><td>簡明局面</td><td>較短</td><td>~5,000</td></tr><tr><td>讀秒</td><td>固定</td><td>~1,600</td></tr></tbody></table>
<p>更多的 MCTS 模擬通常意味著更好的落子品質。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="通訊與同步">通訊與同步<a href="#通訊與同步" class="hash-link" aria-label="通訊與同步的直接連結" title="通訊與同步的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="資料格式">資料格式<a href="#資料格式" class="hash-link" aria-label="資料格式的直接連結" title="資料格式的直接連結" translate="no">​</a></h3>
<p>訓練資料的傳輸格式：</p>
<div class="language-protobuf codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-protobuf codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">message TrainingExample {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    // 棋盤狀態（17 × 19 × 19）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    repeated float board_planes = 1;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    // MCTS 搜索結果（362）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    repeated float mcts_policy = 2;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    // 勝負結果（1 = 當前方勝，-1 = 當前方負）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    float game_result = 3;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="網路頻寬需求">網路頻寬需求<a href="#網路頻寬需求" class="hash-link" aria-label="網路頻寬需求的直接連結" title="網路頻寬需求的直接連結" translate="no">​</a></h3>
<table><thead><tr><th>資料流</th><th>大小</th><th>頻率</th></tr></thead><tbody><tr><td>訓練樣本</td><td>~10 KB/樣本</td><td>每秒數千樣本</td></tr><tr><td>網路權重</td><td>~200 MB</td><td>每小時數次</td></tr><tr><td>控制訊息</td><td>&lt; 1 KB</td><td>持續</td></tr></tbody></table>
<p>總頻寬需求：~100 Mbps（內部網路足夠）</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="故障處理">故障處理<a href="#故障處理" class="hash-link" aria-label="故障處理的直接連結" title="故障處理的直接連結" translate="no">​</a></h3>
<p>分散式系統的故障處理：</p>
<table><thead><tr><th>故障類型</th><th>處理方式</th></tr></thead><tbody><tr><td>Worker 掛掉</td><td>重啟，繼續使用最新 checkpoint</td></tr><tr><td>網路斷線</td><td>緩衝資料，重連後續傳</td></tr><tr><td>TPU 故障</td><td>自動切換到備用 TPU</td></tr><tr><td>資料損壞</td><td>校驗後丟棄，重新生成</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="成本分析">成本分析<a href="#成本分析" class="hash-link" aria-label="成本分析的直接連結" title="成本分析的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="硬體成本估算">硬體成本估算<a href="#硬體成本估算" class="hash-link" aria-label="硬體成本估算的直接連結" title="硬體成本估算的直接連結" translate="no">​</a></h3>
<p>以 Google Cloud 的 TPU 定價估算 AlphaGo Zero 的訓練成本：</p>
<table><thead><tr><th>資源</th><th>數量</th><th>單價/小時</th><th>總價/天</th></tr></thead><tbody><tr><td>TPU v2 Pod</td><td>4</td><td>~$32</td><td>~$3,000</td></tr><tr><td>高記憶體 VM</td><td>數台</td><td>~$5</td><td>~$500</td></tr><tr><td>儲存空間</td><td>10 TB</td><td>~$0.02/GB</td><td>~$200</td></tr><tr><td>網路</td><td>-</td><td>包含</td><td>-</td></tr></tbody></table>
<p><strong>每天約 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo separator="true">,</mo><mn>700</mn><mo>∗</mo><mo>∗</mo><mtext>，完整訓練（</mtext><mn>40</mn><mtext>天）約</mtext><mo>∗</mo><mo>∗</mo></mrow><annotation encoding="application/x-tex">3,700**，完整訓練（40 天）約 **</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">700</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">∗</span><span class="mord cjk_fallback">，完整訓練（</span><span class="mord">40</span><span class="mord cjk_fallback">天）約</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4653em"></span><span class="mord">∗</span></span></span></span>150,000</strong>。</p>
<p>注意：這是 2017 年的估算，DeepMind 作為 Google 子公司可能有內部折扣。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="與人類訓練的對比">與人類訓練的對比<a href="#與人類訓練的對比" class="hash-link" aria-label="與人類訓練的對比的直接連結" title="與人類訓練的對比的直接連結" translate="no">​</a></h3>
<table><thead><tr><th>方面</th><th>AlphaGo Zero</th><th>人類職業棋手</th></tr></thead><tbody><tr><td>達到職業水平</td><td>2 天</td><td>10-15 年</td></tr><tr><td>訓練成本</td><td>~$7,500</td><td>數百萬（學費、生活費、機會成本）</td></tr><tr><td>持續成本</td><td>電費</td><td>生活費</td></tr><tr><td>可複製性</td><td>完美複製</td><td>不可複製</td></tr></tbody></table>
<p>當然，這個對比不完全公平——人類在學棋過程中學到的不只是圍棋。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="推理成本">推理成本<a href="#推理成本" class="hash-link" aria-label="推理成本的直接連結" title="推理成本的直接連結" translate="no">​</a></h3>
<p>正式比賽的推理成本：</p>
<table><thead><tr><th>配置</th><th>每局成本</th></tr></thead><tbody><tr><td>48 TPU（AlphaGo Lee）</td><td>~$500</td></tr><tr><td>4 TPU（AlphaGo Zero）</td><td>~$50</td></tr><tr><td>單 GPU（KataGo）</td><td>~$1</td></tr></tbody></table>
<p>推理成本隨著技術進步大幅下降。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="技術演進">技術演進<a href="#技術演進" class="hash-link" aria-label="技術演進的直接連結" title="技術演進的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="從-alphago-到-alphazero">從 AlphaGo 到 AlphaZero<a href="#從-alphago-到-alphazero" class="hash-link" aria-label="從 AlphaGo 到 AlphaZero的直接連結" title="從 AlphaGo 到 AlphaZero的直接連結" translate="no">​</a></h3>
<table><thead><tr><th>方面</th><th>AlphaGo Lee</th><th>AlphaGo Zero</th><th>AlphaZero</th></tr></thead><tbody><tr><td>訓練 TPU</td><td>50+ GPU → TPU</td><td>4 TPU</td><td>4 TPU</td></tr><tr><td>推理 TPU</td><td>48 TPU</td><td>4 TPU</td><td>4 TPU</td></tr><tr><td>MCTS/步</td><td>~100,000</td><td>~1,600</td><td>~800</td></tr><tr><td>訓練時間</td><td>數月</td><td>40 天</td><td>數小時-數天</td></tr></tbody></table>
<p>效率提升約 100 倍。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="對開源社群的影響">對開源社群的影響<a href="#對開源社群的影響" class="hash-link" aria-label="對開源社群的影響的直接連結" title="對開源社群的影響的直接連結" translate="no">​</a></h3>
<p>AlphaGo 的架構啟發了多個開源項目：</p>
<table><thead><tr><th>項目</th><th>特點</th></tr></thead><tbody><tr><td>Leela Zero</td><td>社群分散式訓練，複現 AlphaGo Zero</td></tr><tr><td>KataGo</td><td>單 GPU 高效訓練，超越 AlphaGo Zero</td></tr><tr><td>ELF OpenGo</td><td>Facebook 開源，使用 PyTorch</td></tr><tr><td>Minigo</td><td>Google 開源，使用 TensorFlow</td></tr></tbody></table>
<p>這些項目讓普通研究者也能訓練強大的圍棋 AI。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="動畫對應">動畫對應<a href="#動畫對應" class="hash-link" aria-label="動畫對應的直接連結" title="動畫對應的直接連結" translate="no">​</a></h2>
<p>本文涉及的核心概念與動畫編號：</p>
<table><thead><tr><th>編號</th><th>概念</th><th>物理/數學對應</th></tr></thead><tbody><tr><td>🎬 C9</td><td>並行 MCTS</td><td>多體問題</td></tr><tr><td>🎬 E9</td><td>分散式訓練</td><td>分散式計算</td></tr><tr><td>🎬 C5</td><td>虛擬損失</td><td>排斥勢</td></tr><tr><td>🎬 D15</td><td>批次推理</td><td>向量化計算</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="延伸閱讀">延伸閱讀<a href="#延伸閱讀" class="hash-link" aria-label="延伸閱讀的直接連結" title="延伸閱讀的直接連結" translate="no">​</a></h2>
<ul>
<li class=""><strong>上一篇</strong>：<a class="" href="/docs/alphago/training-from-scratch/">從零訓練的過程</a> — 訓練曲線的詳細分析</li>
<li class=""><strong>下一篇</strong>：<a class="" href="/docs/alphago/legacy-and-impact/">AlphaGo 的遺產</a> — AlphaGo 對 AI 領域的深遠影響</li>
<li class=""><strong>相關文章</strong>：<a class="" href="/docs/alphago/mcts-neural-combo/">MCTS 與神經網路的結合</a> — MCTS 的基礎知識</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="參考資料">參考資料<a href="#參考資料" class="hash-link" aria-label="參考資料的直接連結" title="參考資料的直接連結" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">Jouppi, N., et al. (2017). &quot;In-Datacenter Performance Analysis of a Tensor Processing Unit.&quot; <em>ISCA 2017</em>.</li>
<li class="">Dean, J., et al. (2012). &quot;Large Scale Distributed Deep Networks.&quot; <em>NeurIPS 2012</em>.</li>
<li class="">Chaslot, G., et al. (2008). &quot;Parallel Monte-Carlo Tree Search.&quot; <em>CIG 2008</em>.</li>
<li class="">Segal, R. (2010). &quot;On the Scalability of Parallel UCT.&quot; <em>CIG 2010</em>.</li>
</ol>
<hr>
<div class="key-takeaway" style="background-color:var(--ifm-color-success-lightest);padding:1rem;border-radius:8px;border:1px solid var(--ifm-color-success);margin-bottom:1rem"><strong style="display:block;margin-bottom:0.5rem">📌 重點摘要</strong><p>本文重點：</p><ul>
<li class="">AlphaGo Zero 的訓練架構：Self-play Workers 持續產生資料、Training Workers 同步訓練、Replay Buffer 儲存最近 50 萬局</li>
<li class="">TPU 的關鍵作用：神經網路推理佔計算量 95%，TPU 的 MXU 單元每週期執行 16K 次乘加運算，是高效搜索的基礎</li>
<li class="">虛擬損失技術：讓多執行緒同時探索不同路徑，透過批次神經網路評估大幅提升並行 MCTS 效率</li>
</ul></div>
<div class="faq-section" style="margin-top:2rem"><h2>常見問題</h2><details style="margin-bottom:1rem;padding:1rem;background-color:var(--ifm-color-gray-100);border-radius:8px"><summary style="font-weight:bold;cursor:pointer;margin-bottom:0.5rem">什麼是虛擬損失（Virtual Loss），它如何讓 MCTS 並行化？</summary><p class="faq-answer-content" style="margin-top:0.5rem">虛擬損失是一種技術，當一個執行緒正在探索某個節點時，臨時降低該節點的價值，讓其他執行緒選擇不同路徑。這樣可以同時收集多個待評估的葉節點，進行批次神經網路推理，大幅提升效率。探索完成後虛擬損失會被真實值覆蓋，不影響最終結果。</p></details><details style="margin-bottom:1rem;padding:1rem;background-color:var(--ifm-color-gray-100);border-radius:8px"><summary style="font-weight:bold;cursor:pointer;margin-bottom:0.5rem">為什麼 AlphaGo 需要 TPU 而不是 GPU？</summary><p class="faq-answer-content" style="margin-top:0.5rem">圍棋 AI 的計算瓶頸是神經網路推理（佔 95%），每步 MCTS 需要執行 1600 次推理。TPU 是 Google 專為矩陣運算設計的加速器，其 MXU 單元特別擅長這類計算，功耗更低、延遲更少。從 AlphaGo Lee 的 48 TPU 到 AlphaGo Zero 的 4 TPU，效率提升超過 10 倍。</p></details><details style="margin-bottom:1rem;padding:1rem;background-color:var(--ifm-color-gray-100);border-radius:8px"><summary style="font-weight:bold;cursor:pointer;margin-bottom:0.5rem">AlphaGo Zero 的完整訓練成本大約是多少？</summary><p class="faq-answer-content" style="margin-top:0.5rem">以 Google Cloud 定價估算，每天約 $3,700（4 TPU v2 Pod + 高記憶體 VM + 儲存空間），完整 40 天訓練約 $150,000。但這只是雲端租用成本，DeepMind 作為 Google 子公司可能有內部折扣。對比之下，培養一位職業棋手需要 10-15 年和數百萬的總成本。</p></details></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/19-distributed-systems.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>編輯此頁</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/alphago/training-from-scratch/"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">從零訓練的過程</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/alphago/legacy-and-impact/"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">AlphaGo 的遺產</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#訓練架構總覽" class="table-of-contents__link toc-highlight">訓練架構總覽</a><ul><li><a href="#原版-alphago-的訓練架構" class="table-of-contents__link toc-highlight">原版 AlphaGo 的訓練架構</a></li><li><a href="#alphago-zero-的訓練架構" class="table-of-contents__link toc-highlight">AlphaGo Zero 的訓練架構</a></li></ul></li><li><a href="#自我對弈工作站self-play-workers" class="table-of-contents__link toc-highlight">自我對弈工作站（Self-play Workers）</a><ul><li><a href="#任務分配" class="table-of-contents__link toc-highlight">任務分配</a></li><li><a href="#工作流程" class="table-of-contents__link toc-highlight">工作流程</a></li><li><a href="#負載平衡" class="table-of-contents__link toc-highlight">負載平衡</a></li></ul></li><li><a href="#訓練工作站training-workers" class="table-of-contents__link toc-highlight">訓練工作站（Training Workers）</a><ul><li><a href="#任務分配-1" class="table-of-contents__link toc-highlight">任務分配</a></li><li><a href="#分散式訓練" class="table-of-contents__link toc-highlight">分散式訓練</a></li><li><a href="#同步-vs-非同步更新" class="table-of-contents__link toc-highlight">同步 vs. 非同步更新</a></li></ul></li><li><a href="#tpu-的角色" class="table-of-contents__link toc-highlight">TPU 的角色</a><ul><li><a href="#什麼是-tpu" class="table-of-contents__link toc-highlight">什麼是 TPU？</a></li><li><a href="#tpu-的架構" class="table-of-contents__link toc-highlight">TPU 的架構</a></li><li><a href="#為什麼-alphago-需要-tpu" class="table-of-contents__link toc-highlight">為什麼 AlphaGo 需要 TPU？</a></li><li><a href="#tpu-使用的演進" class="table-of-contents__link toc-highlight">TPU 使用的演進</a></li></ul></li><li><a href="#並行-mcts-與虛擬損失" class="table-of-contents__link toc-highlight">並行 MCTS 與虛擬損失</a><ul><li><a href="#並行化的挑戰" class="table-of-contents__link toc-highlight">並行化的挑戰</a></li><li><a href="#葉節點並行leaf-parallelization" class="table-of-contents__link toc-highlight">葉節點並行（Leaf Parallelization）</a></li><li><a href="#虛擬損失virtual-loss" class="table-of-contents__link toc-highlight">虛擬損失（Virtual Loss）</a></li><li><a href="#批次神經網路評估" class="table-of-contents__link toc-highlight">批次神經網路評估</a></li></ul></li><li><a href="#推理架構" class="table-of-contents__link toc-highlight">推理架構</a><ul><li><a href="#比賽時的配置" class="table-of-contents__link toc-highlight">比賽時的配置</a></li><li><a href="#分散式推理流程" class="table-of-contents__link toc-highlight">分散式推理流程</a></li><li><a href="#思考時間管理" class="table-of-contents__link toc-highlight">思考時間管理</a></li></ul></li><li><a href="#通訊與同步" class="table-of-contents__link toc-highlight">通訊與同步</a><ul><li><a href="#資料格式" class="table-of-contents__link toc-highlight">資料格式</a></li><li><a href="#網路頻寬需求" class="table-of-contents__link toc-highlight">網路頻寬需求</a></li><li><a href="#故障處理" class="table-of-contents__link toc-highlight">故障處理</a></li></ul></li><li><a href="#成本分析" class="table-of-contents__link toc-highlight">成本分析</a><ul><li><a href="#硬體成本估算" class="table-of-contents__link toc-highlight">硬體成本估算</a></li><li><a href="#與人類訓練的對比" class="table-of-contents__link toc-highlight">與人類訓練的對比</a></li><li><a href="#推理成本" class="table-of-contents__link toc-highlight">推理成本</a></li></ul></li><li><a href="#技術演進" class="table-of-contents__link toc-highlight">技術演進</a><ul><li><a href="#從-alphago-到-alphazero" class="table-of-contents__link toc-highlight">從 AlphaGo 到 AlphaZero</a></li><li><a href="#對開源社群的影響" class="table-of-contents__link toc-highlight">對開源社群的影響</a></li></ul></li><li><a href="#動畫對應" class="table-of-contents__link toc-highlight">動畫對應</a></li><li><a href="#延伸閱讀" class="table-of-contents__link toc-highlight">延伸閱讀</a></li><li><a href="#參考資料" class="table-of-contents__link toc-highlight">參考資料</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>