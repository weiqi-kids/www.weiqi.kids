<!doctype html>
<html lang="zh-tw" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-alphago/explained/dual-head-resnet" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">雙頭網路與殘差網路 | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/docs/alphago/explained/dual-head-resnet/"><meta data-rh="true" property="og:locale" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="zh-tw"><meta data-rh="true" name="docsearch:language" content="zh-tw"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="雙頭網路與殘差網路 | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="深入解析 AlphaGo Zero 的神經網路架構 - 共享主幹、Policy Head、Value Head 與 40 層 ResNet"><meta data-rh="true" property="og:description" content="深入解析 AlphaGo Zero 的神經網路架構 - 共享主幹、Policy Head、Value Head 與 40 層 ResNet"><meta data-rh="true" name="keywords" content="雙頭網路,殘差網路,ResNet,Policy Head,Value Head,深度學習,神經網路架構"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/docs/alphago/explained/dual-head-resnet/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/explained/dual-head-resnet/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/alphago/explained/dual-head-resnet/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/alphago/explained/dual-head-resnet/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/alphago/explained/dual-head-resnet/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/alphago/explained/dual-head-resnet/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/alphago/explained/dual-head-resnet/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/alphago/explained/dual-head-resnet/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/alphago/explained/dual-head-resnet/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/alphago/explained/dual-head-resnet/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/alphago/explained/dual-head-resnet/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/alphago/explained/dual-head-resnet/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/explained/dual-head-resnet/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AlphaGo","item":"https://www.weiqi.kids/docs/alphago/"},{"@type":"ListItem","position":2,"name":"完整解析","item":"https://www.weiqi.kids/docs/alphago/explained/"},{"@type":"ListItem","position":3,"name":"雙頭網路與殘差網路","item":"https://www.weiqi.kids/docs/alphago/explained/dual-head-resnet"}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"WebPage","@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/#webpage","url":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/","name":"雙頭網路與殘差網路","description":"深入解析 AlphaGo Zero 的神經網路架構 - 共享主幹、Policy Head、Value Head 與 40 層 ResNet","inLanguage":"zh-TW","isPartOf":{"@id":"https://www.weiqi.kids#website"},"primaryImageOfPage":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/social-card.png"},"datePublished":"2024-01-15","dateModified":"2024-02-22","speakable":{"@type":"SpeakableSpecification","cssSelector":[".article-summary",".speakable-content",".key-takeaway",".key-answer",".expert-quote",".actionable-steps li",".faq-answer-content"]}},{"@type":"Article","@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/#article","mainEntityOfPage":{"@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/#webpage","significantLink":["https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/"]},"headline":"雙頭網路與殘差網路","description":"深入解析 AlphaGo Zero 的神經網路架構 - 共享主幹、Policy Head、Value Head 與 40 層 ResNet","image":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/social-card.png","width":1200,"height":630},"author":{"@id":"https://www.weiqi.kids/docs/about/#person"},"publisher":{"@id":"https://www.weiqi.kids#organization"},"datePublished":"2024-01-15","dateModified":"2024-02-22","articleSection":"AlphaGo 完整解析","keywords":"雙頭網路, 殘差網路, ResNet, Policy Head, Value Head, 深度學習, 多任務學習, 卷積神經網路","wordCount":4500,"inLanguage":"zh-TW","isAccessibleForFree":true,"isPartOf":{"@type":"WebSite","@id":"https://www.weiqi.kids#website"}},{"@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首頁","item":"https://www.weiqi.kids"},{"@type":"ListItem","position":2,"name":"給工程師","item":"https://www.weiqi.kids/docs/for-engineers/"},{"@type":"ListItem","position":3,"name":"技術原理","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/"},{"@type":"ListItem","position":4,"name":"AlphaGo 完整解析","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/"}]},{"@type":"Person","@id":"https://www.weiqi.kids/docs/about/#person","name":"好棋寶寶協會編輯團隊","url":"https://www.weiqi.kids/docs/about/","worksFor":{"@id":"https://www.weiqi.kids#organization"},"description":"專注於圍棋 AI 研究與教育推廣的技術團隊","knowsAbout":["圍棋 AI","AlphaGo","KataGo","機器學習","深度學習"],"hasCredential":[{"@type":"EducationalOccupationalCredential","name":"AI 圍棋研究專家","credentialCategory":"技術認證"}]},{"@type":"ImageObject","@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/#primaryimage","url":"https://www.weiqi.kids/img/social-card.png","width":1200,"height":630,"caption":"雙頭網路與殘差網路 - 好棋寶寶協會","representativeOfPage":true,"license":"https://creativecommons.org/licenses/by-nc-sa/4.0/","creditText":"台灣好棋寶寶協會"}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"FAQPage","mainEntity":[{"@type":"Question","name":"為什麼雙頭網路比分離的兩個網路更好？","acceptedAnswer":{"@type":"Answer","text":"「下一步下哪裡」（Policy）和「誰會贏」（Value）需要理解相同的棋盤模式。共享主幹讓這些底層特徵只需學習一次，兩個任務都能使用。實驗顯示雙頭網路比分離網路提升約 300 ELO，相當於 65% 的勝率差距。"}},{"@type":"Question","name":"殘差連接（Skip Connection）如何解決深度網路訓練問題？","acceptedAnswer":{"@type":"Answer","text":"殘差連接讓輸出 = F(x) + x，反向傳播時梯度變成 ∂L/∂y × (1 + ∂F/∂x)。關鍵在於那個 +1，即使 ∂F/∂x 很小，梯度仍能直接傳回，形成「梯度高速公路」。這讓 152 層的 ResNet 都能有效訓練。"}},{"@type":"Question","name":"AlphaGo Zero 網路的訓練損失函數是什麼？","acceptedAnswer":{"@type":"Answer","text":"總損失 = Policy Loss + Value Loss + L2 正則化。Policy Loss 使用交叉熵，讓網路輸出逼近 MCTS 搜索機率；Value Loss 使用均方誤差，讓網路輸出逼近實際勝負結果（+1 或 -1）；L2 正則化防止過擬合。"}}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.f23bf74b.css">
<script src="/assets/js/runtime~main.81008aed.js" defer="defer"></script>
<script src="/assets/js/main.8936d553.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.svg"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="好棋寶寶協會標誌" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/img/logo.svg" alt="好棋寶寶協會標誌" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">好棋寶寶</b></a><a class="navbar__item navbar__link" href="/docs/learn/">學圍棋</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/alphago/">AlphaGo</a><a class="navbar__item navbar__link" href="/docs/animations/">動畫教室</a><a class="navbar__item navbar__link" href="/docs/tech/">技術文件</a><a class="navbar__item navbar__link" href="/docs/about/">關於我們</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>繁體中文</a><ul class="dropdown__menu"><li><a href="/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/alphago/explained/dual-head-resnet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro/"><span title="使用指南" class="linkLabel_REp1">使用指南</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/learn/"><span title="學圍棋" class="categoryLinkLabel_ezQx">學圍棋</span></a><button aria-label="展開側邊欄分類 &#x27;學圍棋&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/docs/alphago/"><span title="AlphaGo" class="categoryLinkLabel_ezQx">AlphaGo</span></a><button aria-label="收起側邊欄分類 &#x27;AlphaGo&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/alphago/explained/"><span title="完整解析" class="categoryLinkLabel_ezQx">完整解析</span></a><button aria-label="收起側邊欄分類 &#x27;完整解析&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/birth-of-alphago/"><span title="AlphaGo 的誕生" class="linkLabel_REp1">AlphaGo 的誕生</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/key-matches/"><span title="關鍵對局回顧" class="linkLabel_REp1">關鍵對局回顧</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/move-37/"><span title="「神之一手」深度分析" class="linkLabel_REp1">「神之一手」深度分析</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/why-go-is-hard/"><span title="圍棋為什麼難？" class="linkLabel_REp1">圍棋為什麼難？</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/traditional-limits/"><span title="傳統方法的極限" class="linkLabel_REp1">傳統方法的極限</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/board-representation/"><span title="棋盤狀態表示" class="linkLabel_REp1">棋盤狀態表示</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/policy-network/"><span title="Policy Network 詳解" class="linkLabel_REp1">Policy Network 詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/value-network/"><span title="Value Network 詳解" class="linkLabel_REp1">Value Network 詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/input-features/"><span title="輸入特徵設計" class="linkLabel_REp1">輸入特徵設計</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/cnn-and-go/"><span title="CNN 與圍棋的結合" class="linkLabel_REp1">CNN 與圍棋的結合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/supervised-learning/"><span title="監督學習階段" class="linkLabel_REp1">監督學習階段</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/reinforcement-intro/"><span title="強化學習入門" class="linkLabel_REp1">強化學習入門</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/self-play/"><span title="自我對弈" class="linkLabel_REp1">自我對弈</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/mcts-neural-combo/"><span title="MCTS 與神經網路的結合" class="linkLabel_REp1">MCTS 與神經網路的結合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/puct-formula/"><span title="PUCT 公式詳解" class="linkLabel_REp1">PUCT 公式詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/alphago-zero/"><span title="AlphaGo Zero 概述" class="linkLabel_REp1">AlphaGo Zero 概述</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/alphago/explained/dual-head-resnet/"><span title="雙頭網路與殘差網路" class="linkLabel_REp1">雙頭網路與殘差網路</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/training-from-scratch/"><span title="從零訓練的過程" class="linkLabel_REp1">從零訓練的過程</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/distributed-systems/"><span title="分散式系統與 TPU" class="linkLabel_REp1">分散式系統與 TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/alphago/explained/legacy-and-impact/"><span title="AlphaGo 的遺產" class="linkLabel_REp1">AlphaGo 的遺產</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/animations/"><span title="動畫教室" class="linkLabel_REp1">動畫教室</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/tech/"><span title="技術文件" class="categoryLinkLabel_ezQx">技術文件</span></a><button aria-label="展開側邊欄分類 &#x27;技術文件&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/about/"><span title="關於我們" class="categoryLinkLabel_ezQx">關於我們</span></a><button aria-label="展開側邊欄分類 &#x27;關於我們&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="頁面路徑"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/alphago/"><span>AlphaGo</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/alphago/explained/"><span>完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">雙頭網路與殘差網路</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">本頁導覽</button></div><div class="theme-doc-markdown markdown">
<header><h1>雙頭網路與殘差網路</h1></header>
<p>AlphaGo Zero 最重要的架構創新之一，是使用<strong>雙頭網路</strong>（Dual-Head Network）取代原版 AlphaGo 的雙網路設計。這個看似簡單的改變，卻帶來了顯著的效能提升和更優雅的學習過程。</p>
<p>本文將深入解析這個架構的設計原理、數學基礎，以及為什麼它如此有效。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="雙頭網路設計">雙頭網路設計<a href="#雙頭網路設計" class="hash-link" aria-label="雙頭網路設計的直接連結" title="雙頭網路設計的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="整體架構">整體架構<a href="#整體架構" class="hash-link" aria-label="整體架構的直接連結" title="整體架構的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 的神經網路可以分為三個部分：</p>
<!-- -->
<p>讓我們逐一解析每個部分。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="共享主幹shared-backbone">共享主幹（Shared Backbone）<a href="#共享主幹shared-backbone" class="hash-link" aria-label="共享主幹（Shared Backbone）的直接連結" title="共享主幹（Shared Backbone）的直接連結" translate="no">​</a></h3>
<p>共享主幹是一個深層的<strong>殘差網路（ResNet）</strong>，負責從棋盤狀態中提取特徵。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="架構細節">架構細節<a href="#架構細節" class="hash-link" aria-label="架構細節的直接連結" title="架構細節的直接連結" translate="no">​</a></h4>
<table><thead><tr><th>組件</th><th>規格</th></tr></thead><tbody><tr><td>輸入層</td><td>3×3 卷積，256 通道</td></tr><tr><td>殘差塊</td><td>40 個（或 20 個精簡版）</td></tr><tr><td>每個殘差塊</td><td>2 層 3×3 卷積，256 通道</td></tr><tr><td>激活函數</td><td>ReLU</td></tr><tr><td>正規化</td><td>Batch Normalization</td></tr></tbody></table>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="數學表示">數學表示<a href="#數學表示" class="hash-link" aria-label="數學表示的直接連結" title="數學表示的直接連結" translate="no">​</a></h4>
<p>設輸入為 x（維度 17 x 19 x 19），共享主幹的輸出為：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">f(x) = ResNet_40(Conv_3x3(x))</span><br></span></code></pre></div></div>
<p>其中 f(x)（維度 256 x 19 x 19）是高維特徵表示。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="policy-head策略頭">Policy Head（策略頭）<a href="#policy-head策略頭" class="hash-link" aria-label="Policy Head（策略頭）的直接連結" title="Policy Head（策略頭）的直接連結" translate="no">​</a></h3>
<p>Policy Head 負責預測每個位置的落子機率。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="架構細節-1">架構細節<a href="#架構細節-1" class="hash-link" aria-label="架構細節的直接連結" title="架構細節的直接連結" translate="no">​</a></h4>
<!-- -->
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="數學表示-1">數學表示<a href="#數學表示-1" class="hash-link" aria-label="數學表示的直接連結" title="數學表示的直接連結" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">π = Softmax(FC(Flatten(ReLU(BN(Conv_1x1(f(x)))))))</span><br></span></code></pre></div></div>
<p>輸出 π 是一個 362 維向量，滿足所有元素非負且和為 1。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="value-head價值頭">Value Head（價值頭）<a href="#value-head價值頭" class="hash-link" aria-label="Value Head（價值頭）的直接連結" title="Value Head（價值頭）的直接連結" translate="no">​</a></h3>
<p>Value Head 負責預測當前局面的勝率。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="架構細節-2">架構細節<a href="#架構細節-2" class="hash-link" aria-label="架構細節的直接連結" title="架構細節的直接連結" translate="no">​</a></h4>
<!-- -->
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="數學表示-2">數學表示<a href="#數學表示-2" class="hash-link" aria-label="數學表示的直接連結" title="數學表示的直接連結" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">v = Tanh(FC_1(ReLU(FC_2(Flatten(ReLU(BN(Conv_1x1(f(x)))))))))</span><br></span></code></pre></div></div>
<p>輸出 v 在 [-1, 1] 範圍內：</p>
<ul>
<li class="">v = 1：當前方必勝</li>
<li class="">v = -1：當前方必敗</li>
<li class="">v = 0：勢均力敵</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="為什麼要共享主幹">為什麼要共享主幹？<a href="#為什麼要共享主幹" class="hash-link" aria-label="為什麼要共享主幹？的直接連結" title="為什麼要共享主幹？的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="直覺理解">直覺理解<a href="#直覺理解" class="hash-link" aria-label="直覺理解的直接連結" title="直覺理解的直接連結" translate="no">​</a></h3>
<p>「下一步應該下哪裡」（Policy）和「誰會贏」（Value）這兩個問題，其實需要理解相同的棋盤模式：</p>
<ul>
<li class=""><strong>棋形</strong>：哪些形狀是好的，哪些是壞的</li>
<li class=""><strong>勢力</strong>：哪邊更大，哪些地方還有空間</li>
<li class=""><strong>死活</strong>：哪些棋已經活了，哪些還在打劫</li>
<li class=""><strong>戰鬥</strong>：哪裡有攻殺，局部勝負如何</li>
</ul>
<p>如果用兩個獨立的網路，這些特徵需要學習兩次。共享主幹讓這些底層特徵只需學習一次，兩個任務都能使用。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="多任務學習視角">多任務學習視角<a href="#多任務學習視角" class="hash-link" aria-label="多任務學習視角的直接連結" title="多任務學習視角的直接連結" translate="no">​</a></h3>
<p>從機器學習的角度，這是一種<strong>多任務學習（Multi-task Learning）</strong>：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L = L_policy + L_value</span><br></span></code></pre></div></div>
<p>兩個任務共享底層表示，這帶來幾個好處：</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-正則化效果">1. 正則化效果<a href="#1-正則化效果" class="hash-link" aria-label="1. 正則化效果的直接連結" title="1. 正則化效果的直接連結" translate="no">​</a></h4>
<p>共享參數相當於隱式的正則化。如果一個特徵只對 Policy 有用而對 Value 無用（或反之），它更難被過度放大。</p>
<p>有效參數量小於兩個獨立網路的參數量。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-資料效率">2. 資料效率<a href="#2-資料效率" class="hash-link" aria-label="2. 資料效率的直接連結" title="2. 資料效率的直接連結" translate="no">​</a></h4>
<p>每一局棋同時產生 Policy 標籤（MCTS 搜索機率）和 Value 標籤（最終勝負）。共享主幹讓兩個標籤都用於訓練共享特徵，提高了資料利用效率。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-梯度訊號豐富">3. 梯度訊號豐富<a href="#3-梯度訊號豐富" class="hash-link" aria-label="3. 梯度訊號豐富的直接連結" title="3. 梯度訊號豐富的直接連結" translate="no">​</a></h4>
<p>兩個任務的梯度都會流向共享主幹：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">∂L/∂θ_shared = ∂L_policy/∂θ_shared + ∂L_value/∂θ_shared</span><br></span></code></pre></div></div>
<p>這提供了更豐富的監督訊號，讓共享特徵更加穩健。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="實驗證據">實驗證據<a href="#實驗證據" class="hash-link" aria-label="實驗證據的直接連結" title="實驗證據的直接連結" translate="no">​</a></h3>
<p>DeepMind 的消融實驗顯示，雙頭網路的表現顯著優於分離的雙網路：</p>
<table><thead><tr><th>配置</th><th>ELO 評分</th><th>相對差距</th></tr></thead><tbody><tr><td>分離的 Policy + Value 網路</td><td>基準</td><td>-</td></tr><tr><td>雙頭網路（共享主幹）</td><td>+300 ELO</td><td>~65% 勝率差距</td></tr></tbody></table>
<p>300 ELO 的差距意味著雙頭網路對分離網路有約 65% 的勝率。這是一個顯著的提升。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="殘差網路原理">殘差網路原理<a href="#殘差網路原理" class="hash-link" aria-label="殘差網路原理的直接連結" title="殘差網路原理的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="深度網路的困境">深度網路的困境<a href="#深度網路的困境" class="hash-link" aria-label="深度網路的困境的直接連結" title="深度網路的困境的直接連結" translate="no">​</a></h3>
<p>在 ResNet 發明之前，深層神經網路面臨一個悖論：</p>
<blockquote>
<p>理論上，更深的網路應該至少和淺層網路一樣好（最差情況下，額外的層可以學習恆等映射）。但實際上，更深的網路往往表現更差。</p>
</blockquote>
<p>這就是<strong>退化問題（Degradation Problem）</strong>：</p>
<ul>
<li class="">訓練誤差隨深度增加而增加（不是過擬合，是優化困難）</li>
<li class="">梯度在反向傳播時逐漸消失（Vanishing Gradient）</li>
<li class="">深層的參數幾乎無法被有效更新</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="殘差塊的設計">殘差塊的設計<a href="#殘差塊的設計" class="hash-link" aria-label="殘差塊的設計的直接連結" title="殘差塊的設計的直接連結" translate="no">​</a></h3>
<p>何愷明等人在 2015 年提出了一個簡潔而優雅的解決方案：<strong>殘差連接（Skip Connection）</strong>。</p>
<!-- -->
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="數學表示-3">數學表示<a href="#數學表示-3" class="hash-link" aria-label="數學表示的直接連結" title="數學表示的直接連結" translate="no">​</a></h4>
<p>傳統網路：學習目標映射 H(x)</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">y = H(x)</span><br></span></code></pre></div></div>
<p>殘差網路：學習<strong>殘差映射</strong> F(x) = H(x) - x</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">y = F(x) + x</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="為什麼殘差連接有效">為什麼殘差連接有效？<a href="#為什麼殘差連接有效" class="hash-link" aria-label="為什麼殘差連接有效？的直接連結" title="為什麼殘差連接有效？的直接連結" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-梯度高速公路">1. 梯度高速公路<a href="#1-梯度高速公路" class="hash-link" aria-label="1. 梯度高速公路的直接連結" title="1. 梯度高速公路的直接連結" translate="no">​</a></h4>
<p>考慮反向傳播的梯度：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">∂L/∂x = ∂L/∂y × ∂y/∂x = ∂L/∂y × (1 + ∂F(x)/∂x)</span><br></span></code></pre></div></div>
<p>關鍵在於那個 <strong>+1</strong>。即使 ∂F(x)/∂x 很小或為零，梯度仍然可以透過 +1 直接傳回去。</p>
<p>這就像修了一條「梯度高速公路」，讓梯度可以暢通無阻地從輸出層傳回輸入層。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-恆等映射更容易學習">2. 恆等映射更容易學習<a href="#2-恆等映射更容易學習" class="hash-link" aria-label="2. 恆等映射更容易學習的直接連結" title="2. 恆等映射更容易學習的直接連結" translate="no">​</a></h4>
<p>如果最優解接近恆等映射（H(x) 約等於 x），那麼：</p>
<ul>
<li class="">傳統網路：需要學習 H(x) = x，可能很難</li>
<li class="">殘差網路：只需學習 F(x) 約等於 0，相對容易</li>
</ul>
<p>將權重初始化為零或接近零，殘差塊就自然趨向恆等映射。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-集成效應">3. 集成效應<a href="#3-集成效應" class="hash-link" aria-label="3. 集成效應的直接連結" title="3. 集成效應的直接連結" translate="no">​</a></h4>
<p>深層 ResNet 可以視為許多淺層網路的<strong>隱式集成</strong>。如果有 n 個殘差塊，資訊可以透過 2^n 種不同的路徑流動。</p>
<p>這種集成效應增加了模型的穩健性。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="resnet-在-imagenet-上的突破">ResNet 在 ImageNet 上的突破<a href="#resnet-在-imagenet-上的突破" class="hash-link" aria-label="ResNet 在 ImageNet 上的突破的直接連結" title="ResNet 在 ImageNet 上的突破的直接連結" translate="no">​</a></h3>
<p>ResNet 在 2015 年 ImageNet 競賽中取得了驚人的成績：</p>
<table><thead><tr><th>深度</th><th>Top-5 錯誤率</th></tr></thead><tbody><tr><td>VGG-19（無殘差）</td><td>7.3%</td></tr><tr><td>ResNet-34</td><td>5.7%</td></tr><tr><td>ResNet-152</td><td>4.5%</td></tr><tr><td>人類水平</td><td>~5.1%</td></tr></tbody></table>
<p><strong>152 層</strong>的 ResNet 不僅可以訓練，還比 19 層的 VGG 好得多。這證明了殘差連接確實解決了深度網路的訓練問題。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero-的-40-層-resnet">AlphaGo Zero 的 40 層 ResNet<a href="#alphago-zero-的-40-層-resnet" class="hash-link" aria-label="AlphaGo Zero 的 40 層 ResNet的直接連結" title="AlphaGo Zero 的 40 層 ResNet的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="為什麼選擇-40-層">為什麼選擇 40 層？<a href="#為什麼選擇-40-層" class="hash-link" aria-label="為什麼選擇 40 層？的直接連結" title="為什麼選擇 40 層？的直接連結" translate="no">​</a></h3>
<p>DeepMind 測試了不同深度的 ResNet：</p>
<table><thead><tr><th>殘差塊數量</th><th>總層數</th><th>ELO 評分</th></tr></thead><tbody><tr><td>5</td><td>11</td><td>基準</td></tr><tr><td>10</td><td>21</td><td>+200</td></tr><tr><td>20</td><td>41</td><td>+400</td></tr><tr><td>40</td><td>81</td><td>+500</td></tr></tbody></table>
<p>更深的網路確實更強，但邊際效益遞減。AlphaGo Zero 使用 20 或 40 個殘差塊：</p>
<ul>
<li class=""><strong>AlphaGo Zero（論文版）</strong>：40 個殘差塊，256 通道</li>
<li class=""><strong>精簡版</strong>：20 個殘差塊，256 通道</li>
</ul>
<p>40 層的配置在棋力和訓練成本之間取得了良好的平衡。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="具體配置">具體配置<a href="#具體配置" class="hash-link" aria-label="具體配置的直接連結" title="具體配置的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 的 ResNet 配置如下：</p>
<!-- -->
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="參數量估計">參數量估計<a href="#參數量估計" class="hash-link" aria-label="參數量估計的直接連結" title="參數量估計的直接連結" translate="no">​</a></h4>
<table><thead><tr><th>組件</th><th>參數量（約）</th></tr></thead><tbody><tr><td>輸入卷積</td><td>17 × 3 × 3 × 256 ≈ 39K</td></tr><tr><td>每個殘差塊</td><td>2 × 256 × 3 × 3 × 256 ≈ 1.2M</td></tr><tr><td>40 個殘差塊</td><td>40 × 1.2M ≈ 47M</td></tr><tr><td>Policy Head</td><td>~1M</td></tr><tr><td>Value Head</td><td>~0.2M</td></tr><tr><td><strong>總計</strong></td><td><strong>~48M</strong></td></tr></tbody></table>
<p>約 4800 萬參數，以現代標準來看是中等規模的神經網路。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="batch-normalization-的作用">Batch Normalization 的作用<a href="#batch-normalization-的作用" class="hash-link" aria-label="Batch Normalization 的作用的直接連結" title="Batch Normalization 的作用的直接連結" translate="no">​</a></h3>
<p>每個卷積層之後都有 <strong>Batch Normalization（BN）</strong>，這對訓練穩定性至關重要：</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-正規化啟動值">1. 正規化啟動值<a href="#1-正規化啟動值" class="hash-link" aria-label="1. 正規化啟動值的直接連結" title="1. 正規化啟動值的直接連結" translate="no">​</a></h4>
<p>BN 將每一層的啟動值正規化到均值為 0、方差為 1：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">x_hat = (x - μ_B) / sqrt(σ_B² + ε)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">y = γ × x_hat + β</span><br></span></code></pre></div></div>
<p>其中 γ 和 β 是可學習的參數。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-緩解內部協變量偏移">2. 緩解內部協變量偏移<a href="#2-緩解內部協變量偏移" class="hash-link" aria-label="2. 緩解內部協變量偏移的直接連結" title="2. 緩解內部協變量偏移的直接連結" translate="no">​</a></h4>
<p>深層網路中，每一層的輸入分布會隨著前面層的參數更新而改變。BN 讓每一層的輸入分布保持穩定，加速訓練收斂。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-正則化效果">3. 正則化效果<a href="#3-正則化效果" class="hash-link" aria-label="3. 正則化效果的直接連結" title="3. 正則化效果的直接連結" translate="no">​</a></h4>
<p>BN 在訓練時使用 mini-batch 的統計量，引入了隨機性，有輕微的正則化效果。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="與其他架構的比較">與其他架構的比較<a href="#與其他架構的比較" class="hash-link" aria-label="與其他架構的比較的直接連結" title="與其他架構的比較的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="vs-原版-alphago-的-cnn">vs. 原版 AlphaGo 的 CNN<a href="#vs-原版-alphago-的-cnn" class="hash-link" aria-label="vs. 原版 AlphaGo 的 CNN的直接連結" title="vs. 原版 AlphaGo 的 CNN的直接連結" translate="no">​</a></h3>
<table><thead><tr><th>特性</th><th>AlphaGo 原版</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>架構類型</td><td>標準 CNN</td><td>ResNet</td></tr><tr><td>深度</td><td>13 層</td><td>41-81 層</td></tr><tr><td>殘差連接</td><td>無</td><td>有</td></tr><tr><td>網路數量</td><td>2（分離）</td><td>1（共享）</td></tr><tr><td>BN</td><td>無</td><td>有</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="vs-vgg-風格網路">vs. VGG 風格網路<a href="#vs-vgg-風格網路" class="hash-link" aria-label="vs. VGG 風格網路的直接連結" title="vs. VGG 風格網路的直接連結" translate="no">​</a></h3>
<p>VGG 是 2014 年 ImageNet 亞軍的架構，使用堆疊的 3×3 卷積：</p>
<table><thead><tr><th>特性</th><th>VGG</th><th>ResNet</th></tr></thead><tbody><tr><td>最大可訓練深度</td><td>~19 層</td><td>152+ 層</td></tr><tr><td>梯度流動</td><td>逐層遞減</td><td>有高速公路</td></tr><tr><td>訓練難度</td><td>深層困難</td><td>深層可訓練</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="vs-inception--googlenet">vs. Inception / GoogLeNet<a href="#vs-inception--googlenet" class="hash-link" aria-label="vs. Inception / GoogLeNet的直接連結" title="vs. Inception / GoogLeNet的直接連結" translate="no">​</a></h3>
<p>Inception 使用多尺度卷積並行：</p>
<table><thead><tr><th>特性</th><th>Inception</th><th>ResNet</th></tr></thead><tbody><tr><td>特點</td><td>多尺度特徵</td><td>深度堆疊</td></tr><tr><td>複雜度</td><td>較高</td><td>簡潔</td></tr><tr><td>圍棋適用性</td><td>一般</td><td>優秀</td></tr></tbody></table>
<p>ResNet 的簡潔設計更適合圍棋這種需要深層推理的任務。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="vs-transformer">vs. Transformer<a href="#vs-transformer" class="hash-link" aria-label="vs. Transformer的直接連結" title="vs. Transformer的直接連結" translate="no">​</a></h3>
<p>2017 年提出的 Transformer 架構在 NLP 領域取得了巨大成功。有人嘗試將 Transformer 應用於圍棋：</p>
<table><thead><tr><th>特性</th><th>ResNet</th><th>Transformer</th></tr></thead><tbody><tr><td>歸納偏置</td><td>局部性（卷積）</td><td>全局注意力</td></tr><tr><td>位置編碼</td><td>隱式（卷積）</td><td>顯式</td></tr><tr><td>圍棋表現</td><td>優秀</td><td>可行但不優於 ResNet</td></tr><tr><td>計算效率</td><td>較高</td><td>較低（O(n²)）</td></tr></tbody></table>
<p>對於圍棋這種有明顯空間結構的問題，CNN/ResNet 的歸納偏置更加合適。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="設計選擇的深入分析">設計選擇的深入分析<a href="#設計選擇的深入分析" class="hash-link" aria-label="設計選擇的深入分析的直接連結" title="設計選擇的深入分析的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="為什麼用-33-卷積">為什麼用 3×3 卷積？<a href="#為什麼用-33-卷積" class="hash-link" aria-label="為什麼用 3×3 卷積？的直接連結" title="為什麼用 3×3 卷積？的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 全程使用 3×3 卷積，而非更大的卷積核：</p>
<ol>
<li class=""><strong>參數效率</strong>：兩個 3×3 卷積的感受野等於一個 5×5，但參數量更少（18 vs 25）</li>
<li class=""><strong>更深的網路</strong>：相同參數量下，可以堆疊更多層</li>
<li class=""><strong>更多非線性</strong>：每層之間有 ReLU，增加表達能力</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="為什麼用-256-通道">為什麼用 256 通道？<a href="#為什麼用-256-通道" class="hash-link" aria-label="為什麼用 256 通道？的直接連結" title="為什麼用 256 通道？的直接連結" translate="no">​</a></h3>
<p>256 通道是一個經驗性的選擇：</p>
<ul>
<li class=""><strong>太少</strong>（如 64）：表達能力不足，無法捕捉複雜模式</li>
<li class=""><strong>太多</strong>（如 512）：參數量翻倍，訓練成本大增，但棋力提升有限</li>
</ul>
<p>後來的 KataGo 實驗顯示，通道數可以根據訓練資源調整：</p>
<ul>
<li class="">低資源：128 通道，20 塊</li>
<li class="">高資源：256 通道，40 塊</li>
<li class="">更高資源：384 通道，60 塊</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="為什麼-policy-head-用-softmaxvalue-head-用-tanh">為什麼 Policy Head 用 Softmax、Value Head 用 Tanh？<a href="#為什麼-policy-head-用-softmaxvalue-head-用-tanh" class="hash-link" aria-label="為什麼 Policy Head 用 Softmax、Value Head 用 Tanh？的直接連結" title="為什麼 Policy Head 用 Softmax、Value Head 用 Tanh？的直接連結" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="policy-headsoftmax">Policy Head：Softmax<a href="#policy-headsoftmax" class="hash-link" aria-label="Policy Head：Softmax的直接連結" title="Policy Head：Softmax的直接連結" translate="no">​</a></h4>
<p>落子是一個<strong>分類問題</strong>——361 個位置（加 Pass）中選擇一個。Softmax 輸出滿足：</p>
<ul>
<li class="">所有機率非負：π_i &gt;= 0</li>
<li class="">機率和為 1：Σπ_i = 1</li>
</ul>
<p>這與機率分布的定義一致。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="value-headtanh">Value Head：Tanh<a href="#value-headtanh" class="hash-link" aria-label="Value Head：Tanh的直接連結" title="Value Head：Tanh的直接連結" translate="no">​</a></h4>
<p>勝率是一個<strong>回歸問題</strong>——預測一個連續值。Tanh 輸出範圍是 [-1, 1]：</p>
<ul>
<li class="">有界：不會產生極端值</li>
<li class="">對稱：勝和負對稱處理</li>
<li class="">可微：方便梯度計算</li>
</ul>
<p>使用 Tanh 而非無界輸出（如線性層）可以防止訓練不穩定。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="訓練細節">訓練細節<a href="#訓練細節" class="hash-link" aria-label="訓練細節的直接連結" title="訓練細節的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="損失函數">損失函數<a href="#損失函數" class="hash-link" aria-label="損失函數的直接連結" title="損失函數的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 的總損失是三項之和：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L = L_policy + L_value + L_reg</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="policy-loss">Policy Loss<a href="#policy-loss" class="hash-link" aria-label="Policy Loss的直接連結" title="Policy Loss的直接連結" translate="no">​</a></h4>
<p>使用<strong>交叉熵損失</strong>，讓網路輸出逼近 MCTS 搜索機率：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L_policy = -Σ π_MCTS(a) × log(π_net(a))</span><br></span></code></pre></div></div>
<p>其中：</p>
<ul>
<li class="">π_MCTS(a) 是 MCTS 對動作 a 的搜索機率</li>
<li class="">π_net(a) 是網路輸出的機率</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="value-loss">Value Loss<a href="#value-loss" class="hash-link" aria-label="Value Loss的直接連結" title="Value Loss的直接連結" translate="no">​</a></h4>
<p>使用<strong>均方誤差（MSE）</strong>，讓網路輸出逼近實際勝負：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L_value = (v_net - z)²</span><br></span></code></pre></div></div>
<p>其中：</p>
<ul>
<li class="">v_net 是網路預測的勝率</li>
<li class="">z 是實際比賽結果（+1 或 -1）</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="regularization-loss">Regularization Loss<a href="#regularization-loss" class="hash-link" aria-label="Regularization Loss的直接連結" title="Regularization Loss的直接連結" translate="no">​</a></h4>
<p>使用 <strong>L2 正則化</strong>防止過擬合：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">L_reg = c × ||θ||²</span><br></span></code></pre></div></div>
<p>其中 c 是正則化係數，θ 是網路參數。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="優化器配置">優化器配置<a href="#優化器配置" class="hash-link" aria-label="優化器配置的直接連結" title="優化器配置的直接連結" translate="no">​</a></h3>
<table><thead><tr><th>參數</th><th>值</th></tr></thead><tbody><tr><td>優化器</td><td>SGD + Momentum</td></tr><tr><td>動量</td><td>0.9</td></tr><tr><td>初始學習率</td><td>0.01</td></tr><tr><td>學習率衰減</td><td>每 X 步減半</td></tr><tr><td>Batch Size</td><td>32 × 2048 = 64K（分散式）</td></tr><tr><td>L2 正則化係數</td><td>1e-4</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="資料增強">資料增強<a href="#資料增強" class="hash-link" aria-label="資料增強的直接連結" title="資料增強的直接連結" translate="no">​</a></h3>
<p>圍棋棋盤有 8 重對稱性（4 次旋轉 × 2 次翻轉）。訓練時，每個局面可以產生 8 個等價的訓練樣本。</p>
<p>這讓有效訓練資料增加 8 倍，且不需要額外的自我對弈。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="實作考量">實作考量<a href="#實作考量" class="hash-link" aria-label="實作考量的直接連結" title="實作考量的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="記憶體優化">記憶體優化<a href="#記憶體優化" class="hash-link" aria-label="記憶體優化的直接連結" title="記憶體優化的直接連結" translate="no">​</a></h3>
<p>40 層 ResNet 的訓練需要大量記憶體：</p>
<ul>
<li class=""><strong>前向傳播</strong>：需要儲存每層的啟動值（用於反向傳播）</li>
<li class=""><strong>反向傳播</strong>：需要儲存梯度</li>
</ul>
<p>優化策略：</p>
<ol>
<li class=""><strong>梯度檢查點（Gradient Checkpointing）</strong>：只儲存部分啟動值，需要時重新計算</li>
<li class=""><strong>混合精度訓練</strong>：使用 FP16 減少記憶體佔用</li>
<li class=""><strong>分散式訓練</strong>：將 batch 分散到多個 GPU/TPU</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="推理優化">推理優化<a href="#推理優化" class="hash-link" aria-label="推理優化的直接連結" title="推理優化的直接連結" translate="no">​</a></h3>
<p>推理時不需要 BN 的 mini-batch 統計量，可以使用訓練時累積的移動平均：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">x_hat = (x - μ_moving) / sqrt(σ_moving² + ε)</span><br></span></code></pre></div></div>
<p>這讓推理速度更快且結果確定性。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="量化與壓縮">量化與壓縮<a href="#量化與壓縮" class="hash-link" aria-label="量化與壓縮的直接連結" title="量化與壓縮的直接連結" translate="no">​</a></h3>
<p>部署時可以進一步壓縮網路：</p>
<ul>
<li class=""><strong>權重量化</strong>：FP32 → INT8，記憶體減少 4 倍</li>
<li class=""><strong>剪枝</strong>：移除小權重連接</li>
<li class=""><strong>知識蒸餾</strong>：用大網路訓練小網路</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="動畫對應">動畫對應<a href="#動畫對應" class="hash-link" aria-label="動畫對應的直接連結" title="動畫對應的直接連結" translate="no">​</a></h2>
<p>本文涉及的核心概念與動畫編號：</p>
<table><thead><tr><th>編號</th><th>概念</th><th>物理/數學對應</th></tr></thead><tbody><tr><td>🎬 E3</td><td>雙頭網路</td><td>多任務學習</td></tr><tr><td>🎬 D12</td><td>殘差連接</td><td>梯度高速公路</td></tr><tr><td>🎬 D8</td><td>卷積神經網路</td><td>局部感受野</td></tr><tr><td>🎬 D10</td><td>Batch Normalization</td><td>分布正規化</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="延伸閱讀">延伸閱讀<a href="#延伸閱讀" class="hash-link" aria-label="延伸閱讀的直接連結" title="延伸閱讀的直接連結" translate="no">​</a></h2>
<ul>
<li class=""><strong>上一篇</strong>：<a class="" href="/docs/alphago/explained/alphago-zero/">AlphaGo Zero 概述</a> — 為什麼不需要人類棋譜</li>
<li class=""><strong>下一篇</strong>：<a class="" href="/docs/alphago/explained/training-from-scratch/">從零訓練的過程</a> — Day 0-3 的詳細演進</li>
<li class=""><strong>技術深入</strong>：<a class="" href="/docs/alphago/explained/cnn-and-go/">CNN 與圍棋的結合</a> — 為什麼 CNN 適合棋盤</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="參考資料">參考資料<a href="#參考資料" class="hash-link" aria-label="參考資料的直接連結" title="參考資料的直接連結" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">He, K., et al. (2016). &quot;Deep Residual Learning for Image Recognition.&quot; <em>CVPR 2016</em>.</li>
<li class="">Ioffe, S., &amp; Szegedy, C. (2015). &quot;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.&quot; <em>ICML 2015</em>.</li>
<li class="">Caruana, R. (1997). &quot;Multitask Learning.&quot; <em>Machine Learning</em>, 28(1), 41-75.</li>
<li class="">Veit, A., et al. (2016). &quot;Residual Networks Behave Like Ensembles of Relatively Shallow Networks.&quot; <em>NeurIPS 2016</em>.</li>
</ol>
<hr>
<div class="key-takeaway" style="background-color:var(--ifm-color-success-lightest);padding:1rem;border-radius:8px;border:1px solid var(--ifm-color-success);margin-bottom:1rem"><strong style="display:block;margin-bottom:0.5rem">📌 重點摘要</strong><p>本文重點：</p><ul>
<li class="">雙頭網路：共享 40 層 ResNet 主幹，分出 Policy Head（預測落子機率）和 Value Head（預測勝率），比分離網路提升約 300 ELO</li>
<li class="">殘差連接的關鍵：透過 skip connection 形成「梯度高速公路」，讓 40+ 層深度網路得以有效訓練</li>
<li class="">設計選擇：全程使用 3x3 卷積、256 通道、Batch Normalization，總參數約 4800 萬</li>
</ul></div>
<div class="faq-section" style="margin-top:2rem"><h2>常見問題</h2><details style="margin-bottom:1rem;padding:1rem;background-color:var(--ifm-color-gray-100);border-radius:8px"><summary style="font-weight:bold;cursor:pointer;margin-bottom:0.5rem">為什麼雙頭網路比分離的兩個網路更好？</summary><p class="faq-answer-content" style="margin-top:0.5rem">「下一步下哪裡」（Policy）和「誰會贏」（Value）需要理解相同的棋盤模式。共享主幹讓這些底層特徵只需學習一次，兩個任務都能使用。實驗顯示雙頭網路比分離網路提升約 300 ELO，相當於 65% 的勝率差距。</p></details><details style="margin-bottom:1rem;padding:1rem;background-color:var(--ifm-color-gray-100);border-radius:8px"><summary style="font-weight:bold;cursor:pointer;margin-bottom:0.5rem">殘差連接（Skip Connection）如何解決深度網路訓練問題？</summary><p class="faq-answer-content" style="margin-top:0.5rem">殘差連接讓輸出 = F(x) + x，反向傳播時梯度變成 ∂L/∂y × (1 + ∂F/∂x)。關鍵在於那個 +1，即使 ∂F/∂x 很小，梯度仍能直接傳回，形成「梯度高速公路」。這讓 152 層的 ResNet 都能有效訓練。</p></details><details style="margin-bottom:1rem;padding:1rem;background-color:var(--ifm-color-gray-100);border-radius:8px"><summary style="font-weight:bold;cursor:pointer;margin-bottom:0.5rem">AlphaGo Zero 網路的訓練損失函數是什麼？</summary><p class="faq-answer-content" style="margin-top:0.5rem">總損失 = Policy Loss + Value Loss + L2 正則化。Policy Loss 使用交叉熵，讓網路輸出逼近 MCTS 搜索機率；Value Loss 使用均方誤差，讓網路輸出逼近實際勝負結果（+1 或 -1）；L2 正則化防止過擬合。</p></details></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/explained/17-dual-head-resnet.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>編輯此頁</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/alphago/explained/alphago-zero/"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">AlphaGo Zero 概述</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/alphago/explained/training-from-scratch/"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">從零訓練的過程</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#雙頭網路設計" class="table-of-contents__link toc-highlight">雙頭網路設計</a><ul><li><a href="#整體架構" class="table-of-contents__link toc-highlight">整體架構</a></li><li><a href="#共享主幹shared-backbone" class="table-of-contents__link toc-highlight">共享主幹（Shared Backbone）</a></li><li><a href="#policy-head策略頭" class="table-of-contents__link toc-highlight">Policy Head（策略頭）</a></li><li><a href="#value-head價值頭" class="table-of-contents__link toc-highlight">Value Head（價值頭）</a></li></ul></li><li><a href="#為什麼要共享主幹" class="table-of-contents__link toc-highlight">為什麼要共享主幹？</a><ul><li><a href="#直覺理解" class="table-of-contents__link toc-highlight">直覺理解</a></li><li><a href="#多任務學習視角" class="table-of-contents__link toc-highlight">多任務學習視角</a></li><li><a href="#實驗證據" class="table-of-contents__link toc-highlight">實驗證據</a></li></ul></li><li><a href="#殘差網路原理" class="table-of-contents__link toc-highlight">殘差網路原理</a><ul><li><a href="#深度網路的困境" class="table-of-contents__link toc-highlight">深度網路的困境</a></li><li><a href="#殘差塊的設計" class="table-of-contents__link toc-highlight">殘差塊的設計</a></li><li><a href="#為什麼殘差連接有效" class="table-of-contents__link toc-highlight">為什麼殘差連接有效？</a></li><li><a href="#resnet-在-imagenet-上的突破" class="table-of-contents__link toc-highlight">ResNet 在 ImageNet 上的突破</a></li></ul></li><li><a href="#alphago-zero-的-40-層-resnet" class="table-of-contents__link toc-highlight">AlphaGo Zero 的 40 層 ResNet</a><ul><li><a href="#為什麼選擇-40-層" class="table-of-contents__link toc-highlight">為什麼選擇 40 層？</a></li><li><a href="#具體配置" class="table-of-contents__link toc-highlight">具體配置</a></li><li><a href="#batch-normalization-的作用" class="table-of-contents__link toc-highlight">Batch Normalization 的作用</a></li></ul></li><li><a href="#與其他架構的比較" class="table-of-contents__link toc-highlight">與其他架構的比較</a><ul><li><a href="#vs-原版-alphago-的-cnn" class="table-of-contents__link toc-highlight">vs. 原版 AlphaGo 的 CNN</a></li><li><a href="#vs-vgg-風格網路" class="table-of-contents__link toc-highlight">vs. VGG 風格網路</a></li><li><a href="#vs-inception--googlenet" class="table-of-contents__link toc-highlight">vs. Inception / GoogLeNet</a></li><li><a href="#vs-transformer" class="table-of-contents__link toc-highlight">vs. Transformer</a></li></ul></li><li><a href="#設計選擇的深入分析" class="table-of-contents__link toc-highlight">設計選擇的深入分析</a><ul><li><a href="#為什麼用-33-卷積" class="table-of-contents__link toc-highlight">為什麼用 3×3 卷積？</a></li><li><a href="#為什麼用-256-通道" class="table-of-contents__link toc-highlight">為什麼用 256 通道？</a></li><li><a href="#為什麼-policy-head-用-softmaxvalue-head-用-tanh" class="table-of-contents__link toc-highlight">為什麼 Policy Head 用 Softmax、Value Head 用 Tanh？</a></li></ul></li><li><a href="#訓練細節" class="table-of-contents__link toc-highlight">訓練細節</a><ul><li><a href="#損失函數" class="table-of-contents__link toc-highlight">損失函數</a></li><li><a href="#優化器配置" class="table-of-contents__link toc-highlight">優化器配置</a></li><li><a href="#資料增強" class="table-of-contents__link toc-highlight">資料增強</a></li></ul></li><li><a href="#實作考量" class="table-of-contents__link toc-highlight">實作考量</a><ul><li><a href="#記憶體優化" class="table-of-contents__link toc-highlight">記憶體優化</a></li><li><a href="#推理優化" class="table-of-contents__link toc-highlight">推理優化</a></li><li><a href="#量化與壓縮" class="table-of-contents__link toc-highlight">量化與壓縮</a></li></ul></li><li><a href="#動畫對應" class="table-of-contents__link toc-highlight">動畫對應</a></li><li><a href="#延伸閱讀" class="table-of-contents__link toc-highlight">延伸閱讀</a></li><li><a href="#參考資料" class="table-of-contents__link toc-highlight">參考資料</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>