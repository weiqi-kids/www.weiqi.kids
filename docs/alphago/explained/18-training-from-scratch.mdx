---
sidebar_position: 19
title: 從零訓練的過程
description: 見證 AlphaGo Zero 如何在三天內從隨機亂下到超越人類，重新發現並超越千年棋理
keywords: [AlphaGo Zero, 訓練過程, 自我對弈, 棋力成長, 圍棋 AI, 深度學習]
---

import { EloChart } from '@site/src/components/D3Charts';
import { ArticleSchema, KeyTakeaway, FAQ } from '@site/src/components/SEO';

<ArticleSchema
  title="從零訓練的過程"
  description="見證 AlphaGo Zero 如何在三天內從隨機亂下到超越人類，重新發現並超越千年棋理"
  slug="for-engineers/how-it-works/alphago-explained/training-from-scratch"
  datePublished="2024-01-15"
  dateModified="2024-02-22"
  section="AlphaGo 完整解析"
  keywords={["AlphaGo Zero", "訓練過程", "自我對弈", "棋力成長", "圍棋 AI", "從零開始", "棋理發現", "強化學習"]}
  wordCount={4200}
/>

# 從零訓練的過程

AlphaGo Zero 最令人驚嘆的不只是最終的棋力，而是它的**成長過程**——從完全隨機的狀態開始，在短短三天內經歷了人類數千年才完成的圍棋知識積累，然後超越人類所有的理解。

本文將帶你一步步見證這個驚人的蛻變過程。

---

## 訓練曲線

首先，讓我們看看 AlphaGo Zero 的棋力成長曲線：

<EloChart mode="zero" width={700} height={400} />

這條曲線展示了 AlphaGo Zero 在 72 小時內的棋力變化。注意幾個關鍵里程碑：

| 時間 | ELO 評分 | 相當於 |
|------|----------|--------|
| 0 小時 | 0 | 隨機亂下 |
| 3 小時 | ~1000 | 發現基本規則 |
| 12 小時 | ~3000 | 發現定式和棋形 |
| 36 小時 | ~4500 | 超越樊麾版 AlphaGo |
| 60 小時 | ~5200 | 超越李世乭版 AlphaGo |
| 72 小時 | ~5400 | 超越所有先前版本 |

**三天，從零到超越人類頂峰。**

---

## Day 0：混沌的開始

### 完全隨機的初始狀態

訓練開始時，神經網路的權重是隨機初始化的。這意味著：

- **Policy Head**：輸出接近均勻分布，每個位置的落子機率約為 1/361
- **Value Head**：輸出接近 0，無法區分好局面和壞局面

此時的 AlphaGo Zero 下棋完全是亂下——比一個從未見過棋盤的人還要差。

### 第一局自我對弈

想像一下第一局自我對弈是什麼樣子：

```
黑 1：隨機落在某處（可能是天元，可能是角上，可能是一線）
白 2：隨機落在另一處
黑 3：隨機...
...
第 200 手：棋盤上到處是孤立的棋子，沒有任何連接
最終：勝負由隨機因素決定
```

這局棋的「品質」極低，但它包含了寶貴的資訊：**最終誰贏了**。

### 第一個訓練訊號

雖然雙方都在亂下，但勝負結果是確定的。神經網路開始學習：

> 「在這個局面下，最終黑方贏了。雖然我不知道為什麼，但這個局面對黑方可能比較好。」

這是一個非常弱的訊號，但它是真實的。經過數千局這樣的「垃圾棋」之後，網路開始發現一些統計規律。

---

## Hour 1-3：發現遊戲規則

### 湧現的規則意識

經過數萬局自我對弈後，AlphaGo Zero 開始「發現」圍棋的基本規則（雖然這些規則早就內建在遊戲引擎中）：

#### 1. 連接的重要性

```
觀察：當棋子相連時，比較不容易被吃掉
學習：開始優先在已有棋子旁邊落子
```

這不是被教導的，而是從勝負結果中學到的。散落的棋子容易被各個擊破，連成一片的棋子更容易存活。

#### 2. 氣的概念

```
觀察：當棋子的鄰接空點都被佔據時，棋子會消失
學習：開始避免氣很少的位置，開始攻擊對手氣少的棋子
```

網路學會了追蹤氣數——雖然輸入中沒有明確的「氣數」特徵，但從歷史棋盤狀態中可以推斷出來。

#### 3. 眼的雛形

```
觀察：某些形狀特別難被吃掉
學習：開始在角落和邊上形成有空間的形狀
```

這是活棋概念的萌芽。網路發現，有內部空間的棋子群更容易存活。

### 棋力評估

此時的 AlphaGo Zero 大約是：
- **ELO**：~1000
- **相當於**：剛學會規則的初學者
- **特徵**：知道要連接棋子，知道要吃對方的棋

---

## Hour 3-12：發現定式與棋形

### 角部的覺醒

經過更多訓練，網路發現了角部的重要性：

```
觀察：角部的棋子只需要 2 個眼就能活
     邊上需要 2 個眼較難
     中央需要 2 個眼最難
學習：開局時優先佔角
```

這就是人類棋理中「金角銀邊草肚皮」的發現過程。網路沒有被告知這個原則，而是從數十萬局對弈中自己發現的。

### 定式的湧現

更令人驚奇的是，網路開始「發明」定式——雙方在角部的標準下法：

#### 觀察到的現象

```
訓練初期：角部下法五花八門
訓練中期：某些下法反覆出現
訓練後期：形成穩定的角部定式
```

這些定式與人類數百年累積的定式**高度相似**，驗證了這些定式確實是雙方最優解的近似。

### 典型的湧現定式

以小目定式為例：

```
  A B C D E F G H J
9 . . . . . . . . .
8 . . . . . . . . .
7 . . . . . . . . .
6 . . . ● . . . . .   ● = 黑
5 . . . . . . . . .   ○ = 白
4 . . . ○ . ● . . .
3 . . . . . . . . .
2 . . . . . . . . .
1 . . . . . . . . .
```

黑方佔據小目，白方掛角，黑方夾擊——這個序列在訓練過程中自然湧現。

### 棋形知識

除了定式，網路也學會了好形與壞形的區別：

| 形狀 | 人類評價 | Zero 的學習 |
|------|----------|-------------|
| 空三角 | 愚形 | 逐漸避免 |
| 虎口 | 好形 | 逐漸偏好 |
| 雙飛燕 | 經典攻擊形 | 自然發現 |
| 鎮神頭 | 強力攻擊 | 自然發現 |

### 棋力評估

此時的 AlphaGo Zero：
- **ELO**：~3000
- **相當於**：業餘高段
- **特徵**：有基本的定式知識，懂得基本棋形

---

## Hour 12-36：棋理的成熟

### 全局觀的形成

進入第二天，網路開始展現出**全局觀**：

#### 勢力與實地

```
觀察：圍住空間可以得到目數
     但勢力也有價值——可以攻擊對方
學習：在取地和取勢之間尋找平衡
```

這是圍棋中最深奧的概念之一。網路學會了評估「虛」和「實」的價值。

#### 厚薄判斷

```
觀察：「厚」的棋可以支援遠處的戰鬥
     「薄」的棋需要補強，否則會被攻擊
學習：主動建立厚勢，攻擊對方的薄弱
```

### 中盤戰術

網路的中盤戰鬥能力大幅提升：

| 技術 | 描述 |
|------|------|
| 攻擊弱棋 | 識別對方的孤棋，發動攻勢 |
| 利用厚味 | 用厚勢支援攻擊，獲得利益 |
| 轉換 | 放棄局部損失，換取全局優勢 |
| 打入 | 侵消對方的模樣 |

### 官子技巧

收官階段的精確計算也在提升：

```
觀察：官子階段每一手的價值可以精確計算
學習：按照價值大小依序收官
```

網路學會了「雙方先手」「單方先手」「後手」等官子概念。

### 棋力評估

此時的 AlphaGo Zero：
- **ELO**：~4500
- **相當於**：職業棋手水平
- **特徵**：有完整的圍棋理解，能下出高品質的對局

---

## Hour 36-72：超越人類

### 突破職業水平

在 36 小時左右，AlphaGo Zero 的棋力達到了職業棋手水平。但訓練並沒有停止——它繼續自我對弈，繼續提升。

接下來發生的事情更加有趣：**它開始發現人類從未想過的下法**。

### 顛覆性的開局

傳統圍棋開局有許多「定見」：

| 傳統觀點 | AlphaGo Zero 的發現 |
|----------|---------------------|
| 開局先佔角 | 某些情況下先佔邊更好 |
| 小目最穩健 | 三三直接佔角可行 |
| 定式要記熟 | 可以主動偏離定式 |
| 點三三太早貪 | 某些局面下點三三正確 |

這些「發現」在 AlphaGo 之後被人類職業棋手廣泛研究，許多已經被納入現代棋理。

### 反直覺的棋形

AlphaGo Zero 有時會下出人類認為「不好看」的形狀：

```
人類：「這是愚形，不可能是好棋」
Zero：（下了那步棋）
分析後：「原來這樣更有效率」
```

這揭示了人類棋理的侷限：有些「壞形」其實是特定局面下的最優解。

### 激進的棄子

Zero 比人類更願意棄子換取其他利益：

```
局部虧損 3 目
全局獲得主動權
最終勝率提升
```

人類棋手往往過度在意局部得失，而 Zero 始終盯著最終勝率。

### 棋力評估

72 小時後的 AlphaGo Zero：
- **ELO**：~5400
- **相當於**：超越所有人類棋手
- **特徵**：發現人類未知的下法，創造新的棋理

---

## 重新發現人類棋理

### 數千年 vs. 三天

人類圍棋發展了數千年：
- 公元前 2000 年左右起源於中國
- 唐朝傳入日本，發展出精密的棋理
- 20 世紀出現職業體系，棋理進一步深化
- 2016 年，人類認為已經相當理解圍棋

AlphaGo Zero 用三天走完了這段路程。更驚人的是，它發現的棋理與人類的**高度一致**。

### 驗證與超越

| 人類知識 | Zero 的態度 |
|----------|-------------|
| 金角銀邊草肚皮 | 確認（角落確實重要） |
| 基本定式 | 大部分確認，少數改進 |
| 好形壞形 | 大部分確認，特例存在 |
| 棄子轉換 | 比人類更激進 |
| 厚薄判斷 | 大致一致，細節不同 |

這說明人類數千年累積的棋理**大方向是正確的**。但也有一些領域，人類的理解需要修正。

### 對人類學習的啟示

AlphaGo Zero 的訓練過程給人類學習帶來啟示：

1. **從基礎開始**：Zero 先學會規則，再學會棋形，最後發展全局觀
2. **大量練習**：490 萬局自我對弈相當於數十萬年的人類對局量
3. **專注勝負**：不追求「漂亮的棋」，只追求贏
4. **不受傳統束縛**：敢於嘗試「不可能」的下法

---

## 訓練過程的技術細節

### 自我對弈的機制

每一局自我對弈的流程：

```
初始化：空棋盤
↓
每一步：
  1. 用神經網路評估當前局面
  2. 執行 MCTS 搜索（1600 次模擬）
  3. 根據搜索結果選擇落子
  4. 記錄 (局面, MCTS機率, -)
↓
遊戲結束：
  1. 判定勝負 z ∈ {-1, +1}
  2. 為所有記錄補上勝負 (局面, MCTS機率, z)
  3. 將資料加入訓練池
```

### 訓練的節奏

AlphaGo Zero 的訓練是**持續進行**的：

```
Self-play Workers:       不斷產生自我對弈資料
Training Workers:        不斷從資料池取樣訓練
Network Updates:         定期更新自我對弈用的網路
```

這三個過程同時進行，形成一個持續改進的循環。

### 資料池管理

訓練資料池的管理：

| 參數 | 值 |
|------|-----|
| 池大小 | 最近 50 萬局 |
| 每局樣本 | ~200 步 |
| 總樣本數 | ~1 億 |
| 取樣方式 | 均勻隨機 |

舊的資料會被新資料替換，確保訓練資料反映當前網路的水平。

### 網路更新策略

不是每訓練一步就更新自我對弈的網路。而是：

1. 訓練一段時間後，產生候選網路
2. 用候選網路對戰當前網路（400 局）
3. 如果候選網路勝率 > 55%，更新
4. 否則繼續訓練

這確保了自我對弈始終使用**足夠強**的網路。

---

## 學習速度的分析

### 為什麼這麼快？

AlphaGo Zero 學習速度驚人的原因：

#### 1. 計算資源

- 4 個 TPU，每秒數萬次推理
- 每天產生數十萬局自我對弈
- 相當於人類數千年的對局量

#### 2. 完美的對手

自我對弈意味著：
- 對手水平始終與自己相當
- 不會太弱（學不到東西）也不會太強（無法獲勝）
- 這是理想的學習條件

#### 3. 直接的目標

只有一個目標：贏。沒有：
- 老師的偏好
- 風格的追求
- 美學的考量

#### 4. 高效的表示學習

殘差網路能夠學習非常抽象的棋盤特徵，比手工設計的特徵更有效。

### 與人類的對比

| 方面 | 人類 | AlphaGo Zero |
|------|------|--------------|
| 學習速度 | 每天 ~10 局 | 每天 ~100,000 局 |
| 記憶保留 | 有遺忘 | 完美保留 |
| 精力限制 | 需要休息 | 24/7 運行 |
| 創新能力 | 受傳統影響 | 無預設限制 |

---

## 訓練過程中的有趣現象

### 階段性停滯

訓練曲線不是完全平滑的，有時會出現**停滯期**：

```
ELO: 2000 -----> 2000 -----> 2500 ---->
          (停滯)       (突破)
```

這可能是因為網路在學習某個新概念，需要時間「消化」。

### 策略的湧現和消失

某些策略會在訓練過程中湧現，然後又消失：

```
階段 1：發現某個攻擊手段
階段 2：對手學會防守
階段 3：該手段使用頻率降低
階段 4：發現新的攻擊手段
```

這是軍備競賽的縮影。

### 「重新發明輪子」

訓練過程中，Zero 會「重新發明」人類已知的概念：

- **征子**：發現連續叫吃可以吃掉棋子
- **倒脫靴**：發現可以先送子再反殺
- **打劫**：發現迴避規則的利用方式

這些發現的順序與人類學棋的順序類似。

---

## 動畫對應

本文涉及的核心概念與動畫編號：

| 編號 | 概念 | 物理/數學對應 |
|------|------|--------------|
| 🎬 E12 | 棋力成長曲線 | S 型增長（邏輯斯蒂） |
| 🎬 E7 | 從零開始 | 自組織現象 |
| 🎬 E5 | 自我對弈 | 不動點收斂 |
| 🎬 F8 | 湧現能力 | 相變 |

---

## 延伸閱讀

- **上一篇**：[雙頭網路與殘差網路](../dual-head-resnet) — 支撐這一切的神經網路架構
- **下一篇**：[分散式系統與 TPU](../distributed-systems) — 讓這一切成為可能的硬體
- **相關文章**：[自我對弈](../self-play) — 為什麼自我對弈如此有效

---

## 參考資料

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
3. DeepMind. (2017). "AlphaGo Zero: Learning from scratch." *YouTube*.
4. Wang, F., et al. (2019). "A Survey on the Evolution of AlphaGo." *arXiv:1907.11180*.

---

<KeyTakeaway>
本文重點：
- 訓練里程碑：0-3 小時發現基本規則、3-12 小時發現定式和棋形、36 小時超越 AlphaGo Fan、72 小時超越所有前代版本
- AlphaGo Zero 自然「重新發現」人類數千年累積的棋理（金角銀邊、定式、好形壞形），驗證這些知識的正確性
- 關鍵效率因素：完美的自我對弈對手、直接的勝率目標、高效的 ResNet 表示學習
</KeyTakeaway>

<FAQ items={[
  { question: "AlphaGo Zero 訓練過程中是如何發現圍棋規則的？", answer: "AlphaGo Zero 透過數萬局自我對弈的勝負結果，逐漸發現「連接的棋子比較難被吃」「氣少的棋子危險」「有內部空間的棋更容易存活」等規律。這些規則不是被教導的，而是從統計規律中自然湧現的。" },
  { question: "為什麼 AlphaGo Zero 的學習速度這麼快？", answer: "四個關鍵因素：1）計算資源讓它每天產生數十萬局自我對弈，相當於人類數千年的對局量；2）自我對弈的對手水平總是與自己相當，是理想的學習條件；3）只追求勝率，不受人類偏見干擾；4）ResNet 能學習非常抽象的特徵表示。" },
  { question: "AlphaGo Zero 發現的定式和人類的定式有什麼異同？", answer: "AlphaGo Zero 「重新發現」了許多人類的經典定式（如小目定式），驗證這些確實接近雙方最優解。但它也發現了人類未曾想過的變化，如開局直接點三三、反直覺的「愚形」其實是特定局面的最優解等，這些發現正在改變現代棋理。" }
]} />
