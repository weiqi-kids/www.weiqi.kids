<!doctype html>
<html lang="zh-tw" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-for-engineers/how-it-works/alphago-explained/alphago-zero" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">AlphaGo Zero 概述 | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><meta data-rh="true" property="og:locale" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="zh-tw"><meta data-rh="true" name="docsearch:language" content="zh-tw"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="AlphaGo Zero 概述 | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="從零開始、完全自學，AlphaGo Zero 如何在沒有人類棋譜的情況下超越所有前代版本"><meta data-rh="true" property="og:description" content="從零開始、完全自學，AlphaGo Zero 如何在沒有人類棋譜的情況下超越所有前代版本"><meta data-rh="true" name="keywords" content="AlphaGo Zero,自我對弈,強化學習,深度學習,圍棋 AI,無監督學習"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"給工程師的圍棋 AI 指南","item":"https://www.weiqi.kids/docs/for-engineers/"},{"@type":"ListItem","position":2,"name":"一篇文章搞懂圍棋 AI","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/"},{"@type":"ListItem","position":3,"name":"AlphaGo 完整解析","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/"},{"@type":"ListItem","position":4,"name":"AlphaGo Zero 概述","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero"}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"WebPage","@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/#webpage","url":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/","name":"AlphaGo Zero 概述","description":"從零開始、完全自學，AlphaGo Zero 如何在沒有人類棋譜的情況下超越所有前代版本","inLanguage":"zh-TW","isPartOf":{"@id":"https://www.weiqi.kids#website"},"primaryImageOfPage":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/social-card.png"},"datePublished":"2024-01-15","dateModified":"2024-02-22","speakable":{"@type":"SpeakableSpecification","cssSelector":[".article-summary",".speakable-content",".key-takeaway",".key-answer",".expert-quote",".actionable-steps li",".faq-answer-content"]}},{"@type":"Article","@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/#article","mainEntityOfPage":{"@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/#webpage","significantLink":["https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/"]},"headline":"AlphaGo Zero 概述","description":"從零開始、完全自學，AlphaGo Zero 如何在沒有人類棋譜的情況下超越所有前代版本","image":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/social-card.png","width":1200,"height":630},"author":{"@id":"https://www.weiqi.kids/docs/about/#person"},"publisher":{"@id":"https://www.weiqi.kids#organization"},"datePublished":"2024-01-15","dateModified":"2024-02-22","articleSection":"AlphaGo 完整解析","keywords":"AlphaGo Zero, 自我對弈, 強化學習, 深度學習, 圍棋 AI, 從零開始學習, AlphaZero, 神經網路","wordCount":4800,"inLanguage":"zh-TW","isAccessibleForFree":true,"isPartOf":{"@type":"WebSite","@id":"https://www.weiqi.kids#website"}},{"@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首頁","item":"https://www.weiqi.kids"},{"@type":"ListItem","position":2,"name":"給工程師","item":"https://www.weiqi.kids/docs/for-engineers/"},{"@type":"ListItem","position":3,"name":"技術原理","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/"},{"@type":"ListItem","position":4,"name":"AlphaGo 完整解析","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/"}]},{"@type":"Person","@id":"https://www.weiqi.kids/docs/about/#person","name":"好棋寶寶協會編輯團隊","url":"https://www.weiqi.kids/docs/about/","worksFor":{"@id":"https://www.weiqi.kids#organization"},"description":"專注於圍棋 AI 研究與教育推廣的技術團隊","knowsAbout":["圍棋 AI","AlphaGo","KataGo","機器學習","深度學習"],"hasCredential":[{"@type":"EducationalOccupationalCredential","name":"AI 圍棋研究專家","credentialCategory":"技術認證"}]},{"@type":"ImageObject","@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/#primaryimage","url":"https://www.weiqi.kids/img/social-card.png","width":1200,"height":630,"caption":"AlphaGo Zero 概述 - 好棋寶寶協會","representativeOfPage":true,"license":"https://creativecommons.org/licenses/by-nc-sa/4.0/","creditText":"台灣好棋寶寶協會"}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"FAQPage","mainEntity":[{"@type":"Question","name":"AlphaGo Zero 為什麼不需要人類棋譜也能學會圍棋？","acceptedAnswer":{"@type":"Answer","text":"AlphaGo Zero 透過純強化學習，只用勝負結果作為訓練訊號。它從隨機落子開始自我對弈，逐漸發現什麼是好棋。這種方法的優勢是不受人類偏見限制，能發現人類未知的下法，最終超越人類知識的上限。"}},{"@type":"Question","name":"AlphaGo Zero 的訓練效率為何比原版高這麼多？","acceptedAnswer":{"@type":"Answer","text":"三個主要原因：1）使用單一雙頭網路共享特徵學習，參數效率更高；2）純神經網路評估取代 rollout，搜索更準確；3）只有一個目標（勝率最大化），不像原版需要先模仿人類再強化學習。這讓 AlphaGo Zero 用更少資源達到更強棋力。"}},{"@type":"Question","name":"AlphaGo Zero 發現了哪些人類沒想過的下法？","acceptedAnswer":{"@type":"Answer","text":"AlphaGo Zero 發現許多顛覆傳統的下法，如：開局直接點三三、主動偏離傳統定式、使用看似「愚形」但實際更有效率的形狀、比人類更激進的棄子換取全局優勢等。這些發現正在改變現代職業圍棋的下法。"}}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.f23bf74b.css">
<script src="/assets/js/runtime~main.8112459a.js" defer="defer"></script>
<script src="/assets/js/main.05ef375d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.svg"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="好棋寶寶協會標誌" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/img/logo.svg" alt="好棋寶寶協會標誌" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">好棋寶寶</b></a><a class="navbar__item navbar__link" href="/docs/for-players/">給棋友</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/for-engineers/">給工程師</a><a class="navbar__item navbar__link" href="/docs/about/">關於協會</a><a class="navbar__item navbar__link" href="/docs/activities/">活動實績</a><a class="navbar__item navbar__link" href="/docs/references/">參考資料</a><a class="navbar__item navbar__link" href="/docs/sop/">標準作業流程</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>繁體中文</a><ul class="dropdown__menu"><li><a href="/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro/"><span title="使用指南" class="linkLabel_REp1">使用指南</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/about/"><span title="關於協會" class="categoryLinkLabel_ezQx">關於協會</span></a><button aria-label="展開側邊欄分類 &#x27;關於協會&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/activities/"><span title="活動實績" class="categoryLinkLabel_ezQx">活動實績</span></a><button aria-label="展開側邊欄分類 &#x27;活動實績&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/for-players/"><span title="給圍棋棋友" class="categoryLinkLabel_ezQx">給圍棋棋友</span></a><button aria-label="展開側邊欄分類 &#x27;給圍棋棋友&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/references/"><span title="參考資料" class="categoryLinkLabel_ezQx">參考資料</span></a><button aria-label="展開側邊欄分類 &#x27;參考資料&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/sop/"><span title="標準作業流程" class="categoryLinkLabel_ezQx">標準作業流程</span></a><button aria-label="展開側邊欄分類 &#x27;標準作業流程&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/docs/for-engineers/"><span title="給工程師的圍棋 AI 指南" class="categoryLinkLabel_ezQx">給工程師的圍棋 AI 指南</span></a><button aria-label="收起側邊欄分類 &#x27;給工程師的圍棋 AI 指南&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/docs/for-engineers/deep-dive/"><span title="給想深入研究的人" class="categoryLinkLabel_ezQx">給想深入研究的人</span></a><button aria-label="展開側邊欄分類 &#x27;給想深入研究的人&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/docs/for-engineers/hands-on/"><span title="30 分鐘跑起第一個圍棋 AI" class="categoryLinkLabel_ezQx">30 分鐘跑起第一個圍棋 AI</span></a><button aria-label="展開側邊欄分類 &#x27;30 分鐘跑起第一個圍棋 AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/for-engineers/how-it-works/"><span title="一篇文章搞懂圍棋 AI" class="categoryLinkLabel_ezQx">一篇文章搞懂圍棋 AI</span></a><button aria-label="收起側邊欄分類 &#x27;一篇文章搞懂圍棋 AI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/"><span title="AlphaGo 完整解析" class="categoryLinkLabel_ezQx">AlphaGo 完整解析</span></a><button aria-label="收起側邊欄分類 &#x27;AlphaGo 完整解析&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/birth-of-alphago/"><span title="AlphaGo 的誕生" class="linkLabel_REp1">AlphaGo 的誕生</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/key-matches/"><span title="關鍵對局回顧" class="linkLabel_REp1">關鍵對局回顧</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/move-37/"><span title="「神之一手」深度分析" class="linkLabel_REp1">「神之一手」深度分析</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/why-go-is-hard/"><span title="圍棋為什麼難？" class="linkLabel_REp1">圍棋為什麼難？</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/traditional-limits/"><span title="傳統方法的極限" class="linkLabel_REp1">傳統方法的極限</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/board-representation/"><span title="棋盤狀態表示" class="linkLabel_REp1">棋盤狀態表示</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/policy-network/"><span title="Policy Network 詳解" class="linkLabel_REp1">Policy Network 詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/value-network/"><span title="Value Network 詳解" class="linkLabel_REp1">Value Network 詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/input-features/"><span title="輸入特徵設計" class="linkLabel_REp1">輸入特徵設計</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/cnn-and-go/"><span title="CNN 與圍棋的結合" class="linkLabel_REp1">CNN 與圍棋的結合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/supervised-learning/"><span title="監督學習階段" class="linkLabel_REp1">監督學習階段</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/"><span title="強化學習入門" class="linkLabel_REp1">強化學習入門</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/self-play/"><span title="自我對弈" class="linkLabel_REp1">自我對弈</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/"><span title="MCTS 與神經網路的結合" class="linkLabel_REp1">MCTS 與神經網路的結合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/puct-formula/"><span title="PUCT 公式詳解" class="linkLabel_REp1">PUCT 公式詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><span title="AlphaGo Zero 概述" class="linkLabel_REp1">AlphaGo Zero 概述</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/"><span title="雙頭網路與殘差網路" class="linkLabel_REp1">雙頭網路與殘差網路</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/training-from-scratch/"><span title="從零訓練的過程" class="linkLabel_REp1">從零訓練的過程</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/distributed-systems/"><span title="分散式系統與 TPU" class="linkLabel_REp1">分散式系統與 TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/legacy-and-impact/"><span title="AlphaGo 的遺產" class="linkLabel_REp1">AlphaGo 的遺產</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/katago-innovations/"><span title="KataGo 的關鍵創新" class="linkLabel_REp1">KataGo 的關鍵創新</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/concepts/"><span title="概念速查表" class="linkLabel_REp1">概念速查表</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/docs/for-engineers/industry/"><span title="圍棋 AI 產業現況" class="categoryLinkLabel_ezQx">圍棋 AI 產業現況</span></a><button aria-label="展開側邊欄分類 &#x27;圍棋 AI 產業現況&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/docs/for-engineers/overview/"><span title="圍棋 AI 能做什麼？" class="categoryLinkLabel_ezQx">圍棋 AI 能做什麼？</span></a><button aria-label="展開側邊欄分類 &#x27;圍棋 AI 能做什麼？&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="頁面路徑"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/for-engineers/"><span>給工程師的圍棋 AI 指南</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/for-engineers/how-it-works/"><span>一篇文章搞懂圍棋 AI</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/for-engineers/how-it-works/alphago-explained/"><span>AlphaGo 完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">AlphaGo Zero 概述</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">本頁導覽</button></div><div class="theme-doc-markdown markdown">
<header><h1>AlphaGo Zero 概述</h1></header>
<p>2017 年 10 月，DeepMind 發表了一個震驚 AI 界的成果：<strong>AlphaGo Zero</strong> 在沒有使用任何人類棋譜的情況下，從完全隨機的狀態開始訓練，僅僅三天就超越了擊敗李世乭的原版 AlphaGo，並以 <strong>100:0</strong> 的比分完勝。</p>
<p>這不只是數字上的進步。這代表一個全新的範式：<strong>AI 不需要人類知識，可以從零發現一切</strong>。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="為什麼不需要人類棋譜">為什麼不需要人類棋譜？<a href="#為什麼不需要人類棋譜" class="hash-link" aria-label="為什麼不需要人類棋譜？的直接連結" title="為什麼不需要人類棋譜？的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="人類棋譜的限制">人類棋譜的限制<a href="#人類棋譜的限制" class="hash-link" aria-label="人類棋譜的限制的直接連結" title="人類棋譜的限制的直接連結" translate="no">​</a></h3>
<p>原版 AlphaGo 的訓練過程分為兩個階段：</p>
<ol>
<li class=""><strong>監督學習</strong>：用 3000 萬局人類棋譜訓練 Policy Network</li>
<li class=""><strong>強化學習</strong>：透過自我對弈進一步提升</li>
</ol>
<p>這個方法有幾個根本性的問題：</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-人類棋譜有上限">1. 人類棋譜有上限<a href="#1-人類棋譜有上限" class="hash-link" aria-label="1. 人類棋譜有上限的直接連結" title="1. 人類棋譜有上限的直接連結" translate="no">​</a></h4>
<p>人類棋手的棋力有極限，棋譜中包含的是人類的理解，也包含人類的錯誤和偏見。當 AI 從人類棋譜學習時，它學到的是：</p>
<ul>
<li class="">人類認為好的下法（但不一定是最優的）</li>
<li class="">人類的思維模式（但可能限制創新）</li>
<li class="">人類的錯誤（會被當作正確的樣本學習）</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-監督學習的瓶頸">2. 監督學習的瓶頸<a href="#2-監督學習的瓶頸" class="hash-link" aria-label="2. 監督學習的瓶頸的直接連結" title="2. 監督學習的瓶頸的直接連結" translate="no">​</a></h4>
<p>監督學習的目標是「模仿人類」——預測人類棋手會下哪一步。這意味著 AI 的能力上限被人類棋手的能力所限制。</p>
<p>就像一個學徒只能模仿師傅，永遠無法超越師傅一樣。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-資料收集成本">3. 資料收集成本<a href="#3-資料收集成本" class="hash-link" aria-label="3. 資料收集成本的直接連結" title="3. 資料收集成本的直接連結" translate="no">​</a></h4>
<p>高品質的人類棋譜需要多年累積，而且只存在於圍棋這類有悠久歷史的遊戲中。如果要將 AI 應用到新領域（如蛋白質結構預測），根本沒有「人類專家棋譜」可用。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="zero-的突破">Zero 的突破<a href="#zero-的突破" class="hash-link" aria-label="Zero 的突破的直接連結" title="Zero 的突破的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 完全跳過監督學習階段，直接從<strong>隨機初始化</strong>開始自我對弈。這解決了上述所有問題：</p>
<table><thead><tr><th>問題</th><th>原版 AlphaGo</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>人類知識上限</td><td>受限於棋譜品質</td><td>無此限制</td></tr><tr><td>學習目標</td><td>模仿人類</td><td>最大化勝率</td></tr><tr><td>資料需求</td><td>3000 萬局棋譜</td><td>0</td></tr><tr><td>可推廣性</td><td>僅限圍棋</td><td>可推廣至其他領域</td></tr></tbody></table>
<p>這是一個根本性的範式轉變：從「學習人類知識」轉向「從第一性原理發現知識」。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="與原版-alphago-的對比1000">與原版 AlphaGo 的對比：100:0<a href="#與原版-alphago-的對比1000" class="hash-link" aria-label="與原版 AlphaGo 的對比：100:0的直接連結" title="與原版 AlphaGo 的對比：100:0的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="碾壓性的勝利">碾壓性的勝利<a href="#碾壓性的勝利" class="hash-link" aria-label="碾壓性的勝利的直接連結" title="碾壓性的勝利的直接連結" translate="no">​</a></h3>
<p>DeepMind 讓訓練完成的 AlphaGo Zero 與各個版本的 AlphaGo 對弈：</p>
<table><thead><tr><th>對手</th><th>AlphaGo Zero 戰績</th></tr></thead><tbody><tr><td>AlphaGo Fan（擊敗樊麾版本）</td><td>100:0</td></tr><tr><td>AlphaGo Lee（擊敗李世乭版本）</td><td>100:0</td></tr><tr><td>AlphaGo Master（60 連勝版本）</td><td>89:11</td></tr></tbody></table>
<p><strong>100:0</strong>——這意味著在 100 盤比賽中，原版 AlphaGo 連一盤都贏不了。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="更少的資源更強的棋力">更少的資源，更強的棋力<a href="#更少的資源更強的棋力" class="hash-link" aria-label="更少的資源，更強的棋力的直接連結" title="更少的資源，更強的棋力的直接連結" translate="no">​</a></h3>
<p>不只是贏，AlphaGo Zero 還用更少的資源達成更強的棋力：</p>
<table><thead><tr><th>指標</th><th>AlphaGo Lee</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>訓練時間</td><td>數月</td><td>40 天（3 天超越 AlphaGo Lee）</td></tr><tr><td>訓練局數</td><td>3000 萬人類棋譜 + 自我對弈</td><td>490 萬局自我對弈</td></tr><tr><td>TPU 數量（訓練）</td><td>50+</td><td>4</td></tr><tr><td>TPU 數量（推理）</td><td>48</td><td>4</td></tr><tr><td>輸入特徵</td><td>48 個平面</td><td>17 個平面</td></tr><tr><td>神經網路</td><td>SL + RL 雙網路</td><td>單一雙頭網路</td></tr></tbody></table>
<p>這是一個驚人的效率提升：<strong>資源減少 10 倍以上，棋力卻大幅提升</strong>。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="為什麼-zero-更強">為什麼 Zero 更強？<a href="#為什麼-zero-更強" class="hash-link" aria-label="為什麼 Zero 更強？的直接連結" title="為什麼 Zero 更強？的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 更強的原因可以從幾個角度理解：</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-無偏見的學習">1. 無偏見的學習<a href="#1-無偏見的學習" class="hash-link" aria-label="1. 無偏見的學習的直接連結" title="1. 無偏見的學習的直接連結" translate="no">​</a></h4>
<p>原版 AlphaGo 從人類棋譜學習，繼承了人類的偏見。例如，人類棋手可能過度重視某些定式，或對某些局面有錯誤的評估。</p>
<p>AlphaGo Zero 沒有這些包袱。它從白紙開始，只透過勝負結果來學習什麼是好棋。這讓它能夠發現人類從未想過的下法。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-一致的學習目標">2. 一致的學習目標<a href="#2-一致的學習目標" class="hash-link" aria-label="2. 一致的學習目標的直接連結" title="2. 一致的學習目標的直接連結" translate="no">​</a></h4>
<p>原版 AlphaGo 的訓練有兩個不同的目標：</p>
<ul>
<li class="">監督學習：最大化對人類落子的預測準確率</li>
<li class="">強化學習：最大化勝率</li>
</ul>
<p>這兩個目標可能互相衝突。AlphaGo Zero 只有一個目標：<strong>勝率最大化</strong>。這讓學習過程更加一致和有效。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-更簡潔的架構">3. 更簡潔的架構<a href="#3-更簡潔的架構" class="hash-link" aria-label="3. 更簡潔的架構的直接連結" title="3. 更簡潔的架構的直接連結" translate="no">​</a></h4>
<p>原版 AlphaGo 使用分離的 Policy Network 和 Value Network。AlphaGo Zero 使用單一的雙頭網路（詳見下一篇），讓特徵表示能夠被共享，提高了學習效率。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="簡化的輸入特徵從-48-到-17">簡化的輸入特徵：從 48 到 17<a href="#簡化的輸入特徵從-48-到-17" class="hash-link" aria-label="簡化的輸入特徵：從 48 到 17的直接連結" title="簡化的輸入特徵：從 48 到 17的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="原版-alphago-的-48-個特徵平面">原版 AlphaGo 的 48 個特徵平面<a href="#原版-alphago-的-48-個特徵平面" class="hash-link" aria-label="原版 AlphaGo 的 48 個特徵平面的直接連結" title="原版 AlphaGo 的 48 個特徵平面的直接連結" translate="no">​</a></h3>
<p>原版 AlphaGo 的神經網路輸入包含 48 個 19x19 的特徵平面，編碼了大量人類設計的特徵：</p>
<table><thead><tr><th>類別</th><th>特徵數</th><th>內容</th></tr></thead><tbody><tr><td>棋子位置</td><td>3</td><td>黑子、白子、空點</td></tr><tr><td>氣數</td><td>8</td><td>1-8 氣的棋串</td></tr><tr><td>提子</td><td>8</td><td>能提 1-8 顆子</td></tr><tr><td>打劫</td><td>1</td><td>劫爭位置</td></tr><tr><td>邊線距離</td><td>4</td><td>一線到四線</td></tr><tr><td>落子合法性</td><td>1</td><td>哪些位置可以下</td></tr><tr><td>歷史狀態</td><td>8</td><td>過去 8 手的位置</td></tr><tr><td>輪次</td><td>1</td><td>黑方或白方</td></tr><tr><td>其他</td><td>14</td><td>征子、眼位等</td></tr></tbody></table>
<p>這 48 個特徵是圍棋專家精心設計的，包含了大量領域知識。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero-的-17-個特徵平面">AlphaGo Zero 的 17 個特徵平面<a href="#alphago-zero-的-17-個特徵平面" class="hash-link" aria-label="AlphaGo Zero 的 17 個特徵平面的直接連結" title="AlphaGo Zero 的 17 個特徵平面的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 大幅簡化了輸入，只使用 17 個特徵平面：</p>
<table><thead><tr><th>平面編號</th><th>內容</th><th>數量</th></tr></thead><tbody><tr><td>1-8</td><td>黑子位置（最近 8 步）</td><td>8</td></tr><tr><td>9-16</td><td>白子位置（最近 8 步）</td><td>8</td></tr><tr><td>17</td><td>當前輪次（全 1 或全 0）</td><td>1</td></tr></tbody></table>
<p>這 17 個特徵只包含：</p>
<ul>
<li class=""><strong>當前棋盤狀態</strong>：每個位置有黑子、白子或空</li>
<li class=""><strong>歷史資訊</strong>：過去 8 步的棋盤狀態</li>
<li class=""><strong>輪次資訊</strong>：輪到誰下</li>
</ul>
<p>沒有氣數、沒有征子判斷、沒有邊線距離——所有這些「圍棋知識」都讓神經網路自己學習。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="為什麼簡化是好的">為什麼簡化是好的？<a href="#為什麼簡化是好的" class="hash-link" aria-label="為什麼簡化是好的？的直接連結" title="為什麼簡化是好的？的直接連結" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-讓網路自己發現特徵">1. 讓網路自己發現特徵<a href="#1-讓網路自己發現特徵" class="hash-link" aria-label="1. 讓網路自己發現特徵的直接連結" title="1. 讓網路自己發現特徵的直接連結" translate="no">​</a></h4>
<p>複雜的手工特徵可能遺漏重要資訊，或編碼錯誤的假設。讓神經網路從原始資料學習，它可能發現更好的特徵表示。</p>
<p>事實證明，AlphaGo Zero 學會了人類設計的所有特徵（氣數、征子等），還學到了一些人類沒有明確意識到的模式。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-更好的可推廣性">2. 更好的可推廣性<a href="#2-更好的可推廣性" class="hash-link" aria-label="2. 更好的可推廣性的直接連結" title="2. 更好的可推廣性的直接連結" translate="no">​</a></h4>
<p>48 個特徵中的許多是圍棋專用的（如征子、邊線距離）。17 個簡化特徵則是通用的——任何棋盤遊戲都可以用類似的方式編碼。</p>
<p>這為後來的 <strong>AlphaZero</strong>（通用遊戲 AI）奠定了基礎。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-減少人為錯誤">3. 減少人為錯誤<a href="#3-減少人為錯誤" class="hash-link" aria-label="3. 減少人為錯誤的直接連結" title="3. 減少人為錯誤的直接連結" translate="no">​</a></h4>
<p>手工設計的特徵可能包含錯誤或不完整的定義。簡化輸入消除了這類問題的可能性。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="單一網路架構">單一網路架構<a href="#單一網路架構" class="hash-link" aria-label="單一網路架構的直接連結" title="單一網路架構的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="原版的雙網路設計">原版的雙網路設計<a href="#原版的雙網路設計" class="hash-link" aria-label="原版的雙網路設計的直接連結" title="原版的雙網路設計的直接連結" translate="no">​</a></h3>
<p>原版 AlphaGo 使用兩個獨立的神經網路：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Policy Network:  輸入 → CNN → 19x19 落子機率</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Value Network:   輸入 → CNN → 勝率評估（-1 到 1）</span><br></span></code></pre></div></div>
<p>這兩個網路：</p>
<ul>
<li class="">有不同的架構（層數、通道數略有不同）</li>
<li class="">獨立訓練（先訓練 Policy，再訓練 Value）</li>
<li class="">不共享任何參數</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="zero-的雙頭網路">Zero 的雙頭網路<a href="#zero-的雙頭網路" class="hash-link" aria-label="Zero 的雙頭網路的直接連結" title="Zero 的雙頭網路的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 使用單一網路，但有兩個輸出頭（heads）：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">輸入 → ResNet 共享主幹 → Policy Head → 19x19 落子機率</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                       → Value Head  → 勝率評估</span><br></span></code></pre></div></div>
<p>兩個 Head 共享同一個 ResNet 主幹（詳見<a class="" href="/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/">下一篇：雙頭網路與殘差網路</a>），這帶來幾個好處：</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-參數效率">1. 參數效率<a href="#1-參數效率" class="hash-link" aria-label="1. 參數效率的直接連結" title="1. 參數效率的直接連結" translate="no">​</a></h4>
<p>共享主幹意味著大部分參數被兩個任務共用。這減少了總參數量，降低了過擬合風險。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-特徵共享">2. 特徵共享<a href="#2-特徵共享" class="hash-link" aria-label="2. 特徵共享的直接連結" title="2. 特徵共享的直接連結" translate="no">​</a></h4>
<p>「應該下哪裡」（Policy）和「誰會贏」（Value）需要理解類似的棋盤模式。共享主幹讓這些特徵能被兩個任務同時學習和利用。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-訓練穩定性">3. 訓練穩定性<a href="#3-訓練穩定性" class="hash-link" aria-label="3. 訓練穩定性的直接連結" title="3. 訓練穩定性的直接連結" translate="no">​</a></h4>
<p>聯合訓練讓梯度訊號來自兩個來源，提供了更豐富的監督訊號，讓訓練更加穩定。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="殘差網路的威力">殘差網路的威力<a href="#殘差網路的威力" class="hash-link" aria-label="殘差網路的威力的直接連結" title="殘差網路的威力的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 的主幹使用 <strong>40 層殘差網路（ResNet）</strong>，比原版 AlphaGo 的 13 層 CNN 深得多。</p>
<p>殘差連接（skip connections）讓深層網路得以有效訓練，避免了梯度消失問題。這是 2015 年 ImageNet 競賽的突破性技術，被 AlphaGo Zero 成功應用到圍棋領域。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="訓練效率的提升">訓練效率的提升<a href="#訓練效率的提升" class="hash-link" aria-label="訓練效率的提升的直接連結" title="訓練效率的提升的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="自我對弈的指數增長">自我對弈的指數增長<a href="#自我對弈的指數增長" class="hash-link" aria-label="自我對弈的指數增長的直接連結" title="自我對弈的指數增長的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 的訓練過程展示了令人驚嘆的效率：</p>
<table><thead><tr><th>訓練時間</th><th>ELO 評分</th><th>相當於</th></tr></thead><tbody><tr><td>0 小時</td><td>0</td><td>隨機亂下</td></tr><tr><td>3 小時</td><td>~1000</td><td>發現基本規則</td></tr><tr><td>12 小時</td><td>~3000</td><td>發現定式</td></tr><tr><td>36 小時</td><td>~4500</td><td>超越樊麾版</td></tr><tr><td>60 小時</td><td>~5200</td><td>超越李世乭版</td></tr><tr><td>72 小時</td><td>~5400</td><td>超越原版 AlphaGo</td></tr><tr><td>40 天</td><td>~5600</td><td>最強版本</td></tr></tbody></table>
<p><strong>三天超越人類、三天超越之前花費數月訓練的 AI</strong>——這是指數級的效率提升。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="為什麼這麼快">為什麼這麼快？<a href="#為什麼這麼快" class="hash-link" aria-label="為什麼這麼快？的直接連結" title="為什麼這麼快？的直接連結" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-更強的搜索引導">1. 更強的搜索引導<a href="#1-更強的搜索引導" class="hash-link" aria-label="1. 更強的搜索引導的直接連結" title="1. 更強的搜索引導的直接連結" translate="no">​</a></h4>
<p>AlphaGo Zero 的 MCTS 完全由神經網路引導，不再使用快速走子策略（rollout）。這讓搜索更加高效和準確。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-更快的自我對弈">2. 更快的自我對弈<a href="#2-更快的自我對弈" class="hash-link" aria-label="2. 更快的自我對弈的直接連結" title="2. 更快的自我對弈的直接連結" translate="no">​</a></h4>
<p>由於只需要一個網路（而非兩個），每局自我對弈的計算成本降低。這意味著在相同時間內可以產生更多訓練資料。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-更有效的學習">3. 更有效的學習<a href="#3-更有效的學習" class="hash-link" aria-label="3. 更有效的學習的直接連結" title="3. 更有效的學習的直接連結" translate="no">​</a></h4>
<p>雙頭網路的聯合訓練讓每一局棋的資訊被更有效地利用。Policy 和 Value 的梯度相互強化，加速了收斂。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="與人類學習的對比">與人類學習的對比<a href="#與人類學習的對比" class="hash-link" aria-label="與人類學習的對比的直接連結" title="與人類學習的對比的直接連結" translate="no">​</a></h3>
<p>人類棋手需要多長時間達到不同水平？</p>
<table><thead><tr><th>水平</th><th>人類所需時間</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>入門</td><td>數週</td><td>幾分鐘</td></tr><tr><td>業餘初段</td><td>數年</td><td>數小時</td></tr><tr><td>職業水平</td><td>10-20 年</td><td>1-2 天</td></tr><tr><td>世界冠軍</td><td>20+ 年全職投入</td><td>3 天</td></tr><tr><td>超越人類</td><td>不可能</td><td>3 天</td></tr></tbody></table>
<p>這個對比不是要貶低人類棋手——他們用的是生物神經元，而 AlphaGo Zero 用的是專門設計的 TPU 和幾千瓦的電力。但它確實展示了正確的學習方法可以多麼高效。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="通用性西洋棋將棋">通用性：西洋棋、將棋<a href="#通用性西洋棋將棋" class="hash-link" aria-label="通用性：西洋棋、將棋的直接連結" title="通用性：西洋棋、將棋的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphazero-的誕生">AlphaZero 的誕生<a href="#alphazero-的誕生" class="hash-link" aria-label="AlphaZero 的誕生的直接連結" title="AlphaZero 的誕生的直接連結" translate="no">​</a></h3>
<p>2017 年 12 月，DeepMind 發表了 <strong>AlphaZero</strong>——AlphaGo Zero 的通用版本。同一套演算法，只需修改遊戲規則，就能在三種棋類遊戲中達到世界頂級水平：</p>
<table><thead><tr><th>遊戲</th><th>訓練時間</th><th>對手</th><th>戰績</th></tr></thead><tbody><tr><td>圍棋</td><td>8 小時</td><td>AlphaGo Zero</td><td>60:40</td></tr><tr><td>西洋棋</td><td>4 小時</td><td>Stockfish 8</td><td>28 勝 72 和 0 負</td></tr><tr><td>將棋</td><td>2 小時</td><td>Elmo</td><td>90:8:2</td></tr></tbody></table>
<p>注意這裡的對手：</p>
<ul>
<li class=""><strong>Stockfish</strong> 是當時最強的西洋棋引擎，使用幾十年人類知識和優化</li>
<li class=""><strong>Elmo</strong> 是當時最強的將棋 AI</li>
</ul>
<p>AlphaZero 用幾小時訓練，就超越了這些耗費多年開發的專用系統。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="通用性的意義">通用性的意義<a href="#通用性的意義" class="hash-link" aria-label="通用性的意義的直接連結" title="通用性的意義的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero / AlphaZero 證明了一件重要的事：</p>
<blockquote>
<p><strong>同一套學習演算法，可以在不同領域達到超人水平。</strong></p>
</blockquote>
<p>這不是三個不同的 AI，而是一個通用的學習框架：</p>
<ol>
<li class=""><strong>自我對弈</strong>產生經驗</li>
<li class=""><strong>蒙地卡羅樹搜索</strong>探索可能性</li>
<li class=""><strong>神經網路</strong>學習策略和價值函數</li>
<li class=""><strong>強化學習</strong>優化目標函數</li>
</ol>
<p>這個框架不依賴領域特定的知識，這為 AI 的通用化邁出了重要一步。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="對傳統-ai-的衝擊">對傳統 AI 的衝擊<a href="#對傳統-ai-的衝擊" class="hash-link" aria-label="對傳統 AI 的衝擊的直接連結" title="對傳統 AI 的衝擊的直接連結" translate="no">​</a></h3>
<p>在 AlphaZero 之前，西洋棋和將棋的最強 AI 都是「專家系統」風格的：</p>
<ul>
<li class=""><strong>大量人類知識</strong>：開局庫、殘局庫、評估函數</li>
<li class=""><strong>數十年優化</strong>：無數棋手和工程師的心血</li>
<li class=""><strong>極度專業化</strong>：Stockfish 不能下圍棋，Elmo 不能下西洋棋</li>
</ul>
<p>AlphaZero 用一個通用演算法在幾小時內超越了這一切。這讓許多 AI 研究者重新思考：</p>
<blockquote>
<p>我們應該投入更多精力在「通用學習演算法」，還是「專家知識編碼」？</p>
</blockquote>
<p>答案似乎越來越清楚：讓機器自己學習，比教它知識更有效。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero-的下棋風格">AlphaGo Zero 的下棋風格<a href="#alphago-zero-的下棋風格" class="hash-link" aria-label="AlphaGo Zero 的下棋風格的直接連結" title="AlphaGo Zero 的下棋風格的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="超越人類的審美">超越人類的審美<a href="#超越人類的審美" class="hash-link" aria-label="超越人類的審美的直接連結" title="超越人類的審美的直接連結" translate="no">​</a></h3>
<p>圍棋界對 AlphaGo Zero 的下法有一個普遍評價：<strong>更加優美</strong>。</p>
<p>AlphaGo Lee 的下法有時顯得「怪異」——像第 37 手那樣的落子，人類需要事後分析才能理解其妙處。但 AlphaGo Zero 的下法常常在事後被評價為「一眼就知道是好棋」。</p>
<p>這可能是因為：</p>
<ol>
<li class=""><strong>更強的棋力</strong>：Zero 能看得更深，落子更加從容</li>
<li class=""><strong>無人類偏見</strong>：不受傳統定式的束縛</li>
<li class=""><strong>一致的目標</strong>：只追求勝率，不模仿人類</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="重新發現人類棋理">重新發現人類棋理<a href="#重新發現人類棋理" class="hash-link" aria-label="重新發現人類棋理的直接連結" title="重新發現人類棋理的直接連結" translate="no">​</a></h3>
<p>有趣的是，AlphaGo Zero 在訓練過程中「重新發現」了人類數千年累積的圍棋知識：</p>
<ul>
<li class=""><strong>定式</strong>：Zero 自己發現了許多常見定式，因為這些確實是雙方最優解</li>
<li class=""><strong>佈局原則</strong>：角、邊、中央的重要性順序</li>
<li class=""><strong>棋形知識</strong>：愚形與好形的區別</li>
</ul>
<p>這驗證了人類棋理的合理性——這些知識不是偶然的，而是圍棋本質的反映。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="超越人類的創新">超越人類的創新<a href="#超越人類的創新" class="hash-link" aria-label="超越人類的創新的直接連結" title="超越人類的創新的直接連結" translate="no">​</a></h3>
<p>但 Zero 也發現了人類從未想過的下法：</p>
<ul>
<li class=""><strong>非常規開局</strong>：在傳統開局基礎上的變化</li>
<li class=""><strong>激進的棄子</strong>：比人類更願意放棄局部換取全局優勢</li>
<li class=""><strong>反直覺的形狀</strong>：表面上的「壞形」其實是最優解</li>
</ul>
<p>這些創新正在改變人類對圍棋的理解。許多職業棋手表示，研究 AlphaGo Zero 的棋譜讓他們對圍棋有了全新的認識。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="技術細節總結">技術細節總結<a href="#技術細節總結" class="hash-link" aria-label="技術細節總結的直接連結" title="技術細節總結的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="與原版-alphago-的完整對比">與原版 AlphaGo 的完整對比<a href="#與原版-alphago-的完整對比" class="hash-link" aria-label="與原版 AlphaGo 的完整對比的直接連結" title="與原版 AlphaGo 的完整對比的直接連結" translate="no">​</a></h3>
<table><thead><tr><th>方面</th><th>AlphaGo（原版）</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td><strong>訓練資料</strong></td><td>人類棋譜 + 自我對弈</td><td>純自我對弈</td></tr><tr><td><strong>學習方法</strong></td><td>監督學習 + 強化學習</td><td>純強化學習</td></tr><tr><td><strong>輸入特徵</strong></td><td>48 個平面</td><td>17 個平面</td></tr><tr><td><strong>網路架構</strong></td><td>分離的 Policy/Value</td><td>雙頭 ResNet</td></tr><tr><td><strong>網路深度</strong></td><td>13 層</td><td>40 層（或更多）</td></tr><tr><td><strong>MCTS 評估</strong></td><td>神經網路 + Rollout</td><td>純神經網路</td></tr><tr><td><strong>搜索次數</strong></td><td>每步 ~100,000</td><td>每步 ~1,600</td></tr><tr><td><strong>訓練 TPU</strong></td><td>50+</td><td>4</td></tr><tr><td><strong>推理 TPU</strong></td><td>48</td><td>4（可擴展）</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="核心算法">核心算法<a href="#核心算法" class="hash-link" aria-label="核心算法的直接連結" title="核心算法的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 的訓練循環非常簡潔：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. 自我對弈</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 用當前網路進行 MCTS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 按 MCTS 搜索機率選擇落子</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 記錄每一步的 (局面, MCTS機率, 勝負結果)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. 訓練網路</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 從經驗池中取樣</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Policy Head：最小化與 MCTS 機率的交叉熵</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Value Head：最小化與實際勝負的均方誤差</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 聯合優化兩個目標</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. 更新網路</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 用新網路替換舊網路（通過對弈驗證新網路更強）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - 回到步驟 1</span><br></span></code></pre></div></div>
<p>這個循環持續運行，網路不斷變強。沒有人類數據、沒有人類知識，只有遊戲規則和勝負目標。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="對-ai-研究的啟示">對 AI 研究的啟示<a href="#對-ai-研究的啟示" class="hash-link" aria-label="對 AI 研究的啟示的直接連結" title="對 AI 研究的啟示的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="第一性原理學習">第一性原理學習<a href="#第一性原理學習" class="hash-link" aria-label="第一性原理學習的直接連結" title="第一性原理學習的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 展示了一種「第一性原理」的學習方法：</p>
<blockquote>
<p>不要告訴 AI 怎麼做，只告訴它目標是什麼，讓它自己發現方法。</p>
</blockquote>
<p>這與傳統的專家系統方法形成鮮明對比。專家系統試圖將人類知識編碼進 AI，而 AlphaGo Zero 讓 AI 自己發現知識。</p>
<p>結果是：AI 發現的知識可能比人類知識更完整、更準確。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="自我對弈的威力">自我對弈的威力<a href="#自我對弈的威力" class="hash-link" aria-label="自我對弈的威力的直接連結" title="自我對弈的威力的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 證明了自我對弈可以產生無限的訓練資料，而且這些資料的品質會隨著網路的提升而提升。</p>
<p>這是一個「正向循環」：</p>
<ul>
<li class="">更強的網路 → 更好的自我對弈資料</li>
<li class="">更好的資料 → 更強的網路</li>
</ul>
<p>這個循環可以持續運行，直到達到遊戲的理論上限（如果存在的話）。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="簡化的重要性">簡化的重要性<a href="#簡化的重要性" class="hash-link" aria-label="簡化的重要性的直接連結" title="簡化的重要性的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 的成功證明了「簡化」的重要性：</p>
<ul>
<li class="">簡化輸入（48 → 17）</li>
<li class="">簡化架構（雙網路 → 單網路）</li>
<li class="">簡化訓練（監督 + 強化 → 純強化）</li>
</ul>
<p>每一次簡化都讓系統更加強大。這告訴我們：複雜不等於好，最簡單的解決方案往往是最好的。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="動畫對應">動畫對應<a href="#動畫對應" class="hash-link" aria-label="動畫對應的直接連結" title="動畫對應的直接連結" translate="no">​</a></h2>
<p>本文涉及的核心概念與動畫編號：</p>
<table><thead><tr><th>編號</th><th>概念</th><th>物理/數學對應</th></tr></thead><tbody><tr><td>🎬 E7</td><td>從零開始訓練</td><td>自組織現象</td></tr><tr><td>🎬 E5</td><td>自我對弈</td><td>不動點收斂</td></tr><tr><td>🎬 E12</td><td>棋力成長曲線</td><td>S 型增長</td></tr><tr><td>🎬 D12</td><td>殘差網路</td><td>梯度高速公路</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="延伸閱讀">延伸閱讀<a href="#延伸閱讀" class="hash-link" aria-label="延伸閱讀的直接連結" title="延伸閱讀的直接連結" translate="no">​</a></h2>
<ul>
<li class=""><strong>下一篇</strong>：<a class="" href="/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/">雙頭網路與殘差網路</a> — 詳解 AlphaGo Zero 的神經網路架構</li>
<li class=""><strong>相關文章</strong>：<a class="" href="/docs/for-engineers/how-it-works/alphago-explained/self-play/">自我對弈</a> — 為什麼自我對弈能產生超人水平</li>
<li class=""><strong>技術深入</strong>：<a class="" href="/docs/for-engineers/how-it-works/alphago-explained/training-from-scratch/">從零訓練的過程</a> — Day 0-3 的詳細演進</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="參考資料">參考資料<a href="#參考資料" class="hash-link" aria-label="參考資料的直接連結" title="參考資料的直接連結" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">Silver, D., et al. (2018). &quot;A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.&quot; <em>Science</em>, 362(6419), 1140-1144.</li>
<li class="">DeepMind. (2017). &quot;AlphaGo Zero: Starting from scratch.&quot; <em>DeepMind Blog</em>.</li>
<li class="">Schrittwieser, J., et al. (2020). &quot;Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.&quot; <em>Nature</em>, 588, 604-609.</li>
</ol>
<hr>
<div class="key-takeaway" style="background-color:var(--ifm-color-success-lightest);padding:1rem;border-radius:8px;border:1px solid var(--ifm-color-success);margin-bottom:1rem"><strong style="display:block;margin-bottom:0.5rem">📌 重點摘要</strong><p>本文重點：</p><ul>
<li class="">AlphaGo Zero 不需要人類棋譜，從隨機初始化開始純自我對弈，3 天就超越原版 AlphaGo，以 100:0 完勝</li>
<li class="">關鍵簡化：輸入特徵從 48 減到 17 個平面，使用單一雙頭網路取代分離的 Policy/Value 網路，TPU 從 50+ 減到 4 個</li>
<li class="">同一套演算法推廣到 AlphaZero，在圍棋、西洋棋、將棋三種棋類都達到超人水平</li>
</ul></div>
<div class="faq-section" style="margin-top:2rem"><h2>常見問題</h2><details style="margin-bottom:1rem;padding:1rem;background-color:var(--ifm-color-gray-100);border-radius:8px"><summary style="font-weight:bold;cursor:pointer;margin-bottom:0.5rem">AlphaGo Zero 為什麼不需要人類棋譜也能學會圍棋？</summary><p class="faq-answer-content" style="margin-top:0.5rem">AlphaGo Zero 透過純強化學習，只用勝負結果作為訓練訊號。它從隨機落子開始自我對弈，逐漸發現什麼是好棋。這種方法的優勢是不受人類偏見限制，能發現人類未知的下法，最終超越人類知識的上限。</p></details><details style="margin-bottom:1rem;padding:1rem;background-color:var(--ifm-color-gray-100);border-radius:8px"><summary style="font-weight:bold;cursor:pointer;margin-bottom:0.5rem">AlphaGo Zero 的訓練效率為何比原版高這麼多？</summary><p class="faq-answer-content" style="margin-top:0.5rem">三個主要原因：1）使用單一雙頭網路共享特徵學習，參數效率更高；2）純神經網路評估取代 rollout，搜索更準確；3）只有一個目標（勝率最大化），不像原版需要先模仿人類再強化學習。這讓 AlphaGo Zero 用更少資源達到更強棋力。</p></details><details style="margin-bottom:1rem;padding:1rem;background-color:var(--ifm-color-gray-100);border-radius:8px"><summary style="font-weight:bold;cursor:pointer;margin-bottom:0.5rem">AlphaGo Zero 發現了哪些人類沒想過的下法？</summary><p class="faq-answer-content" style="margin-top:0.5rem">AlphaGo Zero 發現許多顛覆傳統的下法，如：開局直接點三三、主動偏離傳統定式、使用看似「愚形」但實際更有效率的形狀、比人類更激進的棄子換取全局優勢等。這些發現正在改變現代職業圍棋的下法。</p></details></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/16-alphago-zero.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>編輯此頁</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/for-engineers/how-it-works/alphago-explained/puct-formula/"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">PUCT 公式詳解</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">雙頭網路與殘差網路</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#為什麼不需要人類棋譜" class="table-of-contents__link toc-highlight">為什麼不需要人類棋譜？</a><ul><li><a href="#人類棋譜的限制" class="table-of-contents__link toc-highlight">人類棋譜的限制</a></li><li><a href="#zero-的突破" class="table-of-contents__link toc-highlight">Zero 的突破</a></li></ul></li><li><a href="#與原版-alphago-的對比1000" class="table-of-contents__link toc-highlight">與原版 AlphaGo 的對比：100:0</a><ul><li><a href="#碾壓性的勝利" class="table-of-contents__link toc-highlight">碾壓性的勝利</a></li><li><a href="#更少的資源更強的棋力" class="table-of-contents__link toc-highlight">更少的資源，更強的棋力</a></li><li><a href="#為什麼-zero-更強" class="table-of-contents__link toc-highlight">為什麼 Zero 更強？</a></li></ul></li><li><a href="#簡化的輸入特徵從-48-到-17" class="table-of-contents__link toc-highlight">簡化的輸入特徵：從 48 到 17</a><ul><li><a href="#原版-alphago-的-48-個特徵平面" class="table-of-contents__link toc-highlight">原版 AlphaGo 的 48 個特徵平面</a></li><li><a href="#alphago-zero-的-17-個特徵平面" class="table-of-contents__link toc-highlight">AlphaGo Zero 的 17 個特徵平面</a></li><li><a href="#為什麼簡化是好的" class="table-of-contents__link toc-highlight">為什麼簡化是好的？</a></li></ul></li><li><a href="#單一網路架構" class="table-of-contents__link toc-highlight">單一網路架構</a><ul><li><a href="#原版的雙網路設計" class="table-of-contents__link toc-highlight">原版的雙網路設計</a></li><li><a href="#zero-的雙頭網路" class="table-of-contents__link toc-highlight">Zero 的雙頭網路</a></li><li><a href="#殘差網路的威力" class="table-of-contents__link toc-highlight">殘差網路的威力</a></li></ul></li><li><a href="#訓練效率的提升" class="table-of-contents__link toc-highlight">訓練效率的提升</a><ul><li><a href="#自我對弈的指數增長" class="table-of-contents__link toc-highlight">自我對弈的指數增長</a></li><li><a href="#為什麼這麼快" class="table-of-contents__link toc-highlight">為什麼這麼快？</a></li><li><a href="#與人類學習的對比" class="table-of-contents__link toc-highlight">與人類學習的對比</a></li></ul></li><li><a href="#通用性西洋棋將棋" class="table-of-contents__link toc-highlight">通用性：西洋棋、將棋</a><ul><li><a href="#alphazero-的誕生" class="table-of-contents__link toc-highlight">AlphaZero 的誕生</a></li><li><a href="#通用性的意義" class="table-of-contents__link toc-highlight">通用性的意義</a></li><li><a href="#對傳統-ai-的衝擊" class="table-of-contents__link toc-highlight">對傳統 AI 的衝擊</a></li></ul></li><li><a href="#alphago-zero-的下棋風格" class="table-of-contents__link toc-highlight">AlphaGo Zero 的下棋風格</a><ul><li><a href="#超越人類的審美" class="table-of-contents__link toc-highlight">超越人類的審美</a></li><li><a href="#重新發現人類棋理" class="table-of-contents__link toc-highlight">重新發現人類棋理</a></li><li><a href="#超越人類的創新" class="table-of-contents__link toc-highlight">超越人類的創新</a></li></ul></li><li><a href="#技術細節總結" class="table-of-contents__link toc-highlight">技術細節總結</a><ul><li><a href="#與原版-alphago-的完整對比" class="table-of-contents__link toc-highlight">與原版 AlphaGo 的完整對比</a></li><li><a href="#核心算法" class="table-of-contents__link toc-highlight">核心算法</a></li></ul></li><li><a href="#對-ai-研究的啟示" class="table-of-contents__link toc-highlight">對 AI 研究的啟示</a><ul><li><a href="#第一性原理學習" class="table-of-contents__link toc-highlight">第一性原理學習</a></li><li><a href="#自我對弈的威力" class="table-of-contents__link toc-highlight">自我對弈的威力</a></li><li><a href="#簡化的重要性" class="table-of-contents__link toc-highlight">簡化的重要性</a></li></ul></li><li><a href="#動畫對應" class="table-of-contents__link toc-highlight">動畫對應</a></li><li><a href="#延伸閱讀" class="table-of-contents__link toc-highlight">延伸閱讀</a></li><li><a href="#參考資料" class="table-of-contents__link toc-highlight">參考資料</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>