---
sidebar_position: 9
title: Value Network è©³è§£
description: æ·±å…¥ç†è§£ AlphaGo çš„åƒ¹å€¼ç¶²è·¯æ¶æ§‹ã€è¨“ç·´æŒ‘æˆ°èˆ‡åœ¨ MCTS ä¸­çš„é—œéµä½œç”¨
---

# Value Network è©³è§£

å¦‚æœèªª Policy Network å‘Šè¨´ AlphaGoã€Œä¸‹ä¸€æ­¥æ‡‰è©²ä¸‹å“ªè£¡ã€ï¼Œé‚£éº¼ Value Network å›ç­”çš„æ˜¯ä¸€å€‹æ›´æ ¹æœ¬çš„å•é¡Œï¼š

> **ã€Œé€™ç›¤æ£‹ï¼Œæˆ‘æœƒè´å—ï¼Ÿã€**

---

## ä»€éº¼æ˜¯ Value Networkï¼Ÿ

### æ ¸å¿ƒåŠŸèƒ½

Value Network æ˜¯ä¸€å€‹æ·±åº¦å·ç©ç¥ç¶“ç¶²è·¯ï¼Œå®ƒçš„ä»»å‹™æ˜¯ï¼š

> **çµ¦å®šç•¶å‰æ£‹ç›¤ç‹€æ…‹ï¼Œé æ¸¬æœ€çµ‚çš„å‹ç‡**

ç”¨æ•¸å­¸è¡¨ç¤ºï¼š

```
v = f_Î¸(s)
```

å…¶ä¸­ï¼š
- `s`ï¼šç•¶å‰æ£‹ç›¤ç‹€æ…‹
- `f_Î¸`ï¼šValue Networkï¼ˆÎ¸ æ˜¯ç¶²è·¯åƒæ•¸ï¼‰
- `v`ï¼šä¸€å€‹ä»‹æ–¼ -1 åˆ° +1 ä¹‹é–“çš„æ•¸å€¼

### è¼¸å‡ºçš„å«ç¾©

| è¼¸å‡ºå€¼ | å«ç¾© |
|--------|------|
| +1 | ç•¶å‰ç©å®¶å¿…å‹ |
| +0.5 | ç•¶å‰ç©å®¶ç´„ 75% å‹ç‡ |
| 0 | é›™æ–¹å‹ç‡ç›¸ç­‰ |
| -0.5 | ç•¶å‰ç©å®¶ç´„ 25% å‹ç‡ |
| -1 | ç•¶å‰ç©å®¶å¿…æ•— |

### ç‚ºä»€éº¼éœ€è¦å–®ä¸€æ•¸å€¼ï¼Ÿ

#### æ¯”è¼ƒä¸åŒé¸æ“‡

åœ¨ä¸‹æ£‹æ™‚ï¼Œæˆ‘å€‘ç¶“å¸¸éœ€è¦åœ¨å¤šå€‹é¸é …ä¸­åšé¸æ“‡ã€‚Value Network è®“é€™å€‹æ¯”è¼ƒè®Šå¾—ç°¡å–®ï¼š

```
é¸é … A çš„å±€é¢åƒ¹å€¼ï¼š0.3
é¸é … B çš„å±€é¢åƒ¹å€¼ï¼š0.5
é¸é … C çš„å±€é¢åƒ¹å€¼ï¼š0.2

â†’ é¸æ“‡ Bï¼ˆæœ€é«˜çš„åƒ¹å€¼ï¼‰
```

å¦‚æœæ²’æœ‰å–®ä¸€æ•¸å€¼ï¼Œæˆ‘å€‘å¦‚ä½•æ¯”è¼ƒã€Œåƒæ‰å°æ–¹ä¸€å¡Šæ£‹ã€å’Œã€Œåœä½ä¸€å¤§å¡Šç©ºã€å“ªå€‹æ›´å¥½ï¼Ÿ

#### å–ä»£å¤§é‡æ¨¡æ“¬

åœ¨å‚³çµ±çš„è’™åœ°å¡ç¾…æ¨¹æœç´¢ä¸­ï¼Œè©•ä¼°ä¸€å€‹å±€é¢éœ€è¦é€²è¡Œ **éš¨æ©Ÿæ¨¡æ“¬ï¼ˆrolloutï¼‰**ï¼š

1. å¾ç•¶å‰å±€é¢é–‹å§‹
2. é›™æ–¹éš¨æ©Ÿä¸‹æ£‹ç›´åˆ°éŠæˆ²çµæŸ
3. è¨˜éŒ„å‹è² 
4. é‡è¤‡æ•¸åƒæ¬¡ï¼Œè¨ˆç®—å‹ç‡

é€™éå¸¸æ…¢ã€‚Value Network å¯ä»¥**ä¸€æ¬¡å‰å‘å‚³æ’­**å°±çµ¦å‡ºè©•ä¼°ï¼Œé€Ÿåº¦å¿«å¹¾å€‹æ•¸é‡ç´šã€‚

| æ–¹æ³• | è©•ä¼°æ™‚é–“ | ç²¾åº¦ |
|------|---------|------|
| 1000 æ¬¡éš¨æ©Ÿæ¨¡æ“¬ | ~2000 æ¯«ç§’ | è¼ƒä½ |
| 15000 æ¬¡éš¨æ©Ÿæ¨¡æ“¬ | ~30000 æ¯«ç§’ | ä¸­ç­‰ |
| Value Network | ~3 æ¯«ç§’ | é«˜ï¼ˆç­‰åƒ¹æ–¼ 15000 æ¬¡æ¨¡æ“¬ï¼‰ |

---

## ç¶²è·¯æ¶æ§‹

### èˆ‡ Policy Network çš„ç›¸ä¼¼æ€§

Value Network çš„æ¶æ§‹èˆ‡ Policy Network éå¸¸ç›¸ä¼¼ï¼Œéƒ½æ˜¯æ·±åº¦å·ç©ç¥ç¶“ç¶²è·¯ï¼š

```
è¼¸å…¥å±¤ â†’ å·ç©å±¤ Ã—12 â†’ å…¨é€£æ¥å±¤ â†’ è¼¸å‡º
   â†“         â†“           â†“         â†“
19Ã—19Ã—48   19Ã—19Ã—192    256ç¶­     å–®ä¸€æ•¸å€¼
```

### è¼¸å…¥å±¤

èˆ‡ Policy Network ç›¸åŒï¼Œè¼¸å…¥æ˜¯ **19Ã—19Ã—49** çš„ç‰¹å¾µå¼µé‡ï¼š

- **19Ã—19**ï¼šæ£‹ç›¤å¤§å°
- **49**ï¼š48 å€‹ç‰¹å¾µå¹³é¢ + 1 å€‹è¡¨ç¤ºç•¶å‰è¼ªåˆ°èª°çš„å¹³é¢

å¤šå‡ºçš„ 1 å€‹å¹³é¢å¾ˆé‡è¦ï¼šValue Network éœ€è¦çŸ¥é“æ˜¯èª°çš„å›åˆï¼Œå› ç‚ºåŒä¸€å±€é¢å°é»‘æ£‹å’Œç™½æ£‹çš„åƒ¹å€¼æ˜¯ç›¸åçš„ã€‚

### å·ç©å±¤

èˆ‡ Policy Network ç›¸åŒï¼š
- **12 å±¤å·ç©å±¤**
- **192 å€‹æ¿¾æ³¢å™¨**
- **3Ã—3 å·ç©æ ¸**ï¼ˆç¬¬ä¸€å±¤ 5Ã—5ï¼‰
- **ReLU æ¿€æ´»å‡½æ•¸**

### è¼¸å‡ºå±¤çš„å·®ç•°

é€™æ˜¯ Value Network èˆ‡ Policy Network çš„é—œéµå·®ç•°ï¼š

#### Policy Network è¼¸å‡º
```
19Ã—19Ã—192 â†’ 1Ã—1 å·ç© â†’ 19Ã—19Ã—1 â†’ å±•å¹³ â†’ 361ç¶­ â†’ Softmax â†’ æ©Ÿç‡åˆ†å¸ƒ
```

#### Value Network è¼¸å‡º
```
19Ã—19Ã—192 â†’ 1Ã—1 å·ç© â†’ 19Ã—19Ã—1 â†’ å±•å¹³ â†’ 361ç¶­ â†’ å…¨é€£æ¥256 â†’ ReLU â†’ å…¨é€£æ¥1 â†’ Tanh â†’ å–®ä¸€æ•¸å€¼
```

### Tanh æ¿€æ´»å‡½æ•¸

Value Network çš„æœ€å¾Œä¸€å±¤ä½¿ç”¨ **Tanh**ï¼ˆé›™æ›²æ­£åˆ‡ï¼‰å‡½æ•¸ï¼š

```
Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```

Tanh çš„è¼¸å‡ºç¯„åœæ˜¯ **(-1, +1)**ï¼Œæ­£å¥½å°æ‡‰å‹è² ã€‚

#### ç‚ºä»€éº¼ç”¨ Tanh è€Œé Sigmoidï¼Ÿ

Sigmoid çš„è¼¸å‡ºç¯„åœæ˜¯ (0, 1)ï¼Œä¹Ÿå¯ä»¥è¡¨ç¤ºå‹ç‡ã€‚ä½† Tanh æœ‰å¹¾å€‹å„ªé»ï¼š

1. **å°ç¨±æ€§**ï¼šä»¥ 0 ç‚ºä¸­å¿ƒï¼Œè¼¸å‡ºå¯æ­£å¯è² 
2. **æ¢¯åº¦æ›´å¥½**ï¼šåœ¨ 0 é™„è¿‘æ¢¯åº¦æ¥è¿‘ 1
3. **èªæ„æ¸…æ™°**ï¼šæ­£å€¼è´ã€è² å€¼è¼¸ã€é›¶æ˜¯å¹³å±€

### å®Œæ•´æ¶æ§‹åœ–

```
è¼¸å…¥: 19Ã—19Ã—49
        â†“
    Conv 5Ã—5, 192 filters
        â†“
    ReLU
        â†“
    Conv 3Ã—3, 192 filters (Ã—11)
        â†“
    ReLU
        â†“
    Conv 1Ã—1, 1 filter
        â†“
    å±•å¹³ (361 ç¶­)
        â†“
    å…¨é€£æ¥ (256 ç¶­)
        â†“
    ReLU
        â†“
    å…¨é€£æ¥ (1 ç¶­)
        â†“
    Tanh
        â†“
è¼¸å‡º: [-1, +1]
```

### åƒæ•¸æ•¸é‡

| å±¤ | è¨ˆç®— | åƒæ•¸æ•¸é‡ |
|---|------|---------|
| å·ç©å±¤ | åŒ Policy Network | ~3.9M |
| å…¨é€£æ¥å±¤ 1 | 361Ã—256 + 256 | 92,672 |
| å…¨é€£æ¥å±¤ 2 | 256Ã—1 + 1 | 257 |
| **ç¸½è¨ˆ** | | **~4.0M** |

ç´„ 400 è¬å€‹åƒæ•¸ï¼Œæ¯” Policy Network ç•¥å¤šã€‚

---

## è¨“ç·´çš„æŒ‘æˆ°

### éæ“¬åˆå•é¡Œ

Value Network çš„è¨“ç·´æ¯” Policy Network å›°é›£å¾—å¤šã€‚ä¸»è¦å•é¡Œæ˜¯**éæ“¬åˆ**ã€‚

#### ä»€éº¼æ˜¯éæ“¬åˆï¼Ÿ

éæ“¬åˆæ˜¯æŒ‡æ¨¡å‹ã€Œè¨˜ä½ã€äº†è¨“ç·´è³‡æ–™ï¼Œè€Œéå­¸æœƒæ³›åŒ–ã€‚è¡¨ç¾ç‚ºï¼š
- è¨“ç·´é›†ä¸Šè¡¨ç¾å¾ˆå¥½
- æ¸¬è©¦é›†ä¸Šè¡¨ç¾å¾ˆå·®

#### ç‚ºä»€éº¼ Value Network å®¹æ˜“éæ“¬åˆï¼Ÿ

è€ƒæ…®ä¸€ç›¤æ£‹çš„è³‡æ–™ï¼š

```
å±€é¢ 1 â†’ å±€é¢ 2 â†’ å±€é¢ 3 â†’ ... â†’ å±€é¢ 200 â†’ çµæœï¼šé»‘å‹
```

å¦‚æœç›´æ¥ç”¨é€™äº›è³‡æ–™è¨“ç·´ï¼š
- é€™ 200 å€‹å±€é¢æœ‰å¾ˆå¼·çš„ç›¸é—œæ€§
- å®ƒå€‘ä¾†è‡ªåŒä¸€ç›¤æ£‹ï¼Œæœ‰ç›¸åŒçš„çµæœ
- æ¨¡å‹å¯èƒ½å­¸æœƒã€Œèªå‡ºã€é€™ç›¤æ£‹ï¼Œè€Œéç†è§£å±€é¢

DeepMind ç™¼ç¾ï¼šå¦‚æœç”¨ç›¸åŒçš„äººé¡æ£‹è­œè¨“ç·´ Policy å’Œ Value Networkï¼ŒValue Network æœƒåš´é‡éæ“¬åˆã€‚

### è§£æ±ºæ–¹æ¡ˆï¼šè‡ªæˆ‘å°å¼ˆè³‡æ–™

DeepMind çš„è§£æ±ºæ–¹æ¡ˆæ˜¯ç”¨**è‡ªæˆ‘å°å¼ˆ**ç”Ÿæˆæ–°çš„è¨“ç·´è³‡æ–™ï¼š

```
1. ç”¨è¨“ç·´å¥½çš„ RL Policy Network è‡ªæˆ‘å°å¼ˆ
2. å¾æ¯ç›¤æ£‹ä¸­åªå–ä¸€å€‹å±€é¢ï¼ˆé¿å…ç›¸é—œæ€§ï¼‰
3. é€™å€‹å±€é¢çš„æ¨™ç±¤æ˜¯è©²ç›¤æ£‹çš„æœ€çµ‚çµæœ
4. ç”Ÿæˆ 3000 è¬å€‹é€™æ¨£çš„æ¨£æœ¬
```

#### ç‚ºä»€éº¼é€™èƒ½è§£æ±ºéæ“¬åˆï¼Ÿ

1. **è³‡æ–™é‡å¤§**ï¼š3000 è¬å€‹ç¨ç«‹çš„å±€é¢
2. **ç„¡ç›¸é—œæ€§**ï¼šæ¯ç›¤æ£‹åªå–ä¸€å€‹å±€é¢
3. **åˆ†å¸ƒä¸åŒ**ï¼šè‡ªæˆ‘å°å¼ˆçš„å±€é¢åˆ†å¸ƒä¸åŒæ–¼äººé¡æ£‹è­œ

### è¨“ç·´è³‡æ–™çš„ç”¢ç”Ÿ

```python
# å½ä»£ç¢¼
training_data = []

for game_id in range(30_000_000):
    # è‡ªæˆ‘å°å¼ˆä¸€ç›¤
    states, result = self_play(rl_policy_network)

    # éš¨æ©Ÿé¸å–ä¸€å€‹å±€é¢
    random_index = random.randint(0, len(states) - 1)
    state = states[random_index]

    # è¨˜éŒ„å±€é¢å’Œçµæœ
    training_data.append((state, result))
```

---

## è¨“ç·´ç›®æ¨™èˆ‡æ–¹æ³•

### å‡æ–¹èª¤å·®æå¤±

Value Network ä½¿ç”¨**å‡æ–¹èª¤å·®ï¼ˆMSEï¼‰**ä½œç‚ºæå¤±å‡½æ•¸ï¼š

```
L(Î¸) = (1/n) Ã— Î£ (v_Î¸(s) - z)Â²
```

å…¶ä¸­ï¼š
- `v_Î¸(s)`ï¼šæ¨¡å‹é æ¸¬çš„åƒ¹å€¼
- `z`ï¼šå¯¦éš›çµæœï¼ˆ+1 æˆ– -1ï¼‰

#### ç‚ºä»€éº¼ç”¨ MSE è€Œéäº¤å‰ç†µï¼Ÿ

- **äº¤å‰ç†µ**é©åˆåˆ†é¡å•é¡Œï¼ˆé›¢æ•£çš„æ¨™ç±¤ï¼‰
- **MSE** é©åˆå›æ­¸å•é¡Œï¼ˆé€£çºŒçš„æ•¸å€¼ï¼‰

é›–ç„¶çµæœåªæœ‰ +1 æˆ– -1ï¼Œä½†æ¨¡å‹é æ¸¬çš„æ˜¯é€£çºŒå€¼ï¼ˆ-1 åˆ° +1 ä¹‹é–“çš„ä»»ä½•æ•¸ï¼‰ã€‚MSE è®“æ¨¡å‹å­¸æœƒé æ¸¬æ¥è¿‘ +1 æˆ– -1 çš„å€¼ã€‚

### è¨“ç·´éç¨‹

```python
# å½ä»£ç¢¼
for epoch in range(num_epochs):
    for batch in dataloader:
        states, outcomes = batch

        # å‰å‘å‚³æ’­
        values = network(states)  # (batch, 1)

        # è¨ˆç®—æå¤±ï¼ˆMSEï¼‰
        loss = mse_loss(values, outcomes)

        # åå‘å‚³æ’­
        loss.backward()
        optimizer.step()
```

è¨“ç·´ç´°ç¯€ï¼š
- **å„ªåŒ–å™¨**ï¼šSGD with momentum
- **å­¸ç¿’ç‡**ï¼š0.003
- **æ‰¹æ¬¡å¤§å°**ï¼š32
- **è¨“ç·´æ™‚é–“**ï¼šç´„ 1 é€±ï¼ˆ50 GPUsï¼‰

---

## æº–ç¢ºåº¦åˆ†æ

### èˆ‡éš¨æ©Ÿæ¨¡æ“¬çš„æ¯”è¼ƒ

DeepMind åœ¨è«–æ–‡ä¸­é€²è¡Œäº†è©³ç´°çš„æ¯”è¼ƒï¼š

| è©•ä¼°æ–¹æ³• | é æ¸¬èª¤å·® |
|---------|---------|
| 1000 æ¬¡éš¨æ©Ÿæ¨¡æ“¬ | è¼ƒé«˜ |
| 15000 æ¬¡éš¨æ©Ÿæ¨¡æ“¬ | ä¸­ç­‰ |
| Value Network | èˆ‡ 15000 æ¬¡æ¨¡æ“¬ç›¸ç•¶ |

é€™æ„å‘³è‘—ä¸€æ¬¡ Value Network è©•ä¼° â‰ˆ 15000 æ¬¡éš¨æ©Ÿæ¨¡æ“¬ï¼Œä½†é€Ÿåº¦å¿«ç´„ 1000 å€ã€‚

### å„éšæ®µçš„æº–ç¢ºåº¦

Value Network çš„æº–ç¢ºåº¦å–æ±ºæ–¼éŠæˆ²é€²ç¨‹ï¼š

| éšæ®µ | å‰©é¤˜æ‰‹æ•¸ | é æ¸¬é›£åº¦ | æº–ç¢ºåº¦ |
|------|---------|---------|--------|
| é–‹å±€ | ~300 | å¾ˆé›£ | è¼ƒä½ |
| ä¸­ç›¤ | ~150 | å›°é›£ | ä¸­ç­‰ |
| æ”¶å®˜ | ~50 | è¼ƒæ˜“ | è¼ƒé«˜ |
| çµ‚å±€ | ~10 | ç°¡å–® | å¾ˆé«˜ |

é€™æ˜¯ç›´è¦ºä¸Šåˆç†çš„ï¼šè¶Šæ¥è¿‘éŠæˆ²çµæŸï¼Œçµæœè¶Šç¢ºå®šã€‚

### è¼¸å‡ºåˆ†å¸ƒ

ä¸€å€‹è¨“ç·´è‰¯å¥½çš„ Value Network çš„è¼¸å‡ºåˆ†å¸ƒï¼š

```
        é »ç‡
          |
          |    *
          |   * *
          |  *   *
          | *     *
          |*       *
          +----+----+---- è¼¸å‡ºå€¼
         -1    0   +1

å¤§å¤šæ•¸è¼¸å‡ºé›†ä¸­åœ¨ -1 å’Œ +1 é™„è¿‘
ï¼ˆå› ç‚ºå¤§å¤šæ•¸å±€é¢æœ‰æ˜ç¢ºçš„å‹è² å‚¾å‘ï¼‰
```

### ä¸ç¢ºå®šçš„å±€é¢

ç•¶ Value Network è¼¸å‡ºæ¥è¿‘ 0 æ™‚ï¼Œè¡¨ç¤ºå±€é¢éå¸¸è¤‡é›œï¼Œå‹è² é›£æ–™ã€‚é€™äº›å±€é¢é€šå¸¸æ˜¯ï¼š
- å¤§å‹æˆ°é¬¥ä¸­
- é›™æ–¹å‹¢å‡åŠ›æ•µ
- å­˜åœ¨å¤šå€‹å¯èƒ½çš„è®ŠåŒ–

åœ¨ MCTS ä¸­ï¼Œé€™äº›ç¯€é»æœƒç²å¾—æ›´å¤šçš„æœç´¢è³‡æºï¼ˆå› ç‚ºä¸ç¢ºå®šæ€§é«˜ï¼‰ã€‚

---

## åœ¨ MCTS ä¸­çš„ä½œç”¨

### è‘‰ç¯€é»è©•ä¼°

Value Network åœ¨ MCTS çš„ **Evaluation** éšæ®µç™¼æ®é—œéµä½œç”¨ï¼š

```
MCTS æœç´¢æ¨¹ï¼š

        æ ¹ç¯€é» (ç•¶å‰å±€é¢)
           /    \
         A        B
        /  \    /  \
       A1  A2  B1  B2 â† è‘‰ç¯€é»
        â†“   â†“   â†“   â†“
       è©•ä¼°  è©•ä¼°  è©•ä¼°  è©•ä¼°
```

ç•¶ MCTS åˆ°é”ä¸€å€‹è‘‰ç¯€é»æ™‚ï¼Œéœ€è¦è©•ä¼°é€™å€‹å±€é¢çš„åƒ¹å€¼ã€‚æœ‰å…©ç¨®æ–¹æ³•ï¼š

1. **éš¨æ©Ÿæ¨¡æ“¬ï¼ˆRolloutï¼‰**ï¼šå¾è‘‰ç¯€é»éš¨æ©Ÿä¸‹åˆ°éŠæˆ²çµæŸ
2. **Value Network è©•ä¼°**ï¼šç›´æ¥ç”¨ç¥ç¶“ç¶²è·¯é æ¸¬

AlphaGo çµåˆäº†å…©è€…ï¼š

```
V(leaf) = (1-Î») Ã— V_network(leaf) + Î» Ã— V_rollout(leaf)
```

å…¶ä¸­ Î» = 0.5ï¼Œå³å„ä½”ä¸€åŠæ¬Šé‡ã€‚

#### ç‚ºä»€éº¼è¦çµåˆï¼Ÿ

- **Value Network** æ›´æº–ç¢ºï¼Œä½†å¯èƒ½æœ‰ç³»çµ±æ€§åå·®
- **éš¨æ©Ÿæ¨¡æ“¬** è¼ƒä¸æº–ç¢ºï¼Œä½†æä¾›ç¨ç«‹çš„ä¼°è¨ˆ
- çµåˆå…©è€…å¯ä»¥äº’è£œ

### AlphaGo Zero çš„ç°¡åŒ–

å¾Œä¾†çš„ AlphaGo Zero å®Œå…¨æ£„ç”¨äº†éš¨æ©Ÿæ¨¡æ“¬ï¼š

```
V(leaf) = V_network(leaf)
```

é€™å¤§å¤§ç°¡åŒ–äº†ç³»çµ±ï¼ŒåŒæ™‚æ£‹åŠ›æ›´å¼·ã€‚é€™è­‰æ˜äº† Value Network è¶³å¤ å¯é ï¼Œä¸éœ€è¦éš¨æ©Ÿæ¨¡æ“¬çš„ã€Œä¿éšªã€ã€‚

### å›æº¯æ›´æ–°

è©•ä¼°å®Œè‘‰ç¯€é»å¾Œï¼Œé€™å€‹å€¼æœƒæ²¿è·¯å¾‘å›æº¯æ›´æ–°ï¼š

```
v3 = V(leaf) = 0.6
      â†‘
A2 çš„ Q å€¼æ›´æ–°
      â†‘
A çš„ Q å€¼æ›´æ–°
      â†‘
æ ¹ç¯€é»çš„çµ±è¨ˆæ›´æ–°
```

æ¯å€‹ç¯€é»ç¶­è­·çš„ Q å€¼æ˜¯æ‰€æœ‰ç¶“éå®ƒçš„è‘‰ç¯€é»è©•ä¼°çš„å¹³å‡ï¼š

```
Q(s, a) = (1/N(s,a)) Ã— Î£ V(leaf)
```

---

## è¦–è¦ºåŒ–åˆ†æ

### åƒ¹å€¼æ›²é¢

æƒ³åƒä¸€å€‹ç°¡åŒ–çš„ 3Ã—3 æ£‹ç›¤ã€‚Value Network å­¸åˆ°çš„æ˜¯ä¸€å€‹ã€Œåƒ¹å€¼æ›²é¢ã€ï¼š

```
        ç™½å­ä½ç½®
       1   2   3
    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
  1 â”‚+0.3â”‚-0.1â”‚+0.2â”‚
é»‘  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
å­  2 â”‚-0.2â”‚+0.5â”‚-0.3â”‚
ä½  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
ç½®  3 â”‚+0.1â”‚-0.2â”‚+0.4â”‚
    â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
```

é€™å€‹æ›²é¢å‘Šè¨´æˆ‘å€‘æ¯å€‹ä½ç½®çµ„åˆçš„åƒ¹å€¼ã€‚æ­£å€¼æœ‰åˆ©æ–¼é»‘æ£‹ï¼Œè² å€¼æœ‰åˆ©æ–¼ç™½æ£‹ã€‚

### è¨“ç·´éç¨‹ä¸­çš„æ¼”è®Š

éš¨è‘—è¨“ç·´é€²è¡Œï¼ŒValue Network çš„é æ¸¬é€æ¼¸è®Šå¾—æ›´æº–ç¢ºï¼š

```
       é æ¸¬èª¤å·®
          |
     1.0  |*
          | *
     0.5  |  *
          |   *
     0.1  |    * * * * *
          +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¨“ç·´æ­¥æ•¸
          0   100K  500K  1M
```

èª¤å·®æœƒå¿«é€Ÿä¸‹é™ï¼Œç„¶å¾Œè¶¨æ–¼ç©©å®šã€‚

### å›°é›£å±€é¢çš„è­˜åˆ¥

Value Network å¯ä»¥å¹«åŠ©è­˜åˆ¥å›°é›£å±€é¢ï¼š

| è¼¸å‡º | å«ç¾© | æ‡‰å°ç­–ç•¥ |
|------|------|---------|
| æ¥è¿‘ +1 | å¤§å„ª | ç©©å¥ä¸‹æ³• |
| æ¥è¿‘ -1 | å¤§åŠ£ | å°‹æ‰¾ç¿»ç›¤æ©Ÿæœƒ |
| æ¥è¿‘ 0 | è¤‡é›œå±€é¢ | éœ€è¦æ·±å…¥è¨ˆç®— |

AlphaGo æœƒåœ¨æ¥è¿‘ 0 çš„å±€é¢æŠ•å…¥æ›´å¤šæ€è€ƒæ™‚é–“ã€‚

---

## å¯¦ä½œè¦é»

### PyTorch å¯¦ç¾

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ValueNetwork(nn.Module):
    def __init__(self, input_channels=49, num_filters=192, num_layers=12):
        super().__init__()

        # ç¬¬ä¸€å·ç©å±¤ï¼ˆ5Ã—5ï¼‰
        self.conv1 = nn.Conv2d(input_channels, num_filters,
                               kernel_size=5, padding=2)

        # ä¸­é–“å·ç©å±¤ï¼ˆ3Ã—3ï¼‰Ã—11
        self.conv_layers = nn.ModuleList([
            nn.Conv2d(num_filters, num_filters,
                     kernel_size=3, padding=1)
            for _ in range(num_layers - 1)
        ])

        # è¼¸å‡ºå·ç©å±¤
        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)

        # å…¨é€£æ¥å±¤
        self.fc1 = nn.Linear(361, 256)
        self.fc2 = nn.Linear(256, 1)

    def forward(self, x):
        # x: (batch, 49, 19, 19)

        # å·ç©å±¤
        x = F.relu(self.conv1(x))
        for conv in self.conv_layers:
            x = F.relu(conv(x))
        x = self.conv_out(x)

        # å±•å¹³
        x = x.view(x.size(0), -1)  # (batch, 361)

        # å…¨é€£æ¥å±¤
        x = F.relu(self.fc1(x))
        x = torch.tanh(self.fc2(x))

        return x.squeeze(-1)  # (batch,)
```

### è¨“ç·´å¾ªç’°

```python
def train_value_network(model, optimizer, states, outcomes):
    """
    states: (batch, 49, 19, 19) - æ£‹ç›¤ç‰¹å¾µ
    outcomes: (batch,) - éŠæˆ²çµæœ (+1 æˆ– -1)
    """
    # å‰å‘å‚³æ’­
    values = model(states)  # (batch,)

    # MSE æå¤±
    loss = F.mse_loss(values, outcomes)

    # åå‘å‚³æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # è¨ˆç®—æº–ç¢ºç‡ï¼ˆé æ¸¬æ­£ç¢ºçš„å‹è² ï¼‰
    predictions = (values > 0).float() * 2 - 1  # è½‰æ›ç‚º +1/-1
    accuracy = (predictions == outcomes).float().mean()

    return loss.item(), accuracy.item()
```

### é¿å…éæ“¬åˆçš„æŠ€å·§

```python
# 1. è³‡æ–™å¢å¼·ï¼ˆ8é‡å°ç¨±æ€§ï¼‰
def augment(state, outcome):
    augmented = []
    for rotation in [0, 90, 180, 270]:
        s = rotate(state, rotation)
        augmented.append((s, outcome))
        augmented.append((flip(s), outcome))
    return augmented

# 2. Dropout
class ValueNetworkWithDropout(ValueNetwork):
    def __init__(self, *args, dropout_rate=0.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        # ... å·ç©å±¤ ...
        x = self.dropout(x)  # åœ¨å…¨é€£æ¥å±¤å‰ dropout
        # ... å…¨é€£æ¥å±¤ ...

# 3. æ—©åœï¼ˆEarly Stoppingï¼‰
best_val_loss = float('inf')
patience = 10
counter = 0

for epoch in range(max_epochs):
    train_loss = train_one_epoch()
    val_loss = evaluate()

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        save_model()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping!")
            break
```

---

## èˆ‡ Policy Network çš„å”ä½œ

### äº’è£œé—œä¿‚

Policy Network å’Œ Value Network åœ¨ AlphaGo ä¸­äº’è£œï¼š

| ç¶²è·¯ | å›ç­”çš„å•é¡Œ | è¼¸å‡º | MCTS è§’è‰² |
|------|-----------|------|----------|
| Policy | ä¸‹ä¸€æ­¥ä¸‹å“ªè£¡ï¼Ÿ | æ©Ÿç‡åˆ†å¸ƒ | å¼•å°æœç´¢æ–¹å‘ |
| Value | é€™ç›¤æ£‹æœƒè´å—ï¼Ÿ | å–®ä¸€æ•¸å€¼ | è©•ä¼°è‘‰ç¯€é» |

### çµ±ä¸€çš„é›™é ­ç¶²è·¯

åœ¨ AlphaGo Zero ä¸­ï¼Œé€™å…©å€‹ç¶²è·¯è¢«åˆä½µæˆä¸€å€‹**é›™é ­ç¶²è·¯**ï¼š

```
       å…±äº«çš„ç‰¹å¾µæå–å±¤
              |
       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
       â†“              â†“
  Policy Head    Value Head
       â†“              â†“
  361å€‹æ©Ÿç‡       å–®ä¸€æ•¸å€¼
```

é€™ç¨®è¨­è¨ˆçš„å„ªé»ï¼š
- **åƒæ•¸å…±äº«**ï¼šæ¸›å°‘è¨ˆç®—é‡
- **ç‰¹å¾µå…±äº«**ï¼šPolicy å’Œ Value ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾µ
- **è¨“ç·´æ›´ç©©å®š**ï¼šå…©å€‹ç›®æ¨™äº’ç›¸æ­£å‰‡åŒ–

è©³è¦‹ [é›™é ­ç¶²è·¯èˆ‡æ®˜å·®ç¶²è·¯](../dual-head-resnet)ã€‚

---

## å‹•ç•«å°æ‡‰

æœ¬æ–‡æ¶‰åŠçš„æ ¸å¿ƒæ¦‚å¿µèˆ‡å‹•ç•«ç·¨è™Ÿï¼š

| ç·¨è™Ÿ | æ¦‚å¿µ | ç‰©ç†/æ•¸å­¸å°æ‡‰ |
|------|------|--------------|
| ğŸ¬ E2 | Value Network | å‹¢èƒ½é¢ |
| ğŸ¬ D4 | åƒ¹å€¼å‡½æ•¸ | æœŸæœ›å ±é…¬ |
| ğŸ¬ C6 | è‘‰ç¯€é»è©•ä¼° | å‡½æ•¸é€¼è¿‘ |
| ğŸ¬ H3 | æ™‚åºå·®åˆ† | å¼•å°å­¸ç¿’ |

---

## å»¶ä¼¸é–±è®€

- **ä¸Šä¸€ç¯‡**ï¼š[Policy Network è©³è§£](../policy-network) â€” ç­–ç•¥ç¶²è·¯å¦‚ä½•é¸æ“‡è‘—æ³•
- **ä¸‹ä¸€ç¯‡**ï¼š[è¼¸å…¥ç‰¹å¾µè¨­è¨ˆ](../input-features) â€” 48 å€‹ç‰¹å¾µå¹³é¢è©³è§£
- **é€²éšä¸»é¡Œ**ï¼š[MCTS èˆ‡ç¥ç¶“ç¶²è·¯çš„çµåˆ](../mcts-neural-combo) â€” å®Œæ•´çš„æœç´¢æµç¨‹

---

## é—œéµè¦é»

1. **Value Network é æ¸¬å‹ç‡**ï¼šè¼¸å‡º -1 åˆ° +1 ä¹‹é–“çš„å–®ä¸€æ•¸å€¼
2. **Tanh è¼¸å‡º**ï¼šç¢ºä¿è¼¸å‡ºåœ¨æ­£ç¢ºçš„ç¯„åœå…§
3. **MSE æå¤±**ï¼šå°‡é æ¸¬å€¼é€¼è¿‘å¯¦éš›çµæœ
4. **éæ“¬åˆæŒ‘æˆ°**ï¼šéœ€è¦ç”¨è‡ªæˆ‘å°å¼ˆè³‡æ–™ä¾†é¿å…
5. **å–ä»£éš¨æ©Ÿæ¨¡æ“¬**ï¼šä¸€æ¬¡è©•ä¼° â‰ˆ 15000 æ¬¡æ¨¡æ“¬

Value Network æ˜¯ AlphaGo çš„ã€Œåˆ¤æ–·åŠ›ã€â€”â€”å®ƒè®“ AI èƒ½å¤ è©•ä¼°ä»»ä½•å±€é¢çš„å¥½å£ï¼Œè€Œä¸éœ€è¦çª®ç›¡æ‰€æœ‰å¯èƒ½ã€‚

---

## åƒè€ƒè³‡æ–™

1. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529, 484-489.
2. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 551, 354-359.
3. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.
4. Tesauro, G. (1995). "Temporal difference learning and TD-Gammon." *Communications of the ACM*, 38(3), 58-68.
