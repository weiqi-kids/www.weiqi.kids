---
sidebar_position: 17
title: AlphaGo Zero 概述
description: 從零開始、完全自學，AlphaGo Zero 如何在沒有人類棋譜的情況下超越所有前代版本
keywords: [AlphaGo Zero, 自我對弈, 強化學習, 深度學習, 圍棋 AI, 無監督學習]
---

# AlphaGo Zero 概述

2017 年 10 月，DeepMind 發表了一個震驚 AI 界的成果：**AlphaGo Zero** 在沒有使用任何人類棋譜的情況下，從完全隨機的狀態開始訓練，僅僅三天就超越了擊敗李世乭的原版 AlphaGo，並以 **100:0** 的比分完勝。

這不只是數字上的進步。這代表一個全新的範式：**AI 不需要人類知識，可以從零發現一切**。

---

## 為什麼不需要人類棋譜？

### 人類棋譜的限制

原版 AlphaGo 的訓練過程分為兩個階段：

1. **監督學習**：用 3000 萬局人類棋譜訓練 Policy Network
2. **強化學習**：透過自我對弈進一步提升

這個方法有幾個根本性的問題：

#### 1. 人類棋譜有上限

人類棋手的棋力有極限，棋譜中包含的是人類的理解，也包含人類的錯誤和偏見。當 AI 從人類棋譜學習時，它學到的是：

- 人類認為好的下法（但不一定是最優的）
- 人類的思維模式（但可能限制創新）
- 人類的錯誤（會被當作正確的樣本學習）

#### 2. 監督學習的瓶頸

監督學習的目標是「模仿人類」——預測人類棋手會下哪一步。這意味著 AI 的能力上限被人類棋手的能力所限制。

就像一個學徒只能模仿師傅，永遠無法超越師傅一樣。

#### 3. 資料收集成本

高品質的人類棋譜需要多年累積，而且只存在於圍棋這類有悠久歷史的遊戲中。如果要將 AI 應用到新領域（如蛋白質結構預測），根本沒有「人類專家棋譜」可用。

### Zero 的突破

AlphaGo Zero 完全跳過監督學習階段，直接從**隨機初始化**開始自我對弈。這解決了上述所有問題：

| 問題 | 原版 AlphaGo | AlphaGo Zero |
|------|-------------|--------------|
| 人類知識上限 | 受限於棋譜品質 | 無此限制 |
| 學習目標 | 模仿人類 | 最大化勝率 |
| 資料需求 | 3000 萬局棋譜 | 0 |
| 可推廣性 | 僅限圍棋 | 可推廣至其他領域 |

這是一個根本性的範式轉變：從「學習人類知識」轉向「從第一性原理發現知識」。

---

## 與原版 AlphaGo 的對比：100:0

### 碾壓性的勝利

DeepMind 讓訓練完成的 AlphaGo Zero 與各個版本的 AlphaGo 對弈：

| 對手 | AlphaGo Zero 戰績 |
|------|-------------------|
| AlphaGo Fan（擊敗樊麾版本） | 100:0 |
| AlphaGo Lee（擊敗李世乭版本） | 100:0 |
| AlphaGo Master（60 連勝版本） | 89:11 |

**100:0**——這意味著在 100 盤比賽中，原版 AlphaGo 連一盤都贏不了。

### 更少的資源，更強的棋力

不只是贏，AlphaGo Zero 還用更少的資源達成更強的棋力：

| 指標 | AlphaGo Lee | AlphaGo Zero |
|------|-------------|--------------|
| 訓練時間 | 數月 | 40 天（3 天超越 AlphaGo Lee） |
| 訓練局數 | 3000 萬人類棋譜 + 自我對弈 | 490 萬局自我對弈 |
| TPU 數量（訓練） | 50+ | 4 |
| TPU 數量（推理） | 48 | 4 |
| 輸入特徵 | 48 個平面 | 17 個平面 |
| 神經網路 | SL + RL 雙網路 | 單一雙頭網路 |

這是一個驚人的效率提升：**資源減少 10 倍以上，棋力卻大幅提升**。

### 為什麼 Zero 更強？

AlphaGo Zero 更強的原因可以從幾個角度理解：

#### 1. 無偏見的學習

原版 AlphaGo 從人類棋譜學習，繼承了人類的偏見。例如，人類棋手可能過度重視某些定式，或對某些局面有錯誤的評估。

AlphaGo Zero 沒有這些包袱。它從白紙開始，只透過勝負結果來學習什麼是好棋。這讓它能夠發現人類從未想過的下法。

#### 2. 一致的學習目標

原版 AlphaGo 的訓練有兩個不同的目標：
- 監督學習：最大化對人類落子的預測準確率
- 強化學習：最大化勝率

這兩個目標可能互相衝突。AlphaGo Zero 只有一個目標：**勝率最大化**。這讓學習過程更加一致和有效。

#### 3. 更簡潔的架構

原版 AlphaGo 使用分離的 Policy Network 和 Value Network。AlphaGo Zero 使用單一的雙頭網路（詳見下一篇），讓特徵表示能夠被共享，提高了學習效率。

---

## 簡化的輸入特徵：從 48 到 17

### 原版 AlphaGo 的 48 個特徵平面

原版 AlphaGo 的神經網路輸入包含 48 個 19x19 的特徵平面，編碼了大量人類設計的特徵：

| 類別 | 特徵數 | 內容 |
|------|--------|------|
| 棋子位置 | 3 | 黑子、白子、空點 |
| 氣數 | 8 | 1-8 氣的棋串 |
| 提子 | 8 | 能提 1-8 顆子 |
| 打劫 | 1 | 劫爭位置 |
| 邊線距離 | 4 | 一線到四線 |
| 落子合法性 | 1 | 哪些位置可以下 |
| 歷史狀態 | 8 | 過去 8 手的位置 |
| 輪次 | 1 | 黑方或白方 |
| 其他 | 14 | 征子、眼位等 |

這 48 個特徵是圍棋專家精心設計的，包含了大量領域知識。

### AlphaGo Zero 的 17 個特徵平面

AlphaGo Zero 大幅簡化了輸入，只使用 17 個特徵平面：

| 平面編號 | 內容 | 數量 |
|----------|------|------|
| 1-8 | 黑子位置（最近 8 步） | 8 |
| 9-16 | 白子位置（最近 8 步） | 8 |
| 17 | 當前輪次（全 1 或全 0） | 1 |

這 17 個特徵只包含：
- **當前棋盤狀態**：每個位置有黑子、白子或空
- **歷史資訊**：過去 8 步的棋盤狀態
- **輪次資訊**：輪到誰下

沒有氣數、沒有征子判斷、沒有邊線距離——所有這些「圍棋知識」都讓神經網路自己學習。

### 為什麼簡化是好的？

#### 1. 讓網路自己發現特徵

複雜的手工特徵可能遺漏重要資訊，或編碼錯誤的假設。讓神經網路從原始資料學習，它可能發現更好的特徵表示。

事實證明，AlphaGo Zero 學會了人類設計的所有特徵（氣數、征子等），還學到了一些人類沒有明確意識到的模式。

#### 2. 更好的可推廣性

48 個特徵中的許多是圍棋專用的（如征子、邊線距離）。17 個簡化特徵則是通用的——任何棋盤遊戲都可以用類似的方式編碼。

這為後來的 **AlphaZero**（通用遊戲 AI）奠定了基礎。

#### 3. 減少人為錯誤

手工設計的特徵可能包含錯誤或不完整的定義。簡化輸入消除了這類問題的可能性。

---

## 單一網路架構

### 原版的雙網路設計

原版 AlphaGo 使用兩個獨立的神經網路：

```
Policy Network:  輸入 → CNN → 19x19 落子機率
Value Network:   輸入 → CNN → 勝率評估（-1 到 1）
```

這兩個網路：
- 有不同的架構（層數、通道數略有不同）
- 獨立訓練（先訓練 Policy，再訓練 Value）
- 不共享任何參數

### Zero 的雙頭網路

AlphaGo Zero 使用單一網路，但有兩個輸出頭（heads）：

```
輸入 → ResNet 共享主幹 → Policy Head → 19x19 落子機率
                       → Value Head  → 勝率評估
```

兩個 Head 共享同一個 ResNet 主幹（詳見[下一篇：雙頭網路與殘差網路](../dual-head-resnet)），這帶來幾個好處：

#### 1. 參數效率

共享主幹意味著大部分參數被兩個任務共用。這減少了總參數量，降低了過擬合風險。

#### 2. 特徵共享

「應該下哪裡」（Policy）和「誰會贏」（Value）需要理解類似的棋盤模式。共享主幹讓這些特徵能被兩個任務同時學習和利用。

#### 3. 訓練穩定性

聯合訓練讓梯度訊號來自兩個來源，提供了更豐富的監督訊號，讓訓練更加穩定。

### 殘差網路的威力

AlphaGo Zero 的主幹使用 **40 層殘差網路（ResNet）**，比原版 AlphaGo 的 13 層 CNN 深得多。

殘差連接（skip connections）讓深層網路得以有效訓練，避免了梯度消失問題。這是 2015 年 ImageNet 競賽的突破性技術，被 AlphaGo Zero 成功應用到圍棋領域。

---

## 訓練效率的提升

### 自我對弈的指數增長

AlphaGo Zero 的訓練過程展示了令人驚嘆的效率：

| 訓練時間 | ELO 評分 | 相當於 |
|----------|----------|--------|
| 0 小時 | 0 | 隨機亂下 |
| 3 小時 | ~1000 | 發現基本規則 |
| 12 小時 | ~3000 | 發現定式 |
| 36 小時 | ~4500 | 超越樊麾版 |
| 60 小時 | ~5200 | 超越李世乭版 |
| 72 小時 | ~5400 | 超越原版 AlphaGo |
| 40 天 | ~5600 | 最強版本 |

**三天超越人類、三天超越之前花費數月訓練的 AI**——這是指數級的效率提升。

### 為什麼這麼快？

#### 1. 更強的搜索引導

AlphaGo Zero 的 MCTS 完全由神經網路引導，不再使用快速走子策略（rollout）。這讓搜索更加高效和準確。

#### 2. 更快的自我對弈

由於只需要一個網路（而非兩個），每局自我對弈的計算成本降低。這意味著在相同時間內可以產生更多訓練資料。

#### 3. 更有效的學習

雙頭網路的聯合訓練讓每一局棋的資訊被更有效地利用。Policy 和 Value 的梯度相互強化，加速了收斂。

### 與人類學習的對比

人類棋手需要多長時間達到不同水平？

| 水平 | 人類所需時間 | AlphaGo Zero |
|------|-------------|--------------|
| 入門 | 數週 | 幾分鐘 |
| 業餘初段 | 數年 | 數小時 |
| 職業水平 | 10-20 年 | 1-2 天 |
| 世界冠軍 | 20+ 年全職投入 | 3 天 |
| 超越人類 | 不可能 | 3 天 |

這個對比不是要貶低人類棋手——他們用的是生物神經元，而 AlphaGo Zero 用的是專門設計的 TPU 和幾千瓦的電力。但它確實展示了正確的學習方法可以多麼高效。

---

## 通用性：西洋棋、將棋

### AlphaZero 的誕生

2017 年 12 月，DeepMind 發表了 **AlphaZero**——AlphaGo Zero 的通用版本。同一套演算法，只需修改遊戲規則，就能在三種棋類遊戲中達到世界頂級水平：

| 遊戲 | 訓練時間 | 對手 | 戰績 |
|------|----------|------|------|
| 圍棋 | 8 小時 | AlphaGo Zero | 60:40 |
| 西洋棋 | 4 小時 | Stockfish 8 | 28 勝 72 和 0 負 |
| 將棋 | 2 小時 | Elmo | 90:8:2 |

注意這裡的對手：
- **Stockfish** 是當時最強的西洋棋引擎，使用幾十年人類知識和優化
- **Elmo** 是當時最強的將棋 AI

AlphaZero 用幾小時訓練，就超越了這些耗費多年開發的專用系統。

### 通用性的意義

AlphaGo Zero / AlphaZero 證明了一件重要的事：

> **同一套學習演算法，可以在不同領域達到超人水平。**

這不是三個不同的 AI，而是一個通用的學習框架：

1. **自我對弈**產生經驗
2. **蒙地卡羅樹搜索**探索可能性
3. **神經網路**學習策略和價值函數
4. **強化學習**優化目標函數

這個框架不依賴領域特定的知識，這為 AI 的通用化邁出了重要一步。

### 對傳統 AI 的衝擊

在 AlphaZero 之前，西洋棋和將棋的最強 AI 都是「專家系統」風格的：

- **大量人類知識**：開局庫、殘局庫、評估函數
- **數十年優化**：無數棋手和工程師的心血
- **極度專業化**：Stockfish 不能下圍棋，Elmo 不能下西洋棋

AlphaZero 用一個通用演算法在幾小時內超越了這一切。這讓許多 AI 研究者重新思考：

> 我們應該投入更多精力在「通用學習演算法」，還是「專家知識編碼」？

答案似乎越來越清楚：讓機器自己學習，比教它知識更有效。

---

## AlphaGo Zero 的下棋風格

### 超越人類的審美

圍棋界對 AlphaGo Zero 的下法有一個普遍評價：**更加優美**。

AlphaGo Lee 的下法有時顯得「怪異」——像第 37 手那樣的落子，人類需要事後分析才能理解其妙處。但 AlphaGo Zero 的下法常常在事後被評價為「一眼就知道是好棋」。

這可能是因為：

1. **更強的棋力**：Zero 能看得更深，落子更加從容
2. **無人類偏見**：不受傳統定式的束縛
3. **一致的目標**：只追求勝率，不模仿人類

### 重新發現人類棋理

有趣的是，AlphaGo Zero 在訓練過程中「重新發現」了人類數千年累積的圍棋知識：

- **定式**：Zero 自己發現了許多常見定式，因為這些確實是雙方最優解
- **佈局原則**：角、邊、中央的重要性順序
- **棋形知識**：愚形與好形的區別

這驗證了人類棋理的合理性——這些知識不是偶然的，而是圍棋本質的反映。

### 超越人類的創新

但 Zero 也發現了人類從未想過的下法：

- **非常規開局**：在傳統開局基礎上的變化
- **激進的棄子**：比人類更願意放棄局部換取全局優勢
- **反直覺的形狀**：表面上的「壞形」其實是最優解

這些創新正在改變人類對圍棋的理解。許多職業棋手表示，研究 AlphaGo Zero 的棋譜讓他們對圍棋有了全新的認識。

---

## 技術細節總結

### 與原版 AlphaGo 的完整對比

| 方面 | AlphaGo（原版） | AlphaGo Zero |
|------|----------------|--------------|
| **訓練資料** | 人類棋譜 + 自我對弈 | 純自我對弈 |
| **學習方法** | 監督學習 + 強化學習 | 純強化學習 |
| **輸入特徵** | 48 個平面 | 17 個平面 |
| **網路架構** | 分離的 Policy/Value | 雙頭 ResNet |
| **網路深度** | 13 層 | 40 層（或更多） |
| **MCTS 評估** | 神經網路 + Rollout | 純神經網路 |
| **搜索次數** | 每步 ~100,000 | 每步 ~1,600 |
| **訓練 TPU** | 50+ | 4 |
| **推理 TPU** | 48 | 4（可擴展） |

### 核心算法

AlphaGo Zero 的訓練循環非常簡潔：

```
1. 自我對弈
   - 用當前網路進行 MCTS
   - 按 MCTS 搜索機率選擇落子
   - 記錄每一步的 (局面, MCTS機率, 勝負結果)

2. 訓練網路
   - 從經驗池中取樣
   - Policy Head：最小化與 MCTS 機率的交叉熵
   - Value Head：最小化與實際勝負的均方誤差
   - 聯合優化兩個目標

3. 更新網路
   - 用新網路替換舊網路（通過對弈驗證新網路更強）
   - 回到步驟 1
```

這個循環持續運行，網路不斷變強。沒有人類數據、沒有人類知識，只有遊戲規則和勝負目標。

---

## 對 AI 研究的啟示

### 第一性原理學習

AlphaGo Zero 展示了一種「第一性原理」的學習方法：

> 不要告訴 AI 怎麼做，只告訴它目標是什麼，讓它自己發現方法。

這與傳統的專家系統方法形成鮮明對比。專家系統試圖將人類知識編碼進 AI，而 AlphaGo Zero 讓 AI 自己發現知識。

結果是：AI 發現的知識可能比人類知識更完整、更準確。

### 自我對弈的威力

AlphaGo Zero 證明了自我對弈可以產生無限的訓練資料，而且這些資料的品質會隨著網路的提升而提升。

這是一個「正向循環」：
- 更強的網路 → 更好的自我對弈資料
- 更好的資料 → 更強的網路

這個循環可以持續運行，直到達到遊戲的理論上限（如果存在的話）。

### 簡化的重要性

AlphaGo Zero 的成功證明了「簡化」的重要性：

- 簡化輸入（48 → 17）
- 簡化架構（雙網路 → 單網路）
- 簡化訓練（監督 + 強化 → 純強化）

每一次簡化都讓系統更加強大。這告訴我們：複雜不等於好，最簡單的解決方案往往是最好的。

---

## 動畫對應

本文涉及的核心概念與動畫編號：

| 編號 | 概念 | 物理/數學對應 |
|------|------|--------------|
| 🎬 E7 | 從零開始訓練 | 自組織現象 |
| 🎬 E5 | 自我對弈 | 不動點收斂 |
| 🎬 E12 | 棋力成長曲線 | S 型增長 |
| 🎬 D12 | 殘差網路 | 梯度高速公路 |

---

## 延伸閱讀

- **下一篇**：[雙頭網路與殘差網路](../dual-head-resnet) — 詳解 AlphaGo Zero 的神經網路架構
- **相關文章**：[自我對弈](../self-play) — 為什麼自我對弈能產生超人水平
- **技術深入**：[從零訓練的過程](../training-from-scratch) — Day 0-3 的詳細演進

---

## 參考資料

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." *Science*, 362(6419), 1140-1144.
3. DeepMind. (2017). "AlphaGo Zero: Starting from scratch." *DeepMind Blog*.
4. Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." *Nature*, 588, 604-609.
