---
sidebar_position: 18
title: 雙頭網路與殘差網路
description: 深入解析 AlphaGo Zero 的神經網路架構 - 共享主幹、Policy Head、Value Head 與 40 層 ResNet
keywords: [雙頭網路, 殘差網路, ResNet, Policy Head, Value Head, 深度學習, 神經網路架構]
---

import { ArticleSchema, KeyTakeaway, FAQ } from '@site/src/components/SEO';

<ArticleSchema
  title="雙頭網路與殘差網路"
  description="深入解析 AlphaGo Zero 的神經網路架構 - 共享主幹、Policy Head、Value Head 與 40 層 ResNet"
  slug="for-engineers/how-it-works/alphago-explained/dual-head-resnet"
  datePublished="2024-01-15"
  dateModified="2024-02-22"
  section="AlphaGo 完整解析"
  keywords={["雙頭網路", "殘差網路", "ResNet", "Policy Head", "Value Head", "深度學習", "多任務學習", "卷積神經網路"]}
  wordCount={4500}
/>

# 雙頭網路與殘差網路

AlphaGo Zero 最重要的架構創新之一，是使用**雙頭網路**（Dual-Head Network）取代原版 AlphaGo 的雙網路設計。這個看似簡單的改變，卻帶來了顯著的效能提升和更優雅的學習過程。

本文將深入解析這個架構的設計原理、數學基礎，以及為什麼它如此有效。

---

## 雙頭網路設計

### 整體架構

AlphaGo Zero 的神經網路可以分為三個部分：

```
輸入（17 × 19 × 19）
       ↓
┌─────────────────────────────────────────┐
│            共享主幹（ResNet）              │
│        40 個殘差塊，256 通道              │
└─────────────────────────────────────────┘
       ↓                    ↓
┌─────────────┐      ┌─────────────┐
│  Policy Head │      │  Value Head  │
│  (策略頭)    │      │  (價值頭)    │
└─────────────┘      └─────────────┘
       ↓                    ↓
  19×19 機率分布          勝率 [-1, 1]
  + 1 Pass 機率
```

讓我們逐一解析每個部分。

### 共享主幹（Shared Backbone）

共享主幹是一個深層的**殘差網路（ResNet）**，負責從棋盤狀態中提取特徵。

#### 架構細節

| 組件 | 規格 |
|------|------|
| 輸入層 | 3×3 卷積，256 通道 |
| 殘差塊 | 40 個（或 20 個精簡版） |
| 每個殘差塊 | 2 層 3×3 卷積，256 通道 |
| 激活函數 | ReLU |
| 正規化 | Batch Normalization |

#### 數學表示

設輸入為 x（維度 17 x 19 x 19），共享主幹的輸出為：

```
f(x) = ResNet_40(Conv_3x3(x))
```

其中 f(x)（維度 256 x 19 x 19）是高維特徵表示。

### Policy Head（策略頭）

Policy Head 負責預測每個位置的落子機率。

#### 架構細節

```
共享主幹輸出（256 × 19 × 19）
       ↓
1×1 卷積（2 通道）
       ↓
Batch Normalization
       ↓
ReLU
       ↓
展平（2 × 19 × 19 = 722）
       ↓
全連接層（362）
       ↓
Softmax
       ↓
輸出：362 個機率（361 個位置 + Pass）
```

#### 數學表示

```
π = Softmax(FC(Flatten(ReLU(BN(Conv_1x1(f(x)))))))
```

輸出 π 是一個 362 維向量，滿足所有元素非負且和為 1。

### Value Head（價值頭）

Value Head 負責預測當前局面的勝率。

#### 架構細節

```
共享主幹輸出（256 × 19 × 19）
       ↓
1×1 卷積（1 通道）
       ↓
Batch Normalization
       ↓
ReLU
       ↓
展平（1 × 19 × 19 = 361）
       ↓
全連接層（256）
       ↓
ReLU
       ↓
全連接層（1）
       ↓
Tanh
       ↓
輸出：勝率 [-1, 1]
```

#### 數學表示

```
v = Tanh(FC_1(ReLU(FC_2(Flatten(ReLU(BN(Conv_1x1(f(x)))))))))
```

輸出 v 在 [-1, 1] 範圍內：
- v = 1：當前方必勝
- v = -1：當前方必敗
- v = 0：勢均力敵

---

## 為什麼要共享主幹？

### 直覺理解

「下一步應該下哪裡」（Policy）和「誰會贏」（Value）這兩個問題，其實需要理解相同的棋盤模式：

- **棋形**：哪些形狀是好的，哪些是壞的
- **勢力**：哪邊更大，哪些地方還有空間
- **死活**：哪些棋已經活了，哪些還在打劫
- **戰鬥**：哪裡有攻殺，局部勝負如何

如果用兩個獨立的網路，這些特徵需要學習兩次。共享主幹讓這些底層特徵只需學習一次，兩個任務都能使用。

### 多任務學習視角

從機器學習的角度，這是一種**多任務學習（Multi-task Learning）**：

```
L = L_policy + L_value
```

兩個任務共享底層表示，這帶來幾個好處：

#### 1. 正則化效果

共享參數相當於隱式的正則化。如果一個特徵只對 Policy 有用而對 Value 無用（或反之），它更難被過度放大。

有效參數量小於兩個獨立網路的參數量。

#### 2. 資料效率

每一局棋同時產生 Policy 標籤（MCTS 搜索機率）和 Value 標籤（最終勝負）。共享主幹讓兩個標籤都用於訓練共享特徵，提高了資料利用效率。

#### 3. 梯度訊號豐富

兩個任務的梯度都會流向共享主幹：

```
∂L/∂θ_shared = ∂L_policy/∂θ_shared + ∂L_value/∂θ_shared
```

這提供了更豐富的監督訊號，讓共享特徵更加穩健。

### 實驗證據

DeepMind 的消融實驗顯示，雙頭網路的表現顯著優於分離的雙網路：

| 配置 | ELO 評分 | 相對差距 |
|------|----------|----------|
| 分離的 Policy + Value 網路 | 基準 | - |
| 雙頭網路（共享主幹） | +300 ELO | ~65% 勝率差距 |

300 ELO 的差距意味著雙頭網路對分離網路有約 65% 的勝率。這是一個顯著的提升。

---

## 殘差網路原理

### 深度網路的困境

在 ResNet 發明之前，深層神經網路面臨一個悖論：

> 理論上，更深的網路應該至少和淺層網路一樣好（最差情況下，額外的層可以學習恆等映射）。但實際上，更深的網路往往表現更差。

這就是**退化問題（Degradation Problem）**：

- 訓練誤差隨深度增加而增加（不是過擬合，是優化困難）
- 梯度在反向傳播時逐漸消失（Vanishing Gradient）
- 深層的參數幾乎無法被有效更新

### 殘差塊的設計

何愷明等人在 2015 年提出了一個簡潔而優雅的解決方案：**殘差連接（Skip Connection）**。

```
輸入 x
   ↓
┌─────────────┐
│  卷積層     │
│  BN + ReLU  │
│  卷積層     │
│  BN        │
└─────────────┘
   ↓ F(x)
   ↓←────────────── x（跳躍連接）
   +
   ↓
 ReLU
   ↓
輸出 x + F(x)
```

#### 數學表示

傳統網路：學習目標映射 H(x)

```
y = H(x)
```

殘差網路：學習**殘差映射** F(x) = H(x) - x

```
y = F(x) + x
```

### 為什麼殘差連接有效？

#### 1. 梯度高速公路

考慮反向傳播的梯度：

```
∂L/∂x = ∂L/∂y × ∂y/∂x = ∂L/∂y × (1 + ∂F(x)/∂x)
```

關鍵在於那個 **+1**。即使 ∂F(x)/∂x 很小或為零，梯度仍然可以透過 +1 直接傳回去。

這就像修了一條「梯度高速公路」，讓梯度可以暢通無阻地從輸出層傳回輸入層。

#### 2. 恆等映射更容易學習

如果最優解接近恆等映射（H(x) 約等於 x），那麼：
- 傳統網路：需要學習 H(x) = x，可能很難
- 殘差網路：只需學習 F(x) 約等於 0，相對容易

將權重初始化為零或接近零，殘差塊就自然趨向恆等映射。

#### 3. 集成效應

深層 ResNet 可以視為許多淺層網路的**隱式集成**。如果有 n 個殘差塊，資訊可以透過 2^n 種不同的路徑流動。

這種集成效應增加了模型的穩健性。

### ResNet 在 ImageNet 上的突破

ResNet 在 2015 年 ImageNet 競賽中取得了驚人的成績：

| 深度 | Top-5 錯誤率 |
|------|-------------|
| VGG-19（無殘差） | 7.3% |
| ResNet-34 | 5.7% |
| ResNet-152 | 4.5% |
| 人類水平 | ~5.1% |

**152 層**的 ResNet 不僅可以訓練，還比 19 層的 VGG 好得多。這證明了殘差連接確實解決了深度網路的訓練問題。

---

## AlphaGo Zero 的 40 層 ResNet

### 為什麼選擇 40 層？

DeepMind 測試了不同深度的 ResNet：

| 殘差塊數量 | 總層數 | ELO 評分 |
|------------|--------|----------|
| 5 | 11 | 基準 |
| 10 | 21 | +200 |
| 20 | 41 | +400 |
| 40 | 81 | +500 |

更深的網路確實更強，但邊際效益遞減。AlphaGo Zero 使用 20 或 40 個殘差塊：

- **AlphaGo Zero（論文版）**：40 個殘差塊，256 通道
- **精簡版**：20 個殘差塊，256 通道

40 層的配置在棋力和訓練成本之間取得了良好的平衡。

### 具體配置

AlphaGo Zero 的 ResNet 配置如下：

```
輸入：17 × 19 × 19
↓
卷積層：3×3, 256 通道, BN, ReLU
↓
殘差塊 ×40：
  ├─ 卷積層：3×3, 256 通道, BN, ReLU
  ├─ 卷積層：3×3, 256 通道, BN
  └─ 跳躍連接 + ReLU
↓
Policy Head / Value Head
```

#### 參數量估計

| 組件 | 參數量（約） |
|------|-------------|
| 輸入卷積 | 17 × 3 × 3 × 256 ≈ 39K |
| 每個殘差塊 | 2 × 256 × 3 × 3 × 256 ≈ 1.2M |
| 40 個殘差塊 | 40 × 1.2M ≈ 47M |
| Policy Head | ~1M |
| Value Head | ~0.2M |
| **總計** | **~48M** |

約 4800 萬參數，以現代標準來看是中等規模的神經網路。

### Batch Normalization 的作用

每個卷積層之後都有 **Batch Normalization（BN）**，這對訓練穩定性至關重要：

#### 1. 正規化啟動值

BN 將每一層的啟動值正規化到均值為 0、方差為 1：

```
x_hat = (x - μ_B) / sqrt(σ_B² + ε)
y = γ × x_hat + β
```

其中 γ 和 β 是可學習的參數。

#### 2. 緩解內部協變量偏移

深層網路中，每一層的輸入分布會隨著前面層的參數更新而改變。BN 讓每一層的輸入分布保持穩定，加速訓練收斂。

#### 3. 正則化效果

BN 在訓練時使用 mini-batch 的統計量，引入了隨機性，有輕微的正則化效果。

---

## 與其他架構的比較

### vs. 原版 AlphaGo 的 CNN

| 特性 | AlphaGo 原版 | AlphaGo Zero |
|------|-------------|--------------|
| 架構類型 | 標準 CNN | ResNet |
| 深度 | 13 層 | 41-81 層 |
| 殘差連接 | 無 | 有 |
| 網路數量 | 2（分離） | 1（共享） |
| BN | 無 | 有 |

### vs. VGG 風格網路

VGG 是 2014 年 ImageNet 亞軍的架構，使用堆疊的 3×3 卷積：

| 特性 | VGG | ResNet |
|------|-----|--------|
| 最大可訓練深度 | ~19 層 | 152+ 層 |
| 梯度流動 | 逐層遞減 | 有高速公路 |
| 訓練難度 | 深層困難 | 深層可訓練 |

### vs. Inception / GoogLeNet

Inception 使用多尺度卷積並行：

| 特性 | Inception | ResNet |
|------|-----------|--------|
| 特點 | 多尺度特徵 | 深度堆疊 |
| 複雜度 | 較高 | 簡潔 |
| 圍棋適用性 | 一般 | 優秀 |

ResNet 的簡潔設計更適合圍棋這種需要深層推理的任務。

### vs. Transformer

2017 年提出的 Transformer 架構在 NLP 領域取得了巨大成功。有人嘗試將 Transformer 應用於圍棋：

| 特性 | ResNet | Transformer |
|------|--------|-------------|
| 歸納偏置 | 局部性（卷積） | 全局注意力 |
| 位置編碼 | 隱式（卷積） | 顯式 |
| 圍棋表現 | 優秀 | 可行但不優於 ResNet |
| 計算效率 | 較高 | 較低（O(n²)） |

對於圍棋這種有明顯空間結構的問題，CNN/ResNet 的歸納偏置更加合適。

---

## 設計選擇的深入分析

### 為什麼用 3×3 卷積？

AlphaGo Zero 全程使用 3×3 卷積，而非更大的卷積核：

1. **參數效率**：兩個 3×3 卷積的感受野等於一個 5×5，但參數量更少（18 vs 25）
2. **更深的網路**：相同參數量下，可以堆疊更多層
3. **更多非線性**：每層之間有 ReLU，增加表達能力

### 為什麼用 256 通道？

256 通道是一個經驗性的選擇：

- **太少**（如 64）：表達能力不足，無法捕捉複雜模式
- **太多**（如 512）：參數量翻倍，訓練成本大增，但棋力提升有限

後來的 KataGo 實驗顯示，通道數可以根據訓練資源調整：
- 低資源：128 通道，20 塊
- 高資源：256 通道，40 塊
- 更高資源：384 通道，60 塊

### 為什麼 Policy Head 用 Softmax、Value Head 用 Tanh？

#### Policy Head：Softmax

落子是一個**分類問題**——361 個位置（加 Pass）中選擇一個。Softmax 輸出滿足：
- 所有機率非負：π_i >= 0
- 機率和為 1：Σπ_i = 1

這與機率分布的定義一致。

#### Value Head：Tanh

勝率是一個**回歸問題**——預測一個連續值。Tanh 輸出範圍是 [-1, 1]：
- 有界：不會產生極端值
- 對稱：勝和負對稱處理
- 可微：方便梯度計算

使用 Tanh 而非無界輸出（如線性層）可以防止訓練不穩定。

---

## 訓練細節

### 損失函數

AlphaGo Zero 的總損失是三項之和：

```
L = L_policy + L_value + L_reg
```

#### Policy Loss

使用**交叉熵損失**，讓網路輸出逼近 MCTS 搜索機率：

```
L_policy = -Σ π_MCTS(a) × log(π_net(a))
```

其中：
- π_MCTS(a) 是 MCTS 對動作 a 的搜索機率
- π_net(a) 是網路輸出的機率

#### Value Loss

使用**均方誤差（MSE）**，讓網路輸出逼近實際勝負：

```
L_value = (v_net - z)²
```

其中：
- v_net 是網路預測的勝率
- z 是實際比賽結果（+1 或 -1）

#### Regularization Loss

使用 **L2 正則化**防止過擬合：

```
L_reg = c × ||θ||²
```

其中 c 是正則化係數，θ 是網路參數。

### 優化器配置

| 參數 | 值 |
|------|-----|
| 優化器 | SGD + Momentum |
| 動量 | 0.9 |
| 初始學習率 | 0.01 |
| 學習率衰減 | 每 X 步減半 |
| Batch Size | 32 × 2048 = 64K（分散式）|
| L2 正則化係數 | 1e-4 |

### 資料增強

圍棋棋盤有 8 重對稱性（4 次旋轉 × 2 次翻轉）。訓練時，每個局面可以產生 8 個等價的訓練樣本。

這讓有效訓練資料增加 8 倍，且不需要額外的自我對弈。

---

## 實作考量

### 記憶體優化

40 層 ResNet 的訓練需要大量記憶體：
- **前向傳播**：需要儲存每層的啟動值（用於反向傳播）
- **反向傳播**：需要儲存梯度

優化策略：
1. **梯度檢查點（Gradient Checkpointing）**：只儲存部分啟動值，需要時重新計算
2. **混合精度訓練**：使用 FP16 減少記憶體佔用
3. **分散式訓練**：將 batch 分散到多個 GPU/TPU

### 推理優化

推理時不需要 BN 的 mini-batch 統計量，可以使用訓練時累積的移動平均：

```
x_hat = (x - μ_moving) / sqrt(σ_moving² + ε)
```

這讓推理速度更快且結果確定性。

### 量化與壓縮

部署時可以進一步壓縮網路：
- **權重量化**：FP32 → INT8，記憶體減少 4 倍
- **剪枝**：移除小權重連接
- **知識蒸餾**：用大網路訓練小網路

---

## 動畫對應

本文涉及的核心概念與動畫編號：

| 編號 | 概念 | 物理/數學對應 |
|------|------|--------------|
| 🎬 E3 | 雙頭網路 | 多任務學習 |
| 🎬 D12 | 殘差連接 | 梯度高速公路 |
| 🎬 D8 | 卷積神經網路 | 局部感受野 |
| 🎬 D10 | Batch Normalization | 分布正規化 |

---

## 延伸閱讀

- **上一篇**：[AlphaGo Zero 概述](../alphago-zero) — 為什麼不需要人類棋譜
- **下一篇**：[從零訓練的過程](../training-from-scratch) — Day 0-3 的詳細演進
- **技術深入**：[CNN 與圍棋的結合](../cnn-and-go) — 為什麼 CNN 適合棋盤

---

## 參考資料

1. Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." *Nature*, 550, 354-359.
2. He, K., et al. (2016). "Deep Residual Learning for Image Recognition." *CVPR 2016*.
3. Ioffe, S., & Szegedy, C. (2015). "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift." *ICML 2015*.
4. Caruana, R. (1997). "Multitask Learning." *Machine Learning*, 28(1), 41-75.
5. Veit, A., et al. (2016). "Residual Networks Behave Like Ensembles of Relatively Shallow Networks." *NeurIPS 2016*.

---

<KeyTakeaway>
本文重點：
- 雙頭網路：共享 40 層 ResNet 主幹，分出 Policy Head（預測落子機率）和 Value Head（預測勝率），比分離網路提升約 300 ELO
- 殘差連接的關鍵：透過 skip connection 形成「梯度高速公路」，讓 40+ 層深度網路得以有效訓練
- 設計選擇：全程使用 3x3 卷積、256 通道、Batch Normalization，總參數約 4800 萬
</KeyTakeaway>

<FAQ items={[
  { question: "為什麼雙頭網路比分離的兩個網路更好？", answer: "「下一步下哪裡」（Policy）和「誰會贏」（Value）需要理解相同的棋盤模式。共享主幹讓這些底層特徵只需學習一次，兩個任務都能使用。實驗顯示雙頭網路比分離網路提升約 300 ELO，相當於 65% 的勝率差距。" },
  { question: "殘差連接（Skip Connection）如何解決深度網路訓練問題？", answer: "殘差連接讓輸出 = F(x) + x，反向傳播時梯度變成 ∂L/∂y × (1 + ∂F/∂x)。關鍵在於那個 +1，即使 ∂F/∂x 很小，梯度仍能直接傳回，形成「梯度高速公路」。這讓 152 層的 ResNet 都能有效訓練。" },
  { question: "AlphaGo Zero 網路的訓練損失函數是什麼？", answer: "總損失 = Policy Loss + Value Loss + L2 正則化。Policy Loss 使用交叉熵，讓網路輸出逼近 MCTS 搜索機率；Value Loss 使用均方誤差，讓網路輸出逼近實際勝負結果（+1 或 -1）；L2 正則化防止過擬合。" }
]} />
