---
sidebar_position: 1
title: AlphaGo 完整解析
description: 從歷史背景到技術細節，20 篇文章帶你徹底理解 AlphaGo
---

import { PolicyHeatmap, EloChart } from '@site/src/components/D3Charts';

# AlphaGo 完整解析

2016 年 3 月，AlphaGo 以 4:1 擊敗世界冠軍李世乭，震撼全球。這不只是一場圍棋比賽的勝利，更標誌著人工智慧的重大突破。

本系列 **20 篇深度文章**，將帶你從歷史背景、技術原理、到實作細節，完整理解 AlphaGo 的一切。

---

## 系列導覽

### 模組 1：歷史與突破

| 文章 | 說明 |
|------|------|
| [AlphaGo 的誕生](./birth-of-alphago) | DeepMind 創立、Google 收購、團隊組成 |
| [關鍵對局回顧](./key-matches) | 樊麾、李世乭、柯潔、Master 60連勝 |
| [「神之一手」深度分析](./move-37) | 第 37 手的棋理與 AI 視角解讀 |

### 模組 2：圍棋的挑戰

| 文章 | 說明 |
|------|------|
| [圍棋為什麼難？](./why-go-is-hard) | 狀態空間 10^170、分支因子 ~250 |
| [傳統方法的極限](./traditional-limits) | Minimax、Alpha-Beta、純 MCTS |
| [棋盤狀態表示](./board-representation) | Zobrist Hashing、Union-Find、特徵編碼 |

### 模組 3：神經網路核心

| 文章 | 說明 |
|------|------|
| [Policy Network 詳解](./policy-network) | 架構、Softmax 輸出、訓練目標 |
| [Value Network 詳解](./value-network) | 架構、Tanh 輸出、避免過擬合 |
| [輸入特徵設計](./input-features) | 48→17 個特徵平面的演進 |
| [CNN 與圍棋的結合](./cnn-and-go) | 為什麼 CNN 適合棋盤 |
| [監督學習階段](./supervised-learning) | KGS 資料集、57% 預測準確率 |

### 模組 4：強化學習與搜索

| 文章 | 說明 |
|------|------|
| [強化學習入門](./reinforcement-intro) | MDP、策略梯度、價值函數 |
| [自我對弈](./self-play) | 為什麼有效、ELO 成長曲線 |
| [MCTS 與神經網路的結合](./mcts-neural-combo) | Selection→Expansion→Evaluation→Backup |
| [PUCT 公式詳解](./puct-formula) | 數學推導、探索 vs 利用 |

### 模組 5：AlphaGo Zero 演進

| 文章 | 說明 |
|------|------|
| [AlphaGo Zero 概述](./alphago-zero) | 為什麼不需要人類棋譜 |
| [雙頭網路與殘差網路](./dual-head-resnet) | 共享表示、梯度流動、40 層 ResNet |
| [從零訓練的過程](./training-from-scratch) | Day 0-3 的變化、3 天超越人類 |

### 模組 6：技術細節與延伸

| 文章 | 說明 |
|------|------|
| [分散式系統與 TPU](./distributed-systems) | 訓練架構、推理架構、並行 MCTS |
| [AlphaGo 的遺產](./legacy-and-impact) | 對圍棋界影響、AlphaZero、MuZero、AlphaFold |

---

## 快速預覽

### Policy Network 輸出示例

Policy Network 會輸出每個位置的落子機率：

<PolicyHeatmap initialPosition="corner" size={400} />

### 訓練曲線

AlphaGo Zero 在 3 天內從零開始超越人類：

<EloChart mode="zero" width={600} height={350} />

---

## 閱讀建議

### 依背景選擇起點

| 你的背景 | 建議起點 |
|---------|---------|
| **完全新手** | 從 [AlphaGo 的誕生](./birth-of-alphago) 開始，按順序閱讀 |
| **了解圍棋** | 從 [圍棋為什麼難？](./why-go-is-hard) 開始 |
| **有機器學習基礎** | 從 [Policy Network 詳解](./policy-network) 開始 |
| **想快速了解精華** | 閱讀 [MCTS 與神經網路的結合](./mcts-neural-combo) |
| **想了解 Zero 的突破** | 從 [AlphaGo Zero 概述](./alphago-zero) 開始 |

### 預計閱讀時間

- **完整閱讀**：約 8-10 小時
- **快速瀏覽**：約 2-3 小時
- **每篇文章**：約 15-25 分鐘

---

## 動畫對應

本系列文章引用了 [109 個動畫概念](../concepts/) 中的以下系列：

| 系列 | 主題 | 相關文章 |
|------|------|---------|
| **C 系列** | 蒙地卡羅方法 | #5, #14, #15 |
| **D 系列** | 神經網路 | #7, #8, #10, #11 |
| **E 系列** | AlphaGo 架構 | #13, #16, #17, #18 |
| **H 系列** | 強化學習 | #12, #13 |

---

## 參考資料

### 論文

1. Silver, D., et al. (2016). ["Mastering the game of Go with deep neural networks and tree search."](https://www.nature.com/articles/nature16961) *Nature*.
2. Silver, D., et al. (2017). ["Mastering the game of Go without human knowledge."](https://www.nature.com/articles/nature24270) *Nature*.
3. Silver, D., et al. (2018). ["A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play."](https://www.science.org/doi/10.1126/science.aar6404) *Science*.

### 延伸閱讀

- [KataGo 的關鍵創新](../katago-innovations) — 如何用更少資源達到更強棋力
- [概念速查表](../concepts/) — 109 個動畫概念的完整列表
- [30 分鐘跑起第一個圍棋 AI](../../hands-on/) — 動手實作
