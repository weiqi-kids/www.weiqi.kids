<!doctype html>
<html lang="zh-tw" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-for-engineers/how-it-works/alphago-explained/self-play" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">自我對弈 | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/self-play/"><meta data-rh="true" property="og:locale" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="zh-tw"><meta data-rh="true" name="docsearch:language" content="zh-tw"><meta data-rh="true" name="keywords" content="圍棋, Go, 好棋寶寶, AI, KataGo, AlphaGo, 圍棋教學, 圍棋入門"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="自我對弈 | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="深入理解 AlphaGo 如何透過自我對弈突破人類棋力的極限"><meta data-rh="true" property="og:description" content="深入理解 AlphaGo 如何透過自我對弈突破人類棋力的極限"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/self-play/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/self-play/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"給工程師的圍棋 AI 指南","item":"https://www.weiqi.kids/docs/for-engineers/"},{"@type":"ListItem","position":2,"name":"一篇文章搞懂圍棋 AI","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/"},{"@type":"ListItem","position":3,"name":"AlphaGo 完整解析","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/"},{"@type":"ListItem","position":4,"name":"自我對弈","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/self-play"}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"WebPage","@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/13-self-play/#webpage","url":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/13-self-play/","name":"自我對弈","description":"深入理解 AlphaGo 如何透過自我對弈突破人類棋力的極限","inLanguage":"zh-TW","isPartOf":{"@id":"https://www.weiqi.kids#website"},"primaryImageOfPage":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/social-card.png"},"datePublished":"2024-01-15","dateModified":"2024-02-22","speakable":{"@type":"SpeakableSpecification","cssSelector":[".article-summary",".speakable-content",".key-takeaway",".key-answer",".expert-quote",".actionable-steps li",".faq-answer-content"]}},{"@type":"Article","@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/13-self-play/#article","mainEntityOfPage":{"@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/13-self-play/#webpage","significantLink":["https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/"]},"headline":"自我對弈","description":"深入理解 AlphaGo 如何透過自我對弈突破人類棋力的極限","image":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/social-card.png","width":1200,"height":630},"author":{"@id":"https://www.weiqi.kids/docs/about/#person"},"publisher":{"@id":"https://www.weiqi.kids#organization"},"datePublished":"2024-01-15","dateModified":"2024-02-22","articleSection":"AlphaGo 完整解析","keywords":"自我對弈, Self-Play, AlphaGo Zero, 強化學習, Elo評分, 對抗性學習, 深度學習, 圍棋AI","wordCount":5500,"inLanguage":"zh-TW","isAccessibleForFree":true,"isPartOf":{"@type":"WebSite","@id":"https://www.weiqi.kids#website"}},{"@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首頁","item":"https://www.weiqi.kids"},{"@type":"ListItem","position":2,"name":"給工程師","item":"https://www.weiqi.kids/docs/for-engineers/"},{"@type":"ListItem","position":3,"name":"技術原理","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/"},{"@type":"ListItem","position":4,"name":"AlphaGo 完整解析","item":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/"}]},{"@type":"Person","@id":"https://www.weiqi.kids/docs/about/#person","name":"好棋寶寶協會編輯團隊","url":"https://www.weiqi.kids/docs/about/","worksFor":{"@id":"https://www.weiqi.kids#organization"},"description":"專注於圍棋 AI 研究與教育推廣的技術團隊","knowsAbout":["圍棋 AI","AlphaGo","KataGo","機器學習","深度學習"],"hasCredential":[{"@type":"EducationalOccupationalCredential","name":"AI 圍棋研究專家","credentialCategory":"技術認證"}]},{"@type":"ImageObject","@id":"https://www.weiqi.kids/docs/for-engineers/how-it-works/alphago-explained/13-self-play/#primaryimage","url":"https://www.weiqi.kids/img/social-card.png","width":1200,"height":630,"caption":"自我對弈 - 好棋寶寶協會","representativeOfPage":true,"license":"https://creativecommons.org/licenses/by-nc-sa/4.0/","creditText":"台灣好棋寶寶協會"}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"FAQPage","mainEntity":[{"@type":"Question","name":"AI 和自己下棋怎麼會變強？","acceptedAnswer":{"@type":"Answer","text":"關鍵在於對抗性學習：即使雙方是同一個 AI，每盤棋的結果仍包含資訊。好的選擇被強化，差的被抑制。隨著 AI 發現有效戰術，作為對手的自己會學會防守，迫使發現更好的策略。"}},{"@type":"Question","name":"自我對弈會收斂嗎？","acceptedAnswer":{"@type":"Answer","text":"理論上對於簡單遊戲可以證明收斂到納什均衡，但圍棋太複雜無法嚴格證明。實際觀察顯示：棋力持續提升、沒有振盪或退化、最終超越人類，表明在實踐中確實有效。"}},{"@type":"Question","name":"為什麼 AlphaGo Zero 不需要人類棋譜？","acceptedAnswer":{"@type":"Answer","text":"自我對弈提供無限的訓練資料，且對手水平隨自己提升而提升。關鍵在於規則定義了什麼是好棋，自我對弈只是揭示這些隱含在規則中的結構。3 天訓練就能超越人類。"}}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.f23bf74b.css">
<script src="/assets/js/runtime~main.8112459a.js" defer="defer"></script>
<script src="/assets/js/main.05ef375d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.svg"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="好棋寶寶協會標誌" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/img/logo.svg" alt="好棋寶寶協會標誌" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">好棋寶寶</b></a><a class="navbar__item navbar__link" href="/docs/for-players/">給棋友</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/for-engineers/">給工程師</a><a class="navbar__item navbar__link" href="/docs/about/">關於協會</a><a class="navbar__item navbar__link" href="/docs/activities/">活動實績</a><a class="navbar__item navbar__link" href="/docs/references/">參考資料</a><a class="navbar__item navbar__link" href="/docs/sop/">標準作業流程</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>繁體中文</a><ul class="dropdown__menu"><li><a href="/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/for-engineers/how-it-works/alphago-explained/self-play/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro/"><span title="使用指南" class="linkLabel_REp1">使用指南</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/about/"><span title="關於協會" class="categoryLinkLabel_ezQx">關於協會</span></a><button aria-label="展開側邊欄分類 &#x27;關於協會&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/activities/"><span title="活動實績" class="categoryLinkLabel_ezQx">活動實績</span></a><button aria-label="展開側邊欄分類 &#x27;活動實績&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/for-players/"><span title="給圍棋棋友" class="categoryLinkLabel_ezQx">給圍棋棋友</span></a><button aria-label="展開側邊欄分類 &#x27;給圍棋棋友&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/references/"><span title="參考資料" class="categoryLinkLabel_ezQx">參考資料</span></a><button aria-label="展開側邊欄分類 &#x27;參考資料&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/docs/sop/"><span title="標準作業流程" class="categoryLinkLabel_ezQx">標準作業流程</span></a><button aria-label="展開側邊欄分類 &#x27;標準作業流程&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/docs/for-engineers/"><span title="給工程師的圍棋 AI 指南" class="categoryLinkLabel_ezQx">給工程師的圍棋 AI 指南</span></a><button aria-label="收起側邊欄分類 &#x27;給工程師的圍棋 AI 指南&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/docs/for-engineers/deep-dive/"><span title="給想深入研究的人" class="categoryLinkLabel_ezQx">給想深入研究的人</span></a><button aria-label="展開側邊欄分類 &#x27;給想深入研究的人&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/docs/for-engineers/hands-on/"><span title="30 分鐘跑起第一個圍棋 AI" class="categoryLinkLabel_ezQx">30 分鐘跑起第一個圍棋 AI</span></a><button aria-label="展開側邊欄分類 &#x27;30 分鐘跑起第一個圍棋 AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/for-engineers/how-it-works/"><span title="一篇文章搞懂圍棋 AI" class="categoryLinkLabel_ezQx">一篇文章搞懂圍棋 AI</span></a><button aria-label="收起側邊欄分類 &#x27;一篇文章搞懂圍棋 AI&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/"><span title="AlphaGo 完整解析" class="categoryLinkLabel_ezQx">AlphaGo 完整解析</span></a><button aria-label="收起側邊欄分類 &#x27;AlphaGo 完整解析&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/birth-of-alphago/"><span title="AlphaGo 的誕生" class="linkLabel_REp1">AlphaGo 的誕生</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/key-matches/"><span title="關鍵對局回顧" class="linkLabel_REp1">關鍵對局回顧</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/move-37/"><span title="「神之一手」深度分析" class="linkLabel_REp1">「神之一手」深度分析</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/why-go-is-hard/"><span title="圍棋為什麼難？" class="linkLabel_REp1">圍棋為什麼難？</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/traditional-limits/"><span title="傳統方法的極限" class="linkLabel_REp1">傳統方法的極限</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/board-representation/"><span title="棋盤狀態表示" class="linkLabel_REp1">棋盤狀態表示</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/policy-network/"><span title="Policy Network 詳解" class="linkLabel_REp1">Policy Network 詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/value-network/"><span title="Value Network 詳解" class="linkLabel_REp1">Value Network 詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/input-features/"><span title="輸入特徵設計" class="linkLabel_REp1">輸入特徵設計</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/cnn-and-go/"><span title="CNN 與圍棋的結合" class="linkLabel_REp1">CNN 與圍棋的結合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/supervised-learning/"><span title="監督學習階段" class="linkLabel_REp1">監督學習階段</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/"><span title="強化學習入門" class="linkLabel_REp1">強化學習入門</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/self-play/"><span title="自我對弈" class="linkLabel_REp1">自我對弈</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/"><span title="MCTS 與神經網路的結合" class="linkLabel_REp1">MCTS 與神經網路的結合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/puct-formula/"><span title="PUCT 公式詳解" class="linkLabel_REp1">PUCT 公式詳解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/"><span title="AlphaGo Zero 概述" class="linkLabel_REp1">AlphaGo Zero 概述</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/dual-head-resnet/"><span title="雙頭網路與殘差網路" class="linkLabel_REp1">雙頭網路與殘差網路</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/training-from-scratch/"><span title="從零訓練的過程" class="linkLabel_REp1">從零訓練的過程</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/distributed-systems/"><span title="分散式系統與 TPU" class="linkLabel_REp1">分散式系統與 TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/alphago-explained/legacy-and-impact/"><span title="AlphaGo 的遺產" class="linkLabel_REp1">AlphaGo 的遺產</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/katago-innovations/"><span title="KataGo 的關鍵創新" class="linkLabel_REp1">KataGo 的關鍵創新</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/for-engineers/how-it-works/concepts/"><span title="概念速查表" class="linkLabel_REp1">概念速查表</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/docs/for-engineers/industry/"><span title="圍棋 AI 產業現況" class="categoryLinkLabel_ezQx">圍棋 AI 產業現況</span></a><button aria-label="展開側邊欄分類 &#x27;圍棋 AI 產業現況&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" tabindex="0" href="/docs/for-engineers/overview/"><span title="圍棋 AI 能做什麼？" class="categoryLinkLabel_ezQx">圍棋 AI 能做什麼？</span></a><button aria-label="展開側邊欄分類 &#x27;圍棋 AI 能做什麼？&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="頁面路徑"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/for-engineers/"><span>給工程師的圍棋 AI 指南</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/for-engineers/how-it-works/"><span>一篇文章搞懂圍棋 AI</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/for-engineers/how-it-works/alphago-explained/"><span>AlphaGo 完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">自我對弈</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">本頁導覽</button></div><div class="theme-doc-markdown markdown">
<header><h1>自我對弈</h1></header>
<p>在前一篇文章中，我們介紹了強化學習的基本概念。現在，讓我們探討 AlphaGo 成功的關鍵之一——<strong>自我對弈（Self-Play）</strong>。</p>
<p>這是一個看似矛盾的概念：<strong>AI 怎麼能透過和自己下棋變得更強？</strong></p>
<p>答案既深刻又優雅，涉及博弈論、演化動力學、以及學習的本質。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="為什麼自我對弈有效">為什麼自我對弈有效？<a href="#為什麼自我對弈有效" class="hash-link" aria-label="為什麼自我對弈有效？的直接連結" title="為什麼自我對弈有效？的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="直覺解釋">直覺解釋<a href="#直覺解釋" class="hash-link" aria-label="直覺解釋的直接連結" title="直覺解釋的直接連結" translate="no">​</a></h3>
<p>想像你是一位圍棋初學者，在一個荒島上獨自練習：</p>
<ol>
<li class="">你下了一盤棋，自己同時扮演黑白雙方</li>
<li class="">對局結束後，你分析哪些棋下得好、哪些下得差</li>
<li class="">下一盤棋時，你嘗試避免之前的錯誤</li>
<li class="">你重複這個過程數百萬次</li>
</ol>
<p>直覺上，這似乎有問題：</p>
<ul>
<li class="">如果你的水平很差，黑白雙方都下差棋，能學到什麼？</li>
<li class="">會不會陷入「錯誤的平衡」——雙方都下錯棋但能互相抵消？</li>
</ul>
<p>但實際上，自我對弈能夠產生持續的進步。原因如下：</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="漸進式發現弱點">漸進式發現弱點<a href="#漸進式發現弱點" class="hash-link" aria-label="漸進式發現弱點的直接連結" title="漸進式發現弱點的直接連結" translate="no">​</a></h3>
<p>關鍵洞見是：<strong>即使雙方都是同一個 AI，每盤棋的結果仍然包含資訊</strong>。</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">局面 A：AI 選擇了走法 X，最終獲勝</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">局面 A：AI 選擇了走法 Y，最終失敗</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">→ 結論：在局面 A 中，X 比 Y 好</span><br></span></code></pre></div></div>
<p>透過統計大量對局，AI 能夠學習到每個局面下哪些選擇更優。這就是<strong>策略梯度</strong>的本質：好的選擇會被強化，差的選擇會被抑制。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="對抗性學習">對抗性學習<a href="#對抗性學習" class="hash-link" aria-label="對抗性學習的直接連結" title="對抗性學習的直接連結" translate="no">​</a></h3>
<p>自我對弈有一個特殊的性質：<strong>訓練對手會自動適應你的水平</strong>。</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">訓練週期 1：AI 發現了一個有效的戰術 T</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">訓練週期 2：作為對手的 AI 學會了如何防守 T</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">訓練週期 3：原版 AI 被迫尋找更好的戰術 T&#x27;</span><br></span></code></pre></div></div>
<p>這形成了一個<strong>軍備競賽（Arms Race）</strong>，雙方不斷發現並克服彼此的弱點。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="與人類棋譜的比較">與人類棋譜的比較<a href="#與人類棋譜的比較" class="hash-link" aria-label="與人類棋譜的比較的直接連結" title="與人類棋譜的比較的直接連結" translate="no">​</a></h3>
<table><thead><tr><th>訓練方式</th><th>優點</th><th>缺點</th></tr></thead><tbody><tr><td><strong>人類棋譜</strong></td><td>學習人類智慧的結晶</td><td>受限於人類水平</td></tr><tr><td><strong>自我對弈</strong></td><td>無上限的提升潛力</td><td>可能陷入局部最優</td></tr><tr><td><strong>兩者結合</strong></td><td>快速起步 + 持續提升</td><td>最佳策略</td></tr></tbody></table>
<p>AlphaGo 原版先用人類棋譜做監督學習，再用自我對弈做強化學習。AlphaGo Zero 則證明了只用自我對弈也能達到超人水平。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="博弈論視角">博弈論視角<a href="#博弈論視角" class="hash-link" aria-label="博弈論視角的直接連結" title="博弈論視角的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="納什均衡">納什均衡<a href="#納什均衡" class="hash-link" aria-label="納什均衡的直接連結" title="納什均衡的直接連結" translate="no">​</a></h3>
<p>在博弈論中，<strong>納什均衡（Nash Equilibrium）</strong> 是一種穩定狀態：在這個狀態下，沒有任何玩家有動機單方面改變策略。</p>
<p>對於圍棋這樣的<strong>零和、完美資訊博弈</strong>，納什均衡有特殊的意義：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>π</mi><mo>∗</mo></msup><mo>=</mo><mi>arg</mi><mo>⁡</mo><msub><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>π</mi></msub><msub><mrow><mi>min</mi><mo>⁡</mo></mrow><msup><mi>π</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mi>V</mi><mo stretchy="false">(</mo><mi>π</mi><mo separator="true">,</mo><msup><mi>π</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi^* = \arg\max_\pi \min_{\pi&#x27;} V(\pi, \pi&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mop">ar<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop">min</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em"><span style="top:-2.786em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><mi>π</mi><mo separator="true">,</mo><msup><mi>π</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(\pi, \pi&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 是當策略 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">π</span></span></span></span> 對上策略 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>π</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">\pi&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> 時的預期價值。</p>
<p>這就是著名的 <strong>Minimax 原則</strong>：最佳策略是那個能在最壞情況下表現最好的策略。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="自我對弈與納什均衡">自我對弈與納什均衡<a href="#自我對弈與納什均衡" class="hash-link" aria-label="自我對弈與納什均衡的直接連結" title="自我對弈與納什均衡的直接連結" translate="no">​</a></h3>
<p>理論上，如果自我對弈能夠收斂，它應該收斂到納什均衡。對於圍棋這樣的確定性博弈，納什均衡就是<strong>完美下法</strong>。</p>
<p>但圍棋的狀態空間太大了（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>10</mn><mn>170</mn></msup></mrow><annotation encoding="application/x-tex">10^{170}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">170</span></span></span></span></span></span></span></span></span></span></span></span>），我們不可能找到真正的納什均衡。自我對弈實際上是在<strong>近似</strong>這個均衡。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="虛擬對弈fictitious-play">虛擬對弈（Fictitious Play）<a href="#虛擬對弈fictitious-play" class="hash-link" aria-label="虛擬對弈（Fictitious Play）的直接連結" title="虛擬對弈（Fictitious Play）的直接連結" translate="no">​</a></h3>
<p>自我對弈與博弈論中的<strong>虛擬對弈</strong>概念相關：</p>
<ol>
<li class="">每個玩家觀察對手的歷史策略</li>
<li class="">計算對手策略的平均分佈</li>
<li class="">選擇對抗這個平均分佈的最佳回應</li>
</ol>
<p>在某些條件下，虛擬對弈可以證明會收斂到納什均衡。</p>
<p>AlphaGo 的自我對弈可以看作是這個概念的神經網路實現。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="自我對弈的機制">自我對弈的機制<a href="#自我對弈的機制" class="hash-link" aria-label="自我對弈的機制的直接連結" title="自我對弈的機制的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="基本流程">基本流程<a href="#基本流程" class="hash-link" aria-label="基本流程的直接連結" title="基本流程的直接連結" translate="no">​</a></h3>
<p>AlphaGo 的自我對弈流程：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">演算法：Self-Play Training</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">初始化：Policy Network π_θ（可從監督學習或隨機初始化開始）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">重複以下步驟直到收斂：</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1. 產生對弈資料</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   對於 i = 1 到 N（並行進行）：</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     a. 用當前策略 π_θ 進行一局自我對弈</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     b. 收集軌跡：τ_i = (s_0, a_0, r_1, s_1, a_1, ...)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     c. 記錄最終結果 z_i ∈ {-1, +1}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. 更新策略</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   a. 計算策略梯度：</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ∇J = (1/N) Σ_i Σ_t ∇_θ log π_θ(a_t|s_t) · z_i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   b. 更新參數：θ ← θ + α · ∇J</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. 更新價值網路</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   a. 用 (s, z) 對訓練 Value Network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   b. 最小化：L = E[(V_φ(s) - z)²]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">4. 可選：評估並保存檢查點</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   a. 讓新策略對抗舊版本</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   b. 如果贏率 &gt; 55%，更新對手池</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="訓練資料的產生">訓練資料的產生<a href="#訓練資料的產生" class="hash-link" aria-label="訓練資料的產生的直接連結" title="訓練資料的產生的直接連結" translate="no">​</a></h3>
<p>每局自我對弈產生一個<strong>軌跡（trajectory）</strong>：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>a</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>s</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>a</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>s</mi><mi>T</mi></msub><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose">)</span></span></span></span></p>
<p>其中：</p>
<ul>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">s_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>：時間步 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span> 的棋盤狀態</li>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">a_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>：時間步 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span> 選擇的動作</li>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span>：最終結果（+1 勝利，-1 失敗）</li>
</ul>
<p>一局 200 手的對弈就產生了 200 個訓練樣本。每天進行數十萬局自我對弈，訓練資料量是驚人的。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="策略更新">策略更新<a href="#策略更新" class="hash-link" aria-label="策略更新的直接連結" title="策略更新的直接連結" translate="no">​</a></h3>
<p>使用策略梯度更新 Policy Network：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>+</mo><mi>α</mi><mo>⋅</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi mathvariant="double-struck">E</mi><mrow><mo fence="true">[</mo><msub><mo>∑</mo><mi>t</mi></msub><mi>log</mi><mo>⁡</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>⋅</mo><mi>z</mi><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\theta \leftarrow \theta + \alpha \cdot \nabla_\theta \mathbb{E}\left[\sum_t \log \pi_\theta(a_t|s_t) \cdot z\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathbb">E</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">[</span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1308em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose delimcenter" style="top:0em">]</span></span></span></span></span></p>
<p>這個更新的效果：</p>
<ul>
<li class="">如果最終獲勝（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">z = +1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">+</span><span class="mord">1</span></span></span></span>），增加所有落子的機率</li>
<li class="">如果最終失敗（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">z = -1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">−</span><span class="mord">1</span></span></span></span>），減少所有落子的機率</li>
</ul>
<p>這看起來很粗糙——贏棋時可能也有些差棋，輸棋時可能也有些好棋。但透過大量對局的統計，這些「雜訊」會被平均掉，真正的好棋會被識別出來。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="價值網路訓練">價值網路訓練<a href="#價值網路訓練" class="hash-link" aria-label="價值網路訓練的直接連結" title="價值網路訓練的直接連結" translate="no">​</a></h3>
<p>Value Network 使用<strong>回歸（regression）</strong> 訓練：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mo>←</mo><mi>ϕ</mi><mo>−</mo><mi>β</mi><mo>⋅</mo><msub><mi mathvariant="normal">∇</mi><mi>ϕ</mi></msub><mi mathvariant="double-struck">E</mi><mrow><mo fence="true">[</mo><mo stretchy="false">(</mo><msub><mi>V</mi><mi>ϕ</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>−</mo><mi>z</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\phi \leftarrow \phi - \beta \cdot \nabla_\phi \mathbb{E}\left[(V_\phi(s) - z)^2\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">ϕ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">ϕ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord mathbb">E</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size1">[</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size1">]</span></span></span></span></span></span></p>
<p>這讓 Value Network 學會預測：從當前局面開始，最終獲勝的機率是多少？</p>
<p>Value Network 的作用是：</p>
<ol>
<li class="">在 MCTS 中提供葉節點評估</li>
<li class="">作為策略梯度的基準線（baseline）</li>
<li class="">直接用於局面評估</li>
</ol>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="隨機化的重要性">隨機化的重要性<a href="#隨機化的重要性" class="hash-link" aria-label="隨機化的重要性的直接連結" title="隨機化的重要性的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="避免確定性循環">避免確定性循環<a href="#避免確定性循環" class="hash-link" aria-label="避免確定性循環的直接連結" title="避免確定性循環的直接連結" translate="no">​</a></h3>
<p>如果自我對弈是完全確定性的，可能會陷入循環：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">策略 A 總是下固定的開局</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">策略 A 對上策略 A 總是產生相同的棋局</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">只有一局棋被反覆學習</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">AI 無法探索其他可能性</span><br></span></code></pre></div></div>
<p>這就是為什麼<strong>隨機性</strong>在自我對弈中至關重要。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="隨機化的來源">隨機化的來源<a href="#隨機化的來源" class="hash-link" aria-label="隨機化的來源的直接連結" title="隨機化的來源的直接連結" translate="no">​</a></h3>
<p>AlphaGo 在自我對弈中引入隨機性的方式：</p>
<p><strong>1. 策略網路本身是隨機的</strong></p>
<p>Policy Network 輸出的是機率分佈，而非確定性選擇：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>∼</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a \sim \pi_\theta(a|s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span></p>
<p>同樣的局面，每次可能選擇不同的落子。</p>
<p><strong>2. 溫度參數</strong></p>
<p>在訓練時使用較高的溫度（temperature）來增加多樣性：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>τ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><msup><mo stretchy="false">)</mo><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>τ</mi></mrow></msup></mrow><mrow><msub><mo>∑</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mi mathvariant="normal">∣</mi><mi>s</mi><msup><mo stretchy="false">)</mo><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>τ</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\pi_\tau(a|s) = \frac{\pi_\theta(a|s)^{1/\tau}}{\sum_{a&#x27;} \pi_\theta(a&#x27;|s)^{1/\tau}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.1132em">τ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.7721em;vertical-align:-0.6104em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1617em"><span style="top:-2.6146em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2854em"><span style="top:-2.2854em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.6068em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8496em"><span style="top:-2.8496em;margin-right:0.1em"><span class="pstrut" style="height:2.5556em"></span><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3214em"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em"></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em"><span style="top:-2.786em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.822em"><span style="top:-2.822em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1/</span><span class="mord mathnormal mtight" style="margin-right:0.1132em">τ</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.485em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">a</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9667em"><span style="top:-2.9667em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1/</span><span class="mord mathnormal mtight" style="margin-right:0.1132em">τ</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6104em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<ul>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau &gt; 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>：更隨機，更多探索</li>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau &lt; 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>：更確定，更多利用</li>
<li class=""><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>：原始分佈</li>
</ul>
<p><strong>3. 狄利克雷噪音（Dirichlet Noise）</strong></p>
<p>AlphaGo Zero 在自我對弈時，在根節點的先驗機率上加入狄利克雷噪音：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>ε</mi><mo stretchy="false">)</mo><mo>⋅</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo><mo>+</mo><mi>ε</mi><mo>⋅</mo><msub><mi>η</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">P(s, a) = (1 - \varepsilon) \cdot \pi_\theta(a|s) + \varepsilon \cdot \eta_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">ε</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal">ε</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></p>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>∼</mo><mtext>Dir</mtext><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\eta \sim \text{Dir}(\alpha)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">Dir</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mclose">)</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ε</mi><mo>=</mo><mn>0.25</mn></mrow><annotation encoding="application/x-tex">\varepsilon = 0.25</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">ε</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.25</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.03</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.03</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.03</span></span></span></span>（針對圍棋的 361 個動作）。</p>
<p>這確保了即使是非常低機率的走法，也有機會被探索。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="對弈池population方法">對弈池（Population）方法<a href="#對弈池population方法" class="hash-link" aria-label="對弈池（Population）方法的直接連結" title="對弈池（Population）方法的直接連結" translate="no">​</a></h3>
<p>另一種增加多樣性的方法是維護一個<strong>對弈池</strong>：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">對弈池 = [π_1, π_2, π_3, ..., π_k]（不同版本的策略）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">每局對弈：</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1. 從池中隨機選擇一個對手</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. 與該對手進行對弈</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. 用結果更新當前策略</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">4. 定期將改進的策略加入池中</span><br></span></code></pre></div></div>
<p>這種方法的好處：</p>
<ul>
<li class=""><strong>多樣性</strong>：不同風格的對手</li>
<li class=""><strong>穩定性</strong>：避免對特定對手過擬合</li>
<li class=""><strong>魯棒性</strong>：學會應對各種策略</li>
</ul>
<p>AlphaGo 原版和 AlphaGo Zero 都使用了類似的技術。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="棋力成長曲線">棋力成長曲線<a href="#棋力成長曲線" class="hash-link" aria-label="棋力成長曲線的直接連結" title="棋力成長曲線的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="elo-評分系統">Elo 評分系統<a href="#elo-評分系統" class="hash-link" aria-label="Elo 評分系統的直接連結" title="Elo 評分系統的直接連結" translate="no">​</a></h3>
<p>為了追蹤 AI 棋力的變化，AlphaGo 使用了 <strong>Elo 評分系統</strong>。</p>
<p>Elo 系統的基本原理：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>A 勝</mtext><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mn>10</mn><mrow><mo stretchy="false">(</mo><msub><mi>R</mi><mi>B</mi></msub><mo>−</mo><msub><mi>R</mi><mi>A</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mn>400</mn></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(\text{A 勝}) = \frac{1}{1 + 10^{(R_B - R_A)/400}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">A </span><span class="mord cjk_fallback">勝</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.3331em;vertical-align:-0.488em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.5703em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mord mtight"><span class="mord mtight">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8853em"><span style="top:-2.8853em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3448em;margin-left:-0.0077em;margin-right:0.1em"><span class="pstrut" style="height:2.6833em"></span><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3385em"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3448em;margin-left:-0.0077em;margin-right:0.1em"><span class="pstrut" style="height:2.6833em"></span><span class="mord mathnormal mtight">A</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3385em"><span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mord mtight">/400</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.488em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">R_A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">R_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 是雙方的 Elo 分數。</p>
<ul>
<li class="">分差 200：強者預期贏 75%</li>
<li class="">分差 400：強者預期贏 90%</li>
<li class="">分差 800：強者預期贏 99%</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-的棋力成長">AlphaGo 的棋力成長<a href="#alphago-的棋力成長" class="hash-link" aria-label="AlphaGo 的棋力成長的直接連結" title="AlphaGo 的棋力成長的直接連結" translate="no">​</a></h3>
<p>讓我們視覺化 AlphaGo 各版本的棋力成長：</p>
<div>載入中...</div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="成長速度分析">成長速度分析<a href="#成長速度分析" class="hash-link" aria-label="成長速度分析的直接連結" title="成長速度分析的直接連結" translate="no">​</a></h3>
<p>從曲線可以觀察到幾個有趣的現象：</p>
<p><strong>1. 初期快速增長</strong></p>
<p>在訓練的最初幾個小時，AI 學會了基本規則和簡單戰術。這是<strong>低掛果實</strong>階段——有太多明顯的錯誤可以修正。</p>
<p><strong>2. 中期穩定增長</strong></p>
<p>隨著基本錯誤被消除，AI 開始學習更精妙的戰術和定式。增長速度變慢，但仍然穩定。</p>
<p><strong>3. 後期增長放緩</strong></p>
<p>當 AI 已經很強時，進一步提升變得困難。可能需要發現全新的策略，而不只是修正錯誤。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="超越人類的時刻">超越人類的時刻<a href="#超越人類的時刻" class="hash-link" aria-label="超越人類的時刻的直接連結" title="超越人類的時刻的直接連結" translate="no">​</a></h3>
<p>AlphaGo 訓練曲線中的關鍵里程碑：</p>
<table><thead><tr><th>里程碑</th><th>相當於</th><th>達成時間</th></tr></thead><tbody><tr><td>超越業餘強豪</td><td>Elo ~2700</td><td>約 3 小時</td></tr><tr><td>超越 Fan Hui</td><td>Elo ~3500</td><td>約 36 小時</td></tr><tr><td>超越 Lee Sedol</td><td>Elo ~4500</td><td>約 60 小時</td></tr><tr><td>超越原版 AlphaGo</td><td>Elo ~5000</td><td>約 72 小時</td></tr></tbody></table>
<p>這些數字（來自 AlphaGo Zero）令人震驚：<strong>AI 在 3 天內從零開始超越了人類數千年的圍棋智慧</strong>。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="收斂性分析">收斂性分析<a href="#收斂性分析" class="hash-link" aria-label="收斂性分析的直接連結" title="收斂性分析的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="自我對弈會收斂嗎">自我對弈會收斂嗎？<a href="#自我對弈會收斂嗎" class="hash-link" aria-label="自我對弈會收斂嗎？的直接連結" title="自我對弈會收斂嗎？的直接連結" translate="no">​</a></h3>
<p>這是一個重要的理論問題。簡短的答案是：<strong>在某些條件下會，但圍棋太複雜了，我們無法嚴格證明</strong>。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="理論保證">理論保證<a href="#理論保證" class="hash-link" aria-label="理論保證的直接連結" title="理論保證的直接連結" translate="no">​</a></h3>
<p>對於較簡單的遊戲（如井字棋），可以證明：</p>
<ol>
<li class=""><strong>存在性</strong>：存在納什均衡（Minimax 定理）</li>
<li class=""><strong>收斂性</strong>：某些演算法（如虛擬對弈）會收斂到納什均衡</li>
</ol>
<p>對於圍棋，我們沒有嚴格的收斂保證，但實驗證據顯示：</p>
<ul>
<li class="">棋力持續提升</li>
<li class="">沒有出現明顯的振盪或退化</li>
<li class="">最終棋力超越所有已知人類</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="可能的失敗模式">可能的失敗模式<a href="#可能的失敗模式" class="hash-link" aria-label="可能的失敗模式的直接連結" title="可能的失敗模式的直接連結" translate="no">​</a></h3>
<p>自我對弈可能遇到的問題：</p>
<p><strong>1. 策略循環（Strategy Cycling）</strong></p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">策略 A 打敗策略 B</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">策略 B 打敗策略 C</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">策略 C 打敗策略 A</span><br></span></code></pre></div></div>
<p>這在某些遊戲中確實會發生（如剪刀石頭布）。但圍棋有足夠的複雜性，這種純粹的循環似乎不會發生。</p>
<p><strong>2. 過擬合到自己</strong></p>
<p>AI 可能學會了只針對自己風格的策略，而無法應對其他風格的對手。這是為什麼 AlphaGo 會與不同版本的自己對弈，以及最終與人類棋手測試。</p>
<p><strong>3. 局部最優</strong></p>
<p>AI 可能陷入局部最優——一種「還不錯但不是最好」的策略。隨機化和大量對弈有助於避免這個問題。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="實際觀察">實際觀察<a href="#實際觀察" class="hash-link" aria-label="實際觀察的直接連結" title="實際觀察的直接連結" translate="no">​</a></h3>
<p>從 AlphaGo 的訓練過程觀察到：</p>
<ol>
<li class=""><strong>持續進步</strong>：Elo 分數隨著訓練持續上升</li>
<li class=""><strong>沒有退化</strong>：沒有出現棋力突然下降的情況</li>
<li class=""><strong>風格演化</strong>：AI 的下棋風格隨著訓練逐漸變化</li>
<li class=""><strong>發現新定式</strong>：AI 發現了人類從未使用過的開局和戰術</li>
</ol>
<p>這些觀察表明，雖然我們沒有理論保證，但自我對弈在實踐中確實有效。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="實作細節">實作細節<a href="#實作細節" class="hash-link" aria-label="實作細節的直接連結" title="實作細節的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="並行自我對弈">並行自我對弈<a href="#並行自我對弈" class="hash-link" aria-label="並行自我對弈的直接連結" title="並行自我對弈的直接連結" translate="no">​</a></h3>
<p>為了加速訓練，AlphaGo 使用大規模並行自我對弈：</p>
<!-- -->
<p><strong>關鍵設計決策</strong>：</p>
<ul>
<li class=""><strong>同步 vs 非同步</strong>：AlphaGo 使用非同步更新，Worker 不需要等待彼此</li>
<li class=""><strong>更新頻率</strong>：每完成 N 局對弈就更新一次參數</li>
<li class=""><strong>對手選擇</strong>：隨機選擇最近幾個版本中的一個作為對手</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="檢查點策略">檢查點策略<a href="#檢查點策略" class="hash-link" aria-label="檢查點策略的直接連結" title="檢查點策略的直接連結" translate="no">​</a></h3>
<p>定期保存模型檢查點，用於：</p>
<ol>
<li class=""><strong>對弈池</strong>：維護不同版本的對手</li>
<li class=""><strong>評估</strong>：追蹤棋力變化</li>
<li class=""><strong>故障恢復</strong>：訓練中斷時可以恢復</li>
</ol>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 偽代碼</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">training_loop</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> iteration </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_iterations</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 產生對弈資料</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        trajectories </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> parallel_self_play</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">current_policy</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_games</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1000</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 更新策略</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        update_policy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">trajectories</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 定期評估和保存</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> iteration </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">100</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            elo </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> evaluate_against_pool</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">current_policy</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            save_checkpoint</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">current_policy</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> elo</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> elo </span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> best_elo</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                add_to_pool</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">current_policy</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                best_elo </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> elo</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="訓練資源需求">訓練資源需求<a href="#訓練資源需求" class="hash-link" aria-label="訓練資源需求的直接連結" title="訓練資源需求的直接連結" translate="no">​</a></h3>
<p>AlphaGo 的訓練規模令人印象深刻：</p>
<table><thead><tr><th>版本</th><th>硬體</th><th>訓練時間</th><th>自我對弈局數</th></tr></thead><tbody><tr><td>AlphaGo Fan</td><td>176 GPU</td><td>數月</td><td>~30M</td></tr><tr><td>AlphaGo Lee</td><td>48 TPU</td><td>數週</td><td>~30M</td></tr><tr><td>AlphaGo Zero</td><td>4 TPU</td><td>3 天</td><td>~5M</td></tr><tr><td>AlphaGo Zero (40天版)</td><td>4 TPU</td><td>40 天</td><td>~30M</td></tr></tbody></table>
<p>注意 AlphaGo Zero 用更少的硬體和更短的時間達到了更強的棋力——這是算法效率的提升。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="超參數設置">超參數設置<a href="#超參數設置" class="hash-link" aria-label="超參數設置的直接連結" title="超參數設置的直接連結" translate="no">​</a></h3>
<p>一些關鍵的超參數：</p>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 自我對弈設置</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUM_PARALLEL_GAMES </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">5000</span><span class="token plain">      </span><span class="token comment" style="color:#999988;font-style:italic"># 同時進行的對弈數</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">GAMES_PER_ITERATION </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">25000</span><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 每次迭代的對弈數</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">MCTS_SIMULATIONS </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1600</span><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 每步棋的 MCTS 模擬次數</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 訓練設置</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">BATCH_SIZE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2048</span><span class="token plain">              </span><span class="token comment" style="color:#999988;font-style:italic"># 訓練批次大小</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">LEARNING_RATE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.01</span><span class="token plain">           </span><span class="token comment" style="color:#999988;font-style:italic"># 初始學習率</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">L2_REGULARIZATION </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1e-4</span><span class="token plain">       </span><span class="token comment" style="color:#999988;font-style:italic"># 權重衰減</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 探索設置</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">TEMPERATURE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.0</span><span class="token plain">              </span><span class="token comment" style="color:#999988;font-style:italic"># 開局 30 手的溫度</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">DIRICHLET_ALPHA </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.03</span><span class="token plain">         </span><span class="token comment" style="color:#999988;font-style:italic"># 狄利克雷噪音參數</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">EXPLORATION_FRACTION </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.25</span><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 噪音比例</span><br></span></code></pre></div></div>
<p>這些超參數是經過大量實驗調整的，對訓練效果有顯著影響。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="自我對弈的變體">自我對弈的變體<a href="#自我對弈的變體" class="hash-link" aria-label="自我對弈的變體的直接連結" title="自我對弈的變體的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-原版">AlphaGo 原版<a href="#alphago-原版" class="hash-link" aria-label="AlphaGo 原版的直接連結" title="AlphaGo 原版的直接連結" translate="no">​</a></h3>
<p>AlphaGo 原版的訓練流程：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. 監督學習 (SL)：從人類棋譜學習</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   → 產生 SL Policy Network (π_SL)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. 強化學習 (RL)：自我對弈</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   初始化 π_RL = π_SL</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   對手池 = [π_SL]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   重複：</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     a. π_RL 與池中策略對弈</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     b. 用策略梯度更新 π_RL</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     c. 如果 π_RL 變強，加入池中</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   → 產生 RL Policy Network (π_RL)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. 價值網路訓練：</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   用 π_RL 自我對弈產生局面</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   訓練 V(s) 預測勝率</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-zero">AlphaGo Zero<a href="#alphago-zero" class="hash-link" aria-label="AlphaGo Zero的直接連結" title="AlphaGo Zero的直接連結" translate="no">​</a></h3>
<p>AlphaGo Zero 簡化了這個流程：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. 純自我對弈（無人類資料）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   初始化隨機網路 f_θ</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   重複：</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     a. 用 MCTS + f_θ 進行自我對弈</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     b. 同時訓練策略頭和價值頭</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     c. 更新 f_θ</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   → 單一網路同時輸出策略和價值</span><br></span></code></pre></div></div>
<p>關鍵改進：</p>
<ul>
<li class=""><strong>無需人類資料</strong>：從零開始</li>
<li class=""><strong>單一網路</strong>：策略和價值共享特徵</li>
<li class=""><strong>更簡潔的訓練</strong>：端到端學習</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="alphazero">AlphaZero<a href="#alphazero" class="hash-link" aria-label="AlphaZero的直接連結" title="AlphaZero的直接連結" translate="no">​</a></h3>
<p>AlphaZero 進一步泛化：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">同樣的算法，不同的遊戲：</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- 圍棋：達到超越 AlphaGo Zero 的水平</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- 西洋棋：超越 Stockfish</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- 將棋：超越 Elmo</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">唯一的遊戲特定部分：規則編碼</span><br></span></code></pre></div></div>
<p>這證明了自我對弈是一種<strong>通用的學習範式</strong>，不限於圍棋。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="人類從中學到什麼">人類從中學到什麼？<a href="#人類從中學到什麼" class="hash-link" aria-label="人類從中學到什麼？的直接連結" title="人類從中學到什麼？的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="ai-發現的新定式">AI 發現的新定式<a href="#ai-發現的新定式" class="hash-link" aria-label="AI 發現的新定式的直接連結" title="AI 發現的新定式的直接連結" translate="no">​</a></h3>
<p>自我對弈產生了許多人類從未使用過的下法：</p>
<p><strong>1. 開局創新</strong></p>
<p>AlphaGo 偏好的一些開局：</p>
<ul>
<li class="">3-3 侵入：在早期就侵入角部</li>
<li class="">高位下法：傳統上被認為「不穩定」</li>
<li class="">大雪崩變化：人類認為複雜難以計算</li>
</ul>
<p><strong>2. 新的形勢判斷</strong></p>
<p>AI 對某些局面的評估與人類大相徑庭：</p>
<ul>
<li class="">某些看似「薄弱」的棋形其實很堅實</li>
<li class="">某些「厚勢」的價值被高估</li>
<li class="">對「先手」和「後手」的重新評估</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="對人類圍棋的影響">對人類圍棋的影響<a href="#對人類圍棋的影響" class="hash-link" aria-label="對人類圍棋的影響的直接連結" title="對人類圍棋的影響的直接連結" translate="no">​</a></h3>
<p>AlphaGo 之後，職業圍棋發生了顯著變化：</p>
<ol>
<li class=""><strong>開局多樣化</strong>：職業棋手開始使用 AI 發現的新開局</li>
<li class=""><strong>訓練方式改變</strong>：AI 成為職業棋手的主要訓練工具</li>
<li class=""><strong>棋理重新思考</strong>：許多傳統「棋理」被質疑和修正</li>
<li class=""><strong>新的美學</strong>：開始欣賞 AI 風格的棋</li>
</ol>
<p>柯潔在輸給 AlphaGo 後說：</p>
<blockquote>
<p>「AlphaGo 讓我重新認識圍棋。我以前認為人類理解圍棋，現在我知道我們只是觸及皮毛。」</p>
</blockquote>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="哲學思考">哲學思考<a href="#哲學思考" class="hash-link" aria-label="哲學思考的直接連結" title="哲學思考的直接連結" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="學習的本質">學習的本質<a href="#學習的本質" class="hash-link" aria-label="學習的本質的直接連結" title="學習的本質的直接連結" translate="no">​</a></h3>
<p>自我對弈提出了關於學習的深刻問題：</p>
<p><strong>知識從哪裡來？</strong></p>
<ul>
<li class="">人類學習依賴於外部資訊（老師、書本、經驗）</li>
<li class="">自我對弈的 AI 只有規則，沒有外部知識</li>
<li class="">但它仍然能「發現」知識——這些知識是從哪裡來的？</li>
</ul>
<p>答案可能是：<strong>知識隱含在遊戲規則和結構中</strong>。圍棋的規則定義了什麼是好棋、什麼是壞棋，自我對弈只是揭示了這些隱含的結構。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="創造力與發現">創造力與發現<a href="#創造力與發現" class="hash-link" aria-label="創造力與發現的直接連結" title="創造力與發現的直接連結" translate="no">​</a></h3>
<p>當 AI 下出「神之一手」（Move 37），這算是創造還是發現？</p>
<p>一種觀點是：那步棋一直「存在」於圍棋的規則中，AI 只是「發現」了它。
另一種觀點是：AI 「創造」了這步棋，因為沒有人（包括 AI 自己）事先知道它。</p>
<p>這個問題沒有標準答案，但它挑戰了我們對創造力的傳統理解。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="人類智慧的位置">人類智慧的位置<a href="#人類智慧的位置" class="hash-link" aria-label="人類智慧的位置的直接連結" title="人類智慧的位置的直接連結" translate="no">​</a></h3>
<p>如果 AI 可以從零開始，透過自我對弈超越人類數千年的智慧，這對人類意味著什麼？</p>
<p>樂觀的看法：</p>
<ul>
<li class="">AI 是人類創造的工具</li>
<li class="">AI 的發現可以增強人類的理解</li>
<li class="">人類可以與 AI 合作，達到更高的水平</li>
</ul>
<p>謹慎的看法：</p>
<ul>
<li class="">某些領域，純粹的計算可能超越人類直覺</li>
<li class="">需要重新思考「專業技能」的價值</li>
<li class="">教育和訓練方式可能需要改變</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="動畫對應">動畫對應<a href="#動畫對應" class="hash-link" aria-label="動畫對應的直接連結" title="動畫對應的直接連結" translate="no">​</a></h2>
<p>本文涉及的核心概念與動畫編號：</p>
<table><thead><tr><th>編號</th><th>概念</th><th>物理/數學對應</th></tr></thead><tbody><tr><td>🎬 E5</td><td>自我對弈循環</td><td>不動點迭代</td></tr><tr><td>🎬 E6</td><td>策略演化</td><td>進化動力學</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="總結">總結<a href="#總結" class="hash-link" aria-label="總結的直接連結" title="總結的直接連結" translate="no">​</a></h2>
<p>自我對弈是 AlphaGo 成功的關鍵技術之一。我們學習了：</p>
<ol>
<li class=""><strong>為什麼有效</strong>：對抗性學習、漸進式發現弱點</li>
<li class=""><strong>機制</strong>：軌跡收集、策略梯度、價值網路訓練</li>
<li class=""><strong>隨機化</strong>：溫度參數、狄利克雷噪音、對弈池</li>
<li class=""><strong>棋力成長</strong>：Elo 系統、成長曲線分析</li>
<li class=""><strong>收斂性</strong>：理論保證與實際觀察</li>
<li class=""><strong>實作細節</strong>：並行訓練、檢查點策略、超參數</li>
</ol>
<p>下一篇，我們將探討 AlphaGo 如何將神經網路與 MCTS 結合，發揮兩者的優勢。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="延伸閱讀">延伸閱讀<a href="#延伸閱讀" class="hash-link" aria-label="延伸閱讀的直接連結" title="延伸閱讀的直接連結" translate="no">​</a></h2>
<ul>
<li class=""><strong>下一篇</strong>：<a class="" href="/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/">MCTS 與神經網路的結合</a> — 直覺與推理的完美結合</li>
<li class=""><strong>上一篇</strong>：<a class="" href="/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/">強化學習入門</a> — 強化學習的基本概念</li>
<li class=""><strong>相關</strong>：<a class="" href="/docs/for-engineers/how-it-works/alphago-explained/alphago-zero/">AlphaGo Zero 概述</a> — 從零開始的突破</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="參考資料">參考資料<a href="#參考資料" class="hash-link" aria-label="參考資料的直接連結" title="參考資料的直接連結" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2016). &quot;Mastering the game of Go with deep neural networks and tree search.&quot; <em>Nature</em>, 529, 484-489.</li>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">Silver, D., et al. (2018). &quot;A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.&quot; <em>Science</em>, 362(6419), 1140-1144.</li>
<li class="">Heinrich, J., &amp; Silver, D. (2016). &quot;Deep Reinforcement Learning from Self-Play in Imperfect-Information Games.&quot; <em>arXiv preprint</em>.</li>
<li class="">Lanctot, M., et al. (2017). &quot;A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.&quot; <em>NeurIPS</em>.</li>
</ol>
<hr>
<div class="key-takeaway" style="background-color:var(--ifm-color-success-lightest);padding:1rem;border-radius:8px;border:1px solid var(--ifm-color-success);margin-bottom:1rem"><strong style="display:block;margin-bottom:0.5rem">📌 重點摘要</strong><p>本文重點：</p><ul>
<li class="">自我對弈透過對抗性學習形成軍備競賽，雙方不斷發現並克服彼此弱點</li>
<li class="">AlphaGo Zero 在 3 天內從零開始超越人類數千年的圍棋智慧</li>
<li class="">隨機化（溫度參數、狄利克雷噪音）確保探索多樣性，避免陷入局部最優</li>
</ul></div>
<div class="faq-section" style="margin-top:2rem"><h2>常見問題</h2><details style="margin-bottom:1rem;padding:1rem;background-color:var(--ifm-color-gray-100);border-radius:8px"><summary style="font-weight:bold;cursor:pointer;margin-bottom:0.5rem">AI 和自己下棋怎麼會變強？</summary><p class="faq-answer-content" style="margin-top:0.5rem">關鍵在於對抗性學習：即使雙方是同一個 AI，每盤棋的結果仍包含資訊。好的選擇被強化，差的被抑制。隨著 AI 發現有效戰術，作為對手的自己會學會防守，迫使發現更好的策略。</p></details><details style="margin-bottom:1rem;padding:1rem;background-color:var(--ifm-color-gray-100);border-radius:8px"><summary style="font-weight:bold;cursor:pointer;margin-bottom:0.5rem">自我對弈會收斂嗎？</summary><p class="faq-answer-content" style="margin-top:0.5rem">理論上對於簡單遊戲可以證明收斂到納什均衡，但圍棋太複雜無法嚴格證明。實際觀察顯示：棋力持續提升、沒有振盪或退化、最終超越人類，表明在實踐中確實有效。</p></details><details style="margin-bottom:1rem;padding:1rem;background-color:var(--ifm-color-gray-100);border-radius:8px"><summary style="font-weight:bold;cursor:pointer;margin-bottom:0.5rem">為什麼 AlphaGo Zero 不需要人類棋譜？</summary><p class="faq-answer-content" style="margin-top:0.5rem">自我對弈提供無限的訓練資料，且對手水平隨自己提升而提升。關鍵在於規則定義了什麼是好棋，自我對弈只是揭示這些隱含在規則中的結構。3 天訓練就能超越人類。</p></details></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/13-self-play.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>編輯此頁</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro/"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">強化學習入門</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/for-engineers/how-it-works/alphago-explained/mcts-neural-combo/"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">MCTS 與神經網路的結合</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#為什麼自我對弈有效" class="table-of-contents__link toc-highlight">為什麼自我對弈有效？</a><ul><li><a href="#直覺解釋" class="table-of-contents__link toc-highlight">直覺解釋</a></li><li><a href="#漸進式發現弱點" class="table-of-contents__link toc-highlight">漸進式發現弱點</a></li><li><a href="#對抗性學習" class="table-of-contents__link toc-highlight">對抗性學習</a></li><li><a href="#與人類棋譜的比較" class="table-of-contents__link toc-highlight">與人類棋譜的比較</a></li></ul></li><li><a href="#博弈論視角" class="table-of-contents__link toc-highlight">博弈論視角</a><ul><li><a href="#納什均衡" class="table-of-contents__link toc-highlight">納什均衡</a></li><li><a href="#自我對弈與納什均衡" class="table-of-contents__link toc-highlight">自我對弈與納什均衡</a></li><li><a href="#虛擬對弈fictitious-play" class="table-of-contents__link toc-highlight">虛擬對弈（Fictitious Play）</a></li></ul></li><li><a href="#自我對弈的機制" class="table-of-contents__link toc-highlight">自我對弈的機制</a><ul><li><a href="#基本流程" class="table-of-contents__link toc-highlight">基本流程</a></li><li><a href="#訓練資料的產生" class="table-of-contents__link toc-highlight">訓練資料的產生</a></li><li><a href="#策略更新" class="table-of-contents__link toc-highlight">策略更新</a></li><li><a href="#價值網路訓練" class="table-of-contents__link toc-highlight">價值網路訓練</a></li></ul></li><li><a href="#隨機化的重要性" class="table-of-contents__link toc-highlight">隨機化的重要性</a><ul><li><a href="#避免確定性循環" class="table-of-contents__link toc-highlight">避免確定性循環</a></li><li><a href="#隨機化的來源" class="table-of-contents__link toc-highlight">隨機化的來源</a></li><li><a href="#對弈池population方法" class="table-of-contents__link toc-highlight">對弈池（Population）方法</a></li></ul></li><li><a href="#棋力成長曲線" class="table-of-contents__link toc-highlight">棋力成長曲線</a><ul><li><a href="#elo-評分系統" class="table-of-contents__link toc-highlight">Elo 評分系統</a></li><li><a href="#alphago-的棋力成長" class="table-of-contents__link toc-highlight">AlphaGo 的棋力成長</a></li><li><a href="#成長速度分析" class="table-of-contents__link toc-highlight">成長速度分析</a></li><li><a href="#超越人類的時刻" class="table-of-contents__link toc-highlight">超越人類的時刻</a></li></ul></li><li><a href="#收斂性分析" class="table-of-contents__link toc-highlight">收斂性分析</a><ul><li><a href="#自我對弈會收斂嗎" class="table-of-contents__link toc-highlight">自我對弈會收斂嗎？</a></li><li><a href="#理論保證" class="table-of-contents__link toc-highlight">理論保證</a></li><li><a href="#可能的失敗模式" class="table-of-contents__link toc-highlight">可能的失敗模式</a></li><li><a href="#實際觀察" class="table-of-contents__link toc-highlight">實際觀察</a></li></ul></li><li><a href="#實作細節" class="table-of-contents__link toc-highlight">實作細節</a><ul><li><a href="#並行自我對弈" class="table-of-contents__link toc-highlight">並行自我對弈</a></li><li><a href="#檢查點策略" class="table-of-contents__link toc-highlight">檢查點策略</a></li><li><a href="#訓練資源需求" class="table-of-contents__link toc-highlight">訓練資源需求</a></li><li><a href="#超參數設置" class="table-of-contents__link toc-highlight">超參數設置</a></li></ul></li><li><a href="#自我對弈的變體" class="table-of-contents__link toc-highlight">自我對弈的變體</a><ul><li><a href="#alphago-原版" class="table-of-contents__link toc-highlight">AlphaGo 原版</a></li><li><a href="#alphago-zero" class="table-of-contents__link toc-highlight">AlphaGo Zero</a></li><li><a href="#alphazero" class="table-of-contents__link toc-highlight">AlphaZero</a></li></ul></li><li><a href="#人類從中學到什麼" class="table-of-contents__link toc-highlight">人類從中學到什麼？</a><ul><li><a href="#ai-發現的新定式" class="table-of-contents__link toc-highlight">AI 發現的新定式</a></li><li><a href="#對人類圍棋的影響" class="table-of-contents__link toc-highlight">對人類圍棋的影響</a></li></ul></li><li><a href="#哲學思考" class="table-of-contents__link toc-highlight">哲學思考</a><ul><li><a href="#學習的本質" class="table-of-contents__link toc-highlight">學習的本質</a></li><li><a href="#創造力與發現" class="table-of-contents__link toc-highlight">創造力與發現</a></li><li><a href="#人類智慧的位置" class="table-of-contents__link toc-highlight">人類智慧的位置</a></li></ul></li><li><a href="#動畫對應" class="table-of-contents__link toc-highlight">動畫對應</a></li><li><a href="#總結" class="table-of-contents__link toc-highlight">總結</a></li><li><a href="#延伸閱讀" class="table-of-contents__link toc-highlight">延伸閱讀</a></li><li><a href="#參考資料" class="table-of-contents__link toc-highlight">參考資料</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>