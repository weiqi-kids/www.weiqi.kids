<!doctype html>
<html lang="pt" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-alphago/explained/alphago-zero" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Visão Geral do AlphaGo Zero | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/pt/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/pt/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/pt/docs/alphago/explained/alphago-zero/"><meta data-rh="true" property="og:locale" content="pt"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="pt"><meta data-rh="true" name="docsearch:language" content="pt"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Visão Geral do AlphaGo Zero | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="Começando do zero, completamente autodidata, como o AlphaGo Zero superou todas as versões anteriores sem usar qualquer registro de partidas humanas"><meta data-rh="true" property="og:description" content="Começando do zero, completamente autodidata, como o AlphaGo Zero superou todas as versões anteriores sem usar qualquer registro de partidas humanas"><meta data-rh="true" name="keywords" content="AlphaGo Zero,auto-jogo,aprendizado por reforço,aprendizado profundo,IA de Go,aprendizado não supervisionado"><link data-rh="true" rel="icon" href="/pt/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/pt/docs/alphago/explained/alphago-zero/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/explained/alphago-zero/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/alphago/explained/alphago-zero/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/alphago/explained/alphago-zero/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/alphago/explained/alphago-zero/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/alphago/explained/alphago-zero/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/alphago/explained/alphago-zero/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/alphago/explained/alphago-zero/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/alphago/explained/alphago-zero/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/alphago/explained/alphago-zero/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/alphago/explained/alphago-zero/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/alphago/explained/alphago-zero/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/explained/alphago-zero/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AlphaGo","item":"https://www.weiqi.kids/pt/docs/alphago/"},{"@type":"ListItem","position":2,"name":"完整解析","item":"https://www.weiqi.kids/pt/docs/alphago/explained/"},{"@type":"ListItem","position":3,"name":"Visão Geral do AlphaGo Zero","item":"https://www.weiqi.kids/pt/docs/alphago/explained/alphago-zero"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/pt/assets/css/styles.f23bf74b.css">
<script src="/pt/assets/js/runtime~main.9908e616.js" defer="defer"></script>
<script src="/pt/assets/js/main.c4a2f5c5.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/pt/img/logo.svg"><div role="region" aria-label="Pular para o conteúdo principal"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">Pular para o conteúdo principal</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Alternar a barra de navegação" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/pt/"><div class="navbar__logo"><img src="/pt/img/logo.svg" alt="Logo da Associação Weiqi Kids" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/pt/img/logo.svg" alt="Logo da Associação Weiqi Kids" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">Weiqi.Kids</b></a><a class="navbar__item navbar__link" href="/pt/docs/learn/">Aprender Go</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/pt/docs/alphago/">AlphaGo</a><a class="navbar__item navbar__link" href="/pt/docs/animations/">Estúdio de Animação</a><a class="navbar__item navbar__link" href="/pt/docs/tech/">Documentação Técnica</a><a class="navbar__item navbar__link" href="/pt/docs/about/">Sobre Nós</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>Português</a><ul class="dropdown__menu"><li><a href="/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="pt">Português</a></li><li><a href="/hi/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/alphago/explained/alphago-zero/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="Volte para o topo" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/pt/docs/intro/"><span title="Guia de Uso" class="linkLabel_REp1">Guia de Uso</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/pt/docs/learn/"><span title="學圍棋" class="categoryLinkLabel_ezQx">學圍棋</span></a><button aria-label="Expandir a categoria lateral &#x27;學圍棋&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/pt/docs/alphago/"><span title="AlphaGo" class="categoryLinkLabel_ezQx">AlphaGo</span></a><button aria-label="Fechar a categoria lateral &#x27;AlphaGo&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" tabindex="0" href="/pt/docs/alphago/explained/"><span title="完整解析" class="categoryLinkLabel_ezQx">完整解析</span></a><button aria-label="Fechar a categoria lateral &#x27;完整解析&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/birth-of-alphago/"><span title="O Nascimento do AlphaGo" class="linkLabel_REp1">O Nascimento do AlphaGo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/key-matches/"><span title="Retrospectiva das Partidas-Chave" class="linkLabel_REp1">Retrospectiva das Partidas-Chave</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/move-37/"><span title="Análise Profunda da &quot;Jogada Divina&quot;" class="linkLabel_REp1">Análise Profunda da &quot;Jogada Divina&quot;</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/why-go-is-hard/"><span title="Por Que o Go É Difícil?" class="linkLabel_REp1">Por Que o Go É Difícil?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/traditional-limits/"><span title="Limites dos Metodos Tradicionais" class="linkLabel_REp1">Limites dos Metodos Tradicionais</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/board-representation/"><span title="Representacao do Estado do Tabuleiro" class="linkLabel_REp1">Representacao do Estado do Tabuleiro</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/policy-network/"><span title="Detalhes da Policy Network" class="linkLabel_REp1">Detalhes da Policy Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/value-network/"><span title="Detalhes da Value Network" class="linkLabel_REp1">Detalhes da Value Network</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/input-features/"><span title="Design de Caracteristicas de Entrada" class="linkLabel_REp1">Design de Caracteristicas de Entrada</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/cnn-and-go/"><span title="CNN e Go" class="linkLabel_REp1">CNN e Go</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/supervised-learning/"><span title="Fase de Aprendizado Supervisionado" class="linkLabel_REp1">Fase de Aprendizado Supervisionado</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/reinforcement-intro/"><span title="Introdução ao Aprendizado por Reforço" class="linkLabel_REp1">Introdução ao Aprendizado por Reforço</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/self-play/"><span title="Autopartida" class="linkLabel_REp1">Autopartida</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/mcts-neural-combo/"><span title="A Combinação de MCTS e Redes Neurais" class="linkLabel_REp1">A Combinação de MCTS e Redes Neurais</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/puct-formula/"><span title="Fórmula PUCT em Detalhes" class="linkLabel_REp1">Fórmula PUCT em Detalhes</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/pt/docs/alphago/explained/alphago-zero/"><span title="Visão Geral do AlphaGo Zero" class="linkLabel_REp1">Visão Geral do AlphaGo Zero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/dual-head-resnet/"><span title="Rede de Cabeça Dupla e Rede Residual" class="linkLabel_REp1">Rede de Cabeça Dupla e Rede Residual</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/training-from-scratch/"><span title="O Processo de Treinamento do Zero" class="linkLabel_REp1">O Processo de Treinamento do Zero</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/distributed-systems/"><span title="Sistemas Distribuídos e TPU" class="linkLabel_REp1">Sistemas Distribuídos e TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pt/docs/alphago/explained/legacy-and-impact/"><span title="O Legado do AlphaGo" class="linkLabel_REp1">O Legado do AlphaGo</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/pt/docs/animations/"><span title="動畫教室" class="linkLabel_REp1">動畫教室</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/pt/docs/tech/"><span title="技術文件" class="categoryLinkLabel_ezQx">技術文件</span></a><button aria-label="Expandir a categoria lateral &#x27;技術文件&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/pt/docs/about/"><span title="關於我們" class="categoryLinkLabel_ezQx">關於我們</span></a><button aria-label="Expandir a categoria lateral &#x27;關於我們&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Página Inicial" class="breadcrumbs__link" href="/pt/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/pt/docs/alphago/"><span>AlphaGo</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/pt/docs/alphago/explained/"><span>完整解析</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Visão Geral do AlphaGo Zero</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">Nessa página</button></div><div class="theme-doc-markdown markdown"><header><h1>Visão Geral do AlphaGo Zero</h1></header>
<p>Em outubro de 2017, a DeepMind publicou um resultado que chocou o mundo da IA: <strong>AlphaGo Zero</strong>, sem usar qualquer registro de partidas humanas, começando a treinar de um estado completamente aleatório, superou o AlphaGo original que derrotou Lee Sedol em apenas três dias, e venceu por <strong>100:0</strong>.</p>
<p>Isso não é apenas um progresso numérico. Representa um novo paradigma: <strong>A IA não precisa de conhecimento humano, pode descobrir tudo do zero</strong>.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="por-que-não-precisa-de-registros-de-partidas-humanas">Por que Não Precisa de Registros de Partidas Humanas?<a href="#por-que-não-precisa-de-registros-de-partidas-humanas" class="hash-link" aria-label="Link direto para Por que Não Precisa de Registros de Partidas Humanas?" title="Link direto para Por que Não Precisa de Registros de Partidas Humanas?" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="limitações-dos-registros-de-partidas-humanas">Limitações dos Registros de Partidas Humanas<a href="#limitações-dos-registros-de-partidas-humanas" class="hash-link" aria-label="Link direto para Limitações dos Registros de Partidas Humanas" title="Link direto para Limitações dos Registros de Partidas Humanas" translate="no">​</a></h3>
<p>O processo de treinamento do AlphaGo original era dividido em duas etapas:</p>
<ol>
<li class=""><strong>Aprendizado supervisionado</strong>: Treinar a Policy Network com 30 milhões de partidas humanas</li>
<li class=""><strong>Aprendizado por reforço</strong>: Melhorar ainda mais através de auto-jogo</li>
</ol>
<p>Este método tem vários problemas fundamentais:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-os-registros-humanos-têm-um-teto">1. Os registros humanos têm um teto<a href="#1-os-registros-humanos-têm-um-teto" class="hash-link" aria-label="Link direto para 1. Os registros humanos têm um teto" title="Link direto para 1. Os registros humanos têm um teto" translate="no">​</a></h4>
<p>A habilidade dos jogadores humanos tem limites, os registros contêm a compreensão humana, mas também incluem erros e vieses humanos. Quando a IA aprende com registros humanos, ela aprende:</p>
<ul>
<li class="">Jogadas que os humanos consideram boas (mas não necessariamente ótimas)</li>
<li class="">Padrões de pensamento humano (que podem limitar a inovação)</li>
<li class="">Erros humanos (que são aprendidos como exemplos corretos)</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-o-gargalo-do-aprendizado-supervisionado">2. O gargalo do aprendizado supervisionado<a href="#2-o-gargalo-do-aprendizado-supervisionado" class="hash-link" aria-label="Link direto para 2. O gargalo do aprendizado supervisionado" title="Link direto para 2. O gargalo do aprendizado supervisionado" translate="no">​</a></h4>
<p>O objetivo do aprendizado supervisionado é &quot;imitar humanos&quot; — prever qual jogada um jogador humano faria. Isso significa que o limite de capacidade da IA é limitado pela habilidade dos jogadores humanos.</p>
<p>É como um aprendiz que só pode imitar o mestre, nunca podendo superar o mestre.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-custo-de-coleta-de-dados">3. Custo de coleta de dados<a href="#3-custo-de-coleta-de-dados" class="hash-link" aria-label="Link direto para 3. Custo de coleta de dados" title="Link direto para 3. Custo de coleta de dados" translate="no">​</a></h4>
<p>Registros de partidas humanas de alta qualidade levam muitos anos para acumular, e só existem para jogos com longa história como o Go. Se quisermos aplicar IA a novos campos (como previsão de estrutura de proteínas), simplesmente não existem &quot;registros de especialistas humanos&quot;.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="o-avanço-do-zero">O Avanço do Zero<a href="#o-avanço-do-zero" class="hash-link" aria-label="Link direto para O Avanço do Zero" title="Link direto para O Avanço do Zero" translate="no">​</a></h3>
<p>O AlphaGo Zero pula completamente a etapa de aprendizado supervisionado, começando diretamente o auto-jogo a partir de <strong>inicialização aleatória</strong>. Isso resolve todos os problemas acima:</p>
<table><thead><tr><th>Problema</th><th>AlphaGo Original</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Limite do conhecimento humano</td><td>Limitado pela qualidade dos registros</td><td>Sem esta limitação</td></tr><tr><td>Objetivo de aprendizado</td><td>Imitar humanos</td><td>Maximizar taxa de vitória</td></tr><tr><td>Requisitos de dados</td><td>30 milhões de partidas</td><td>0</td></tr><tr><td>Generalizabilidade</td><td>Apenas Go</td><td>Pode ser generalizado para outros campos</td></tr></tbody></table>
<p>Esta é uma mudança de paradigma fundamental: de &quot;aprender conhecimento humano&quot; para &quot;descobrir conhecimento a partir de primeiros princípios&quot;.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="comparação-com-o-alphago-original-1000">Comparação com o AlphaGo Original: 100:0<a href="#comparação-com-o-alphago-original-1000" class="hash-link" aria-label="Link direto para Comparação com o AlphaGo Original: 100:0" title="Link direto para Comparação com o AlphaGo Original: 100:0" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="vitória-esmagadora">Vitória Esmagadora<a href="#vitória-esmagadora" class="hash-link" aria-label="Link direto para Vitória Esmagadora" title="Link direto para Vitória Esmagadora" translate="no">​</a></h3>
<p>A DeepMind fez o AlphaGo Zero treinado jogar contra várias versões do AlphaGo:</p>
<table><thead><tr><th>Oponente</th><th>Resultado do AlphaGo Zero</th></tr></thead><tbody><tr><td>AlphaGo Fan (versão que derrotou Fan Hui)</td><td>100:0</td></tr><tr><td>AlphaGo Lee (versão que derrotou Lee Sedol)</td><td>100:0</td></tr><tr><td>AlphaGo Master (versão das 60 vitórias consecutivas)</td><td>89:11</td></tr></tbody></table>
<p><strong>100:0</strong> — isso significa que em 100 partidas, o AlphaGo original não conseguiu vencer sequer uma.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="menos-recursos-mais-força">Menos Recursos, Mais Força<a href="#menos-recursos-mais-força" class="hash-link" aria-label="Link direto para Menos Recursos, Mais Força" title="Link direto para Menos Recursos, Mais Força" translate="no">​</a></h3>
<p>Não apenas venceu, o AlphaGo Zero também alcançou maior força de jogo com menos recursos:</p>
<table><thead><tr><th>Métrica</th><th>AlphaGo Lee</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Tempo de treinamento</td><td>Vários meses</td><td>40 dias (3 dias para superar AlphaGo Lee)</td></tr><tr><td>Partidas de treinamento</td><td>30 milhões de partidas humanas + auto-jogo</td><td>4,9 milhões de partidas de auto-jogo</td></tr><tr><td>TPUs (treinamento)</td><td>50+</td><td>4</td></tr><tr><td>TPUs (inferência)</td><td>48</td><td>4</td></tr><tr><td>Características de entrada</td><td>48 planos</td><td>17 planos</td></tr><tr><td>Rede neural</td><td>Redes duplas SL + RL</td><td>Única rede de cabeça dupla</td></tr></tbody></table>
<p>Esta é uma melhoria de eficiência impressionante: <strong>recursos reduzidos em mais de 10 vezes, mas a força de jogo aumentou significativamente</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="por-que-o-zero-é-mais-forte">Por que o Zero é Mais Forte?<a href="#por-que-o-zero-é-mais-forte" class="hash-link" aria-label="Link direto para Por que o Zero é Mais Forte?" title="Link direto para Por que o Zero é Mais Forte?" translate="no">​</a></h3>
<p>As razões pelas quais o AlphaGo Zero é mais forte podem ser entendidas de vários ângulos:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-aprendizado-sem-viés">1. Aprendizado sem viés<a href="#1-aprendizado-sem-viés" class="hash-link" aria-label="Link direto para 1. Aprendizado sem viés" title="Link direto para 1. Aprendizado sem viés" translate="no">​</a></h4>
<p>O AlphaGo original aprendeu com registros humanos, herdando vieses humanos. Por exemplo, jogadores humanos podem supervalorizar certos josekis, ou ter avaliações incorretas de certas posições.</p>
<p>O AlphaGo Zero não tem essa bagagem. Ele começa de uma tela em branco, aprendendo o que é uma boa jogada apenas através dos resultados de vitória/derrota. Isso permite descobrir jogadas que os humanos nunca imaginaram.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-objetivo-de-aprendizado-consistente">2. Objetivo de aprendizado consistente<a href="#2-objetivo-de-aprendizado-consistente" class="hash-link" aria-label="Link direto para 2. Objetivo de aprendizado consistente" title="Link direto para 2. Objetivo de aprendizado consistente" translate="no">​</a></h4>
<p>O treinamento do AlphaGo original tinha dois objetivos diferentes:</p>
<ul>
<li class="">Aprendizado supervisionado: Maximizar a precisão de previsão das jogadas humanas</li>
<li class="">Aprendizado por reforço: Maximizar a taxa de vitória</li>
</ul>
<p>Estes dois objetivos podem entrar em conflito. O AlphaGo Zero tem apenas um objetivo: <strong>maximização da taxa de vitória</strong>. Isso torna o processo de aprendizado mais consistente e eficaz.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-arquitetura-mais-simples">3. Arquitetura mais simples<a href="#3-arquitetura-mais-simples" class="hash-link" aria-label="Link direto para 3. Arquitetura mais simples" title="Link direto para 3. Arquitetura mais simples" translate="no">​</a></h4>
<p>O AlphaGo original usava Policy Network e Value Network separadas. O AlphaGo Zero usa uma única rede de cabeça dupla (veja o próximo artigo), permitindo que a representação de características seja compartilhada, aumentando a eficiência de aprendizado.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="características-de-entrada-simplificadas-de-48-para-17">Características de Entrada Simplificadas: De 48 para 17<a href="#características-de-entrada-simplificadas-de-48-para-17" class="hash-link" aria-label="Link direto para Características de Entrada Simplificadas: De 48 para 17" title="Link direto para Características de Entrada Simplificadas: De 48 para 17" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="48-planos-de-características-do-alphago-original">48 Planos de Características do AlphaGo Original<a href="#48-planos-de-características-do-alphago-original" class="hash-link" aria-label="Link direto para 48 Planos de Características do AlphaGo Original" title="Link direto para 48 Planos de Características do AlphaGo Original" translate="no">​</a></h3>
<p>A entrada da rede neural do AlphaGo original incluía 48 planos de características 19x19, codificando muitas características projetadas por humanos:</p>
<table><thead><tr><th>Categoria</th><th>Número de características</th><th>Conteúdo</th></tr></thead><tbody><tr><td>Posições das pedras</td><td>3</td><td>Pedras pretas, pedras brancas, pontos vazios</td></tr><tr><td>Liberdades</td><td>8</td><td>Grupos com 1-8 liberdades</td></tr><tr><td>Capturas</td><td>8</td><td>Pode capturar 1-8 pedras</td></tr><tr><td>Ko</td><td>1</td><td>Posição do ko</td></tr><tr><td>Distância da borda</td><td>4</td><td>Primeira a quarta linha</td></tr><tr><td>Legalidade de jogada</td><td>1</td><td>Quais posições podem ser jogadas</td></tr><tr><td>Estado histórico</td><td>8</td><td>Posições das últimas 8 jogadas</td></tr><tr><td>Turno</td><td>1</td><td>Pretas ou brancas</td></tr><tr><td>Outros</td><td>14</td><td>Escada, olhos, etc.</td></tr></tbody></table>
<p>Estas 48 características foram cuidadosamente projetadas por especialistas de Go, contendo muito conhecimento do domínio.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="17-planos-de-características-do-alphago-zero">17 Planos de Características do AlphaGo Zero<a href="#17-planos-de-características-do-alphago-zero" class="hash-link" aria-label="Link direto para 17 Planos de Características do AlphaGo Zero" title="Link direto para 17 Planos de Características do AlphaGo Zero" translate="no">​</a></h3>
<p>O AlphaGo Zero simplificou drasticamente a entrada, usando apenas 17 planos de características:</p>
<table><thead><tr><th>Número do plano</th><th>Conteúdo</th><th>Quantidade</th></tr></thead><tbody><tr><td>1-8</td><td>Posições das pedras pretas (últimas 8 jogadas)</td><td>8</td></tr><tr><td>9-16</td><td>Posições das pedras brancas (últimas 8 jogadas)</td><td>8</td></tr><tr><td>17</td><td>Turno atual (todo 1 ou todo 0)</td><td>1</td></tr></tbody></table>
<p>Estes 17 planos contêm apenas:</p>
<ul>
<li class=""><strong>Estado atual do tabuleiro</strong>: Cada posição tem pedra preta, pedra branca ou vazia</li>
<li class=""><strong>Informação histórica</strong>: Estado do tabuleiro das últimas 8 jogadas</li>
<li class=""><strong>Informação de turno</strong>: De quem é a vez de jogar</li>
</ul>
<p>Sem liberdades, sem julgamento de escada, sem distância da borda — todo esse &quot;conhecimento de Go&quot; é deixado para a rede neural aprender sozinha.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="por-que-a-simplificação-é-boa">Por que a Simplificação é Boa?<a href="#por-que-a-simplificação-é-boa" class="hash-link" aria-label="Link direto para Por que a Simplificação é Boa?" title="Link direto para Por que a Simplificação é Boa?" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-deixar-a-rede-descobrir-características">1. Deixar a rede descobrir características<a href="#1-deixar-a-rede-descobrir-características" class="hash-link" aria-label="Link direto para 1. Deixar a rede descobrir características" title="Link direto para 1. Deixar a rede descobrir características" translate="no">​</a></h4>
<p>Características manuais complexas podem perder informações importantes, ou codificar suposições incorretas. Deixar a rede neural aprender a partir de dados brutos pode levar a descobrir melhores representações de características.</p>
<p>De fato, o AlphaGo Zero aprendeu todas as características que os humanos projetaram (liberdades, escadas, etc.), e também aprendeu alguns padrões que os humanos não tinham consciência explícita.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-melhor-generalizabilidade">2. Melhor generalizabilidade<a href="#2-melhor-generalizabilidade" class="hash-link" aria-label="Link direto para 2. Melhor generalizabilidade" title="Link direto para 2. Melhor generalizabilidade" translate="no">​</a></h4>
<p>Muitas das 48 características são específicas do Go (como escadas, distância da borda). Os 17 planos simplificados são genéricos — qualquer jogo de tabuleiro pode ser codificado de forma similar.</p>
<p>Isso estabeleceu as bases para o posterior <strong>AlphaZero</strong> (IA de jogos genérica).</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-redução-de-erros-humanos">3. Redução de erros humanos<a href="#3-redução-de-erros-humanos" class="hash-link" aria-label="Link direto para 3. Redução de erros humanos" title="Link direto para 3. Redução de erros humanos" translate="no">​</a></h4>
<p>Características projetadas manualmente podem conter definições incorretas ou incompletas. Simplificar a entrada elimina a possibilidade desses problemas.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="arquitetura-de-rede-única">Arquitetura de Rede Única<a href="#arquitetura-de-rede-única" class="hash-link" aria-label="Link direto para Arquitetura de Rede Única" title="Link direto para Arquitetura de Rede Única" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="design-de-rede-dupla-da-versão-original">Design de Rede Dupla da Versão Original<a href="#design-de-rede-dupla-da-versão-original" class="hash-link" aria-label="Link direto para Design de Rede Dupla da Versão Original" title="Link direto para Design de Rede Dupla da Versão Original" translate="no">​</a></h3>
<p>O AlphaGo original usava duas redes neurais independentes:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Policy Network:  Entrada → CNN → Probabilidades de jogada 19x19</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Value Network:   Entrada → CNN → Estimativa de taxa de vitória (-1 a 1)</span><br></span></code></pre></div></div>
<p>Estas duas redes:</p>
<ul>
<li class="">Tinham arquiteturas diferentes (número de camadas e canais ligeiramente diferentes)</li>
<li class="">Eram treinadas independentemente (primeiro Policy, depois Value)</li>
<li class="">Não compartilhavam nenhum parâmetro</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="rede-de-cabeça-dupla-do-zero">Rede de Cabeça Dupla do Zero<a href="#rede-de-cabeça-dupla-do-zero" class="hash-link" aria-label="Link direto para Rede de Cabeça Dupla do Zero" title="Link direto para Rede de Cabeça Dupla do Zero" translate="no">​</a></h3>
<p>O AlphaGo Zero usa uma única rede, mas com duas cabeças de saída (heads):</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Entrada → Backbone ResNet compartilhado → Policy Head → Probabilidades de jogada 19x19</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                       → Value Head  → Estimativa de taxa de vitória</span><br></span></code></pre></div></div>
<p>As duas Heads compartilham o mesmo backbone ResNet (veja o próximo artigo: <a class="" href="/pt/docs/alphago/explained/dual-head-resnet/">Rede de Cabeça Dupla e Rede Residual</a>), o que traz várias vantagens:</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-eficiência-de-parâmetros">1. Eficiência de parâmetros<a href="#1-eficiência-de-parâmetros" class="hash-link" aria-label="Link direto para 1. Eficiência de parâmetros" title="Link direto para 1. Eficiência de parâmetros" translate="no">​</a></h4>
<p>Backbone compartilhado significa que a maioria dos parâmetros é usada por ambas as tarefas. Isso reduz o número total de parâmetros e diminui o risco de overfitting.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-compartilhamento-de-características">2. Compartilhamento de características<a href="#2-compartilhamento-de-características" class="hash-link" aria-label="Link direto para 2. Compartilhamento de características" title="Link direto para 2. Compartilhamento de características" translate="no">​</a></h4>
<p>&quot;Onde devo jogar&quot; (Policy) e &quot;Quem vai ganhar&quot; (Value) precisam entender padrões de tabuleiro similares. O backbone compartilhado permite que essas características sejam aprendidas e utilizadas por ambas as tarefas simultaneamente.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-estabilidade-de-treinamento">3. Estabilidade de treinamento<a href="#3-estabilidade-de-treinamento" class="hash-link" aria-label="Link direto para 3. Estabilidade de treinamento" title="Link direto para 3. Estabilidade de treinamento" translate="no">​</a></h4>
<p>O treinamento conjunto faz com que os sinais de gradiente venham de duas fontes, fornecendo sinais de supervisão mais ricos, tornando o treinamento mais estável.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="o-poder-da-rede-residual">O Poder da Rede Residual<a href="#o-poder-da-rede-residual" class="hash-link" aria-label="Link direto para O Poder da Rede Residual" title="Link direto para O Poder da Rede Residual" translate="no">​</a></h3>
<p>O backbone do AlphaGo Zero usa uma <strong>rede residual (ResNet) de 40 camadas</strong>, muito mais profunda que a CNN de 13 camadas do AlphaGo original.</p>
<p>As conexões residuais (skip connections) permitem que redes profundas sejam treinadas efetivamente, evitando o problema do gradiente desvanecente. Esta foi a tecnologia inovadora da competição ImageNet de 2015, aplicada com sucesso pelo AlphaGo Zero ao campo do Go.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="melhoria-na-eficiência-de-treinamento">Melhoria na Eficiência de Treinamento<a href="#melhoria-na-eficiência-de-treinamento" class="hash-link" aria-label="Link direto para Melhoria na Eficiência de Treinamento" title="Link direto para Melhoria na Eficiência de Treinamento" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="crescimento-exponencial-do-auto-jogo">Crescimento Exponencial do Auto-jogo<a href="#crescimento-exponencial-do-auto-jogo" class="hash-link" aria-label="Link direto para Crescimento Exponencial do Auto-jogo" title="Link direto para Crescimento Exponencial do Auto-jogo" translate="no">​</a></h3>
<p>O processo de treinamento do AlphaGo Zero demonstra uma eficiência impressionante:</p>
<table><thead><tr><th>Tempo de Treinamento</th><th>Classificação ELO</th><th>Equivalente a</th></tr></thead><tbody><tr><td>0 horas</td><td>0</td><td>Jogando aleatoriamente</td></tr><tr><td>3 horas</td><td>~1000</td><td>Descobriu regras básicas</td></tr><tr><td>12 horas</td><td>~3000</td><td>Descobriu josekis</td></tr><tr><td>36 horas</td><td>~4500</td><td>Superou versão Fan Hui</td></tr><tr><td>60 horas</td><td>~5200</td><td>Superou versão Lee Sedol</td></tr><tr><td>72 horas</td><td>~5400</td><td>Superou AlphaGo original</td></tr><tr><td>40 dias</td><td>~5600</td><td>Versão mais forte</td></tr></tbody></table>
<p><strong>Três dias para superar humanos, três dias para superar IA que levou meses para treinar</strong> — isso é uma melhoria de eficiência exponencial.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="por-que-tão-rápido">Por que Tão Rápido?<a href="#por-que-tão-rápido" class="hash-link" aria-label="Link direto para Por que Tão Rápido?" title="Link direto para Por que Tão Rápido?" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-guia-de-busca-mais-forte">1. Guia de busca mais forte<a href="#1-guia-de-busca-mais-forte" class="hash-link" aria-label="Link direto para 1. Guia de busca mais forte" title="Link direto para 1. Guia de busca mais forte" translate="no">​</a></h4>
<p>O MCTS do AlphaGo Zero é completamente guiado pela rede neural, não usa mais a política de jogada rápida (rollout). Isso torna a busca mais eficiente e precisa.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-auto-jogo-mais-rápido">2. Auto-jogo mais rápido<a href="#2-auto-jogo-mais-rápido" class="hash-link" aria-label="Link direto para 2. Auto-jogo mais rápido" title="Link direto para 2. Auto-jogo mais rápido" translate="no">​</a></h4>
<p>Como precisa de apenas uma rede (em vez de duas), o custo computacional de cada partida de auto-jogo é reduzido. Isso significa que mais dados de treinamento podem ser gerados no mesmo tempo.</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-aprendizado-mais-eficaz">3. Aprendizado mais eficaz<a href="#3-aprendizado-mais-eficaz" class="hash-link" aria-label="Link direto para 3. Aprendizado mais eficaz" title="Link direto para 3. Aprendizado mais eficaz" translate="no">​</a></h4>
<p>O treinamento conjunto da rede de cabeça dupla faz com que a informação de cada partida seja utilizada de forma mais eficiente. Os gradientes de Policy e Value se reforçam mutuamente, acelerando a convergência.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="comparação-com-aprendizado-humano">Comparação com Aprendizado Humano<a href="#comparação-com-aprendizado-humano" class="hash-link" aria-label="Link direto para Comparação com Aprendizado Humano" title="Link direto para Comparação com Aprendizado Humano" translate="no">​</a></h3>
<p>Quanto tempo jogadores humanos precisam para alcançar diferentes níveis?</p>
<table><thead><tr><th>Nível</th><th>Tempo necessário humano</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td>Iniciante</td><td>Várias semanas</td><td>Alguns minutos</td></tr><tr><td>Amateur 1 dan</td><td>Vários anos</td><td>Algumas horas</td></tr><tr><td>Nível profissional</td><td>10-20 anos</td><td>1-2 dias</td></tr><tr><td>Campeão mundial</td><td>20+ anos de dedicação em tempo integral</td><td>3 dias</td></tr><tr><td>Superar humanos</td><td>Impossível</td><td>3 dias</td></tr></tbody></table>
<p>Esta comparação não é para diminuir jogadores humanos — eles usam neurônios biológicos, enquanto o AlphaGo Zero usa TPUs especialmente projetados e vários quilowatts de eletricidade. Mas isso realmente demonstra quão eficiente o método de aprendizado correto pode ser.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="generalidade-xadrez-shogi">Generalidade: Xadrez, Shogi<a href="#generalidade-xadrez-shogi" class="hash-link" aria-label="Link direto para Generalidade: Xadrez, Shogi" title="Link direto para Generalidade: Xadrez, Shogi" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="o-nascimento-do-alphazero">O Nascimento do AlphaZero<a href="#o-nascimento-do-alphazero" class="hash-link" aria-label="Link direto para O Nascimento do AlphaZero" title="Link direto para O Nascimento do AlphaZero" translate="no">​</a></h3>
<p>Em dezembro de 2017, a DeepMind publicou o <strong>AlphaZero</strong> — a versão genérica do AlphaGo Zero. O mesmo algoritmo, apenas modificando as regras do jogo, alcançou nível mundial em três jogos de tabuleiro:</p>
<table><thead><tr><th>Jogo</th><th>Tempo de Treinamento</th><th>Oponente</th><th>Resultado</th></tr></thead><tbody><tr><td>Go</td><td>8 horas</td><td>AlphaGo Zero</td><td>60:40</td></tr><tr><td>Xadrez</td><td>4 horas</td><td>Stockfish 8</td><td>28 vitórias 72 empates 0 derrotas</td></tr><tr><td>Shogi</td><td>2 horas</td><td>Elmo</td><td>90:8:2</td></tr></tbody></table>
<p>Note os oponentes:</p>
<ul>
<li class=""><strong>Stockfish</strong> era a engine de xadrez mais forte na época, usando décadas de conhecimento humano e otimização</li>
<li class=""><strong>Elmo</strong> era a IA de shogi mais forte na época</li>
</ul>
<p>O AlphaZero com algumas horas de treinamento superou esses sistemas especializados que levaram anos para desenvolver.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="o-significado-da-generalidade">O Significado da Generalidade<a href="#o-significado-da-generalidade" class="hash-link" aria-label="Link direto para O Significado da Generalidade" title="Link direto para O Significado da Generalidade" translate="no">​</a></h3>
<p>AlphaGo Zero / AlphaZero provou algo importante:</p>
<blockquote>
<p><strong>O mesmo algoritmo de aprendizado pode alcançar nível sobre-humano em diferentes domínios.</strong></p>
</blockquote>
<p>Não são três IAs diferentes, mas um framework de aprendizado genérico:</p>
<ol>
<li class=""><strong>Auto-jogo</strong> gera experiência</li>
<li class=""><strong>Busca em Árvore de Monte Carlo</strong> explora possibilidades</li>
<li class=""><strong>Rede Neural</strong> aprende função de política e valor</li>
<li class=""><strong>Aprendizado por reforço</strong> otimiza a função objetivo</li>
</ol>
<p>Este framework não depende de conhecimento específico do domínio, isso é um passo importante para a generalização da IA.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="impacto-na-ia-tradicional">Impacto na IA Tradicional<a href="#impacto-na-ia-tradicional" class="hash-link" aria-label="Link direto para Impacto na IA Tradicional" title="Link direto para Impacto na IA Tradicional" translate="no">​</a></h3>
<p>Antes do AlphaZero, as IAs mais fortes de xadrez e shogi eram do estilo &quot;sistema especialista&quot;:</p>
<ul>
<li class=""><strong>Muito conhecimento humano</strong>: Livros de abertura, tabelas de finais, funções de avaliação</li>
<li class=""><strong>Décadas de otimização</strong>: Sangue e suor de incontáveis jogadores e engenheiros</li>
<li class=""><strong>Altamente especializadas</strong>: Stockfish não consegue jogar Go, Elmo não consegue jogar xadrez</li>
</ul>
<p>O AlphaZero superou tudo isso em horas com um algoritmo genérico. Isso fez muitos pesquisadores de IA reconsiderarem:</p>
<blockquote>
<p>Devemos investir mais esforços em &quot;algoritmos de aprendizado genéricos&quot; ou &quot;codificação de conhecimento especializado&quot;?</p>
</blockquote>
<p>A resposta parece cada vez mais clara: deixar a máquina aprender sozinha é mais eficaz do que ensiná-la conhecimento.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="estilo-de-jogo-do-alphago-zero">Estilo de Jogo do AlphaGo Zero<a href="#estilo-de-jogo-do-alphago-zero" class="hash-link" aria-label="Link direto para Estilo de Jogo do AlphaGo Zero" title="Link direto para Estilo de Jogo do AlphaGo Zero" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="estética-além-dos-humanos">Estética Além dos Humanos<a href="#estética-além-dos-humanos" class="hash-link" aria-label="Link direto para Estética Além dos Humanos" title="Link direto para Estética Além dos Humanos" translate="no">​</a></h3>
<p>O mundo do Go tem uma avaliação comum das jogadas do AlphaGo Zero: <strong>mais elegantes</strong>.</p>
<p>As jogadas do AlphaGo Lee às vezes pareciam &quot;estranhas&quot; — como a jogada 37, onde os humanos precisaram de análise posterior para entender sua profundidade. Mas as jogadas do AlphaGo Zero são frequentemente avaliadas posteriormente como &quot;imediatamente reconhecíveis como boas jogadas&quot;.</p>
<p>Isso pode ser porque:</p>
<ol>
<li class=""><strong>Força de jogo mais forte</strong>: Zero pode ver mais profundamente, jogar com mais calma</li>
<li class=""><strong>Sem vieses humanos</strong>: Não limitado por josekis tradicionais</li>
<li class=""><strong>Objetivo consistente</strong>: Busca apenas taxa de vitória, não imita humanos</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="redescoberta-da-teoria-de-go-humana">Redescoberta da Teoria de Go Humana<a href="#redescoberta-da-teoria-de-go-humana" class="hash-link" aria-label="Link direto para Redescoberta da Teoria de Go Humana" title="Link direto para Redescoberta da Teoria de Go Humana" translate="no">​</a></h3>
<p>Curiosamente, o AlphaGo Zero &quot;redescobriu&quot; o conhecimento de Go que os humanos acumularam ao longo de milhares de anos durante o treinamento:</p>
<ul>
<li class=""><strong>Josekis</strong>: Zero descobriu sozinho muitos josekis comuns, porque estes são de fato as soluções ótimas para ambos os lados</li>
<li class=""><strong>Princípios de abertura</strong>: A ordem de importância de cantos, bordas e centro</li>
<li class=""><strong>Conhecimento de formas</strong>: A diferença entre formas ruins e formas boas</li>
</ul>
<p>Isso valida a racionalidade da teoria de Go humana — este conhecimento não é coincidência, mas reflexo da essência do Go.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="inovação-além-dos-humanos">Inovação Além dos Humanos<a href="#inovação-além-dos-humanos" class="hash-link" aria-label="Link direto para Inovação Além dos Humanos" title="Link direto para Inovação Além dos Humanos" translate="no">​</a></h3>
<p>Mas o Zero também descobriu jogadas que os humanos nunca imaginaram:</p>
<ul>
<li class=""><strong>Aberturas não convencionais</strong>: Variações sobre aberturas tradicionais</li>
<li class=""><strong>Sacrifícios agressivos</strong>: Mais disposto que humanos a desistir localmente em troca de vantagem global</li>
<li class=""><strong>Formas contra-intuitivas</strong>: &quot;Formas ruins&quot; superficiais que na verdade são a solução ótima</li>
</ul>
<p>Estas inovações estão mudando a compreensão humana do Go. Muitos jogadores profissionais dizem que estudar os registros de partidas do AlphaGo Zero lhes deu uma compreensão completamente nova do Go.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="resumo-dos-detalhes-técnicos">Resumo dos Detalhes Técnicos<a href="#resumo-dos-detalhes-técnicos" class="hash-link" aria-label="Link direto para Resumo dos Detalhes Técnicos" title="Link direto para Resumo dos Detalhes Técnicos" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="comparação-completa-com-o-alphago-original">Comparação Completa com o AlphaGo Original<a href="#comparação-completa-com-o-alphago-original" class="hash-link" aria-label="Link direto para Comparação Completa com o AlphaGo Original" title="Link direto para Comparação Completa com o AlphaGo Original" translate="no">​</a></h3>
<table><thead><tr><th>Aspecto</th><th>AlphaGo (Original)</th><th>AlphaGo Zero</th></tr></thead><tbody><tr><td><strong>Dados de treinamento</strong></td><td>Registros humanos + auto-jogo</td><td>Puro auto-jogo</td></tr><tr><td><strong>Método de aprendizado</strong></td><td>Supervisionado + por reforço</td><td>Puro por reforço</td></tr><tr><td><strong>Características de entrada</strong></td><td>48 planos</td><td>17 planos</td></tr><tr><td><strong>Arquitetura de rede</strong></td><td>Policy/Value separadas</td><td>ResNet de cabeça dupla</td></tr><tr><td><strong>Profundidade da rede</strong></td><td>13 camadas</td><td>40 camadas (ou mais)</td></tr><tr><td><strong>Avaliação MCTS</strong></td><td>Rede neural + Rollout</td><td>Pura rede neural</td></tr><tr><td><strong>Simulações</strong></td><td>~100.000 por jogada</td><td>~1.600 por jogada</td></tr><tr><td><strong>TPUs de treinamento</strong></td><td>50+</td><td>4</td></tr><tr><td><strong>TPUs de inferência</strong></td><td>48</td><td>4 (escalável)</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="algoritmo-central">Algoritmo Central<a href="#algoritmo-central" class="hash-link" aria-label="Link direto para Algoritmo Central" title="Link direto para Algoritmo Central" translate="no">​</a></h3>
<p>O ciclo de treinamento do AlphaGo Zero é muito simples:</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">1. Auto-jogo</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Usar rede atual para MCTS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Selecionar jogadas pela probabilidade de busca MCTS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Registrar cada passo (posição, probabilidade MCTS, resultado da partida)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Treinar rede</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Amostrar do pool de experiência</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Policy Head: minimizar entropia cruzada com probabilidades MCTS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Value Head: minimizar erro quadrático médio com resultado real</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Otimizar ambos os objetivos conjuntamente</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Atualizar rede</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Substituir rede antiga pela nova (verificar que nova é mais forte por auto-jogo)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   - Voltar ao passo 1</span><br></span></code></pre></div></div>
<p>Este ciclo roda continuamente, e a rede fica cada vez mais forte. Sem dados humanos, sem conhecimento humano, apenas regras do jogo e objetivo de vitória/derrota.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="lições-para-pesquisa-em-ia">Lições para Pesquisa em IA<a href="#lições-para-pesquisa-em-ia" class="hash-link" aria-label="Link direto para Lições para Pesquisa em IA" title="Link direto para Lições para Pesquisa em IA" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="aprendizado-de-primeiros-princípios">Aprendizado de Primeiros Princípios<a href="#aprendizado-de-primeiros-princípios" class="hash-link" aria-label="Link direto para Aprendizado de Primeiros Princípios" title="Link direto para Aprendizado de Primeiros Princípios" translate="no">​</a></h3>
<p>O AlphaGo Zero demonstrou um método de aprendizado de &quot;primeiros princípios&quot;:</p>
<blockquote>
<p>Não diga à IA como fazer, apenas diga qual é o objetivo, e deixe-a descobrir o método por conta própria.</p>
</blockquote>
<p>Isso forma um contraste marcante com a abordagem tradicional de sistemas especialistas. Sistemas especialistas tentam codificar conhecimento humano na IA, enquanto o AlphaGo Zero deixa a IA descobrir conhecimento por conta própria.</p>
<p>O resultado é: o conhecimento que a IA descobre pode ser mais completo e preciso que o conhecimento humano.</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="o-poder-do-auto-jogo">O Poder do Auto-jogo<a href="#o-poder-do-auto-jogo" class="hash-link" aria-label="Link direto para O Poder do Auto-jogo" title="Link direto para O Poder do Auto-jogo" translate="no">​</a></h3>
<p>O AlphaGo Zero provou que o auto-jogo pode gerar dados de treinamento infinitos, e a qualidade desses dados melhora à medida que a rede melhora.</p>
<p>Este é um &quot;ciclo positivo&quot;:</p>
<ul>
<li class="">Rede mais forte → Dados de auto-jogo melhores</li>
<li class="">Dados melhores → Rede mais forte</li>
</ul>
<p>Este ciclo pode continuar rodando até atingir o limite teórico do jogo (se existir).</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="a-importância-da-simplificação">A Importância da Simplificação<a href="#a-importância-da-simplificação" class="hash-link" aria-label="Link direto para A Importância da Simplificação" title="Link direto para A Importância da Simplificação" translate="no">​</a></h3>
<p>O sucesso do AlphaGo Zero prova a importância da &quot;simplificação&quot;:</p>
<ul>
<li class="">Simplificar entrada (48 → 17)</li>
<li class="">Simplificar arquitetura (rede dupla → rede única)</li>
<li class="">Simplificar treinamento (supervisionado + reforço → puro reforço)</li>
</ul>
<p>Cada simplificação tornou o sistema mais poderoso. Isso nos diz: complexo não significa bom, a solução mais simples frequentemente é a melhor.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="correspondência-de-animações">Correspondência de Animações<a href="#correspondência-de-animações" class="hash-link" aria-label="Link direto para Correspondência de Animações" title="Link direto para Correspondência de Animações" translate="no">​</a></h2>
<p>Conceitos centrais discutidos neste artigo e números de animação:</p>
<table><thead><tr><th>Número</th><th>Conceito</th><th>Correspondência Física/Matemática</th></tr></thead><tbody><tr><td>🎬 E7</td><td>Treinamento do zero</td><td>Fenômeno de auto-organização</td></tr><tr><td>🎬 E5</td><td>Auto-jogo</td><td>Convergência de ponto fixo</td></tr><tr><td>🎬 E12</td><td>Curva de crescimento de força</td><td>Crescimento em forma de S</td></tr><tr><td>🎬 D12</td><td>Rede residual</td><td>Rodovia de gradientes</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="leitura-adicional">Leitura Adicional<a href="#leitura-adicional" class="hash-link" aria-label="Link direto para Leitura Adicional" title="Link direto para Leitura Adicional" translate="no">​</a></h2>
<ul>
<li class=""><strong>Próximo artigo</strong>: <a class="" href="/pt/docs/alphago/explained/dual-head-resnet/">Rede de Cabeça Dupla e Rede Residual</a> — Análise detalhada da arquitetura de rede neural do AlphaGo Zero</li>
<li class=""><strong>Artigo relacionado</strong>: <a class="" href="/pt/docs/alphago/explained/self-play/">Auto-jogo</a> — Por que o auto-jogo pode produzir nível sobre-humano</li>
<li class=""><strong>Aprofundamento técnico</strong>: <a class="" href="/pt/docs/alphago/explained/training-from-scratch/">O Processo de Treinamento do Zero</a> — Evolução detalhada dos Dias 0-3</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="referências">Referências<a href="#referências" class="hash-link" aria-label="Link direto para Referências" title="Link direto para Referências" translate="no">​</a></h2>
<ol>
<li class="">Silver, D., et al. (2017). &quot;Mastering the game of Go without human knowledge.&quot; <em>Nature</em>, 550, 354-359.</li>
<li class="">Silver, D., et al. (2018). &quot;A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.&quot; <em>Science</em>, 362(6419), 1140-1144.</li>
<li class="">DeepMind. (2017). &quot;AlphaGo Zero: Starting from scratch.&quot; <em>DeepMind Blog</em>.</li>
<li class="">Schrittwieser, J., et al. (2020). &quot;Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.&quot; <em>Nature</em>, 588, 604-609.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/explained/16-alphago-zero.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Editar essa página</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Páginas de documentação"><a class="pagination-nav__link pagination-nav__link--prev" href="/pt/docs/alphago/explained/puct-formula/"><div class="pagination-nav__sublabel">Anterior</div><div class="pagination-nav__label">Fórmula PUCT em Detalhes</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/pt/docs/alphago/explained/dual-head-resnet/"><div class="pagination-nav__sublabel">Próxima</div><div class="pagination-nav__label">Rede de Cabeça Dupla e Rede Residual</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#por-que-não-precisa-de-registros-de-partidas-humanas" class="table-of-contents__link toc-highlight">Por que Não Precisa de Registros de Partidas Humanas?</a><ul><li><a href="#limitações-dos-registros-de-partidas-humanas" class="table-of-contents__link toc-highlight">Limitações dos Registros de Partidas Humanas</a></li><li><a href="#o-avanço-do-zero" class="table-of-contents__link toc-highlight">O Avanço do Zero</a></li></ul></li><li><a href="#comparação-com-o-alphago-original-1000" class="table-of-contents__link toc-highlight">Comparação com o AlphaGo Original: 100:0</a><ul><li><a href="#vitória-esmagadora" class="table-of-contents__link toc-highlight">Vitória Esmagadora</a></li><li><a href="#menos-recursos-mais-força" class="table-of-contents__link toc-highlight">Menos Recursos, Mais Força</a></li><li><a href="#por-que-o-zero-é-mais-forte" class="table-of-contents__link toc-highlight">Por que o Zero é Mais Forte?</a></li></ul></li><li><a href="#características-de-entrada-simplificadas-de-48-para-17" class="table-of-contents__link toc-highlight">Características de Entrada Simplificadas: De 48 para 17</a><ul><li><a href="#48-planos-de-características-do-alphago-original" class="table-of-contents__link toc-highlight">48 Planos de Características do AlphaGo Original</a></li><li><a href="#17-planos-de-características-do-alphago-zero" class="table-of-contents__link toc-highlight">17 Planos de Características do AlphaGo Zero</a></li><li><a href="#por-que-a-simplificação-é-boa" class="table-of-contents__link toc-highlight">Por que a Simplificação é Boa?</a></li></ul></li><li><a href="#arquitetura-de-rede-única" class="table-of-contents__link toc-highlight">Arquitetura de Rede Única</a><ul><li><a href="#design-de-rede-dupla-da-versão-original" class="table-of-contents__link toc-highlight">Design de Rede Dupla da Versão Original</a></li><li><a href="#rede-de-cabeça-dupla-do-zero" class="table-of-contents__link toc-highlight">Rede de Cabeça Dupla do Zero</a></li><li><a href="#o-poder-da-rede-residual" class="table-of-contents__link toc-highlight">O Poder da Rede Residual</a></li></ul></li><li><a href="#melhoria-na-eficiência-de-treinamento" class="table-of-contents__link toc-highlight">Melhoria na Eficiência de Treinamento</a><ul><li><a href="#crescimento-exponencial-do-auto-jogo" class="table-of-contents__link toc-highlight">Crescimento Exponencial do Auto-jogo</a></li><li><a href="#por-que-tão-rápido" class="table-of-contents__link toc-highlight">Por que Tão Rápido?</a></li><li><a href="#comparação-com-aprendizado-humano" class="table-of-contents__link toc-highlight">Comparação com Aprendizado Humano</a></li></ul></li><li><a href="#generalidade-xadrez-shogi" class="table-of-contents__link toc-highlight">Generalidade: Xadrez, Shogi</a><ul><li><a href="#o-nascimento-do-alphazero" class="table-of-contents__link toc-highlight">O Nascimento do AlphaZero</a></li><li><a href="#o-significado-da-generalidade" class="table-of-contents__link toc-highlight">O Significado da Generalidade</a></li><li><a href="#impacto-na-ia-tradicional" class="table-of-contents__link toc-highlight">Impacto na IA Tradicional</a></li></ul></li><li><a href="#estilo-de-jogo-do-alphago-zero" class="table-of-contents__link toc-highlight">Estilo de Jogo do AlphaGo Zero</a><ul><li><a href="#estética-além-dos-humanos" class="table-of-contents__link toc-highlight">Estética Além dos Humanos</a></li><li><a href="#redescoberta-da-teoria-de-go-humana" class="table-of-contents__link toc-highlight">Redescoberta da Teoria de Go Humana</a></li><li><a href="#inovação-além-dos-humanos" class="table-of-contents__link toc-highlight">Inovação Além dos Humanos</a></li></ul></li><li><a href="#resumo-dos-detalhes-técnicos" class="table-of-contents__link toc-highlight">Resumo dos Detalhes Técnicos</a><ul><li><a href="#comparação-completa-com-o-alphago-original" class="table-of-contents__link toc-highlight">Comparação Completa com o AlphaGo Original</a></li><li><a href="#algoritmo-central" class="table-of-contents__link toc-highlight">Algoritmo Central</a></li></ul></li><li><a href="#lições-para-pesquisa-em-ia" class="table-of-contents__link toc-highlight">Lições para Pesquisa em IA</a><ul><li><a href="#aprendizado-de-primeiros-princípios" class="table-of-contents__link toc-highlight">Aprendizado de Primeiros Princípios</a></li><li><a href="#o-poder-do-auto-jogo" class="table-of-contents__link toc-highlight">O Poder do Auto-jogo</a></li><li><a href="#a-importância-da-simplificação" class="table-of-contents__link toc-highlight">A Importância da Simplificação</a></li></ul></li><li><a href="#correspondência-de-animações" class="table-of-contents__link toc-highlight">Correspondência de Animações</a></li><li><a href="#leitura-adicional" class="table-of-contents__link toc-highlight">Leitura Adicional</a></li><li><a href="#referências" class="table-of-contents__link toc-highlight">Referências</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>