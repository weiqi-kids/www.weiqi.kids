"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[464],{50309(e,a,r){r.r(a),r.d(a,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>d,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"for-engineers/how-it-works/alphago-explained/supervised-learning","title":"Fase de Aprendizado Supervisionado","description":"Como o AlphaGo aprendeu com 30 milh\xf5es de partidas humanas, alcan\xe7ando 57% de precis\xe3o de previs\xe3o","source":"@site/i18n/pt/docusaurus-plugin-content-docs/current/for-engineers/how-it-works/alphago-explained/11-supervised-learning.mdx","sourceDirName":"for-engineers/how-it-works/alphago-explained","slug":"/for-engineers/how-it-works/alphago-explained/supervised-learning","permalink":"/pt/docs/for-engineers/how-it-works/alphago-explained/supervised-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/11-supervised-learning.mdx","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12,"title":"Fase de Aprendizado Supervisionado","description":"Como o AlphaGo aprendeu com 30 milh\xf5es de partidas humanas, alcan\xe7ando 57% de precis\xe3o de previs\xe3o"},"sidebar":"tutorialSidebar","previous":{"title":"CNN e Go","permalink":"/pt/docs/for-engineers/how-it-works/alphago-explained/cnn-and-go"},"next":{"title":"Introdu\xe7\xe3o ao Aprendizado por Refor\xe7o","permalink":"/pt/docs/for-engineers/how-it-works/alphago-explained/reinforcement-intro"}}');var o=r(62615),s=r(30416),n=r(45695);const d={sidebar_position:12,title:"Fase de Aprendizado Supervisionado",description:"Como o AlphaGo aprendeu com 30 milh\xf5es de partidas humanas, alcan\xe7ando 57% de precis\xe3o de previs\xe3o"},i="Fase de Aprendizado Supervisionado",l={},c=[{value:"Por que come\xe7ar com partidas humanas?",id:"por-que-come\xe7ar-com-partidas-humanas",level:2},{value:"O ponto de partida do aprendizado",id:"o-ponto-de-partida-do-aprendizado",level:3},{value:"O valor das partidas humanas",id:"o-valor-das-partidas-humanas",level:3},{value:"Fonte dos dados de treinamento",id:"fonte-dos-dados-de-treinamento",level:2},{value:"KGS Go Server",id:"kgs-go-server",level:3},{value:"Caracter\xedsticas do KGS",id:"caracter\xedsticas-do-kgs",level:4},{value:"Por que escolher KGS?",id:"por-que-escolher-kgs",level:4},{value:"30 milh\xf5es de posi\xe7\xf5es",id:"30-milh\xf5es-de-posi\xe7\xf5es",level:3},{value:"Formato dos dados",id:"formato-dos-dados",level:3},{value:"Pr\xe9-processamento de dados",id:"pr\xe9-processamento-de-dados",level:2},{value:"An\xe1lise de SGF",id:"an\xe1lise-de-sgf",level:3},{value:"Extra\xe7\xe3o de caracter\xedsticas",id:"extra\xe7\xe3o-de-caracter\xedsticas",level:3},{value:"Aumento de dados",id:"aumento-de-dados",level:3},{value:"Fun\xe7\xe3o de perda",id:"fun\xe7\xe3o-de-perda",level:2},{value:"Perda de entropia cruzada",id:"perda-de-entropia-cruzada",level:3},{value:"Compreens\xe3o intuitiva",id:"compreens\xe3o-intuitiva",level:3},{value:"Compara\xe7\xe3o com MSE",id:"compara\xe7\xe3o-com-mse",level:3},{value:"Processo de treinamento",id:"processo-de-treinamento",level:2},{value:"Configura\xe7\xe3o de hardware",id:"configura\xe7\xe3o-de-hardware",level:3},{value:"Otimizador",id:"otimizador",level:3},{value:"Por que SGD em vez de Adam?",id:"por-que-sgd-em-vez-de-adam",level:4},{value:"Agendamento da taxa de aprendizado",id:"agendamento-da-taxa-de-aprendizado",level:3},{value:"Loop de treinamento",id:"loop-de-treinamento",level:3},{value:"Curva de treinamento",id:"curva-de-treinamento",level:3},{value:"An\xe1lise de resultados",id:"an\xe1lise-de-resultados",level:2},{value:"57% de precis\xe3o",id:"57-de-precis\xe3o",level:3},{value:"O que \xe9 precis\xe3o top-1?",id:"o-que-\xe9-precis\xe3o-top-1",level:4},{value:"Compara\xe7\xe3o com outros programas",id:"compara\xe7\xe3o-com-outros-programas",level:3},{value:"Avalia\xe7\xe3o de for\xe7a de jogo",id:"avalia\xe7\xe3o-de-for\xe7a-de-jogo",level:3},{value:"Precis\xe3o vs for\xe7a de jogo",id:"precis\xe3o-vs-for\xe7a-de-jogo",level:3},{value:"Limita\xe7\xf5es do aprendizado supervisionado",id:"limita\xe7\xf5es-do-aprendizado-supervisionado",level:2},{value:"Problema 1: Efeito teto",id:"problema-1-efeito-teto",level:3},{value:"Problema 2: N\xe3o consegue distinguir boas e m\xe1s jogadas",id:"problema-2-n\xe3o-consegue-distinguir-boas-e-m\xe1s-jogadas",level:3},{value:"Problema 3: Explora\xe7\xe3o insuficiente",id:"problema-3-explora\xe7\xe3o-insuficiente",level:3},{value:"Solu\xe7\xe3o: Aprendizado por refor\xe7o",id:"solu\xe7\xe3o-aprendizado-por-refor\xe7o",level:3},{value:"Pontos de implementa\xe7\xe3o",id:"pontos-de-implementa\xe7\xe3o",level:2},{value:"C\xf3digo de treinamento completo",id:"c\xf3digo-de-treinamento-completo",level:3},{value:"C\xf3digo de avalia\xe7\xe3o",id:"c\xf3digo-de-avalia\xe7\xe3o",level:3},{value:"Problemas comuns e solu\xe7\xf5es",id:"problemas-comuns-e-solu\xe7\xf5es",level:3},{value:"Correspond\xeancia de anima\xe7\xf5es",id:"correspond\xeancia-de-anima\xe7\xf5es",level:2},{value:"Leitura adicional",id:"leitura-adicional",level:2},{value:"Pontos-chave",id:"pontos-chave",level:2},{value:"Refer\xeancias",id:"refer\xeancias",level:2}];function p(e){const a={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(a.header,{children:(0,o.jsx)(a.h1,{id:"fase-de-aprendizado-supervisionado",children:"Fase de Aprendizado Supervisionado"})}),"\n",(0,o.jsxs)(a.p,{children:['Antes que o AlphaGo pudesse jogar contra si mesmo, ele precisou primeiro "assistir" a uma grande quantidade de partidas humanas. Este processo \xe9 chamado de ',(0,o.jsx)(a.strong,{children:"Aprendizado Supervisionado"}),"."]}),"\n",(0,o.jsxs)(a.p,{children:["Ao analisar 30 milh\xf5es de posi\xe7\xf5es de partidas humanas, a Policy Network do AlphaGo alcan\xe7ou ",(0,o.jsx)(a.strong,{children:"57% de precis\xe3o de previs\xe3o"})," \u2014 capaz de adivinhar o pr\xf3ximo movimento do especialista humano em mais da metade dos casos."]}),"\n",(0,o.jsx)(a.p,{children:"Isso pode n\xe3o parecer impressionante, mas considerando que cada posi\xe7\xe3o tem em m\xe9dia 250 movimentos legais, \xe9 uma conquista not\xe1vel."}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"por-que-come\xe7ar-com-partidas-humanas",children:"Por que come\xe7ar com partidas humanas?"}),"\n",(0,o.jsx)(a.h3,{id:"o-ponto-de-partida-do-aprendizado",children:"O ponto de partida do aprendizado"}),"\n",(0,o.jsx)(a.p,{children:"Imagine que voc\xea precisa ensinar algu\xe9m que n\xe3o sabe absolutamente nada sobre Go. Como voc\xea faria?"}),"\n",(0,o.jsx)(a.p,{children:(0,o.jsx)(a.strong,{children:"Op\xe7\xe3o A: Explora\xe7\xe3o aleat\xf3ria"})}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Deix\xe1-lo jogar aleatoriamente, descobrindo gradualmente o que s\xe3o boas jogadas\n\u2192 Efici\xeancia extremamente baixa, pode nunca aprender\n"})}),"\n",(0,o.jsx)(a.p,{children:(0,o.jsx)(a.strong,{children:"Op\xe7\xe3o B: Observar mestres jogando"})}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Deix\xe1-lo assistir um grande n\xfamero de partidas de jogadores profissionais, imitando suas jogadas\n\u2192 Ap\xf3s ter uma base, ent\xe3o explorar por conta pr\xf3pria\n"})}),"\n",(0,o.jsx)(a.p,{children:'AlphaGo escolheu a op\xe7\xe3o B. Aprendizado supervisionado \xe9 a vers\xe3o matem\xe1tica de "observar mestres jogando".'}),"\n",(0,o.jsx)(a.h3,{id:"o-valor-das-partidas-humanas",children:"O valor das partidas humanas"}),"\n",(0,o.jsx)(a.p,{children:"Humanos passaram milhares de anos desenvolvendo a teoria do Go. Todo esse conhecimento est\xe1 codificado nas partidas:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Joseki de abertura"}),": jogadas de abertura verificadas ao longo do tempo"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"T\xe1ticas de meio de jogo"}),": sabedoria de ataque e defesa"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"T\xe9cnicas de endgame"}),": ess\xeancia do c\xe1lculo de pontos"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Vis\xe3o global"}),": intui\xe7\xe3o de julgamento geral"]}),"\n"]}),"\n",(0,o.jsx)(a.p,{children:'O aprendizado supervisionado permite que o AlphaGo "herde" essa sabedoria humana, sem precisar come\xe7ar do zero.'}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"fonte-dos-dados-de-treinamento",children:"Fonte dos dados de treinamento"}),"\n",(0,o.jsx)(a.h3,{id:"kgs-go-server",children:"KGS Go Server"}),"\n",(0,o.jsxs)(a.p,{children:["Os dados de treinamento do AlphaGo vieram principalmente do ",(0,o.jsx)(a.strong,{children:"KGS Go Server"})," (tamb\xe9m conhecido como Kiseido Go Server), uma plataforma de Go online bem conhecida."]}),"\n",(0,o.jsx)(a.h4,{id:"caracter\xedsticas-do-kgs",children:"Caracter\xedsticas do KGS"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Caracter\xedstica"}),(0,o.jsx)(a.th,{children:"Descri\xe7\xe3o"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Usu\xe1rios"}),(0,o.jsx)(a.td,{children:"Principalmente amadores, tamb\xe9m profissionais"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Faixa de n\xedvel"}),(0,o.jsx)(a.td,{children:"De iniciante a 9 dan profissional"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Registros de partidas"}),(0,o.jsx)(a.td,{children:"Partidas completas salvas em SGF"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Per\xedodo ativo"}),(0,o.jsx)(a.td,{children:"2000 at\xe9 hoje"})]})]})]}),"\n",(0,o.jsx)(a.h4,{id:"por-que-escolher-kgs",children:"Por que escolher KGS?"}),"\n",(0,o.jsxs)(a.ol,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Grande volume de dados"}),": milh\xf5es de partidas"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Formato unificado"}),": formato SGF f\xe1cil de analisar"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Etiquetas de n\xedvel"}),": cada usu\xe1rio tem classifica\xe7\xe3o"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Diversidade"}),": jogadores de diferentes estilos"]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"30-milh\xf5es-de-posi\xe7\xf5es",children:"30 milh\xf5es de posi\xe7\xf5es"}),"\n",(0,o.jsxs)(a.p,{children:["Das partidas do KGS, a DeepMind extraiu aproximadamente ",(0,o.jsx)(a.strong,{children:"30 milh\xf5es de posi\xe7\xf5es"}),":"]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Dados brutos:\n- Cerca de 160.000 partidas\n- Cada partida tem cerca de 200 movimentos\n- Total ~32 milh\xf5es de posi\xe7\xf5es\n\nFiltragem de dados:\n- Filtrar partidas de baixo n\xedvel\n- Filtrar posi\xe7\xf5es de desist\xeancia no meio do jogo\n- Aproximadamente 30 milh\xf5es de posi\xe7\xf5es de alta qualidade no final\n"})}),"\n",(0,o.jsx)(a.h3,{id:"formato-dos-dados",children:"Formato dos dados"}),"\n",(0,o.jsx)(a.p,{children:"Cada amostra de treinamento cont\xe9m:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'{\n    "board_state": [[0, 1, 2, ...], ...],  # Tabuleiro 19\xd719\n    "features": [...],                      # 48 planos de caracter\xedsticas\n    "next_move": 123,                       # Posi\xe7\xe3o jogada pelo humano (0-360)\n    "game_result": 1,                       # 1=preto vence, -1=branco vence\n    "player_rank": "5d",                    # N\xedvel do jogador que fez o movimento\n}\n'})}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"pr\xe9-processamento-de-dados",children:"Pr\xe9-processamento de dados"}),"\n",(0,o.jsx)(a.h3,{id:"an\xe1lise-de-sgf",children:"An\xe1lise de SGF"}),"\n",(0,o.jsx)(a.p,{children:"SGF (Smart Game Format) \xe9 o formato padr\xe3o para partidas de Go:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"(;GM[1]FF[4]CA[UTF-8]AP[CGoban:3]ST[2]\nRU[Japanese]SZ[19]KM[6.50]\nPW[White]PB[Black]\n;B[pd];W[dd];B[pq];W[dp];B[qk];W[nc]...\n)\n"})}),"\n",(0,o.jsx)(a.p,{children:"Precisa extrair:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:"Tamanho do tabuleiro (SZ[19])"}),"\n",(0,o.jsx)(a.li,{children:"Cada movimento (B[pd], W[dd]...)"}),"\n",(0,o.jsx)(a.li,{children:"Resultado da partida (RE[B+2.5])"}),"\n"]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:"def parse_sgf(sgf_string):\n    \"\"\"Analisar partida SGF\"\"\"\n    moves = []\n    # Extrair todos os movimentos\n    pattern = r';([BW])\\[([a-s]{2})\\]'\n    for match in re.finditer(pattern, sgf_string):\n        color = match.group(1)  # 'B' or 'W'\n        coord = match.group(2)  # 'pd', 'dd', etc.\n\n        # Converter coordenadas\n        x = ord(coord[0]) - ord('a')\n        y = ord(coord[1]) - ord('a')\n\n        moves.append((color, x, y))\n\n    return moves\n"})}),"\n",(0,o.jsx)(a.h3,{id:"extra\xe7\xe3o-de-caracter\xedsticas",children:"Extra\xe7\xe3o de caracter\xedsticas"}),"\n",(0,o.jsxs)(a.p,{children:["Para cada posi\xe7\xe3o, extrair 48 planos de caracter\xedsticas (veja ",(0,o.jsx)(a.a,{href:"../input-features",children:"Design de caracter\xedsticas de entrada"}),"):"]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'def extract_features(board, history, current_player):\n    """Extrair 48 planos de caracter\xedsticas"""\n    features = np.zeros((48, 19, 19))\n\n    # Posi\xe7\xe3o das pedras\n    features[0] = (board == 1)  # Pedras pretas\n    features[1] = (board == 2)  # Pedras brancas\n    features[2] = (board == 0)  # Pontos vazios\n\n    # Hist\xf3rico\n    for i, hist in enumerate(history[:8]):\n        features[3+i] = (hist == 1)\n        features[11+i] = (hist == 2)\n\n    # Liberdades, atari, escada, etc...\n    # (implementa\xe7\xe3o detalhada omitida)\n\n    return features\n'})}),"\n",(0,o.jsx)(a.h3,{id:"aumento-de-dados",children:"Aumento de dados"}),"\n",(0,o.jsxs)(a.p,{children:["O tabuleiro de Go tem ",(0,o.jsx)(a.strong,{children:"8 simetrias"})," (4 rota\xe7\xf5es \xd7 2 reflex\xf5es). Cada amostra original pode se tornar 8:"]}),"\n",(0,o.jsx)(a.mermaid,{value:'flowchart LR\n    A["Original<br/>\u25cf\xb7\xb7<br/>\xb7\xb7\xb7<br/>\xb7\xb7\xb7"] --\x3e B["Rotacao 90\xb0<br/>\xb7\xb7\xb7<br/>\xb7\xb7\xb7<br/>\u25cf\xb7\xb7"]\n    B --\x3e C["Rotacao 180\xb0<br/>\xb7\xb7\xb7<br/>\xb7\xb7\xb7<br/>\xb7\xb7\u25cf"]\n    C --\x3e D["Rotacao 270\xb0<br/>\xb7\xb7\u25cf<br/>\xb7\xb7\xb7<br/>\xb7\xb7\xb7"]'}),"\n",(0,o.jsx)(a.p,{children:"Cada um espelhado horizontalmente, obtendo 8 amostras de treinamento equivalentes"}),"\n",(0,o.jsx)(a.p,{children:"Isso aumenta os dados de treinamento efetivos em 8 vezes, garantindo que os padr\xf5es aprendidos pelo modelo n\xe3o dependam de uma dire\xe7\xe3o espec\xedfica."}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'def augment(state, action):\n    """Aumento de 8 simetrias"""\n    augmented = []\n\n    for rotation in [0, 1, 2, 3]:  # 0, 90, 180, 270 graus\n        rotated_state = np.rot90(state, rotation, axes=(1, 2))\n        rotated_action = rotate_action(action, rotation)\n        augmented.append((rotated_state, rotated_action))\n\n        # Espelho horizontal\n        flipped_state = np.flip(rotated_state, axis=2)\n        flipped_action = flip_action(rotated_action)\n        augmented.append((flipped_state, flipped_action))\n\n    return augmented\n'})}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"fun\xe7\xe3o-de-perda",children:"Fun\xe7\xe3o de perda"}),"\n",(0,o.jsx)(a.h3,{id:"perda-de-entropia-cruzada",children:"Perda de entropia cruzada"}),"\n",(0,o.jsxs)(a.p,{children:["O aprendizado supervisionado usa ",(0,o.jsx)(a.strong,{children:"Perda de Entropia Cruzada (Cross-Entropy Loss)"})," para treinar a Policy Network:"]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"L(\u03b8) = -\u03a3 log p_\u03b8(a | s)\n"})}),"\n",(0,o.jsx)(a.p,{children:"Onde:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.code,{children:"s"}),": estado do tabuleiro"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.code,{children:"a"}),": posi\xe7\xe3o real jogada pelo humano (r\xf3tulo)"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.code,{children:"p_\u03b8(a | s)"}),": probabilidade prevista pelo modelo para essa posi\xe7\xe3o"]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"compreens\xe3o-intuitiva",children:"Compreens\xe3o intuitiva"}),"\n",(0,o.jsx)(a.p,{children:'A perda de entropia cruzada mede "a diferen\xe7a entre a previs\xe3o do modelo e o r\xf3tulo":'}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Cen\xe1rio"}),(0,o.jsx)(a.th,{children:"Previs\xe3o do modelo"}),(0,o.jsx)(a.th,{children:"Perda"}),(0,o.jsx)(a.th,{children:"Descri\xe7\xe3o"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Previs\xe3o perfeita"}),(0,o.jsx)(a.td,{children:"probabilidade de a = 1.0"}),(0,o.jsx)(a.td,{children:"0"}),(0,o.jsx)(a.td,{children:"Melhor"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Confiante e correto"}),(0,o.jsx)(a.td,{children:"probabilidade de a = 0.9"}),(0,o.jsx)(a.td,{children:"0.1"}),(0,o.jsx)(a.td,{children:"Muito bom"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Incerto mas correto"}),(0,o.jsx)(a.td,{children:"probabilidade de a = 0.5"}),(0,o.jsx)(a.td,{children:"0.7"}),(0,o.jsx)(a.td,{children:"Razo\xe1vel"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Previs\xe3o errada"}),(0,o.jsx)(a.td,{children:"probabilidade de a = 0.1"}),(0,o.jsx)(a.td,{children:"2.3"}),(0,o.jsx)(a.td,{children:"Muito ruim"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Completamente errado"}),(0,o.jsx)(a.td,{children:"probabilidade de a = 0.01"}),(0,o.jsx)(a.td,{children:"4.6"}),(0,o.jsx)(a.td,{children:"Pior"})]})]})]}),"\n",(0,o.jsx)(a.p,{children:"A fun\xe7\xe3o de perda impulsiona o modelo a aumentar a probabilidade da posi\xe7\xe3o correta."}),"\n",(0,o.jsx)(a.h3,{id:"compara\xe7\xe3o-com-mse",children:"Compara\xe7\xe3o com MSE"}),"\n",(0,o.jsx)(a.p,{children:"Por que n\xe3o usar Erro Quadr\xe1tico M\xe9dio (MSE)?"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:"# MSE:\nloss_mse = (prediction - target)^2\n\n# Cross-Entropy:\nloss_ce = -log(prediction[target])\n"})}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Caracter\xedstica"}),(0,o.jsx)(a.th,{children:"MSE"}),(0,o.jsx)(a.th,{children:"Cross-Entropy"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Tipo de objetivo"}),(0,o.jsx)(a.td,{children:"Regress\xe3o (valor cont\xednuo)"}),(0,o.jsx)(a.td,{children:"Classifica\xe7\xe3o (distribui\xe7\xe3o de probabilidade)"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Comportamento do gradiente"}),(0,o.jsx)(a.td,{children:"Erro maior, gradiente maior"}),(0,o.jsx)(a.td,{children:"Erro confiante, gradiente maior"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Cen\xe1rio adequado"}),(0,o.jsx)(a.td,{children:"Value Network"}),(0,o.jsx)(a.td,{children:"Policy Network"})]})]})]}),"\n",(0,o.jsx)(a.p,{children:"A Policy Network produz uma distribui\xe7\xe3o de probabilidade de 361 classes, entropia cruzada \xe9 a escolha natural."}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"processo-de-treinamento",children:"Processo de treinamento"}),"\n",(0,o.jsx)(a.h3,{id:"configura\xe7\xe3o-de-hardware",children:"Configura\xe7\xe3o de hardware"}),"\n",(0,o.jsx)(a.p,{children:"A DeepMind usou recursos computacionais extensivos:"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Recurso"}),(0,o.jsx)(a.th,{children:"Quantidade"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"GPU"}),(0,o.jsx)(a.td,{children:"50"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Tempo de treinamento"}),(0,o.jsx)(a.td,{children:"Cerca de 3 semanas"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Tamanho do lote"}),(0,o.jsx)(a.td,{children:"16"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Total de passos de treinamento"}),(0,o.jsx)(a.td,{children:"~340M"})]})]})]}),"\n",(0,o.jsx)(a.h3,{id:"otimizador",children:"Otimizador"}),"\n",(0,o.jsxs)(a.p,{children:["Usando ",(0,o.jsx)(a.strong,{children:"Descida de Gradiente Estoc\xe1stico (SGD) + momentum"}),":"]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:"optimizer = torch.optim.SGD(\n    model.parameters(),\n    lr=0.003,         # Taxa de aprendizado inicial\n    momentum=0.9,     # Coeficiente de momentum\n    weight_decay=1e-4 # Regulariza\xe7\xe3o L2\n)\n"})}),"\n",(0,o.jsx)(a.h4,{id:"por-que-sgd-em-vez-de-adam",children:"Por que SGD em vez de Adam?"}),"\n",(0,o.jsx)(a.p,{children:"Em 2016, SGD + momentum ainda era a escolha principal para tarefas de imagem. Na verdade, pesquisas posteriores (incluindo KataGo) descobriram que otimizadores do tipo Adam podem ser melhores."}),"\n",(0,o.jsx)(a.h3,{id:"agendamento-da-taxa-de-aprendizado",children:"Agendamento da taxa de aprendizado"}),"\n",(0,o.jsx)(a.p,{children:"A taxa de aprendizado decai gradualmente durante o treinamento:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:"scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=80_000_000,  # A cada 80M passos\n    gamma=0.1              # Taxa de aprendizado multiplicada por 0.1\n)\n"})}),"\n",(0,o.jsx)(a.mermaid,{value:'flowchart LR\n    A["0.003"] --\x3e|"80M passos"| B["0.0003"]\n    B --\x3e|"160M passos"| C["0.00003"]'}),"\n",(0,o.jsx)(a.h3,{id:"loop-de-treinamento",children:"Loop de treinamento"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:"def train_epoch(model, dataloader, optimizer):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    for batch in dataloader:\n        states, actions = batch\n\n        # Forward pass\n        policy = model(states)  # (batch, 361)\n\n        # Calcular perda\n        loss = F.cross_entropy(policy, actions)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Estat\xedsticas\n        total_loss += loss.item()\n        predictions = policy.argmax(dim=1)\n        correct += (predictions == actions).sum().item()\n        total += actions.size(0)\n\n    accuracy = correct / total\n    avg_loss = total_loss / len(dataloader)\n\n    return avg_loss, accuracy\n"})}),"\n",(0,o.jsx)(a.h3,{id:"curva-de-treinamento",children:"Curva de treinamento"}),"\n",(0,o.jsx)(a.p,{children:"Processo de treinamento t\xedpico:"}),"\n",(0,o.jsx)(a.mermaid,{value:'xychart-beta\n    title "Curva de treinamento"\n    x-axis ["0", "100M", "200M", "300M", "340M"]\n    y-axis "Precisao" 30 --\x3e 60\n    line [30, 42, 52, 56, 57]'}),"\n",(0,o.jsx)(a.p,{children:"Perda e precis\xe3o melhoram rapidamente, depois estabilizam."}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"an\xe1lise-de-resultados",children:"An\xe1lise de resultados"}),"\n",(0,o.jsx)(a.h3,{id:"57-de-precis\xe3o",children:"57% de precis\xe3o"}),"\n",(0,o.jsxs)(a.p,{children:["Ap\xf3s treinamento completo, a Policy Network alcan\xe7ou ",(0,o.jsx)(a.strong,{children:"57.0% de precis\xe3o top-1"}),"."]}),"\n",(0,o.jsx)(a.h4,{id:"o-que-\xe9-precis\xe3o-top-1",children:"O que \xe9 precis\xe3o top-1?"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Previs\xe3o: modelo produz 361 probabilidades\nTop-1: posi\xe7\xe3o com maior probabilidade\nPrecis\xe3o: propor\xe7\xe3o em que esta posi\xe7\xe3o \xe9 igual \xe0 posi\xe7\xe3o realmente jogada pelo humano\n"})}),"\n",(0,o.jsx)(a.p,{children:"57% significa: o modelo tem mais da metade de chance de adivinhar o pr\xf3ximo movimento do especialista humano."}),"\n",(0,o.jsx)(a.h3,{id:"compara\xe7\xe3o-com-outros-programas",children:"Compara\xe7\xe3o com outros programas"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Programa"}),(0,o.jsx)(a.th,{children:"Precis\xe3o Top-1"}),(0,o.jsx)(a.th,{children:"Descri\xe7\xe3o"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Sele\xe7\xe3o aleat\xf3ria"}),(0,o.jsx)(a.td,{children:"0.4%"}),(0,o.jsx)(a.td,{children:"Linha de base"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Caracter\xedsticas tradicionais + modelo linear"}),(0,o.jsx)(a.td,{children:"~24%"}),(0,o.jsx)(a.td,{children:"N\xedvel de 2008"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"CNN rasa"}),(0,o.jsx)(a.td,{children:"~44%"}),(0,o.jsx)(a.td,{children:"N\xedvel de 2014"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:(0,o.jsx)(a.strong,{children:"AlphaGo Policy Network"})}),(0,o.jsx)(a.td,{children:(0,o.jsx)(a.strong,{children:"57%"})}),(0,o.jsx)(a.td,{children:"Avan\xe7o de 2016"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"AlphaGo Zero"}),(0,o.jsx)(a.td,{children:"~60%"}),(0,o.jsx)(a.td,{children:"2017"})]})]})]}),"\n",(0,o.jsx)(a.p,{children:"A CNN profunda da DeepMind melhorou 13 pontos percentuais em rela\xe7\xe3o ao melhor m\xe9todo anterior."}),"\n",(0,o.jsx)(a.h3,{id:"avalia\xe7\xe3o-de-for\xe7a-de-jogo",children:"Avalia\xe7\xe3o de for\xe7a de jogo"}),"\n",(0,o.jsx)(a.p,{children:"For\xe7a de jogo usando apenas a Policy Network (sem busca):"}),"\n",(0,o.jsx)(n.$W,{mode:"training",width:600,height:350}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Configura\xe7\xe3o"}),(0,o.jsx)(a.th,{children:"Classifica\xe7\xe3o Elo"}),(0,o.jsx)(a.th,{children:"N\xedvel aproximado"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Melhor tradicional (Pachi)"}),(0,o.jsx)(a.td,{children:"~2500"}),(0,o.jsx)(a.td,{children:"Amador 4-5 dan"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"SL Policy Network"}),(0,o.jsx)(a.td,{children:"~2800"}),(0,o.jsx)(a.td,{children:"Amador 6-7 dan"})]})]})]}),"\n",(0,o.jsx)(a.p,{children:"Aprendizado supervisionado puro j\xe1 alcan\xe7ou n\xedvel amador alto, isso foi um grande avan\xe7o em 2016."}),"\n",(0,o.jsx)(a.h3,{id:"precis\xe3o-vs-for\xe7a-de-jogo",children:"Precis\xe3o vs for\xe7a de jogo"}),"\n",(0,o.jsx)(a.p,{children:"Interessantemente, precis\xe3o e for\xe7a de jogo n\xe3o t\xeam rela\xe7\xe3o linear:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Precis\xe3o:  44% \u2192 57% (melhoria de 13%)\nElo:    ~2500 \u2192 ~2800 (melhoria de ~300)\n\nPropor\xe7\xe3o de melhoria de precis\xe3o: 13% / 44% \u2248 30%\nPropor\xe7\xe3o de melhoria de Elo: 300 / 2500 \u2248 12%\n"})}),"\n",(0,o.jsx)(a.p,{children:"Pequenas melhorias na precis\xe3o podem trazer melhorias significativas na for\xe7a de jogo, porque:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:"Escolhas corretas em posi\xe7\xf5es cr\xedticas s\xe3o mais importantes"}),"\n",(0,o.jsx)(a.li,{children:"Evitar erros \xf3bvios \xe9 mais importante do que jogar muitas boas jogadas"}),"\n"]}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"limita\xe7\xf5es-do-aprendizado-supervisionado",children:"Limita\xe7\xf5es do aprendizado supervisionado"}),"\n",(0,o.jsx)(a.h3,{id:"problema-1-efeito-teto",children:"Problema 1: Efeito teto"}),"\n",(0,o.jsx)(a.p,{children:'O aprendizado supervisionado s\xf3 pode alcan\xe7ar "n\xedvel humano", n\xe3o pode ultrapassar:'}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Objetivo da SL Policy: imitar humanos\n          \u2193\nSe humanos t\xeam h\xe1bitos errados\n          \u2193\nSL Policy tamb\xe9m aprender\xe1 esses erros\n"})}),"\n",(0,o.jsx)(a.p,{children:'Por exemplo, se os jogadores nos dados de treinamento raramente jogam jogadas n\xe3o-tradicionais como "Movimento 37", a SL Policy tamb\xe9m n\xe3o aprender\xe1.'}),"\n",(0,o.jsx)(a.h3,{id:"problema-2-n\xe3o-consegue-distinguir-boas-e-m\xe1s-jogadas",children:"Problema 2: N\xe3o consegue distinguir boas e m\xe1s jogadas"}),"\n",(0,o.jsx)(a.p,{children:'O aprendizado supervisionado s\xf3 olha "o que o humano jogou", n\xe3o se a jogada foi boa:'}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Posi\xe7\xe3o A: humano jogou K10 (na verdade \xe9 jogada ruim)\nPosi\xe7\xe3o B: humano jogou Q4 (boa jogada)\n\nSL Policy trata igualmente, deve aprender ambas\n"})}),"\n",(0,o.jsx)(a.p,{children:"Os dados de treinamento incluem partidas de amadores, que cont\xeam muitos erros. SL Policy aprender\xe1 esses erros."}),"\n",(0,o.jsx)(a.h3,{id:"problema-3-explora\xe7\xe3o-insuficiente",children:"Problema 3: Explora\xe7\xe3o insuficiente"}),"\n",(0,o.jsx)(a.p,{children:"SL Policy s\xf3 aprende jogadas j\xe1 conhecidas pelos humanos:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Conjunto de jogadas humanas: {A, B, C, D, E}\n           \u2193\nSL Policy s\xf3 escolher\xe1 entre essas jogadas\n           \u2193\nPode existir jogada melhor F, mas nunca foi descoberta\n"})}),"\n",(0,o.jsx)(a.p,{children:"Esta \xe9 a limita\xe7\xe3o fundamental do aprendizado supervisionado: s\xf3 pode aprender o que existe nos dados de treinamento."}),"\n",(0,o.jsx)(a.h3,{id:"solu\xe7\xe3o-aprendizado-por-refor\xe7o",children:"Solu\xe7\xe3o: Aprendizado por refor\xe7o"}),"\n",(0,o.jsxs)(a.p,{children:["Para superar humanos, AlphaGo realizou ",(0,o.jsx)(a.strong,{children:"aprendizado por refor\xe7o"})," ap\xf3s o aprendizado supervisionado:"]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"SL Policy (n\xedvel humano)\n      \u2193 Autopartida\nRL Policy (supera humanos)\n"})}),"\n",(0,o.jsxs)(a.p,{children:["Veja ",(0,o.jsx)(a.a,{href:"../reinforcement-intro",children:"Introdu\xe7\xe3o ao Aprendizado por Refor\xe7o"})," e ",(0,o.jsx)(a.a,{href:"../self-play",children:"Autopartida"})," para detalhes."]}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"pontos-de-implementa\xe7\xe3o",children:"Pontos de implementa\xe7\xe3o"}),"\n",(0,o.jsx)(a.h3,{id:"c\xf3digo-de-treinamento-completo",children:"C\xf3digo de treinamento completo"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\nclass GoDataset(Dataset):\n    def __init__(self, data_path):\n        # Carregar dados pr\xe9-processados\n        self.states = np.load(f"{data_path}/states.npy")\n        self.actions = np.load(f"{data_path}/actions.npy")\n\n    def __len__(self):\n        return len(self.states)\n\n    def __getitem__(self, idx):\n        state = torch.FloatTensor(self.states[idx])\n        action = torch.LongTensor([self.actions[idx]])[0]\n        return state, action\n\ndef train_policy_network():\n    # Modelo\n    model = PolicyNetwork(input_channels=48, num_filters=192, num_layers=12)\n    model = model.cuda()\n\n    # Dados\n    dataset = GoDataset("data/kgs")\n    dataloader = DataLoader(\n        dataset, batch_size=16, shuffle=True, num_workers=4\n    )\n\n    # Otimizador\n    optimizer = optim.SGD(\n        model.parameters(),\n        lr=0.003,\n        momentum=0.9,\n        weight_decay=1e-4\n    )\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=80_000_000, gamma=0.1)\n\n    # Loop de treinamento\n    best_accuracy = 0\n\n    for epoch in range(100):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        for states, actions in dataloader:\n            states = states.cuda()\n            actions = actions.cuda()\n\n            # Forward pass\n            policy = model(states)\n            loss = nn.functional.cross_entropy(policy, actions)\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            # Estat\xedsticas\n            total_loss += loss.item()\n            predictions = policy.argmax(dim=1)\n            correct += (predictions == actions).sum().item()\n            total += actions.size(0)\n\n        accuracy = correct / total\n        print(f"Epoch {epoch}: Loss={total_loss/len(dataloader):.4f}, Acc={accuracy:.4f}")\n\n        # Salvar melhor modelo\n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n            torch.save(model.state_dict(), "best_policy.pth")\n\n    print(f"Best accuracy: {best_accuracy:.4f}")\n'})}),"\n",(0,o.jsx)(a.h3,{id:"c\xf3digo-de-avalia\xe7\xe3o",children:"C\xf3digo de avalia\xe7\xe3o"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'def evaluate_policy(model, test_dataloader):\n    model.eval()\n\n    correct_top1 = 0\n    correct_top5 = 0\n    total = 0\n\n    with torch.no_grad():\n        for states, actions in test_dataloader:\n            states = states.cuda()\n            actions = actions.cuda()\n\n            policy = model(states)\n\n            # Precis\xe3o Top-1\n            top1_pred = policy.argmax(dim=1)\n            correct_top1 += (top1_pred == actions).sum().item()\n\n            # Precis\xe3o Top-5\n            top5_pred = policy.topk(5, dim=1)[1]\n            for i, action in enumerate(actions):\n                if action in top5_pred[i]:\n                    correct_top5 += 1\n\n            total += actions.size(0)\n\n    print(f"Top-1 Accuracy: {correct_top1/total:.4f}")\n    print(f"Top-5 Accuracy: {correct_top5/total:.4f}")\n'})}),"\n",(0,o.jsx)(a.h3,{id:"problemas-comuns-e-solu\xe7\xf5es",children:"Problemas comuns e solu\xe7\xf5es"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Problema"}),(0,o.jsx)(a.th,{children:"Sintoma"}),(0,o.jsx)(a.th,{children:"Solu\xe7\xe3o"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Overfitting"}),(0,o.jsx)(a.td,{children:"Alta precis\xe3o de treinamento, baixa precis\xe3o de teste"}),(0,o.jsx)(a.td,{children:"Aumentar aumento de dados, Dropout"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Treinamento inst\xe1vel"}),(0,o.jsx)(a.td,{children:"Perda oscila muito"}),(0,o.jsx)(a.td,{children:"Diminuir taxa de aprendizado, aumentar tamanho do lote"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Converg\xeancia lenta"}),(0,o.jsx)(a.td,{children:"Precis\xe3o estagnada"}),(0,o.jsx)(a.td,{children:"Ajustar taxa de aprendizado, verificar dados"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Mem\xf3ria insuficiente"}),(0,o.jsx)(a.td,{children:"Erro OOM"}),(0,o.jsx)(a.td,{children:"Diminuir tamanho do lote, usar precis\xe3o mista"})]})]})]}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"correspond\xeancia-de-anima\xe7\xf5es",children:"Correspond\xeancia de anima\xe7\xf5es"}),"\n",(0,o.jsx)(a.p,{children:"Os conceitos centrais abordados neste artigo e n\xfameros de anima\xe7\xe3o:"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"N\xfamero"}),(0,o.jsx)(a.th,{children:"Conceito"}),(0,o.jsx)(a.th,{children:"Correspond\xeancia f\xedsica/matem\xe1tica"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"D3"}),(0,o.jsx)(a.td,{children:"Aprendizado supervisionado"}),(0,o.jsx)(a.td,{children:"Estima\xe7\xe3o de m\xe1xima verossimilhan\xe7a"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"D5"}),(0,o.jsx)(a.td,{children:"Perda de entropia cruzada"}),(0,o.jsx)(a.td,{children:"Diverg\xeancia KL"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"D6"}),(0,o.jsx)(a.td,{children:"Descida de gradiente"}),(0,o.jsx)(a.td,{children:"Otimiza\xe7\xe3o"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"A6"}),(0,o.jsx)(a.td,{children:"Pr\xe9-processamento de dados"}),(0,o.jsx)(a.td,{children:"Normaliza\xe7\xe3o"})]})]})]}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"leitura-adicional",children:"Leitura adicional"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Artigo anterior"}),": ",(0,o.jsx)(a.a,{href:"../cnn-and-go",children:"CNN e Go"})," \u2014 Como redes neurais convolucionais processam o tabuleiro"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Pr\xf3ximo artigo"}),": ",(0,o.jsx)(a.a,{href:"../reinforcement-intro",children:"Introdu\xe7\xe3o ao Aprendizado por Refor\xe7o"})," \u2014 A chave para superar humanos"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"T\xf3pico relacionado"}),": ",(0,o.jsx)(a.a,{href:"../policy-network",children:"Detalhamento da Policy Network"})," \u2014 Detalhes da arquitetura da rede"]}),"\n"]}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"pontos-chave",children:"Pontos-chave"}),"\n",(0,o.jsxs)(a.ol,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Partidas do KGS s\xe3o a fonte de dados de treinamento"}),": cerca de 30 milh\xf5es de posi\xe7\xf5es de alta qualidade"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Perda de entropia cruzada impulsiona o aprendizado"}),": faz o modelo aumentar a probabilidade da posi\xe7\xe3o correta"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"57% de precis\xe3o \xe9 um grande avan\xe7o"}),": superando o melhor m\xe9todo anterior em 13 pontos percentuais"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Aumento de 8 simetrias"}),": aumenta efetivamente os dados de treinamento"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Aprendizado supervisionado tem teto"}),": n\xe3o pode superar o n\xedvel dos dados de treinamento"]}),"\n"]}),"\n",(0,o.jsx)(a.p,{children:'O aprendizado supervisionado \xe9 o "ponto de partida" do AlphaGo \u2014 ele herdou milhares de anos de sabedoria humana do Go, estabelecendo a base para o subsequente aprendizado por refor\xe7o.'}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"refer\xeancias",children:"Refer\xeancias"}),"\n",(0,o.jsxs)(a.ol,{children:["\n",(0,o.jsxs)(a.li,{children:['Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." ',(0,o.jsx)(a.em,{children:"Nature"}),", 529, 484-489."]}),"\n",(0,o.jsxs)(a.li,{children:['Maddison, C. J., et al. (2014). "Move Evaluation in Go Using Deep Convolutional Neural Networks." ',(0,o.jsx)(a.em,{children:"arXiv:1412.6564"}),"."]}),"\n",(0,o.jsxs)(a.li,{children:['Clark, C., & Storkey, A. (2015). "Training Deep Convolutional Neural Networks to Play Go." ',(0,o.jsx)(a.em,{children:"ICML"}),"."]}),"\n",(0,o.jsxs)(a.li,{children:["KGS Game Archives: ",(0,o.jsx)(a.a,{href:"https://www.gokgs.com/archives.jsp",children:"https://www.gokgs.com/archives.jsp"})]}),"\n"]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,o.jsx)(a,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},42948(e,a,r){r.d(a,{A:()=>s});r(59471);var t=r(61785),o=r(62615);function s({children:e,fallback:a}){return(0,t.A)()?(0,o.jsx)(o.Fragment,{children:e?.()}):a??null}},45695(e,a,r){r.d(a,{$W:()=>k,tO:()=>i,u8:()=>g,dW:()=>x});var t=r(59471),o=r(90989),s=r(62615);const n=19,d=[[3,3],[3,9],[3,15],[9,3],[9,9],[9,15],[15,3],[15,9],[15,15]];function i({size:e=400,stones:a=[],highlights:r=[],labels:i=[],onCellClick:l=null,showCoordinates:c=!0}){const p=(0,t.useRef)(null),h=c?30:15,m=e-2*h,x=m/18;return(0,t.useEffect)(()=>{if(!p.current)return;const e=o.Ltv(p.current);e.selectAll("*").remove();const t=e.append("g").attr("transform",`translate(${h}, ${h})`);t.append("rect").attr("x",-x/2).attr("y",-x/2).attr("width",m+x).attr("height",m+x).attr("fill","#dcb35c").attr("rx",4);const s=t.append("g").attr("class","grid");for(let a=0;a<n;a++)s.append("line").attr("class","grid-line").attr("x1",0).attr("y1",a*x).attr("x2",18*x).attr("y2",a*x);for(let a=0;a<n;a++)s.append("line").attr("class","grid-line").attr("x1",a*x).attr("y1",0).attr("x2",a*x).attr("y2",18*x);const u=t.append("g").attr("class","star-points");if(d.forEach(([e,a])=>{u.append("circle").attr("class","star-point").attr("cx",e*x).attr("cy",a*x).attr("r",x/8)}),r.length>0){const e=t.append("g").attr("class","highlights");r.forEach(({x:a,y:r,intensity:t})=>{e.append("rect").attr("class","heatmap-cell").attr("x",a*x-x/2).attr("y",r*x-x/2).attr("width",x).attr("height",x).attr("fill",o.Q3(t)).attr("opacity",.7*t)})}const j=t.append("g").attr("class","stones");if(a.forEach(({x:e,y:a,color:r})=>{const t="black"===r?"stone-black":"stone-white";j.append("circle").attr("cx",e*x+2).attr("cy",a*x+2).attr("r",.45*x).attr("fill","rgba(0,0,0,0.2)"),j.append("circle").attr("class",t).attr("cx",e*x).attr("cy",a*x).attr("r",.45*x)}),i.length>0){const e=t.append("g").attr("class","labels");i.forEach(({x:r,y:t,text:o})=>{const s=a.find(e=>e.x===r&&e.y===t),n="black"===s?.color?"#fff":"#000";e.append("text").attr("x",r*x).attr("y",t*x).attr("dy","0.35em").attr("text-anchor","middle").attr("fill",n).attr("font-size",.5*x).attr("font-weight","bold").text(o)})}if(c){const a=e.append("g").attr("class","coordinates"),r="ABCDEFGHJKLMNOPQRST";for(let e=0;e<n;e++)a.append("text").attr("x",h+e*x).attr("y",h/2).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(r[e]);for(let e=0;e<n;e++)a.append("text").attr("x",h/2).attr("y",h+e*x).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(n-e)}l&&t.append("g").attr("class","click-targets").selectAll("rect").data(o.y17(361)).enter().append("rect").attr("x",e=>e%n*x-x/2).attr("y",e=>Math.floor(e/n)*x-x/2).attr("width",x).attr("height",x).attr("fill","transparent").attr("cursor","pointer").on("click",(e,a)=>{const r=a%n,t=Math.floor(a/n);l({x:r,y:t})})},[e,a,r,i,c,l,x,h,m]),(0,s.jsx)("div",{className:"go-board-container",children:(0,s.jsx)("svg",{ref:p,width:e,height:e,className:"go-board"})})}var l=r(42948);const c=19,p={empty:function(){const e=[];for(let a=0;a<c;a++)for(let r=0;r<c;r++)e.push({x:r,y:a,prob:1/361});return e}(),corner:function(){const e=[],a=[[3,3],[3,15],[15,3],[15,15]],r=[[2,4],[4,2],[2,14],[4,16],[14,2],[16,4],[14,16],[16,14]];for(let t=0;t<c;t++)for(let o=0;o<c;o++){let s=.001;a.some(([e,a])=>e===o&&a===t)?s=.15:r.some(([e,a])=>e===o&&a===t)?s=.05:0!==o&&18!==o&&0!==t&&18!==t||(s=5e-4),e.push({x:o,y:t,prob:s})}return h(e)}(),move37:function(){const e=[],a={x:9,y:4},r=[[3,2],[15,2],[10,10],[8,6]];for(let t=0;t<c;t++)for(let o=0;o<c;o++){let s=.001;o===a.x&&t===a.y?s=.08:r.some(([e,a])=>e===o&&a===t)?s=.12:o>=5&&o<=13&&t>=5&&t<=13&&(s=.005+.01*Math.random()),e.push({x:o,y:t,prob:s})}return h(e)}()};function h(e){const a=e.reduce((e,a)=>e+a.prob,0);return e.map(e=>({...e,prob:e.prob/a}))}function m({initialPosition:e="corner",stones:a=[],highlightMoves:r=[],size:n=450,showTopN:d=5,interactive:i=!0}){const l=(0,t.useRef)(null),h=(0,t.useRef)(null),[m,x]=(0,t.useState)(p[e]||p.corner),[u,j]=(0,t.useState)(null),g=35,v=n-70,f=v/18;(0,t.useEffect)(()=>{if(!l.current)return;const e=o.Ltv(l.current);e.selectAll("*").remove();const r=e.append("g").attr("transform","translate(35, 35)");r.append("rect").attr("x",-f/2).attr("y",-f/2).attr("width",v+f).attr("height",v+f).attr("fill","#dcb35c").attr("rx",4);const t=Math.max(...m.map(e=>e.prob)),s=o.exT(o.oKI).domain([0,t]);r.append("g").attr("class","heatmap").selectAll("rect").data(m).enter().append("rect").attr("class","heatmap-cell").attr("x",e=>e.x*f-f/2).attr("y",e=>e.y*f-f/2).attr("width",f).attr("height",f).attr("fill",e=>s(e.prob)).attr("opacity",e=>.3+e.prob/t*.6).attr("cursor",i?"pointer":"default").on("mouseover",function(e,a){if(!i)return;o.Ltv(this).attr("stroke","#333").attr("stroke-width",2);o.Ltv(h.current).style("display","block").style("left",`${e.pageX+10}px`).style("top",e.pageY-10+"px").html(`\u4f4d\u7f6e: ${String.fromCharCode(65+a.x)}${19-a.y}<br>\u6a5f\u7387: ${(100*a.prob).toFixed(2)}%`)}).on("mouseout",function(){o.Ltv(this).attr("stroke","none"),o.Ltv(h.current).style("display","none")}).on("click",function(e,a){i&&j(a)});const n=r.append("g").attr("class","grid");for(let a=0;a<c;a++)n.append("line").attr("class","grid-line").attr("x1",0).attr("y1",a*f).attr("x2",18*f).attr("y2",a*f).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5),n.append("line").attr("class","grid-line").attr("x1",a*f).attr("y1",0).attr("x2",a*f).attr("y2",18*f).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5);const p=r.append("g").attr("class","stones");a.forEach(({x:e,y:a,color:r})=>{p.append("circle").attr("cx",e*f).attr("cy",a*f).attr("r",.45*f).attr("fill","black"===r?"#1a1a1a":"#f5f5f5").attr("stroke","black"===r?"#000":"#333").attr("stroke-width",1)});const x=[...m].sort((e,a)=>a.prob-e.prob).slice(0,d),u=r.append("g").attr("class","top-labels");x.forEach((e,r)=>{a.some(a=>a.x===e.x&&a.y===e.y)||(u.append("circle").attr("cx",e.x*f).attr("cy",e.y*f).attr("r",.3*f).attr("fill","rgba(255,255,255,0.8)").attr("stroke","#e74c3c").attr("stroke-width",2),u.append("text").attr("x",e.x*f).attr("y",e.y*f).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#e74c3c").attr("font-size",.4*f).attr("font-weight","bold").text(r+1))});const b=e.append("g").attr("class","coordinates");for(let a=0;a<c;a++)b.append("text").attr("x",g+a*f).attr("y",17.5).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text("ABCDEFGHJKLMNOPQRST"[a]),b.append("text").attr("x",17.5).attr("y",g+a*f).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(c-a)},[m,a,d,i,f,g,v]);const b=e=>{x(p[e]||p.corner)};return(0,s.jsxs)("div",{children:[i&&(0,s.jsxs)("div",{className:"d3-controls",children:[(0,s.jsx)("button",{className:"empty"===e?"active":"",onClick:()=>b("empty"),children:"\u5747\u52fb\u5206\u5e03"}),(0,s.jsx)("button",{className:"corner"===e?"active":"",onClick:()=>b("corner"),children:"\u958b\u5c40\u661f\u4f4d"}),(0,s.jsx)("button",{className:"move37"===e?"active":"",onClick:()=>b("move37"),children:"\u7b2c 37 \u624b"})]}),(0,s.jsx)("div",{className:"go-board-container",children:(0,s.jsx)("svg",{ref:l,width:n,height:n,className:"go-board"})}),(0,s.jsx)("div",{ref:h,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),u&&(0,s.jsx)("div",{className:"d3-legend",children:(0,s.jsxs)("div",{className:"d3-legend-item",children:["\u5df2\u9078\u64c7: ",String.fromCharCode(65+u.x),19-u.y,"\u2014 \u6a5f\u7387: ",(100*u.prob).toFixed(2),"%"]})}),(0,s.jsxs)("div",{className:"d3-legend",children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#ffffb2"}}),"\u4f4e\u6a5f\u7387"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#fd8d3c"}}),"\u4e2d\u6a5f\u7387"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#bd0026"}}),"\u9ad8\u6a5f\u7387"]})]})]})}function x(e){return(0,s.jsx)(l.A,{fallback:(0,s.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,s.jsx)(m,{...e})})}const u={name:"Root",visits:1600,value:.55,prior:1,children:[{name:"D4",visits:800,value:.62,prior:.35,selected:!0,children:[{name:"Q16",visits:400,value:.58,prior:.3},{name:"R4",visits:300,value:.65,prior:.25,selected:!0},{name:"C16",visits:100,value:.55,prior:.2}]},{name:"Q4",visits:500,value:.52,prior:.3,children:[{name:"D16",visits:300,value:.5,prior:.28},{name:"Q16",visits:200,value:.54,prior:.22}]},{name:"D16",visits:200,value:.48,prior:.2},{name:"Q16",visits:100,value:.45,prior:.15}]};function j({data:e=u,width:a=700,height:r=450,showPUCT:n=!0,cPuct:d=1.5,interactive:i=!0}){const l=(0,t.useRef)(null),c=(0,t.useRef)(null),[p,h]=(0,t.useState)(null),[m,x]=(0,t.useState)(d),j=40,g=40,v=a-g-40,f=r-j-40;return(0,t.useEffect)(()=>{if(!l.current)return;const t=o.Ltv(l.current);t.selectAll("*").remove();const s=o.B22().size([v,f-50]),d=o.Sk5(e);s(d);const p=t.append("g").attr("transform",`translate(${g}, ${j})`);p.append("g").attr("class","links").selectAll("path").data(d.links()).enter().append("path").attr("class",e=>"link "+(e.target.data.selected?"selected":"")).attr("fill","none").attr("stroke",e=>e.target.data.selected?"#4a90d9":"#999").attr("stroke-width",e=>e.target.data.selected?3:1.5).attr("d",o.vu().x(e=>e.x).y(e=>e.y));const x=p.append("g").attr("class","nodes").selectAll("g").data(d.descendants()).enter().append("g").attr("class","node").attr("transform",e=>`translate(${e.x}, ${e.y})`).attr("cursor",i?"pointer":"default").on("mouseover",function(e,a){if(!i)return;o.Ltv(this).select("circle").transition().duration(200).attr("r",30);const r=a.parent?a.parent.data.visits:a.data.visits,t=((e,a)=>{if(!a)return 0;const r=e.value,t=e.prior,o=e.visits;return r+m*t*Math.sqrt(a)/(1+o)})(a.data,r);o.Ltv(c.current).style("display","block").style("left",`${e.pageX+15}px`).style("top",e.pageY-10+"px").html(`\n            <strong>${a.data.name}</strong><br>\n            \u8a2a\u554f\u6b21\u6578 (N): ${a.data.visits}<br>\n            \u5e73\u5747\u50f9\u503c (Q): ${a.data.value.toFixed(3)}<br>\n            \u5148\u9a57\u6a5f\u7387 (P): ${(100*a.data.prior).toFixed(1)}%<br>\n            ${n?`PUCT \u5206\u6578: ${t.toFixed(3)}`:""}\n          `)}).on("mouseout",function(){o.Ltv(this).select("circle").transition().duration(200).attr("r",25),o.Ltv(c.current).style("display","none")}).on("click",function(e,a){i&&h(a.data)});x.append("circle").attr("r",25).attr("fill",e=>e.data.selected?"#4a90d9":"#fff").attr("stroke",a=>{if(a.data.selected)return"#2c5282";const r=a.data.visits/e.visits;return o.dM(.3+.5*r)}).attr("stroke-width",e=>e.data.selected?3:2),x.append("text").attr("dy",-5).attr("text-anchor","middle").attr("fill",e=>e.data.selected?"#fff":"#333").attr("font-size",11).attr("font-weight","bold").text(e=>e.data.name),x.append("text").attr("dy",10).attr("text-anchor","middle").attr("fill",e=>e.data.selected?"#fff":"#666").attr("font-size",9).text(e=>`N=${e.data.visits}`),t.append("text").attr("x",a/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("MCTS \u641c\u7d22\u6a39"),n&&t.append("text").attr("x",a/2).attr("y",r-10).attr("text-anchor","middle").attr("font-size",11).attr("fill","#666").text("\u85cd\u8272\u8def\u5f91\uff1aPUCT \u9078\u64c7\u7684\u6700\u4f73\u8def\u5f91")},[e,a,r,n,m,i,v,f]),(0,s.jsxs)("div",{children:[n&&i&&(0,s.jsx)("div",{className:"d3-controls",children:(0,s.jsxs)("div",{className:"d3-slider",children:[(0,s.jsxs)("label",{children:["c_puct: ",m.toFixed(1)]}),(0,s.jsx)("input",{type:"range",min:"0.5",max:"3",step:"0.1",value:m,onChange:e=>x(parseFloat(e.target.value))})]})}),(0,s.jsx)("div",{className:"mcts-tree-container",children:(0,s.jsx)("svg",{ref:l,width:a,height:r,className:"mcts-tree"})}),(0,s.jsx)("div",{ref:c,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),p&&(0,s.jsxs)("div",{className:"d3-legend",style:{background:"#f5f5f5",padding:"1rem",borderRadius:"4px"},children:[(0,s.jsxs)("strong",{children:["\u5df2\u9078\u64c7\u7bc0\u9ede: ",p.name]}),(0,s.jsxs)("div",{children:["\u8a2a\u554f\u6b21\u6578: ",p.visits]}),(0,s.jsxs)("div",{children:["\u5e73\u5747\u50f9\u503c: ",p.value.toFixed(3)]}),(0,s.jsxs)("div",{children:["\u5148\u9a57\u6a5f\u7387: ",(100*p.prior).toFixed(1),"%"]})]}),(0,s.jsxs)("div",{className:"d3-legend",children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"\u9078\u4e2d\u8def\u5f91"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#fff",border:"2px solid #999"}}),"\u5176\u4ed6\u7bc0\u9ede"]}),(0,s.jsx)("div",{className:"d3-legend-item",children:(0,s.jsx)("span",{style:{fontSize:"12px"},children:"\u7bc0\u9ede\u5927\u5c0f \u221d \u8a2a\u554f\u6b21\u6578"})})]})]})}function g(e){return(0,s.jsx)(l.A,{fallback:(0,s.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,s.jsx)(j,{...e})})}const v=[{hours:0,elo:0,label:"\u96a8\u6a5f"},{hours:3,elo:1e3,label:"\u767c\u73fe\u898f\u5247"},{hours:6,elo:2e3},{hours:12,elo:3e3,label:"\u767c\u73fe\u5b9a\u5f0f"},{hours:24,elo:4e3},{hours:36,elo:4500,label:"\u8d85\u8d8a Fan Hui"},{hours:48,elo:5e3},{hours:60,elo:5200,label:"\u8d85\u8d8a Lee Sedol"},{hours:72,elo:5400,label:"\u8d85\u8d8a\u539f\u7248 AlphaGo"}],f=[{elo:2700,label:"\u696d\u9918\u5f37\u8c6a"},{elo:3500,label:"Fan Hui (\u8077\u696d\u4e8c\u6bb5)"},{elo:4500,label:"Lee Sedol (\u4e16\u754c\u51a0\u8ecd)"},{elo:5e3,label:"\u539f\u7248 AlphaGo"}],b=[{epochs:0,elo:0},{epochs:10,elo:1500},{epochs:20,elo:2500},{epochs:30,elo:3e3},{epochs:40,elo:3200},{epochs:50,elo:3300}],y=[{games:0,elo:3300},{games:1e3,elo:3800},{games:5e3,elo:4200},{games:1e4,elo:4500},{games:5e4,elo:4800},{games:1e5,elo:5e3}];function z({mode:e="zero",width:a=600,height:r=400,animated:n=!0,showMilestones:d=!0}){const i=(0,t.useRef)(null),[l,c]=(0,t.useState)(e),p=40,h=70,m=a-h-100,x=r-p-60;return(0,t.useEffect)(()=>{if(!i.current)return;const e=o.Ltv(i.current);let r,t,s;e.selectAll("*").remove(),"zero"===l?(r=v,t="\u8a13\u7df4\u6642\u9593\uff08\u5c0f\u6642\uff09",s=[0,80]):"sl"===l?(r=b,t="\u8a13\u7df4\u8f2a\u6578\uff08Epochs\uff09",s=[0,60]):(r=y,t="\u81ea\u6211\u5c0d\u5f08\u5c40\u6578",s=[0,12e4]);const c="selfplay"===l?o.ZEH().domain([1,s[1]]).range([0,m]):o.m4Y().domain(s).range([0,m]),u=o.m4Y().domain([0,6e3]).range([x,0]),j=e.append("g").attr("transform",`translate(${h}, ${p})`);if(j.append("g").attr("class","grid").selectAll(".grid-line-y").data(u.ticks(6)).enter().append("line").attr("class","grid-line-y").attr("x1",0).attr("x2",m).attr("y1",e=>u(e)).attr("y2",e=>u(e)).attr("stroke","#ddd").attr("stroke-dasharray","3,3"),d&&"zero"===l){const e=j.append("g").attr("class","human-levels");f.forEach(a=>{e.append("line").attr("x1",0).attr("x2",m).attr("y1",u(a.elo)).attr("y2",u(a.elo)).attr("stroke","#e74c3c").attr("stroke-dasharray","5,5").attr("opacity",.6),e.append("text").attr("x",m+5).attr("y",u(a.elo)).attr("dy","0.35em").attr("fill","#e74c3c").attr("font-size",10).text(a.label)})}const g=o.n8j().x(e=>c("zero"===l?e.hours:"sl"===l?e.epochs:Math.max(1,e.games))).y(e=>u(e.elo)).curve(o.nVG),z=o.Wcw().x(e=>c("zero"===l?e.hours:"sl"===l?e.epochs:Math.max(1,e.games))).y0(x).y1(e=>u(e.elo)).curve(o.nVG);j.append("path").datum(r).attr("class","area").attr("fill","#4a90d9").attr("opacity",.1).attr("d",z);const k=j.append("path").datum(r).attr("class","line").attr("fill","none").attr("stroke","#4a90d9").attr("stroke-width",3).attr("d",g);if(n){const e=k.node().getTotalLength();k.attr("stroke-dasharray",`${e} ${e}`).attr("stroke-dashoffset",e).transition().duration(2e3).ease(o.yfw).attr("stroke-dashoffset",0)}if(d&&"zero"===l){const e=r.filter(e=>e.label),a=j.append("g").attr("class","milestones");a.selectAll("circle").data(e).enter().append("circle").attr("cx",e=>c(e.hours)).attr("cy",e=>u(e.elo)).attr("r",6).attr("fill","#e74c3c").attr("stroke","#fff").attr("stroke-width",2),a.selectAll("text").data(e).enter().append("text").attr("x",e=>c(e.hours)).attr("y",e=>u(e.elo)-15).attr("text-anchor","middle").attr("fill","#333").attr("font-size",10).text(e=>e.label)}const P="selfplay"===l?o.l78(c).ticks(5,"~s"):o.l78(c);j.append("g").attr("class","x-axis").attr("transform",`translate(0, ${x})`).call(P),j.append("text").attr("class","axis-label").attr("x",m/2).attr("y",x+45).attr("text-anchor","middle").attr("fill","#666").text(t),j.append("g").attr("class","y-axis").call(o.V4s(u).ticks(6)),j.append("text").attr("class","axis-label").attr("transform","rotate(-90)").attr("x",-x/2).attr("y",-50).attr("text-anchor","middle").attr("fill","#666").text("ELO \u8a55\u5206"),e.append("text").attr("x",a/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("zero"===l?"AlphaGo Zero \u8a13\u7df4\u66f2\u7dda":"sl"===l?"\u76e3\u7763\u5b78\u7fd2\u68cb\u529b\u6210\u9577":"\u81ea\u6211\u5c0d\u5f08\u68cb\u529b\u6210\u9577")},[l,a,r,n,d,m,x]),(0,s.jsxs)("div",{children:[(0,s.jsxs)("div",{className:"d3-controls",children:[(0,s.jsx)("button",{className:"zero"===l?"active":"",onClick:()=>c("zero"),children:"AlphaGo Zero"}),(0,s.jsx)("button",{className:"sl"===l?"active":"",onClick:()=>c("sl"),children:"\u76e3\u7763\u5b78\u7fd2"}),(0,s.jsx)("button",{className:"selfplay"===l?"active":"",onClick:()=>c("selfplay"),children:"\u81ea\u6211\u5c0d\u5f08"})]}),(0,s.jsx)("div",{className:"elo-chart-container",children:(0,s.jsx)("svg",{ref:i,width:a,height:r,className:"elo-chart"})}),(0,s.jsxs)("div",{className:"d3-legend",children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"ELO \u8a55\u5206"]}),d&&"zero"===l&&(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c"}}),"\u91cc\u7a0b\u7891"]}),(0,s.jsxs)("div",{className:"d3-legend-item",children:[(0,s.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c",opacity:.3}}),"\u4eba\u985e\u6c34\u5e73\u53c3\u8003\u7dda"]})]})]})]})}function k(e){return(0,s.jsx)(l.A,{fallback:(0,s.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,s.jsx)(z,{...e})})}},30416(e,a,r){r.d(a,{R:()=>n,x:()=>d});var t=r(59471);const o={},s=t.createContext(o);function n(e){const a=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function d(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:n(e.components),t.createElement(s.Provider,{value:a},e.children)}}}]);