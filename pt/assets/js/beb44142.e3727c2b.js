"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[5529],{21527(e,n,a){a.r(n),a.d(n,{assets:()=>d,contentTitle:()=>t,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"for-engineers/deep-dive/quantization-deploy","title":"Quantiza\xe7\xe3o e Implanta\xe7\xe3o de Modelos","description":"T\xe9cnicas de quantiza\xe7\xe3o de modelos KataGo, formatos de exporta\xe7\xe3o e solu\xe7\xf5es de implanta\xe7\xe3o para v\xe1rias plataformas","source":"@site/i18n/pt/docusaurus-plugin-content-docs/current/for-engineers/deep-dive/quantization-deploy.md","sourceDirName":"for-engineers/deep-dive","slug":"/for-engineers/deep-dive/quantization-deploy","permalink":"/pt/docs/for-engineers/deep-dive/quantization-deploy","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/deep-dive/quantization-deploy.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"title":"Quantiza\xe7\xe3o e Implanta\xe7\xe3o de Modelos","description":"T\xe9cnicas de quantiza\xe7\xe3o de modelos KataGo, formatos de exporta\xe7\xe3o e solu\xe7\xf5es de implanta\xe7\xe3o para v\xe1rias plataformas"},"sidebar":"tutorialSidebar","previous":{"title":"Arquitetura de Treinamento Distribu\xeddo","permalink":"/pt/docs/for-engineers/deep-dive/distributed-training"},"next":{"title":"Avaliacao e Benchmarks","permalink":"/pt/docs/for-engineers/deep-dive/evaluation"}}');var o=a(62615),r=a(30416);const s={sidebar_position:8,title:"Quantiza\xe7\xe3o e Implanta\xe7\xe3o de Modelos",description:"T\xe9cnicas de quantiza\xe7\xe3o de modelos KataGo, formatos de exporta\xe7\xe3o e solu\xe7\xf5es de implanta\xe7\xe3o para v\xe1rias plataformas"},t="Quantiza\xe7\xe3o e Implanta\xe7\xe3o de Modelos",d={},l=[{value:"Vis\xe3o Geral das T\xe9cnicas de Quantiza\xe7\xe3o",id:"vis\xe3o-geral-das-t\xe9cnicas-de-quantiza\xe7\xe3o",level:2},{value:"Por que Quantizar?",id:"por-que-quantizar",level:3},{value:"Tipos de Quantiza\xe7\xe3o",id:"tipos-de-quantiza\xe7\xe3o",level:3},{value:"FP16 Meia Precis\xe3o",id:"fp16-meia-precis\xe3o",level:2},{value:"Conceito",id:"conceito",level:3},{value:"Configura\xe7\xe3o do KataGo",id:"configura\xe7\xe3o-do-katago",level:3},{value:"Impacto no Desempenho",id:"impacto-no-desempenho",level:3},{value:"Quantiza\xe7\xe3o INT8",id:"quantiza\xe7\xe3o-int8",level:2},{value:"Fluxo de Quantiza\xe7\xe3o",id:"fluxo-de-quantiza\xe7\xe3o",level:3},{value:"Dados de Calibra\xe7\xe3o",id:"dados-de-calibra\xe7\xe3o",level:3},{value:"Observa\xe7\xf5es",id:"observa\xe7\xf5es",level:3},{value:"Implanta\xe7\xe3o com TensorRT",id:"implanta\xe7\xe3o-com-tensorrt",level:2},{value:"Fluxo de Convers\xe3o",id:"fluxo-de-convers\xe3o",level:3},{value:"Usando Engine TensorRT",id:"usando-engine-tensorrt",level:3},{value:"Exporta\xe7\xe3o ONNX",id:"exporta\xe7\xe3o-onnx",level:2},{value:"PyTorch \u2192 ONNX",id:"pytorch--onnx",level:3},{value:"Validar Modelo ONNX",id:"validar-modelo-onnx",level:3},{value:"Implanta\xe7\xe3o em V\xe1rias Plataformas",id:"implanta\xe7\xe3o-em-v\xe1rias-plataformas",level:2},{value:"Implanta\xe7\xe3o em Servidor",id:"implanta\xe7\xe3o-em-servidor",level:3},{value:"Integra\xe7\xe3o com Aplicativo Desktop",id:"integra\xe7\xe3o-com-aplicativo-desktop",level:3},{value:"Implanta\xe7\xe3o em Dispositivos M\xf3veis",id:"implanta\xe7\xe3o-em-dispositivos-m\xf3veis",level:3},{value:"iOS (Core ML)",id:"ios-core-ml",level:4},{value:"Android (TensorFlow Lite)",id:"android-tensorflow-lite",level:4},{value:"Sistemas Embarcados",id:"sistemas-embarcados",level:3},{value:"Raspberry Pi",id:"raspberry-pi",level:4},{value:"NVIDIA Jetson",id:"nvidia-jetson",level:4},{value:"Compara\xe7\xe3o de Desempenho",id:"compara\xe7\xe3o-de-desempenho",level:2},{value:"Desempenho de Diferentes M\xe9todos de Implanta\xe7\xe3o",id:"desempenho-de-diferentes-m\xe9todos-de-implanta\xe7\xe3o",level:3},{value:"Compara\xe7\xe3o de Tamanho de Modelo",id:"compara\xe7\xe3o-de-tamanho-de-modelo",level:3},{value:"Checklist de Implanta\xe7\xe3o",id:"checklist-de-implanta\xe7\xe3o",level:2},{value:"Leitura Adicional",id:"leitura-adicional",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",input:"input",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"quantiza\xe7\xe3o-e-implanta\xe7\xe3o-de-modelos",children:"Quantiza\xe7\xe3o e Implanta\xe7\xe3o de Modelos"})}),"\n",(0,o.jsx)(n.p,{children:"Este artigo apresenta como quantizar modelos KataGo para reduzir requisitos de recursos, al\xe9m de solu\xe7\xf5es de implanta\xe7\xe3o para v\xe1rias plataformas."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"vis\xe3o-geral-das-t\xe9cnicas-de-quantiza\xe7\xe3o",children:"Vis\xe3o Geral das T\xe9cnicas de Quantiza\xe7\xe3o"}),"\n",(0,o.jsx)(n.h3,{id:"por-que-quantizar",children:"Por que Quantizar?"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Precis\xe3o"}),(0,o.jsx)(n.th,{children:"Tamanho"}),(0,o.jsx)(n.th,{children:"Velocidade"}),(0,o.jsx)(n.th,{children:"Perda de Precis\xe3o"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"FP32"}),(0,o.jsx)(n.td,{children:"100%"}),(0,o.jsx)(n.td,{children:"Base"}),(0,o.jsx)(n.td,{children:"0%"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"FP16"}),(0,o.jsx)(n.td,{children:"50%"}),(0,o.jsx)(n.td,{children:"+50%"}),(0,o.jsx)(n.td,{children:"~0%"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"INT8"}),(0,o.jsx)(n.td,{children:"25%"}),(0,o.jsx)(n.td,{children:"+100%"}),(0,o.jsx)(n.td,{children:"<1%"})]})]})]}),"\n",(0,o.jsx)(n.h3,{id:"tipos-de-quantiza\xe7\xe3o",children:"Tipos de Quantiza\xe7\xe3o"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Quantiza\xe7\xe3o P\xf3s-Treinamento (PTQ)\n\u251c\u2500\u2500 Simples e r\xe1pida\n\u251c\u2500\u2500 N\xe3o requer retreinamento\n\u2514\u2500\u2500 Pode ter perda de precis\xe3o\n\nTreinamento com Consci\xeancia de Quantiza\xe7\xe3o (QAT)\n\u251c\u2500\u2500 Maior precis\xe3o\n\u251c\u2500\u2500 Requer retreinamento\n\u2514\u2500\u2500 Mais complexo\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"fp16-meia-precis\xe3o",children:"FP16 Meia Precis\xe3o"}),"\n",(0,o.jsx)(n.h3,{id:"conceito",children:"Conceito"}),"\n",(0,o.jsx)(n.p,{children:"Converter n\xfameros de ponto flutuante de 32 bits para 16 bits:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Convers\xe3o FP32 \u2192 FP16\nmodel_fp16 = model.half()\n\n# Infer\xeancia\nwith torch.cuda.amp.autocast():\n    output = model_fp16(input.half())\n"})}),"\n",(0,o.jsx)(n.h3,{id:"configura\xe7\xe3o-do-katago",children:"Configura\xe7\xe3o do KataGo"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-ini",children:"# config.cfg\nuseFP16 = true           # Habilitar infer\xeancia FP16\nuseFP16Storage = true    # Armazenar resultados intermedi\xe1rios em FP16\n"})}),"\n",(0,o.jsx)(n.h3,{id:"impacto-no-desempenho",children:"Impacto no Desempenho"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"S\xe9rie de GPU"}),(0,o.jsx)(n.th,{children:"Acelera\xe7\xe3o FP16"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"GTX 10xx"}),(0,o.jsx)(n.td,{children:"Nenhuma (sem Tensor Core)"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"RTX 20xx"}),(0,o.jsx)(n.td,{children:"+30-50%"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"RTX 30xx"}),(0,o.jsx)(n.td,{children:"+50-80%"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"RTX 40xx"}),(0,o.jsx)(n.td,{children:"+80-100%"})]})]})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"quantiza\xe7\xe3o-int8",children:"Quantiza\xe7\xe3o INT8"}),"\n",(0,o.jsx)(n.h3,{id:"fluxo-de-quantiza\xe7\xe3o",children:"Fluxo de Quantiza\xe7\xe3o"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import torch.quantization as quant\n\n# 1. Preparar modelo\nmodel.eval()\nmodel.qconfig = quant.get_default_qconfig('fbgemm')\n\n# 2. Preparar quantiza\xe7\xe3o\nmodel_prepared = quant.prepare(model)\n\n# 3. Calibra\xe7\xe3o (usando dados representativos)\nwith torch.no_grad():\n    for data in calibration_loader:\n        model_prepared(data)\n\n# 4. Converter para modelo quantizado\nmodel_quantized = quant.convert(model_prepared)\n"})}),"\n",(0,o.jsx)(n.h3,{id:"dados-de-calibra\xe7\xe3o",children:"Dados de Calibra\xe7\xe3o"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def create_calibration_dataset(num_samples=1000):\n    """Cria conjunto de dados de calibra\xe7\xe3o"""\n    samples = []\n\n    # Amostrar de partidas reais\n    for game in random_games(num_samples):\n        position = random_position(game)\n        features = encode_state(position)\n        samples.append(features)\n\n    return samples\n'})}),"\n",(0,o.jsx)(n.h3,{id:"observa\xe7\xf5es",children:"Observa\xe7\xf5es"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Quantiza\xe7\xe3o INT8 requer dados de calibra\xe7\xe3o"}),"\n",(0,o.jsx)(n.li,{children:"Algumas camadas podem n\xe3o ser adequadas para quantiza\xe7\xe3o"}),"\n",(0,o.jsx)(n.li,{children:"Necess\xe1rio testar perda de precis\xe3o"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"implanta\xe7\xe3o-com-tensorrt",children:"Implanta\xe7\xe3o com TensorRT"}),"\n",(0,o.jsx)(n.h3,{id:"fluxo-de-convers\xe3o",children:"Fluxo de Convers\xe3o"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import tensorrt as trt\n\ndef convert_to_tensorrt(onnx_path, engine_path):\n    logger = trt.Logger(trt.Logger.WARNING)\n    builder = trt.Builder(logger)\n    network = builder.create_network(\n        1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n    )\n    parser = trt.OnnxParser(network, logger)\n\n    # Parsear modelo ONNX\n    with open(onnx_path, 'rb') as f:\n        parser.parse(f.read())\n\n    # Configurar op\xe7\xf5es de otimiza\xe7\xe3o\n    config = builder.create_builder_config()\n    config.max_workspace_size = 1 << 30  # 1GB\n\n    # Habilitar FP16\n    config.set_flag(trt.BuilderFlag.FP16)\n\n    # Construir engine\n    engine = builder.build_engine(network, config)\n\n    # Salvar\n    with open(engine_path, 'wb') as f:\n        f.write(engine.serialize())\n"})}),"\n",(0,o.jsx)(n.h3,{id:"usando-engine-tensorrt",children:"Usando Engine TensorRT"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def inference_with_tensorrt(engine_path, input_data):\n    # Carregar engine\n    with open(engine_path, 'rb') as f:\n        engine = trt.Runtime(logger).deserialize_cuda_engine(f.read())\n\n    context = engine.create_execution_context()\n\n    # Alocar mem\xf3ria\n    d_input = cuda.mem_alloc(input_data.nbytes)\n    d_output = cuda.mem_alloc(output_size)\n\n    # Copiar entrada\n    cuda.memcpy_htod(d_input, input_data)\n\n    # Executar infer\xeancia\n    context.execute_v2([int(d_input), int(d_output)])\n\n    # Obter sa\xedda\n    output = np.empty(output_shape, dtype=np.float32)\n    cuda.memcpy_dtoh(output, d_output)\n\n    return output\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"exporta\xe7\xe3o-onnx",children:"Exporta\xe7\xe3o ONNX"}),"\n",(0,o.jsx)(n.h3,{id:"pytorch--onnx",children:"PyTorch \u2192 ONNX"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import torch.onnx\n\ndef export_to_onnx(model, output_path):\n    model.eval()\n\n    # Criar entrada de exemplo\n    dummy_input = torch.randn(1, 22, 19, 19)\n\n    # Exportar\n    torch.onnx.export(\n        model,\n        dummy_input,\n        output_path,\n        input_names=['input'],\n        output_names=['policy', 'value', 'ownership'],\n        dynamic_axes={\n            'input': {0: 'batch_size'},\n            'policy': {0: 'batch_size'},\n            'value': {0: 'batch_size'},\n            'ownership': {0: 'batch_size'}\n        },\n        opset_version=13\n    )\n"})}),"\n",(0,o.jsx)(n.h3,{id:"validar-modelo-onnx",children:"Validar Modelo ONNX"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import onnx\nimport onnxruntime as ort\n\n# Validar estrutura do modelo\nmodel = onnx.load("model.onnx")\nonnx.checker.check_model(model)\n\n# Testar infer\xeancia\nsession = ort.InferenceSession("model.onnx")\noutput = session.run(None, {\'input\': input_data})\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"implanta\xe7\xe3o-em-v\xe1rias-plataformas",children:"Implanta\xe7\xe3o em V\xe1rias Plataformas"}),"\n",(0,o.jsx)(n.h3,{id:"implanta\xe7\xe3o-em-servidor",children:"Implanta\xe7\xe3o em Servidor"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"# docker-compose.yml\nversion: '3'\nservices:\n  katago:\n    image: katago/katago:latest\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n    volumes:\n      - ./models:/models\n      - ./config:/config\n    command: >\n      katago analysis\n      -model /models/kata-b18c384.bin.gz\n      -config /config/analysis.cfg\n"})}),"\n",(0,o.jsx)(n.h3,{id:"integra\xe7\xe3o-com-aplicativo-desktop",children:"Integra\xe7\xe3o com Aplicativo Desktop"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Embutir KataGo em aplica\xe7\xe3o Python\nimport subprocess\nimport json\n\nclass KataGoProcess:\n    def __init__(self, katago_path, model_path):\n        self.process = subprocess.Popen(\n            [katago_path, 'analysis', '-model', model_path],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            text=True\n        )\n\n    def analyze(self, moves):\n        query = {\n            'id': 'query1',\n            'moves': moves,\n            'rules': 'chinese',\n            'komi': 7.5,\n            'boardXSize': 19,\n            'boardYSize': 19\n        }\n        self.process.stdin.write(json.dumps(query) + '\\n')\n        self.process.stdin.flush()\n\n        response = self.process.stdout.readline()\n        return json.loads(response)\n"})}),"\n",(0,o.jsx)(n.h3,{id:"implanta\xe7\xe3o-em-dispositivos-m\xf3veis",children:"Implanta\xe7\xe3o em Dispositivos M\xf3veis"}),"\n",(0,o.jsx)(n.h4,{id:"ios-core-ml",children:"iOS (Core ML)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import coremltools as ct\n\n# Converter para Core ML\nmlmodel = ct.convert(\n    model,\n    inputs=[ct.TensorType(shape=(1, 22, 19, 19))],\n    minimum_deployment_target=ct.target.iOS15\n)\n\nmlmodel.save("KataGo.mlmodel")\n'})}),"\n",(0,o.jsx)(n.h4,{id:"android-tensorflow-lite",children:"Android (TensorFlow Lite)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import tensorflow as tf\n\n# Converter para TFLite\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_path)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.float16]\n\ntflite_model = converter.convert()\n\nwith open('katago.tflite', 'wb') as f:\n    f.write(tflite_model)\n"})}),"\n",(0,o.jsx)(n.h3,{id:"sistemas-embarcados",children:"Sistemas Embarcados"}),"\n",(0,o.jsx)(n.h4,{id:"raspberry-pi",children:"Raspberry Pi"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Usar backend Eigen (CPU pura)\n./katago gtp -model kata-b10c128.bin.gz -config rpi.cfg\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-ini",children:"# rpi.cfg - Configura\xe7\xe3o otimizada para Raspberry Pi\nnumSearchThreads = 4\nmaxVisits = 100\nnnMaxBatchSize = 1\n"})}),"\n",(0,o.jsx)(n.h4,{id:"nvidia-jetson",children:"NVIDIA Jetson"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Usar backend CUDA\n./katago gtp -model kata-b18c384.bin.gz -config jetson.cfg\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"compara\xe7\xe3o-de-desempenho",children:"Compara\xe7\xe3o de Desempenho"}),"\n",(0,o.jsx)(n.h3,{id:"desempenho-de-diferentes-m\xe9todos-de-implanta\xe7\xe3o",children:"Desempenho de Diferentes M\xe9todos de Implanta\xe7\xe3o"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"M\xe9todo de Implanta\xe7\xe3o"}),(0,o.jsx)(n.th,{children:"Hardware"}),(0,o.jsx)(n.th,{children:"Playouts/seg"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"CUDA FP32"}),(0,o.jsx)(n.td,{children:"RTX 3080"}),(0,o.jsx)(n.td,{children:"~3000"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"CUDA FP16"}),(0,o.jsx)(n.td,{children:"RTX 3080"}),(0,o.jsx)(n.td,{children:"~5000"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"TensorRT FP16"}),(0,o.jsx)(n.td,{children:"RTX 3080"}),(0,o.jsx)(n.td,{children:"~6500"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"OpenCL"}),(0,o.jsx)(n.td,{children:"M1 Pro"}),(0,o.jsx)(n.td,{children:"~1500"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Core ML"}),(0,o.jsx)(n.td,{children:"M1 Pro"}),(0,o.jsx)(n.td,{children:"~1800"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"TFLite"}),(0,o.jsx)(n.td,{children:"Pixel 7"}),(0,o.jsx)(n.td,{children:"~50"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Eigen"}),(0,o.jsx)(n.td,{children:"RPi 4"}),(0,o.jsx)(n.td,{children:"~15"})]})]})]}),"\n",(0,o.jsx)(n.h3,{id:"compara\xe7\xe3o-de-tamanho-de-modelo",children:"Compara\xe7\xe3o de Tamanho de Modelo"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Formato"}),(0,o.jsx)(n.th,{children:"Tamanho b18c384"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Original (.bin.gz)"}),(0,o.jsx)(n.td,{children:"~140 MB"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"ONNX FP32"}),(0,o.jsx)(n.td,{children:"~280 MB"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"ONNX FP16"}),(0,o.jsx)(n.td,{children:"~140 MB"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"TensorRT FP16"}),(0,o.jsx)(n.td,{children:"~100 MB"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"TFLite FP16"}),(0,o.jsx)(n.td,{children:"~140 MB"})]})]})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"checklist-de-implanta\xe7\xe3o",children:"Checklist de Implanta\xe7\xe3o"}),"\n",(0,o.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Escolher precis\xe3o de quantiza\xe7\xe3o adequada"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Preparar dados de calibra\xe7\xe3o (INT8)"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Exportar para formato de destino"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Validar que perda de precis\xe3o \xe9 aceit\xe1vel"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Testar desempenho na plataforma alvo"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Otimizar uso de mem\xf3ria"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Criar fluxo de implanta\xe7\xe3o automatizado"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"leitura-adicional",children:"Leitura Adicional"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"../gpu-optimization",children:"Backend GPU e Otimiza\xe7\xe3o"})," \u2014 Otimiza\xe7\xe3o b\xe1sica de desempenho"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"../evaluation",children:"Avalia\xe7\xe3o e Benchmarks"})," \u2014 Verificar desempenho ap\xf3s implanta\xe7\xe3o"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"../../hands-on/integration",children:"Integra\xe7\xe3o no Seu Projeto"})," \u2014 Exemplos de integra\xe7\xe3o via API"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},30416(e,n,a){a.d(n,{R:()=>s,x:()=>t});var i=a(59471);const o={},r=i.createContext(o);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);