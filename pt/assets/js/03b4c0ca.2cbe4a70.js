"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[9623],{31558(e,a,n){n.r(a),n.d(a,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>t});const r=JSON.parse('{"id":"for-engineers/how-it-works/alphago-explained/value-network","title":"Detalhes da Value Network","description":"Compreens\xe3o aprofundada da arquitetura, desafios de treinamento e papel cr\xedtico da rede de valor do AlphaGo no MCTS","source":"@site/i18n/pt/docusaurus-plugin-content-docs/current/for-engineers/how-it-works/alphago-explained/08-value-network.mdx","sourceDirName":"for-engineers/how-it-works/alphago-explained","slug":"/for-engineers/how-it-works/alphago-explained/value-network","permalink":"/pt/docs/for-engineers/how-it-works/alphago-explained/value-network","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/how-it-works/alphago-explained/08-value-network.mdx","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9,"title":"Detalhes da Value Network","description":"Compreens\xe3o aprofundada da arquitetura, desafios de treinamento e papel cr\xedtico da rede de valor do AlphaGo no MCTS"},"sidebar":"tutorialSidebar","previous":{"title":"Detalhes da Policy Network","permalink":"/pt/docs/for-engineers/how-it-works/alphago-explained/policy-network"},"next":{"title":"Design de Caracteristicas de Entrada","permalink":"/pt/docs/for-engineers/how-it-works/alphago-explained/input-features"}}');var i=n(62615),o=n(30416);const s={sidebar_position:9,title:"Detalhes da Value Network",description:"Compreens\xe3o aprofundada da arquitetura, desafios de treinamento e papel cr\xedtico da rede de valor do AlphaGo no MCTS"},d="Detalhes da Value Network",l={},t=[{value:"O que \xe9 a Value Network?",id:"o-que-\xe9-a-value-network",level:2},{value:"Fun\xe7\xe3o Principal",id:"fun\xe7\xe3o-principal",level:3},{value:"Significado da Sa\xedda",id:"significado-da-sa\xedda",level:3},{value:"Por que precisamos de um \xfanico valor?",id:"por-que-precisamos-de-um-\xfanico-valor",level:3},{value:"Comparar diferentes escolhas",id:"comparar-diferentes-escolhas",level:4},{value:"Substituir muitas simula\xe7\xf5es",id:"substituir-muitas-simula\xe7\xf5es",level:4},{value:"Arquitetura da Rede",id:"arquitetura-da-rede",level:2},{value:"Similaridade com a Policy Network",id:"similaridade-com-a-policy-network",level:3},{value:"Camada de Entrada",id:"camada-de-entrada",level:3},{value:"Camadas Convolucionais",id:"camadas-convolucionais",level:3},{value:"Diferen\xe7a na Camada de Sa\xedda",id:"diferen\xe7a-na-camada-de-sa\xedda",level:3},{value:"Sa\xedda da Policy Network",id:"sa\xedda-da-policy-network",level:4},{value:"Sa\xedda da Value Network",id:"sa\xedda-da-value-network",level:4},{value:"Fun\xe7\xe3o de Ativa\xe7\xe3o Tanh",id:"fun\xe7\xe3o-de-ativa\xe7\xe3o-tanh",level:3},{value:"Por que usar Tanh em vez de Sigmoid?",id:"por-que-usar-tanh-em-vez-de-sigmoid",level:4},{value:"Diagrama Completo da Arquitetura",id:"diagrama-completo-da-arquitetura",level:3},{value:"Contagem de Par\xe2metros",id:"contagem-de-par\xe2metros",level:3},{value:"Desafios do Treinamento",id:"desafios-do-treinamento",level:2},{value:"Problema de Overfitting",id:"problema-de-overfitting",level:3},{value:"O que \xe9 overfitting?",id:"o-que-\xe9-overfitting",level:4},{value:"Por que a Value Network \xe9 propensa a overfitting?",id:"por-que-a-value-network-\xe9-propensa-a-overfitting",level:4},{value:"Solu\xe7\xe3o: Dados de Auto-jogo",id:"solu\xe7\xe3o-dados-de-auto-jogo",level:3},{value:"Por que isso resolve o overfitting?",id:"por-que-isso-resolve-o-overfitting",level:4},{value:"Gera\xe7\xe3o de Dados de Treinamento",id:"gera\xe7\xe3o-de-dados-de-treinamento",level:3},{value:"Objetivo e M\xe9todos de Treinamento",id:"objetivo-e-m\xe9todos-de-treinamento",level:2},{value:"Perda de Erro Quadr\xe1tico M\xe9dio",id:"perda-de-erro-quadr\xe1tico-m\xe9dio",level:3},{value:"Por que usar MSE em vez de entropia cruzada?",id:"por-que-usar-mse-em-vez-de-entropia-cruzada",level:4},{value:"Processo de Treinamento",id:"processo-de-treinamento",level:3},{value:"An\xe1lise de Precis\xe3o",id:"an\xe1lise-de-precis\xe3o",level:2},{value:"Compara\xe7\xe3o com Simula\xe7\xe3o Aleat\xf3ria",id:"compara\xe7\xe3o-com-simula\xe7\xe3o-aleat\xf3ria",level:3},{value:"Precis\xe3o em Diferentes Fases",id:"precis\xe3o-em-diferentes-fases",level:3},{value:"Distribui\xe7\xe3o de Sa\xedda",id:"distribui\xe7\xe3o-de-sa\xedda",level:3},{value:"Posi\xe7\xf5es Incertas",id:"posi\xe7\xf5es-incertas",level:3},{value:"Papel no MCTS",id:"papel-no-mcts",level:2},{value:"Avalia\xe7\xe3o de N\xf3s Folha",id:"avalia\xe7\xe3o-de-n\xf3s-folha",level:3},{value:"Por que combinar?",id:"por-que-combinar",level:4},{value:"Simplifica\xe7\xe3o no AlphaGo Zero",id:"simplifica\xe7\xe3o-no-alphago-zero",level:3},{value:"Atualiza\xe7\xe3o por Backpropagation",id:"atualiza\xe7\xe3o-por-backpropagation",level:3},{value:"An\xe1lise Visual",id:"an\xe1lise-visual",level:2},{value:"Superf\xedcie de Valor",id:"superf\xedcie-de-valor",level:3},{value:"Evolu\xe7\xe3o Durante o Treinamento",id:"evolu\xe7\xe3o-durante-o-treinamento",level:3},{value:"Identifica\xe7\xe3o de Posi\xe7\xf5es Dif\xedceis",id:"identifica\xe7\xe3o-de-posi\xe7\xf5es-dif\xedceis",level:3},{value:"Pontos de Implementa\xe7\xe3o",id:"pontos-de-implementa\xe7\xe3o",level:2},{value:"Implementa\xe7\xe3o em PyTorch",id:"implementa\xe7\xe3o-em-pytorch",level:3},{value:"Loop de Treinamento",id:"loop-de-treinamento",level:3},{value:"T\xe9cnicas para Evitar Overfitting",id:"t\xe9cnicas-para-evitar-overfitting",level:3},{value:"Colabora\xe7\xe3o com a Policy Network",id:"colabora\xe7\xe3o-com-a-policy-network",level:2},{value:"Rela\xe7\xe3o Complementar",id:"rela\xe7\xe3o-complementar",level:3},{value:"Rede de Cabe\xe7a Dupla Unificada",id:"rede-de-cabe\xe7a-dupla-unificada",level:3},{value:"Correspond\xeancia com Anima\xe7\xf5es",id:"correspond\xeancia-com-anima\xe7\xf5es",level:2},{value:"Leitura Adicional",id:"leitura-adicional",level:2},{value:"Pontos-Chave",id:"pontos-chave",level:2},{value:"Refer\xeancias",id:"refer\xeancias",level:2}];function c(e){const a={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(a.header,{children:(0,i.jsx)(a.h1,{id:"detalhes-da-value-network",children:"Detalhes da Value Network"})}),"\n",(0,i.jsx)(a.p,{children:'Se a Policy Network diz ao AlphaGo "onde deve jogar a seguir", a Value Network responde a uma quest\xe3o mais fundamental:'}),"\n",(0,i.jsxs)(a.blockquote,{children:["\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:'"Este jogo, eu vou vencer?"'})}),"\n"]}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"o-que-\xe9-a-value-network",children:"O que \xe9 a Value Network?"}),"\n",(0,i.jsx)(a.h3,{id:"fun\xe7\xe3o-principal",children:"Fun\xe7\xe3o Principal"}),"\n",(0,i.jsx)(a.p,{children:"A Value Network \xe9 uma rede neural convolucional profunda, cuja tarefa \xe9:"}),"\n",(0,i.jsxs)(a.blockquote,{children:["\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"Dado o estado atual do tabuleiro, prever a taxa de vit\xf3ria final"})}),"\n"]}),"\n",(0,i.jsx)(a.p,{children:"Em termos matem\xe1ticos:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"v = f_\u03b8(s)\n"})}),"\n",(0,i.jsx)(a.p,{children:"Onde:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"s"}),": Estado atual do tabuleiro"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"f_\u03b8"}),": Value Network (\u03b8 s\xe3o os par\xe2metros da rede)"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"v"}),": Um valor entre -1 e +1"]}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"significado-da-sa\xedda",children:"Significado da Sa\xedda"}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"Valor de Sa\xedda"}),(0,i.jsx)(a.th,{children:"Significado"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"+1"}),(0,i.jsx)(a.td,{children:"O jogador atual vence com certeza"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"+0,5"}),(0,i.jsx)(a.td,{children:"O jogador atual tem ~75% de taxa de vit\xf3ria"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"0"}),(0,i.jsx)(a.td,{children:"Taxas de vit\xf3ria iguais para ambos"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"-0,5"}),(0,i.jsx)(a.td,{children:"O jogador atual tem ~25% de taxa de vit\xf3ria"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"-1"}),(0,i.jsx)(a.td,{children:"O jogador atual perde com certeza"})]})]})]}),"\n",(0,i.jsx)(a.h3,{id:"por-que-precisamos-de-um-\xfanico-valor",children:"Por que precisamos de um \xfanico valor?"}),"\n",(0,i.jsx)(a.h4,{id:"comparar-diferentes-escolhas",children:"Comparar diferentes escolhas"}),"\n",(0,i.jsx)(a.p,{children:"Ao jogar, frequentemente precisamos escolher entre m\xfaltiplas op\xe7\xf5es. A Value Network torna essa compara\xe7\xe3o simples:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"Valor da posi\xe7\xe3o da Op\xe7\xe3o A: 0,3\nValor da posi\xe7\xe3o da Op\xe7\xe3o B: 0,5\nValor da posi\xe7\xe3o da Op\xe7\xe3o C: 0,2\n\n\u2192 Escolher B (o maior valor)\n"})}),"\n",(0,i.jsx)(a.p,{children:'Se n\xe3o houvesse um \xfanico valor, como comparar\xedamos "capturar um grupo do oponente" com "cercar um grande territ\xf3rio"?'}),"\n",(0,i.jsx)(a.h4,{id:"substituir-muitas-simula\xe7\xf5es",children:"Substituir muitas simula\xe7\xf5es"}),"\n",(0,i.jsxs)(a.p,{children:["Na busca em \xe1rvore de Monte Carlo tradicional, avaliar uma posi\xe7\xe3o requer ",(0,i.jsx)(a.strong,{children:"simula\xe7\xe3o aleat\xf3ria (rollout)"}),":"]}),"\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsx)(a.li,{children:"Come\xe7ar da posi\xe7\xe3o atual"}),"\n",(0,i.jsx)(a.li,{children:"Ambos os lados jogam aleatoriamente at\xe9 o fim do jogo"}),"\n",(0,i.jsx)(a.li,{children:"Registrar vit\xf3ria/derrota"}),"\n",(0,i.jsx)(a.li,{children:"Repetir milhares de vezes, calcular taxa de vit\xf3ria"}),"\n"]}),"\n",(0,i.jsxs)(a.p,{children:["Isso \xe9 muito lento. A Value Network pode dar uma avalia\xe7\xe3o em ",(0,i.jsx)(a.strong,{children:"uma \xfanica propaga\xe7\xe3o direta"}),", ordens de magnitude mais r\xe1pido."]}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"M\xe9todo"}),(0,i.jsx)(a.th,{children:"Tempo de Avalia\xe7\xe3o"}),(0,i.jsx)(a.th,{children:"Precis\xe3o"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"1000 simula\xe7\xf5es aleat\xf3rias"}),(0,i.jsx)(a.td,{children:"~2000 ms"}),(0,i.jsx)(a.td,{children:"Baixa"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"15000 simula\xe7\xf5es aleat\xf3rias"}),(0,i.jsx)(a.td,{children:"~30000 ms"}),(0,i.jsx)(a.td,{children:"M\xe9dia"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Value Network"}),(0,i.jsx)(a.td,{children:"~3 ms"}),(0,i.jsx)(a.td,{children:"Alta (equivalente a 15000 simula\xe7\xf5es)"})]})]})]}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"arquitetura-da-rede",children:"Arquitetura da Rede"}),"\n",(0,i.jsx)(a.h3,{id:"similaridade-com-a-policy-network",children:"Similaridade com a Policy Network"}),"\n",(0,i.jsx)(a.p,{children:"A arquitetura da Value Network \xe9 muito similar \xe0 Policy Network, ambas s\xe3o redes neurais convolucionais profundas:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"Camada de Entrada \u2192 Camadas Conv \xd712 \u2192 Camada Totalmente Conectada \u2192 Sa\xedda\n        \u2193                \u2193                       \u2193                    \u2193\n    19\xd719\xd748         19\xd719\xd7192                 256-dim           Valor \xfanico\n"})}),"\n",(0,i.jsx)(a.h3,{id:"camada-de-entrada",children:"Camada de Entrada"}),"\n",(0,i.jsxs)(a.p,{children:["Similar \xe0 Policy Network, a entrada \xe9 um tensor de caracter\xedsticas ",(0,i.jsx)(a.strong,{children:"19\xd719\xd749"}),":"]}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"19\xd719"}),": Tamanho do tabuleiro"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"49"}),": 48 planos de caracter\xedsticas + 1 plano indicando de quem \xe9 a vez"]}),"\n"]}),"\n",(0,i.jsx)(a.p,{children:"O plano extra \xe9 importante: a Value Network precisa saber de quem \xe9 a vez, porque o valor da mesma posi\xe7\xe3o \xe9 oposto para preto e branco."}),"\n",(0,i.jsx)(a.h3,{id:"camadas-convolucionais",children:"Camadas Convolucionais"}),"\n",(0,i.jsx)(a.p,{children:"Igual \xe0 Policy Network:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:(0,i.jsx)(a.strong,{children:"12 camadas convolucionais"})}),"\n",(0,i.jsx)(a.li,{children:(0,i.jsx)(a.strong,{children:"192 filtros"})}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Kernels 3\xd73"})," (5\xd75 na primeira camada)"]}),"\n",(0,i.jsx)(a.li,{children:(0,i.jsx)(a.strong,{children:"Fun\xe7\xe3o de ativa\xe7\xe3o ReLU"})}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"diferen\xe7a-na-camada-de-sa\xedda",children:"Diferen\xe7a na Camada de Sa\xedda"}),"\n",(0,i.jsx)(a.p,{children:"Esta \xe9 a diferen\xe7a chave entre Value Network e Policy Network:"}),"\n",(0,i.jsx)(a.h4,{id:"sa\xedda-da-policy-network",children:"Sa\xedda da Policy Network"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"19\xd719\xd7192 \u2192 Conv 1\xd71 \u2192 19\xd719\xd71 \u2192 Achatar \u2192 361-dim \u2192 Softmax \u2192 Distribui\xe7\xe3o de probabilidade\n"})}),"\n",(0,i.jsx)(a.h4,{id:"sa\xedda-da-value-network",children:"Sa\xedda da Value Network"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"19\xd719\xd7192 \u2192 Conv 1\xd71 \u2192 19\xd719\xd71 \u2192 Achatar \u2192 361-dim \u2192 FC 256 \u2192 ReLU \u2192 FC 1 \u2192 Tanh \u2192 Valor \xfanico\n"})}),"\n",(0,i.jsx)(a.h3,{id:"fun\xe7\xe3o-de-ativa\xe7\xe3o-tanh",children:"Fun\xe7\xe3o de Ativa\xe7\xe3o Tanh"}),"\n",(0,i.jsxs)(a.p,{children:["A \xfaltima camada da Value Network usa a fun\xe7\xe3o ",(0,i.jsx)(a.strong,{children:"Tanh"})," (tangente hiperb\xf3lica):"]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n"})}),"\n",(0,i.jsxs)(a.p,{children:["O intervalo de sa\xedda do Tanh \xe9 ",(0,i.jsx)(a.strong,{children:"(-1, +1)"}),", correspondendo exatamente a vit\xf3ria/derrota."]}),"\n",(0,i.jsx)(a.h4,{id:"por-que-usar-tanh-em-vez-de-sigmoid",children:"Por que usar Tanh em vez de Sigmoid?"}),"\n",(0,i.jsx)(a.p,{children:"O intervalo de sa\xedda do Sigmoid \xe9 (0, 1), que tamb\xe9m pode representar taxa de vit\xf3ria. Mas Tanh tem v\xe1rias vantagens:"}),"\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Simetria"}),": Centrado em 0, sa\xedda pode ser positiva ou negativa"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Melhor gradiente"}),": Gradiente pr\xf3ximo de 1 em torno de 0"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Sem\xe2ntica clara"}),": Valores positivos = vit\xf3ria, valores negativos = derrota, zero = empate"]}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"diagrama-completo-da-arquitetura",children:"Diagrama Completo da Arquitetura"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"Entrada: 19\xd719\xd749\n        \u2193\n    Conv 5\xd75, 192 filtros\n        \u2193\n    ReLU\n        \u2193\n    Conv 3\xd73, 192 filtros (\xd711)\n        \u2193\n    ReLU\n        \u2193\n    Conv 1\xd71, 1 filtro\n        \u2193\n    Achatar (361-dim)\n        \u2193\n    Totalmente Conectada (256-dim)\n        \u2193\n    ReLU\n        \u2193\n    Totalmente Conectada (1-dim)\n        \u2193\n    Tanh\n        \u2193\nSa\xedda: [-1, +1]\n"})}),"\n",(0,i.jsx)(a.h3,{id:"contagem-de-par\xe2metros",children:"Contagem de Par\xe2metros"}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"Camada"}),(0,i.jsx)(a.th,{children:"C\xe1lculo"}),(0,i.jsx)(a.th,{children:"N\xfamero de Par\xe2metros"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Camadas conv"}),(0,i.jsx)(a.td,{children:"Igual \xe0 Policy Network"}),(0,i.jsx)(a.td,{children:"~3,9M"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Camada FC 1"}),(0,i.jsx)(a.td,{children:"361\xd7256 + 256"}),(0,i.jsx)(a.td,{children:"92.672"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Camada FC 2"}),(0,i.jsx)(a.td,{children:"256\xd71 + 1"}),(0,i.jsx)(a.td,{children:"257"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:(0,i.jsx)(a.strong,{children:"Total"})}),(0,i.jsx)(a.td,{}),(0,i.jsx)(a.td,{children:(0,i.jsx)(a.strong,{children:"~4,0M"})})]})]})]}),"\n",(0,i.jsx)(a.p,{children:"Aproximadamente 4 milh\xf5es de par\xe2metros, ligeiramente mais que a Policy Network."}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"desafios-do-treinamento",children:"Desafios do Treinamento"}),"\n",(0,i.jsx)(a.h3,{id:"problema-de-overfitting",children:"Problema de Overfitting"}),"\n",(0,i.jsxs)(a.p,{children:["O treinamento da Value Network \xe9 muito mais dif\xedcil que o da Policy Network. O principal problema \xe9 ",(0,i.jsx)(a.strong,{children:"overfitting"}),"."]}),"\n",(0,i.jsx)(a.h4,{id:"o-que-\xe9-overfitting",children:"O que \xe9 overfitting?"}),"\n",(0,i.jsx)(a.p,{children:'Overfitting \xe9 quando o modelo "memoriza" os dados de treinamento em vez de aprender a generalizar. Manifesta-se como:'}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"Desempenho muito bom no conjunto de treinamento"}),"\n",(0,i.jsx)(a.li,{children:"Desempenho ruim no conjunto de teste"}),"\n"]}),"\n",(0,i.jsx)(a.h4,{id:"por-que-a-value-network-\xe9-propensa-a-overfitting",children:"Por que a Value Network \xe9 propensa a overfitting?"}),"\n",(0,i.jsx)(a.p,{children:"Considere os dados de um jogo:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"Posi\xe7\xe3o 1 \u2192 Posi\xe7\xe3o 2 \u2192 Posi\xe7\xe3o 3 \u2192 ... \u2192 Posi\xe7\xe3o 200 \u2192 Resultado: Preto vence\n"})}),"\n",(0,i.jsx)(a.p,{children:"Se treinarmos diretamente com esses dados:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"Essas 200 posi\xe7\xf5es t\xeam forte correla\xe7\xe3o"}),"\n",(0,i.jsx)(a.li,{children:"Elas v\xeam do mesmo jogo, t\xeam o mesmo resultado"}),"\n",(0,i.jsx)(a.li,{children:'O modelo pode aprender a "reconhecer" este jogo, em vez de entender posi\xe7\xf5es'}),"\n"]}),"\n",(0,i.jsx)(a.p,{children:"O DeepMind descobriu: se usar os mesmos registros de jogos humanos para treinar Policy e Value Network, a Value Network ter\xe1 s\xe9rio overfitting."}),"\n",(0,i.jsx)(a.h3,{id:"solu\xe7\xe3o-dados-de-auto-jogo",children:"Solu\xe7\xe3o: Dados de Auto-jogo"}),"\n",(0,i.jsxs)(a.p,{children:["A solu\xe7\xe3o do DeepMind \xe9 usar ",(0,i.jsx)(a.strong,{children:"auto-jogo"})," para gerar novos dados de treinamento:"]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"1. Usar a RL Policy Network treinada para auto-jogo\n2. Pegar apenas uma posi\xe7\xe3o de cada jogo (evitar correla\xe7\xe3o)\n3. O r\xf3tulo dessa posi\xe7\xe3o \xe9 o resultado final do jogo\n4. Gerar 30 milh\xf5es de tais amostras\n"})}),"\n",(0,i.jsx)(a.h4,{id:"por-que-isso-resolve-o-overfitting",children:"Por que isso resolve o overfitting?"}),"\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Grande volume de dados"}),": 30 milh\xf5es de posi\xe7\xf5es independentes"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Sem correla\xe7\xe3o"}),": Apenas uma posi\xe7\xe3o de cada jogo"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Distribui\xe7\xe3o diferente"}),": A distribui\xe7\xe3o de posi\xe7\xf5es do auto-jogo \xe9 diferente dos registros humanos"]}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"gera\xe7\xe3o-de-dados-de-treinamento",children:"Gera\xe7\xe3o de Dados de Treinamento"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:"# Pseudoc\xf3digo\ntraining_data = []\n\nfor game_id in range(30_000_000):\n    # Auto-jogar um jogo\n    states, result = self_play(rl_policy_network)\n\n    # Selecionar aleatoriamente uma posi\xe7\xe3o\n    random_index = random.randint(0, len(states) - 1)\n    state = states[random_index]\n\n    # Registrar posi\xe7\xe3o e resultado\n    training_data.append((state, result))\n"})}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"objetivo-e-m\xe9todos-de-treinamento",children:"Objetivo e M\xe9todos de Treinamento"}),"\n",(0,i.jsx)(a.h3,{id:"perda-de-erro-quadr\xe1tico-m\xe9dio",children:"Perda de Erro Quadr\xe1tico M\xe9dio"}),"\n",(0,i.jsxs)(a.p,{children:["A Value Network usa ",(0,i.jsx)(a.strong,{children:"Erro Quadr\xe1tico M\xe9dio (MSE)"})," como fun\xe7\xe3o de perda:"]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"L(\u03b8) = (1/n) \xd7 \u03a3 (v_\u03b8(s) - z)\xb2\n"})}),"\n",(0,i.jsx)(a.p,{children:"Onde:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"v_\u03b8(s)"}),": Valor previsto pelo modelo"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"z"}),": Resultado real (+1 ou -1)"]}),"\n"]}),"\n",(0,i.jsx)(a.h4,{id:"por-que-usar-mse-em-vez-de-entropia-cruzada",children:"Por que usar MSE em vez de entropia cruzada?"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Entropia cruzada"})," \xe9 adequada para problemas de classifica\xe7\xe3o (r\xf3tulos discretos)"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"MSE"})," \xe9 adequada para problemas de regress\xe3o (valores cont\xednuos)"]}),"\n"]}),"\n",(0,i.jsx)(a.p,{children:"Embora o resultado seja apenas +1 ou -1, o modelo prev\xea valores cont\xednuos (qualquer n\xfamero entre -1 e +1). MSE faz o modelo aprender a prever valores pr\xf3ximos de +1 ou -1."}),"\n",(0,i.jsx)(a.h3,{id:"processo-de-treinamento",children:"Processo de Treinamento"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:"# Pseudoc\xf3digo\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        states, outcomes = batch\n\n        # Propaga\xe7\xe3o direta\n        values = network(states)  # (batch, 1)\n\n        # Calcular perda (MSE)\n        loss = mse_loss(values, outcomes)\n\n        # Retropropaga\xe7\xe3o\n        loss.backward()\n        optimizer.step()\n"})}),"\n",(0,i.jsx)(a.p,{children:"Detalhes do treinamento:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Otimizador"}),": SGD com momentum"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Taxa de aprendizado"}),": 0,003"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Tamanho do lote"}),": 32"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Tempo de treinamento"}),": Aproximadamente 1 semana (50 GPUs)"]}),"\n"]}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"an\xe1lise-de-precis\xe3o",children:"An\xe1lise de Precis\xe3o"}),"\n",(0,i.jsx)(a.h3,{id:"compara\xe7\xe3o-com-simula\xe7\xe3o-aleat\xf3ria",children:"Compara\xe7\xe3o com Simula\xe7\xe3o Aleat\xf3ria"}),"\n",(0,i.jsx)(a.p,{children:"O DeepMind realizou compara\xe7\xf5es detalhadas no artigo:"}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"M\xe9todo de Avalia\xe7\xe3o"}),(0,i.jsx)(a.th,{children:"Erro de Previs\xe3o"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"1000 simula\xe7\xf5es aleat\xf3rias"}),(0,i.jsx)(a.td,{children:"Alto"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"15000 simula\xe7\xf5es aleat\xf3rias"}),(0,i.jsx)(a.td,{children:"M\xe9dio"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Value Network"}),(0,i.jsx)(a.td,{children:"Compar\xe1vel a 15000 simula\xe7\xf5es"})]})]})]}),"\n",(0,i.jsx)(a.p,{children:"Isso significa que uma avalia\xe7\xe3o da Value Network \u2248 15000 simula\xe7\xf5es aleat\xf3rias, mas ~1000\xd7 mais r\xe1pida."}),"\n",(0,i.jsx)(a.h3,{id:"precis\xe3o-em-diferentes-fases",children:"Precis\xe3o em Diferentes Fases"}),"\n",(0,i.jsx)(a.p,{children:"A precis\xe3o da Value Network depende do progresso do jogo:"}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"Fase"}),(0,i.jsx)(a.th,{children:"Jogadas Restantes"}),(0,i.jsx)(a.th,{children:"Dificuldade de Previs\xe3o"}),(0,i.jsx)(a.th,{children:"Precis\xe3o"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Abertura"}),(0,i.jsx)(a.td,{children:"~300"}),(0,i.jsx)(a.td,{children:"Muito dif\xedcil"}),(0,i.jsx)(a.td,{children:"Baixa"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Meio de jogo"}),(0,i.jsx)(a.td,{children:"~150"}),(0,i.jsx)(a.td,{children:"Dif\xedcil"}),(0,i.jsx)(a.td,{children:"M\xe9dia"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Yose"}),(0,i.jsx)(a.td,{children:"~50"}),(0,i.jsx)(a.td,{children:"Mais f\xe1cil"}),(0,i.jsx)(a.td,{children:"Alta"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Final"}),(0,i.jsx)(a.td,{children:"~10"}),(0,i.jsx)(a.td,{children:"Simples"}),(0,i.jsx)(a.td,{children:"Muito alta"})]})]})]}),"\n",(0,i.jsx)(a.p,{children:"Isso \xe9 intuitivamente razo\xe1vel: quanto mais perto do fim do jogo, mais determinado \xe9 o resultado."}),"\n",(0,i.jsx)(a.h3,{id:"distribui\xe7\xe3o-de-sa\xedda",children:"Distribui\xe7\xe3o de Sa\xedda"}),"\n",(0,i.jsx)(a.p,{children:"A distribui\xe7\xe3o de sa\xedda de uma Value Network bem treinada:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"        Frequ\xeancia\n          |\n          |    *\n          |   * *\n          |  *   *\n          | *     *\n          |*       *\n          +----+----+---- Valor de sa\xedda\n         -1    0   +1\n\nA maioria das sa\xeddas est\xe1 concentrada perto de -1 e +1\n(porque a maioria das posi\xe7\xf5es tem uma tend\xeancia clara de vit\xf3ria/derrota)\n"})}),"\n",(0,i.jsx)(a.h3,{id:"posi\xe7\xf5es-incertas",children:"Posi\xe7\xf5es Incertas"}),"\n",(0,i.jsx)(a.p,{children:"Quando a sa\xedda da Value Network est\xe1 pr\xf3xima de 0, indica que a posi\xe7\xe3o \xe9 muito complexa, com resultado incerto. Essas posi\xe7\xf5es geralmente s\xe3o:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"No meio de grandes batalhas"}),"\n",(0,i.jsx)(a.li,{children:"For\xe7as equilibradas entre ambos os lados"}),"\n",(0,i.jsx)(a.li,{children:"Existem m\xfaltiplas varia\xe7\xf5es poss\xedveis"}),"\n"]}),"\n",(0,i.jsx)(a.p,{children:"No MCTS, esses n\xf3s recebem mais recursos de busca (por causa da alta incerteza)."}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"papel-no-mcts",children:"Papel no MCTS"}),"\n",(0,i.jsx)(a.h3,{id:"avalia\xe7\xe3o-de-n\xf3s-folha",children:"Avalia\xe7\xe3o de N\xf3s Folha"}),"\n",(0,i.jsxs)(a.p,{children:["A Value Network desempenha um papel cr\xedtico na fase de ",(0,i.jsx)(a.strong,{children:"Avalia\xe7\xe3o"})," do MCTS:"]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"\xc1rvore de busca MCTS:\n\n        N\xf3 raiz (posi\xe7\xe3o atual)\n           /    \\\n         A        B\n        /  \\    /  \\\n       A1  A2  B1  B2 \u2190 N\xf3s folha\n        \u2193   \u2193   \u2193   \u2193\n       Aval Aval Aval Aval\n"})}),"\n",(0,i.jsx)(a.p,{children:"Quando o MCTS alcan\xe7a um n\xf3 folha, precisa avaliar o valor dessa posi\xe7\xe3o. Existem dois m\xe9todos:"}),"\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Simula\xe7\xe3o aleat\xf3ria (Rollout)"}),": Jogar aleatoriamente do n\xf3 folha at\xe9 o fim do jogo"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Avalia\xe7\xe3o pela Value Network"}),": Prever diretamente com a rede neural"]}),"\n"]}),"\n",(0,i.jsx)(a.p,{children:"O AlphaGo combina ambos:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"V(folha) = (1-\u03bb) \xd7 V_network(folha) + \u03bb \xd7 V_rollout(folha)\n"})}),"\n",(0,i.jsx)(a.p,{children:"Onde \u03bb = 0,5, ou seja, cada um com metade do peso."}),"\n",(0,i.jsx)(a.h4,{id:"por-que-combinar",children:"Por que combinar?"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Value Network"})," \xe9 mais precisa, mas pode ter vi\xe9s sistem\xe1tico"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Simula\xe7\xe3o aleat\xf3ria"})," \xe9 menos precisa, mas fornece estimativa independente"]}),"\n",(0,i.jsx)(a.li,{children:"Combinar ambos pode complementar um ao outro"}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"simplifica\xe7\xe3o-no-alphago-zero",children:"Simplifica\xe7\xe3o no AlphaGo Zero"}),"\n",(0,i.jsx)(a.p,{children:"O posterior AlphaGo Zero abandonou completamente a simula\xe7\xe3o aleat\xf3ria:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"V(folha) = V_network(folha)\n"})}),"\n",(0,i.jsx)(a.p,{children:'Isso simplificou muito o sistema, enquanto a for\xe7a de jogo aumentou. Isso prova que a Value Network \xe9 confi\xe1vel o suficiente, n\xe3o precisando do "seguro" da simula\xe7\xe3o aleat\xf3ria.'}),"\n",(0,i.jsx)(a.h3,{id:"atualiza\xe7\xe3o-por-backpropagation",children:"Atualiza\xe7\xe3o por Backpropagation"}),"\n",(0,i.jsx)(a.p,{children:"Ap\xf3s avaliar o n\xf3 folha, esse valor \xe9 propagado de volta pelo caminho:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"v3 = V(folha) = 0,6\n      \u2191\nValor Q de A2 atualizado\n      \u2191\nValor Q de A atualizado\n      \u2191\nEstat\xedsticas do n\xf3 raiz atualizadas\n"})}),"\n",(0,i.jsx)(a.p,{children:"O valor Q mantido por cada n\xf3 \xe9 a m\xe9dia das avalia\xe7\xf5es dos n\xf3s folha que passaram por ele:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"Q(s, a) = (1/N(s,a)) \xd7 \u03a3 V(folha)\n"})}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"an\xe1lise-visual",children:"An\xe1lise Visual"}),"\n",(0,i.jsx)(a.h3,{id:"superf\xedcie-de-valor",children:"Superf\xedcie de Valor"}),"\n",(0,i.jsx)(a.p,{children:'Imagine um tabuleiro 3\xd73 simplificado. A Value Network aprende uma "superf\xedcie de valor":'}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"Posi\xe7\xe3o pedra preta \\ branca"}),(0,i.jsx)(a.th,{children:"1"}),(0,i.jsx)(a.th,{children:"2"}),(0,i.jsx)(a.th,{children:"3"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:(0,i.jsx)(a.strong,{children:"1"})}),(0,i.jsx)(a.td,{children:"+0,3"}),(0,i.jsx)(a.td,{children:"-0,1"}),(0,i.jsx)(a.td,{children:"+0,2"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:(0,i.jsx)(a.strong,{children:"2"})}),(0,i.jsx)(a.td,{children:"-0,2"}),(0,i.jsx)(a.td,{children:"+0,5"}),(0,i.jsx)(a.td,{children:"-0,3"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:(0,i.jsx)(a.strong,{children:"3"})}),(0,i.jsx)(a.td,{children:"+0,1"}),(0,i.jsx)(a.td,{children:"-0,2"}),(0,i.jsx)(a.td,{children:"+0,4"})]})]})]}),"\n",(0,i.jsx)(a.p,{children:"Esta superf\xedcie nos diz o valor de cada combina\xe7\xe3o de posi\xe7\xf5es. Valores positivos favorecem preto, valores negativos favorecem branco."}),"\n",(0,i.jsx)(a.h3,{id:"evolu\xe7\xe3o-durante-o-treinamento",children:"Evolu\xe7\xe3o Durante o Treinamento"}),"\n",(0,i.jsx)(a.p,{children:"\xc0 medida que o treinamento progride, as previs\xf5es da Value Network gradualmente se tornam mais precisas:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"       Erro de previs\xe3o\n          |\n     1,0  |*\n          | *\n     0,5  |  *\n          |   *\n     0,1  |    * * * * *\n          +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Passos de treinamento\n          0   100K  500K  1M\n"})}),"\n",(0,i.jsx)(a.p,{children:"O erro cai rapidamente, depois estabiliza."}),"\n",(0,i.jsx)(a.h3,{id:"identifica\xe7\xe3o-de-posi\xe7\xf5es-dif\xedceis",children:"Identifica\xe7\xe3o de Posi\xe7\xf5es Dif\xedceis"}),"\n",(0,i.jsx)(a.p,{children:"A Value Network pode ajudar a identificar posi\xe7\xf5es dif\xedceis:"}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"Sa\xedda"}),(0,i.jsx)(a.th,{children:"Significado"}),(0,i.jsx)(a.th,{children:"Estrat\xe9gia de Resposta"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Pr\xf3ximo de +1"}),(0,i.jsx)(a.td,{children:"Grande vantagem"}),(0,i.jsx)(a.td,{children:"Jogar solidamente"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Pr\xf3ximo de -1"}),(0,i.jsx)(a.td,{children:"Grande desvantagem"}),(0,i.jsx)(a.td,{children:"Buscar oportunidades de virar"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Pr\xf3ximo de 0"}),(0,i.jsx)(a.td,{children:"Posi\xe7\xe3o complexa"}),(0,i.jsx)(a.td,{children:"Requer c\xe1lculo profundo"})]})]})]}),"\n",(0,i.jsx)(a.p,{children:"O AlphaGo investe mais tempo de pensamento em posi\xe7\xf5es pr\xf3ximas de 0."}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"pontos-de-implementa\xe7\xe3o",children:"Pontos de Implementa\xe7\xe3o"}),"\n",(0,i.jsx)(a.h3,{id:"implementa\xe7\xe3o-em-pytorch",children:"Implementa\xe7\xe3o em PyTorch"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ValueNetwork(nn.Module):\n    def __init__(self, input_channels=49, num_filters=192, num_layers=12):\n        super().__init__()\n\n        # Primeira camada convolucional (5\xd75)\n        self.conv1 = nn.Conv2d(input_channels, num_filters,\n                               kernel_size=5, padding=2)\n\n        # Camadas convolucionais intermedi\xe1rias (3\xd73)\xd711\n        self.conv_layers = nn.ModuleList([\n            nn.Conv2d(num_filters, num_filters,\n                     kernel_size=3, padding=1)\n            for _ in range(num_layers - 1)\n        ])\n\n        # Camada convolucional de sa\xedda\n        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)\n\n        # Camadas totalmente conectadas\n        self.fc1 = nn.Linear(361, 256)\n        self.fc2 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        # x: (batch, 49, 19, 19)\n\n        # Camadas convolucionais\n        x = F.relu(self.conv1(x))\n        for conv in self.conv_layers:\n            x = F.relu(conv(x))\n        x = self.conv_out(x)\n\n        # Achatar\n        x = x.view(x.size(0), -1)  # (batch, 361)\n\n        # Camadas totalmente conectadas\n        x = F.relu(self.fc1(x))\n        x = torch.tanh(self.fc2(x))\n\n        return x.squeeze(-1)  # (batch,)\n"})}),"\n",(0,i.jsx)(a.h3,{id:"loop-de-treinamento",children:"Loop de Treinamento"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'def train_value_network(model, optimizer, states, outcomes):\n    """\n    states: (batch, 49, 19, 19) - Caracter\xedsticas do tabuleiro\n    outcomes: (batch,) - Resultado do jogo (+1 ou -1)\n    """\n    # Propaga\xe7\xe3o direta\n    values = model(states)  # (batch,)\n\n    # Perda MSE\n    loss = F.mse_loss(values, outcomes)\n\n    # Retropropaga\xe7\xe3o\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Calcular precis\xe3o (previs\xe3o correta de vit\xf3ria/derrota)\n    predictions = (values > 0).float() * 2 - 1  # Converter para +1/-1\n    accuracy = (predictions == outcomes).float().mean()\n\n    return loss.item(), accuracy.item()\n'})}),"\n",(0,i.jsx)(a.h3,{id:"t\xe9cnicas-para-evitar-overfitting",children:"T\xe9cnicas para Evitar Overfitting"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:"# 1. Aumento de dados (simetria de 8 dobras)\ndef augment(state, outcome):\n    augmented = []\n    for rotation in [0, 90, 180, 270]:\n        s = rotate(state, rotation)\n        augmented.append((s, outcome))\n        augmented.append((flip(s), outcome))\n    return augmented\n\n# 2. Dropout\nclass ValueNetworkWithDropout(ValueNetwork):\n    def __init__(self, *args, dropout_rate=0.5, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        # ... camadas conv ...\n        x = self.dropout(x)  # Dropout antes das camadas FC\n        # ... camadas FC ...\n\n# 3. Parada antecipada (Early Stopping)\nbest_val_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(max_epochs):\n    train_loss = train_one_epoch()\n    val_loss = evaluate()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        save_model()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping!\")\n            break\n"})}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"colabora\xe7\xe3o-com-a-policy-network",children:"Colabora\xe7\xe3o com a Policy Network"}),"\n",(0,i.jsx)(a.h3,{id:"rela\xe7\xe3o-complementar",children:"Rela\xe7\xe3o Complementar"}),"\n",(0,i.jsx)(a.p,{children:"Policy Network e Value Network s\xe3o complementares no AlphaGo:"}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"Rede"}),(0,i.jsx)(a.th,{children:"Quest\xe3o Respondida"}),(0,i.jsx)(a.th,{children:"Sa\xedda"}),(0,i.jsx)(a.th,{children:"Papel no MCTS"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Policy"}),(0,i.jsx)(a.td,{children:"Onde jogar a seguir?"}),(0,i.jsx)(a.td,{children:"Distribui\xe7\xe3o de probabilidade"}),(0,i.jsx)(a.td,{children:"Guiar dire\xe7\xe3o da busca"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"Value"}),(0,i.jsx)(a.td,{children:"Vou vencer este jogo?"}),(0,i.jsx)(a.td,{children:"Valor \xfanico"}),(0,i.jsx)(a.td,{children:"Avaliar n\xf3s folha"})]})]})]}),"\n",(0,i.jsx)(a.h3,{id:"rede-de-cabe\xe7a-dupla-unificada",children:"Rede de Cabe\xe7a Dupla Unificada"}),"\n",(0,i.jsxs)(a.p,{children:["No AlphaGo Zero, essas duas redes foram fundidas em uma ",(0,i.jsx)(a.strong,{children:"rede de cabe\xe7a dupla"}),":"]}),"\n",(0,i.jsx)(a.mermaid,{value:'flowchart TB\n    Shared["Camadas compartilhadas de<br/>extra\xe7\xe3o de caracter\xedsticas"]\n    Shared --\x3e PolicyHead["Cabe\xe7a Policy"]\n    Shared --\x3e ValueHead["Cabe\xe7a Value"]\n    PolicyHead --\x3e PolicyOut["361 probabilidades"]\n    ValueHead --\x3e ValueOut["Valor \xfanico"]'}),"\n",(0,i.jsx)(a.p,{children:"Vantagens deste design:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Compartilhamento de par\xe2metros"}),": Reduz computa\xe7\xe3o"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Compartilhamento de caracter\xedsticas"}),": Policy e Value usam as mesmas caracter\xedsticas"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Treinamento mais est\xe1vel"}),": Os dois objetivos se regularizam mutuamente"]}),"\n"]}),"\n",(0,i.jsxs)(a.p,{children:["Veja ",(0,i.jsx)(a.a,{href:"../dual-head-resnet",children:"Rede de Cabe\xe7a Dupla e Rede Residual"})," para detalhes."]}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"correspond\xeancia-com-anima\xe7\xf5es",children:"Correspond\xeancia com Anima\xe7\xf5es"}),"\n",(0,i.jsx)(a.p,{children:"Os conceitos principais abordados neste artigo e seus n\xfameros de anima\xe7\xe3o:"}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"N\xfamero"}),(0,i.jsx)(a.th,{children:"Conceito"}),(0,i.jsx)(a.th,{children:"Correspond\xeancia F\xedsica/Matem\xe1tica"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"E2"}),(0,i.jsx)(a.td,{children:"Value Network"}),(0,i.jsx)(a.td,{children:"Superf\xedcie de potencial"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"D4"}),(0,i.jsx)(a.td,{children:"Fun\xe7\xe3o de valor"}),(0,i.jsx)(a.td,{children:"Retorno esperado"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"C6"}),(0,i.jsx)(a.td,{children:"Avalia\xe7\xe3o de n\xf3s folha"}),(0,i.jsx)(a.td,{children:"Aproxima\xe7\xe3o de fun\xe7\xe3o"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"H3"}),(0,i.jsx)(a.td,{children:"Diferen\xe7a temporal"}),(0,i.jsx)(a.td,{children:"Aprendizado por bootstrap"})]})]})]}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"leitura-adicional",children:"Leitura Adicional"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Artigo anterior"}),": ",(0,i.jsx)(a.a,{href:"../policy-network",children:"Detalhes da Policy Network"})," \u2014 Como a rede de pol\xedticas escolhe jogadas"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Pr\xf3ximo artigo"}),": ",(0,i.jsx)(a.a,{href:"../input-features",children:"Design de Caracter\xedsticas de Entrada"})," \u2014 Detalhes dos 48 planos de caracter\xedsticas"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"T\xf3pico avan\xe7ado"}),": ",(0,i.jsx)(a.a,{href:"../mcts-neural-combo",children:"Combina\xe7\xe3o de MCTS com Redes Neurais"})," \u2014 O fluxo de busca completo"]}),"\n"]}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"pontos-chave",children:"Pontos-Chave"}),"\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Value Network prev\xea taxa de vit\xf3ria"}),": Sa\xedda de um \xfanico valor entre -1 e +1"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Sa\xedda Tanh"}),": Garante que a sa\xedda esteja no intervalo correto"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Perda MSE"}),": Aproxima o valor previsto do resultado real"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Desafio de overfitting"}),": Precisa usar dados de auto-jogo para evitar"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Substitui simula\xe7\xe3o aleat\xf3ria"}),": Uma avalia\xe7\xe3o \u2248 15000 simula\xe7\xf5es"]}),"\n"]}),"\n",(0,i.jsx)(a.p,{children:'A Value Network \xe9 o "julgamento" do AlphaGo \u2014 ela permite que a IA avalie a qualidade de qualquer posi\xe7\xe3o sem precisar esgotar todas as possibilidades.'}),"\n",(0,i.jsx)(a.hr,{}),"\n",(0,i.jsx)(a.h2,{id:"refer\xeancias",children:"Refer\xeancias"}),"\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsxs)(a.li,{children:['Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." ',(0,i.jsx)(a.em,{children:"Nature"}),", 529, 484-489."]}),"\n",(0,i.jsxs)(a.li,{children:['Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." ',(0,i.jsx)(a.em,{children:"Nature"}),", 551, 354-359."]}),"\n",(0,i.jsxs)(a.li,{children:["Sutton, R. S., & Barto, A. G. (2018). ",(0,i.jsx)(a.em,{children:"Reinforcement Learning: An Introduction"}),". MIT Press."]}),"\n",(0,i.jsxs)(a.li,{children:['Tesauro, G. (1995). "Temporal difference learning and TD-Gammon." ',(0,i.jsx)(a.em,{children:"Communications of the ACM"}),", 38(3), 58-68."]}),"\n"]})]})}function h(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,i.jsx)(a,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},30416(e,a,n){n.d(a,{R:()=>s,x:()=>d});var r=n(59471);const i={},o=r.createContext(i);function s(e){const a=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function d(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(o.Provider,{value:a},e.children)}}}]);