"use strict";(self.webpackChunktemp_docusaurus=self.webpackChunktemp_docusaurus||[]).push([[191],{691:(e,a,r)=>{r.r(a),r.d(a,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>s,metadata:()=>n,toc:()=>t});const n=JSON.parse('{"id":"for-engineers/background-info/alphago","title":"Analise do Artigo AlphaGo","description":"Este artigo analisa profundamente o artigo classico publicado na Nature pela DeepMind: \\"Mastering the game of Go with deep neural networks and tree search\\", bem como os artigos subsequentes do AlphaGo Zero e AlphaZero.","source":"@site/i18n/pt/docusaurus-plugin-content-docs/current/for-engineers/background-info/alphago.md","sourceDirName":"for-engineers/background-info","slug":"/for-engineers/background-info/alphago","permalink":"/www.weiqi.kids/pt/docs/for-engineers/background-info/alphago","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/background-info/alphago.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Analise do Artigo AlphaGo"},"sidebar":"tutorialSidebar","previous":{"title":"Conhecimento de Fundo","permalink":"/www.weiqi.kids/pt/docs/for-engineers/background-info/"},"next":{"title":"Analise do Artigo KataGo","permalink":"/www.weiqi.kids/pt/docs/for-engineers/background-info/katago-paper"}}');var o=r(3420),i=r(5521);const s={sidebar_position:1,title:"Analise do Artigo AlphaGo"},d="Analise do Artigo AlphaGo",l={},t=[{value:"Significado Historico do AlphaGo",id:"significado-historico-do-alphago",level:2},{value:"Eventos Marcantes",id:"eventos-marcantes",level:3},{value:"Arquitetura Tecnica Central",id:"arquitetura-tecnica-central",level:2},{value:"Policy Network (Rede de Estrategia)",id:"policy-network-rede-de-estrategia",level:3},{value:"Arquitetura da Rede",id:"arquitetura-da-rede",level:4},{value:"Caracteristicas de Entrada",id:"caracteristicas-de-entrada",level:4},{value:"Metodo de Treinamento",id:"metodo-de-treinamento",level:4},{value:"Value Network (Rede de Valor)",id:"value-network-rede-de-valor",level:3},{value:"Arquitetura da Rede",id:"arquitetura-da-rede-1",level:4},{value:"Metodo de Treinamento",id:"metodo-de-treinamento-1",level:4},{value:"Busca por Arvore de Monte Carlo (MCTS)",id:"busca-por-arvore-de-monte-carlo-mcts",level:2},{value:"Quatro Passos do MCTS",id:"quatro-passos-do-mcts",level:3},{value:"Formula de Selecao (PUCT)",id:"formula-de-selecao-puct",level:3},{value:"Detalhes do Processo de Busca",id:"detalhes-do-processo-de-busca",level:3},{value:"Rollout (Simulacao Rapida)",id:"rollout-simulacao-rapida",level:3},{value:"Metodo de Treinamento Self-play",id:"metodo-de-treinamento-self-play",level:2},{value:"Ciclo de Treinamento",id:"ciclo-de-treinamento",level:3},{value:"Por que Self-play e Efetivo?",id:"por-que-self-play-e-efetivo",level:3},{value:"Melhorias do AlphaGo Zero",id:"melhorias-do-alphago-zero",level:2},{value:"Principais Diferencas",id:"principais-diferencas",level:3},{value:"Simplificacao da Arquitetura",id:"simplificacao-da-arquitetura",level:3},{value:"Features de Entrada Simplificadas",id:"features-de-entrada-simplificadas",level:3},{value:"Melhorias no Treinamento",id:"melhorias-no-treinamento",level:3},{value:"Generalizacao do AlphaZero",id:"generalizacao-do-alphazero",level:2},{value:"Caracteristicas-chave",id:"caracteristicas-chave",level:3},{value:"Diferencas do AlphaGo Zero",id:"diferencas-do-alphago-zero",level:3},{value:"Pontos-chave de Implementacao",id:"pontos-chave-de-implementacao",level:2},{value:"Recursos Computacionais",id:"recursos-computacionais",level:3},{value:"Hiperparametros-chave",id:"hiperparametros-chave",level:3},{value:"Problemas Comuns",id:"problemas-comuns",level:3},{value:"Leitura Adicional",id:"leitura-adicional",level:2}];function c(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(a.header,{children:(0,o.jsx)(a.h1,{id:"analise-do-artigo-alphago",children:"Analise do Artigo AlphaGo"})}),"\n",(0,o.jsx)(a.p,{children:'Este artigo analisa profundamente o artigo classico publicado na Nature pela DeepMind: "Mastering the game of Go with deep neural networks and tree search", bem como os artigos subsequentes do AlphaGo Zero e AlphaZero.'}),"\n",(0,o.jsx)(a.h2,{id:"significado-historico-do-alphago",children:"Significado Historico do AlphaGo"}),"\n",(0,o.jsx)(a.p,{children:'O Go foi por muito tempo visto como o desafio "Santo Graal" da inteligencia artificial. Diferente do xadrez, o espaco de busca do Go e extremamente vasto:'}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Jogo"}),(0,o.jsx)(a.th,{children:"Fator de Ramificacao Medio"}),(0,o.jsx)(a.th,{children:"Comprimento Medio do Jogo"}),(0,o.jsx)(a.th,{children:"Espaco de Estados"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Xadrez"}),(0,o.jsx)(a.td,{children:"~35"}),(0,o.jsx)(a.td,{children:"~80"}),(0,o.jsx)(a.td,{children:"~10^47"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Go"}),(0,o.jsx)(a.td,{children:"~250"}),(0,o.jsx)(a.td,{children:"~150"}),(0,o.jsx)(a.td,{children:"~10^170"})]})]})]}),"\n",(0,o.jsx)(a.p,{children:"Metodos tradicionais de busca por forca bruta sao completamente inviaveis no Go. A vitoria do AlphaGo sobre Lee Sedol em 2016 demonstrou o tremendo poder da combinacao de aprendizado profundo com aprendizado por reforco."}),"\n",(0,o.jsx)(a.h3,{id:"eventos-marcantes",children:"Eventos Marcantes"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Outubro 2015"}),": AlphaGo Fan derrota o campeao europeu Fan Hui (profissional 2-dan) por 5:0"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Marco 2016"}),": AlphaGo Lee derrota o campeao mundial Lee Sedol (profissional 9-dan) por 4:1"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Maio 2017"}),": AlphaGo Master derrota Ke Jie, numero 1 do mundo, por 3:0"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Outubro 2017"}),": AlphaGo Zero publicado, treinamento puro por self-play, superando todas as versoes anteriores"]}),"\n"]}),"\n",(0,o.jsx)(a.h2,{id:"arquitetura-tecnica-central",children:"Arquitetura Tecnica Central"}),"\n",(0,o.jsx)(a.p,{children:"A inovacao central do AlphaGo esta em combinar tres tecnologias-chave:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Arquitetura AlphaGo                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502   \u2502   Policy    \u2502    \u2502    Value    \u2502                   \u2502\n\u2502   \u2502   Network   \u2502    \u2502   Network   \u2502                   \u2502\n\u2502   \u2502 (Estrategia \u2502    \u2502 (Avaliacao  \u2502                   \u2502\n\u2502   \u2502  de Jogada) \u2502    \u2502  de Vitoria)\u2502                   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502          \u2502                  \u2502                          \u2502\n\u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2502                   \u2502                                    \u2502\n\u2502                   \u25bc                                    \u2502\n\u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u2502\n\u2502          \u2502      MCTS       \u2502                          \u2502\n\u2502          \u2502(Busca por Arvore\u2502                          \u2502\n\u2502          \u2502 de Monte Carlo) \u2502                          \u2502\n\u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(a.h3,{id:"policy-network-rede-de-estrategia",children:"Policy Network (Rede de Estrategia)"}),"\n",(0,o.jsx)(a.p,{children:"A Policy Network e responsavel por prever a probabilidade de jogada para cada posicao, usada para guiar a direcao da busca."}),"\n",(0,o.jsx)(a.h4,{id:"arquitetura-da-rede",children:"Arquitetura da Rede"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Camada de Entrada: 19\xd719\xd748 planos de caracteristicas\n    \u2502\n    \u25bc\nCamada Conv 1: kernel 5\xd75, 192 filtros\n    \u2502\n    \u25bc\nCamadas Conv 2-12: kernel 3\xd73, 192 filtros\n    \u2502\n    \u25bc\nCamada de Saida: distribuicao de probabilidade 19\xd719 (softmax)\n"})}),"\n",(0,o.jsx)(a.h4,{id:"caracteristicas-de-entrada",children:"Caracteristicas de Entrada"}),"\n",(0,o.jsx)(a.p,{children:"AlphaGo usa 48 planos de caracteristicas como entrada:"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Caracteristica"}),(0,o.jsx)(a.th,{children:"Num Planos"}),(0,o.jsx)(a.th,{children:"Descricao"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Cor da pedra"}),(0,o.jsx)(a.td,{children:"3"}),(0,o.jsx)(a.td,{children:"Preta, branca, vazia"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Liberdades"}),(0,o.jsx)(a.td,{children:"8"}),(0,o.jsx)(a.td,{children:"1, 2, ..., 8+ liberdades"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Liberdades apos captura"}),(0,o.jsx)(a.td,{children:"8"}),(0,o.jsx)(a.td,{children:"Quantas liberdades apos capturar"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Num de capturas"}),(0,o.jsx)(a.td,{children:"8"}),(0,o.jsx)(a.td,{children:"Quantas pedras podem ser capturadas nessa posicao"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Ko"}),(0,o.jsx)(a.td,{children:"1"}),(0,o.jsx)(a.td,{children:"Se e posicao de ko"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Jogada legal"}),(0,o.jsx)(a.td,{children:"1"}),(0,o.jsx)(a.td,{children:"Se a posicao e jogavel"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Posicoes das ultimas 1-8 jogadas"}),(0,o.jsx)(a.td,{children:"8"}),(0,o.jsx)(a.td,{children:"Posicoes das jogadas anteriores"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"De quem e a vez"}),(0,o.jsx)(a.td,{children:"1"}),(0,o.jsx)(a.td,{children:"Atualmente vez de preto ou branco"})]})]})]}),"\n",(0,o.jsx)(a.h4,{id:"metodo-de-treinamento",children:"Metodo de Treinamento"}),"\n",(0,o.jsx)(a.p,{children:"O treinamento da Policy Network e dividido em duas fases:"}),"\n",(0,o.jsx)(a.p,{children:(0,o.jsx)(a.strong,{children:"Fase 1: Aprendizado Supervisionado (SL Policy Network)"})}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:"Usa 30 milhoes de registros de jogos do servidor KGS"}),"\n",(0,o.jsx)(a.li,{children:"Objetivo: Prever a proxima jogada de jogadores humanos"}),"\n",(0,o.jsx)(a.li,{children:"Atinge 57% de precisao de predicao"}),"\n"]}),"\n",(0,o.jsx)(a.p,{children:(0,o.jsx)(a.strong,{children:"Fase 2: Aprendizado por Reforco (RL Policy Network)"})}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:"Comeca da SL Policy Network"}),"\n",(0,o.jsx)(a.li,{children:"Joga contra versoes anteriores de si mesmo"}),"\n",(0,o.jsx)(a.li,{children:"Usa algoritmo REINFORCE para otimizar"}),"\n"]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:"# Policy Gradient update simplificado\n# reward: +1 vitoria, -1 derrota\nloss = -log(policy[action]) * reward\n"})}),"\n",(0,o.jsx)(a.h3,{id:"value-network-rede-de-valor",children:"Value Network (Rede de Valor)"}),"\n",(0,o.jsx)(a.p,{children:"A Value Network avalia a taxa de vitoria da posicao atual, usada para reduzir a profundidade de busca."}),"\n",(0,o.jsx)(a.h4,{id:"arquitetura-da-rede-1",children:"Arquitetura da Rede"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Camada de Entrada: 19\xd719\xd748 planos de caracteristicas (igual a Policy Network)\n    \u2502\n    \u25bc\nCamadas Conv 1-12: similar a Policy Network\n    \u2502\n    \u25bc\nCamada Totalmente Conectada: 256 neuronios\n    \u2502\n    \u25bc\nCamada de Saida: 1 neuronio (tanh, range [-1, 1])\n"})}),"\n",(0,o.jsx)(a.h4,{id:"metodo-de-treinamento-1",children:"Metodo de Treinamento"}),"\n",(0,o.jsx)(a.p,{children:"A Value Network e treinada usando 30 milhoes de posicoes geradas por self-play da RL Policy Network:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:"Amostra aleatoriamente uma posicao de cada jogo"}),"\n",(0,o.jsx)(a.li,{children:"Usa resultado final do jogo como label"}),"\n",(0,o.jsx)(a.li,{children:"Usa funcao de perda MSE"}),"\n"]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:"# Treinamento da Value Network\nvalue_prediction = value_network(position)\nloss = (value_prediction - game_outcome) ** 2\n"})}),"\n",(0,o.jsx)(a.p,{children:(0,o.jsx)(a.strong,{children:"Por que amostrar apenas uma posicao por jogo?"})}),"\n",(0,o.jsx)(a.p,{children:"Se amostrar multiplas posicoes, posicoes adjacentes do mesmo jogo serao altamente correlacionadas, levando a overfitting. Amostragem aleatoria garante diversidade nos dados de treinamento."}),"\n",(0,o.jsx)(a.h2,{id:"busca-por-arvore-de-monte-carlo-mcts",children:"Busca por Arvore de Monte Carlo (MCTS)"}),"\n",(0,o.jsx)(a.p,{children:"MCTS e o nucleo de decisao do AlphaGo, combinando redes neurais para buscar eficientemente a melhor jogada."}),"\n",(0,o.jsx)(a.h3,{id:"quatro-passos-do-mcts",children:"Quatro Passos do MCTS"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"    (1) Selection          (2) Expansion\n         \u2502                      \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502 Selecionar\u2502            \u2502 Expandir \u2502\n    \u2502melhor path\u2502            \u2502 novo no  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n         \u2502                      \u2502\n         \u25bc                      \u25bc\n    (3) Evaluation         (4) Backpropagation\n         \u2502                      \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502Rede Neural\u2502            \u2502 Retornar \u2502\n    \u2502 Avalia   \u2502            \u2502 Atualizar\u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(a.h3,{id:"formula-de-selecao-puct",children:"Formula de Selecao (PUCT)"}),"\n",(0,o.jsx)(a.p,{children:"AlphaGo usa a formula PUCT (Predictor + UCT) para selecionar qual ramo explorar:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"a = argmax[Q(s,a) + u(s,a)]\n\nu(s,a) = c_puct * P(s,a) * sqrt(N(s)) / (1 + N(s,a))\n"})}),"\n",(0,o.jsx)(a.p,{children:"Onde:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Q(s,a)"}),": Valor medio da acao a (exploitation)"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"P(s,a)"}),": Probabilidade prior prevista pela Policy Network"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"N(s)"}),": Numero de visitas do no pai"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"N(s,a)"}),": Numero de visitas desta acao"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"c_puct"}),": Constante de exploracao, equilibrando exploration e exploitation"]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"detalhes-do-processo-de-busca",children:"Detalhes do Processo de Busca"}),"\n",(0,o.jsxs)(a.ol,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Selection"}),": Do no raiz, use a formula PUCT para selecionar acoes ate chegar ao no folha"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Expansion"}),": Expanda novos nos filhos no no folha, inicialize probabilidades prior com Policy Network"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Evaluation"}),": Combine avaliacao da Value Network com simulacao de rollout rapido para avaliar valor"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Backpropagation"}),": Propague valor de avaliacao pelo caminho, atualize valores Q e N"]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"rollout-simulacao-rapida",children:"Rollout (Simulacao Rapida)"}),"\n",(0,o.jsx)(a.p,{children:"AlphaGo (versao nao-Zero) tambem usa uma pequena rede de estrategia rapida para simulacao:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"No folha \u2192 Jogadas rapidas aleatorias ate fim do jogo \u2192 Calcular resultado\n"})}),"\n",(0,o.jsx)(a.p,{children:"O valor final de avaliacao combina Value Network e Rollout:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"V = \u03bb * v_network + (1-\u03bb) * v_rollout\n"})}),"\n",(0,o.jsx)(a.p,{children:"AlphaGo usa \u03bb = 0.5, dando peso igual a ambos."}),"\n",(0,o.jsx)(a.h2,{id:"metodo-de-treinamento-self-play",children:"Metodo de Treinamento Self-play"}),"\n",(0,o.jsx)(a.p,{children:"Self-play e a estrategia central de treinamento do AlphaGo, permitindo que a IA melhore continuamente jogando contra si mesma."}),"\n",(0,o.jsx)(a.h3,{id:"ciclo-de-treinamento",children:"Ciclo de Treinamento"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Ciclo de Treinamento Self-play           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502   \u2502Modelo Atual \u2502 \u2192  \u2502  Self-play  \u2502 \u2192  \u2502 Gerar Dados \u2502\u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502        \u25b2                                    \u2502          \u2502\n\u2502        \u2502                                    \u25bc          \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502   \u2502 Novo Modelo \u2502 \u2190  \u2502  Treinar    \u2502 \u2190  \u2502 Buffer de   \u2502\u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   Dados     \u2502\u2502\n\u2502                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(a.h3,{id:"por-que-self-play-e-efetivo",children:"Por que Self-play e Efetivo?"}),"\n",(0,o.jsxs)(a.ol,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Dados infinitos"}),": Nao limitado pela quantidade de registros humanos"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Dificuldade adaptativa"}),": Forca do oponente melhora junto com voce"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Exploracao e inovacao"}),": Nao restrito por padroes de pensamento humano"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Objetivo claro"}),": Otimiza diretamente taxa de vitoria, nao imitacao de humanos"]}),"\n"]}),"\n",(0,o.jsx)(a.h2,{id:"melhorias-do-alphago-zero",children:"Melhorias do AlphaGo Zero"}),"\n",(0,o.jsx)(a.p,{children:"O AlphaGo Zero publicado em 2017 trouxe melhorias revolucionarias:"}),"\n",(0,o.jsx)(a.h3,{id:"principais-diferencas",children:"Principais Diferencas"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Caracteristica"}),(0,o.jsx)(a.th,{children:"AlphaGo"}),(0,o.jsx)(a.th,{children:"AlphaGo Zero"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Treinamento inicial"}),(0,o.jsx)(a.td,{children:"Aprendizado supervisionado de registros humanos"}),(0,o.jsx)(a.td,{children:"Comeca completamente do zero"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Arquitetura de rede"}),(0,o.jsx)(a.td,{children:"Policy/Value separadas"}),(0,o.jsx)(a.td,{children:"Rede unica de duas cabecas"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Estrutura de rede"}),(0,o.jsx)(a.td,{children:"CNN comum"}),(0,o.jsx)(a.td,{children:"ResNet"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Engenharia de features"}),(0,o.jsx)(a.td,{children:"48 features manuais"}),(0,o.jsx)(a.td,{children:"17 features simples"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Rollout"}),(0,o.jsx)(a.td,{children:"Necessario"}),(0,o.jsx)(a.td,{children:"Nao necessario"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Tempo de treinamento"}),(0,o.jsx)(a.td,{children:"Meses"}),(0,o.jsx)(a.td,{children:"3 dias para superar humanos"})]})]})]}),"\n",(0,o.jsx)(a.h3,{id:"simplificacao-da-arquitetura",children:"Simplificacao da Arquitetura"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Rede de Duas Cabecas AlphaGo Zero           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                         \u2502\n\u2502   Entrada: 19\xd719\xd717 (features simplificadas)            \u2502\n\u2502                      \u2502                                  \u2502\n\u2502                      \u25bc                                  \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n\u2502              \u2502  Backbone     \u2502                         \u2502\n\u2502              \u2502   ResNet      \u2502                         \u2502\n\u2502              \u2502 (40 blocos    \u2502                         \u2502\n\u2502              \u2502  residuais)   \u2502                         \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2502                      \u2502                                  \u2502\n\u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502          \u25bc                       \u25bc                     \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502   \u2502 Policy Head \u2502         \u2502 Value Head  \u2502             \u2502\n\u2502   \u2502 (19\xd719+1)  \u2502         \u2502   ([-1,1])   \u2502             \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(a.h3,{id:"features-de-entrada-simplificadas",children:"Features de Entrada Simplificadas"}),"\n",(0,o.jsx)(a.p,{children:"AlphaGo Zero usa apenas 17 planos de caracteristicas:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:"8 planos: Posicoes das suas ultimas 8 jogadas"}),"\n",(0,o.jsx)(a.li,{children:"8 planos: Posicoes das ultimas 8 jogadas do oponente"}),"\n",(0,o.jsx)(a.li,{children:"1 plano: De quem e a vez (todo 0 ou todo 1)"}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"melhorias-no-treinamento",children:"Melhorias no Treinamento"}),"\n",(0,o.jsxs)(a.ol,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Self-play puro"}),": Nao usa nenhum dado humano"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Usa probabilidades MCTS diretamente como alvo de treinamento"}),": Em vez de vitoria/derrota binaria"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Sem Rollout"}),": Depende completamente da Value Network"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Treinamento de rede unica"}),": Policy e Value compartilham parametros, reforcando-se mutuamente"]}),"\n"]}),"\n",(0,o.jsx)(a.h2,{id:"generalizacao-do-alphazero",children:"Generalizacao do AlphaZero"}),"\n",(0,o.jsx)(a.p,{children:"O AlphaZero publicado no final de 2017 aplicou a mesma arquitetura a Go, xadrez e shogi:"}),"\n",(0,o.jsx)(a.h3,{id:"caracteristicas-chave",children:"Caracteristicas-chave"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Zero conhecimento de dominio"}),": Nao usa nenhum conhecimento especifico do dominio alem das regras do jogo"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Arquitetura unificada"}),": Mesmo algoritmo aplicavel a diferentes jogos de tabuleiro"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Treinamento mais rapido"}),":","\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:"Go: 8 horas para superar AlphaGo Lee"}),"\n",(0,o.jsx)(a.li,{children:"Xadrez: 4 horas para superar Stockfish"}),"\n",(0,o.jsx)(a.li,{children:"Shogi: 2 horas para superar Elmo"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"diferencas-do-alphago-zero",children:"Diferencas do AlphaGo Zero"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Caracteristica"}),(0,o.jsx)(a.th,{children:"AlphaGo Zero"}),(0,o.jsx)(a.th,{children:"AlphaZero"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Jogo alvo"}),(0,o.jsx)(a.td,{children:"Apenas Go"}),(0,o.jsx)(a.td,{children:"Go, xadrez, shogi"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Uso de simetria"}),(0,o.jsx)(a.td,{children:"Usa 8 simetrias do Go"}),(0,o.jsx)(a.td,{children:"Nao assume simetria"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Ajuste de hiperparametros"}),(0,o.jsx)(a.td,{children:"Otimizado para Go"}),(0,o.jsx)(a.td,{children:"Configuracoes universais"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Metodo de treinamento"}),(0,o.jsx)(a.td,{children:"Self-play do melhor modelo"}),(0,o.jsx)(a.td,{children:"Self-play do modelo mais recente"})]})]})]}),"\n",(0,o.jsx)(a.h2,{id:"pontos-chave-de-implementacao",children:"Pontos-chave de Implementacao"}),"\n",(0,o.jsx)(a.p,{children:"Se voce quiser implementar um sistema similar, aqui estao as principais consideracoes:"}),"\n",(0,o.jsx)(a.h3,{id:"recursos-computacionais",children:"Recursos Computacionais"}),"\n",(0,o.jsx)(a.p,{children:"O treinamento do AlphaGo requer recursos computacionais enormes:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"AlphaGo Lee"}),": 176 GPUs + 48 TPUs"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"AlphaGo Zero"}),": 4 TPUs (treinamento) + 1 TPU (self-play)"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"AlphaZero"}),": 5000 TPUs (treinamento)"]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"hiperparametros-chave",children:"Hiperparametros-chave"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:"# Relacionados a MCTS\nnum_simulations = 800     # Simulacoes de busca por jogada\nc_puct = 1.5              # Constante de exploracao\ntemperature = 1.0         # Parametro de temperatura para selecao de acao\n\n# Relacionados a treinamento\nbatch_size = 2048\nlearning_rate = 0.01      # Com decay\nl2_regularization = 1e-4\n"})}),"\n",(0,o.jsx)(a.h3,{id:"problemas-comuns",children:"Problemas Comuns"}),"\n",(0,o.jsxs)(a.ol,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Treinamento instavel"}),": Use learning rate menor, aumente batch size"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Overfitting"}),": Garanta diversidade nos dados de treinamento, use regularizacao"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Eficiencia de busca"}),": Otimize inferencia em batch na GPU, paralelizacao do MCTS"]}),"\n"]}),"\n",(0,o.jsx)(a.h2,{id:"leitura-adicional",children:"Leitura Adicional"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:(0,o.jsx)(a.a,{href:"https://www.nature.com/articles/nature16961",children:"Artigo original: Mastering the game of Go with deep neural networks and tree search"})}),"\n",(0,o.jsx)(a.li,{children:(0,o.jsx)(a.a,{href:"https://www.nature.com/articles/nature24270",children:"Artigo AlphaGo Zero: Mastering the game of Go without human knowledge"})}),"\n",(0,o.jsx)(a.li,{children:(0,o.jsx)(a.a,{href:"https://www.science.org/doi/10.1126/science.aar6404",children:"Artigo AlphaZero: A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"})}),"\n"]}),"\n",(0,o.jsxs)(a.p,{children:["Apos entender a tecnologia do AlphaGo, vamos ver como o ",(0,o.jsx)(a.a,{href:"/www.weiqi.kids/pt/docs/for-engineers/background-info/katago-paper",children:"KataGo fez melhorias com base nisso"}),"."]})]})}function h(e={}){const{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,o.jsx)(a,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},5521:(e,a,r)=>{r.d(a,{R:()=>s,x:()=>d});var n=r(6672);const o={},i=n.createContext(o);function s(e){const a=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function d(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),n.createElement(i.Provider,{value:a},e.children)}}}]);