"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[5988],{98392(e,a,r){r.r(a),r.d(a,{assets:()=>l,contentTitle:()=>d,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"alphago/explained/policy-network","title":"Detalhes da Policy Network","description":"Compreens\xe3o aprofundada da arquitetura, m\xe9todos de treinamento e aplica\xe7\xf5es pr\xe1ticas da rede de pol\xedticas do AlphaGo, desde 13 camadas convolucionais at\xe9 a sa\xedda Softmax","source":"@site/i18n/pt/docusaurus-plugin-content-docs/current/alphago/explained/07-policy-network.mdx","sourceDirName":"alphago/explained","slug":"/alphago/explained/policy-network","permalink":"/pt/docs/alphago/explained/policy-network","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/explained/07-policy-network.mdx","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"title":"Detalhes da Policy Network","description":"Compreens\xe3o aprofundada da arquitetura, m\xe9todos de treinamento e aplica\xe7\xf5es pr\xe1ticas da rede de pol\xedticas do AlphaGo, desde 13 camadas convolucionais at\xe9 a sa\xedda Softmax"},"sidebar":"tutorialSidebar","previous":{"title":"Representacao do Estado do Tabuleiro","permalink":"/pt/docs/alphago/explained/board-representation"},"next":{"title":"Detalhes da Value Network","permalink":"/pt/docs/alphago/explained/value-network"}}');var o=r(62615),n=r(30416),t=r(45695);const i={sidebar_position:8,title:"Detalhes da Policy Network",description:"Compreens\xe3o aprofundada da arquitetura, m\xe9todos de treinamento e aplica\xe7\xf5es pr\xe1ticas da rede de pol\xedticas do AlphaGo, desde 13 camadas convolucionais at\xe9 a sa\xedda Softmax"},d="Detalhes da Policy Network",l={},c=[{value:"O que \xe9 a Policy Network?",id:"o-que-\xe9-a-policy-network",level:2},{value:"Fun\xe7\xe3o Principal",id:"fun\xe7\xe3o-principal",level:3},{value:"Compreens\xe3o Intuitiva",id:"compreens\xe3o-intuitiva",level:3},{value:"Por que precisamos da Policy Network?",id:"por-que-precisamos-da-policy-network",level:3},{value:"Arquitetura da Rede",id:"arquitetura-da-rede",level:2},{value:"Estrutura Geral",id:"estrutura-geral",level:3},{value:"Camada de Entrada",id:"camada-de-entrada",level:3},{value:"Camadas Convolucionais",id:"camadas-convolucionais",level:3},{value:"Por que 192 filtros?",id:"por-que-192-filtros",level:4},{value:"Por que kernels 3\xd73?",id:"por-que-kernels-33",level:4},{value:"Por que a primeira camada usa 5\xd75?",id:"por-que-a-primeira-camada-usa-55",level:4},{value:"Fun\xe7\xe3o de Ativa\xe7\xe3o ReLU",id:"fun\xe7\xe3o-de-ativa\xe7\xe3o-relu",level:3},{value:"Camada de Sa\xedda",id:"camada-de-sa\xedda",level:3},{value:"Convolu\xe7\xe3o 1\xd71",id:"convolu\xe7\xe3o-11",level:4},{value:"Sa\xedda Softmax",id:"sa\xedda-softmax",level:4},{value:"Contagem de Par\xe2metros",id:"contagem-de-par\xe2metros",level:3},{value:"Objetivo e M\xe9todos de Treinamento",id:"objetivo-e-m\xe9todos-de-treinamento",level:2},{value:"Dados de Treinamento",id:"dados-de-treinamento",level:3},{value:"Fun\xe7\xe3o de Perda de Entropia Cruzada",id:"fun\xe7\xe3o-de-perda-de-entropia-cruzada",level:3},{value:"Compreens\xe3o Intuitiva",id:"compreens\xe3o-intuitiva-1",level:4},{value:"Processo de Treinamento",id:"processo-de-treinamento",level:3},{value:"Aumento de Dados",id:"aumento-de-dados",level:3},{value:"Resultados do Treinamento",id:"resultados-do-treinamento",level:2},{value:"57% de Precis\xe3o",id:"57-de-precis\xe3o",level:3},{value:"Esta precis\xe3o \xe9 alta?",id:"esta-precis\xe3o-\xe9-alta",level:4},{value:"Melhoria na For\xe7a de Jogo",id:"melhoria-na-for\xe7a-de-jogo",level:3},{value:"Por que apenas 57%?",id:"por-que-apenas-57",level:3},{value:"1. M\xfaltiplas Boas Jogadas",id:"1-m\xfaltiplas-boas-jogadas",level:4},{value:"2. Diferen\xe7as de Estilo",id:"2-diferen\xe7as-de-estilo",level:4},{value:"3. Humanos Tamb\xe9m Erram",id:"3-humanos-tamb\xe9m-erram",level:4},{value:"Papel no MCTS",id:"papel-no-mcts",level:2},{value:"1. Guiar a Dire\xe7\xe3o da Busca",id:"1-guiar-a-dire\xe7\xe3o-da-busca",level:3},{value:"2. Prior para Expans\xe3o de N\xf3s",id:"2-prior-para-expans\xe3o-de-n\xf3s",level:3},{value:"Vers\xe3o Leve vs. Vers\xe3o Completa",id:"vers\xe3o-leve-vs-vers\xe3o-completa",level:2},{value:"Vers\xe3o Completa (SL Policy Network)",id:"vers\xe3o-completa-sl-policy-network",level:3},{value:"Vers\xe3o Leve (Rollout Policy Network)",id:"vers\xe3o-leve-rollout-policy-network",level:3},{value:"Por que precisamos da vers\xe3o leve?",id:"por-que-precisamos-da-vers\xe3o-leve",level:3},{value:"Caracter\xedsticas da Vers\xe3o Leve",id:"caracter\xedsticas-da-vers\xe3o-leve",level:3},{value:"Melhorias no AlphaGo Zero",id:"melhorias-no-alphago-zero",level:3},{value:"Refinamento com Aprendizado por Refor\xe7o (RL Policy Network)",id:"refinamento-com-aprendizado-por-refor\xe7o-rl-policy-network",level:2},{value:"Limita\xe7\xf5es do Aprendizado Supervisionado",id:"limita\xe7\xf5es-do-aprendizado-supervisionado",level:3},{value:"Auto-jogo com Refor\xe7o",id:"auto-jogo-com-refor\xe7o",level:3},{value:"Algoritmo REINFORCE",id:"algoritmo-reinforce",level:3},{value:"Resultados",id:"resultados",level:3},{value:"De &quot;Imitar&quot; para &quot;Inovar&quot;",id:"de-imitar-para-inovar",level:3},{value:"An\xe1lise Visual",id:"an\xe1lise-visual",level:2},{value:"Distribui\xe7\xf5es de Probabilidade em Diferentes Posi\xe7\xf5es",id:"distribui\xe7\xf5es-de-probabilidade-em-diferentes-posi\xe7\xf5es",level:3},{value:"Abertura (Fase de Fuseki)",id:"abertura-fase-de-fuseki",level:4},{value:"Posi\xe7\xe3o de Combate",id:"posi\xe7\xe3o-de-combate",level:4},{value:"Fase de Yose",id:"fase-de-yose",level:4},{value:"O que as Camadas Ocultas Aprenderam?",id:"o-que-as-camadas-ocultas-aprenderam",level:3},{value:"Pontos de Implementa\xe7\xe3o",id:"pontos-de-implementa\xe7\xe3o",level:2},{value:"Implementa\xe7\xe3o em PyTorch",id:"implementa\xe7\xe3o-em-pytorch",level:3},{value:"Loop de Treinamento",id:"loop-de-treinamento",level:3},{value:"Considera\xe7\xf5es para Infer\xeancia",id:"considera\xe7\xf5es-para-infer\xeancia",level:3},{value:"Correspond\xeancia com Anima\xe7\xf5es",id:"correspond\xeancia-com-anima\xe7\xf5es",level:2},{value:"Leitura Adicional",id:"leitura-adicional",level:2},{value:"Pontos-Chave",id:"pontos-chave",level:2},{value:"Refer\xeancias",id:"refer\xeancias",level:2}];function h(e){const a={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,n.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(a.header,{children:(0,o.jsx)(a.h1,{id:"detalhes-da-policy-network",children:"Detalhes da Policy Network"})}),"\n",(0,o.jsx)(a.p,{children:"Em qualquer posi\xe7\xe3o de Go, existem em m\xe9dia 250 jogadas legais. Se deixarmos o computador escolher aleatoriamente, ele nunca conseguir\xe1 jogar bem."}),"\n",(0,o.jsx)(a.p,{children:'A inova\xe7\xe3o do AlphaGo est\xe1 em: ele aprendeu a "olhar para o tabuleiro e saber quais posi\xe7\xf5es vale a pena considerar".'}),"\n",(0,o.jsxs)(a.p,{children:["Esta capacidade vem da ",(0,o.jsx)(a.strong,{children:"Policy Network (Rede de Pol\xedticas)"}),"."]}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"o-que-\xe9-a-policy-network",children:"O que \xe9 a Policy Network?"}),"\n",(0,o.jsx)(a.h3,{id:"fun\xe7\xe3o-principal",children:"Fun\xe7\xe3o Principal"}),"\n",(0,o.jsx)(a.p,{children:"A Policy Network \xe9 uma rede neural convolucional profunda, cuja tarefa \xe9:"}),"\n",(0,o.jsxs)(a.blockquote,{children:["\n",(0,o.jsx)(a.p,{children:(0,o.jsx)(a.strong,{children:"Dado o estado atual do tabuleiro, produzir a probabilidade de jogar em cada posi\xe7\xe3o"})}),"\n"]}),"\n",(0,o.jsx)(a.p,{children:"Em termos matem\xe1ticos:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"p = f_\u03b8(s)\n"})}),"\n",(0,o.jsx)(a.p,{children:"Onde:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.code,{children:"s"}),": Estado atual do tabuleiro (tabuleiro 19\xd719 + outras caracter\xedsticas)"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.code,{children:"f_\u03b8"}),": Policy Network (\u03b8 s\xe3o os par\xe2metros da rede)"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.code,{children:"p"}),": Distribui\xe7\xe3o de probabilidade para 361 posi\xe7\xf5es (incluindo passar)"]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"compreens\xe3o-intuitiva",children:"Compreens\xe3o Intuitiva"}),"\n",(0,o.jsx)(a.p,{children:'Imagine que voc\xea \xe9 um jogador profissional. Quando voc\xea v\xea uma posi\xe7\xe3o, seu c\xe9rebro automaticamente "ilumina" algumas posi\xe7\xf5es importantes \u2014 estas s\xe3o as que sua intui\xe7\xe3o considera dignas de considera\xe7\xe3o.'}),"\n",(0,o.jsx)(a.p,{children:"A Policy Network est\xe1 simulando este processo."}),"\n",(0,o.jsx)(t.dW,{initialPosition:"corner",size:400}),"\n",(0,o.jsx)(a.p,{children:"O mapa de calor acima mostra a sa\xedda da Policy Network. Quanto mais brilhante a cor, mais o modelo considera que vale a pena jogar ali."}),"\n",(0,o.jsx)(a.h3,{id:"por-que-precisamos-da-policy-network",children:"Por que precisamos da Policy Network?"}),"\n",(0,o.jsx)(a.p,{children:"O espa\xe7o de busca do Go \xe9 imenso. Se buscarmos todas as jogadas poss\xedveis sem filtragem:"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Estrat\xe9gia"}),(0,o.jsx)(a.th,{children:"Jogadas consideradas por lance"}),(0,o.jsx)(a.th,{children:"N\xf3s para buscar 10 lances"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Considerar tudo"}),(0,o.jsx)(a.td,{children:"361"}),(0,o.jsx)(a.td,{children:"361^10 \u2248 10^25"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Filtrado pela Policy Network"}),(0,o.jsx)(a.td,{children:"~20"}),(0,o.jsx)(a.td,{children:"20^10 \u2248 10^13"})]})]})]}),"\n",(0,o.jsxs)(a.p,{children:["A Policy Network reduz o espa\xe7o de busca em ",(0,o.jsx)(a.strong,{children:"10^12 vezes"})," (um trilh\xe3o de vezes)."]}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"arquitetura-da-rede",children:"Arquitetura da Rede"}),"\n",(0,o.jsx)(a.h3,{id:"estrutura-geral",children:"Estrutura Geral"}),"\n",(0,o.jsx)(a.p,{children:"A Policy Network do AlphaGo usa uma arquitetura de rede neural convolucional profunda (CNN):"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Camada de Entrada \u2192 Camadas Conv \xd712 \u2192 Camada Conv de Sa\xedda \u2192 Softmax\n        \u2193                \u2193                    \u2193                  \u2193\n    19\xd719\xd748         19\xd719\xd7192            19\xd719\xd71           362 probabilidades\n"})}),"\n",(0,o.jsx)(a.h3,{id:"camada-de-entrada",children:"Camada de Entrada"}),"\n",(0,o.jsxs)(a.p,{children:["A entrada \xe9 um tensor de caracter\xedsticas ",(0,o.jsx)(a.strong,{children:"19\xd719\xd748"}),":"]}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"19\xd719"}),": Tamanho do tabuleiro"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"48"}),": 48 planos de caracter\xedsticas (veja ",(0,o.jsx)(a.a,{href:"../input-features",children:"Design de Caracter\xedsticas de Entrada"}),")"]}),"\n"]}),"\n",(0,o.jsx)(a.p,{children:"Esses 48 planos incluem:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:"Posi\xe7\xf5es das pedras pretas, posi\xe7\xf5es das pedras brancas"}),"\n",(0,o.jsx)(a.li,{children:"Hist\xf3rico das \xfaltimas 8 jogadas"}),"\n",(0,o.jsx)(a.li,{children:"Liberdades, atari, escadas, etc."}),"\n",(0,o.jsx)(a.li,{children:"Legalidade (quais posi\xe7\xf5es podem ser jogadas)"}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"camadas-convolucionais",children:"Camadas Convolucionais"}),"\n",(0,o.jsxs)(a.p,{children:["A rede cont\xe9m ",(0,o.jsx)(a.strong,{children:"12 camadas convolucionais"}),", cada uma configurada como:"]}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Par\xe2metro"}),(0,o.jsx)(a.th,{children:"Valor"}),(0,o.jsx)(a.th,{children:"Descri\xe7\xe3o"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"N\xfamero de filtros"}),(0,o.jsx)(a.td,{children:"192"}),(0,o.jsx)(a.td,{children:"Cada camada produz 192 mapas de caracter\xedsticas"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Tamanho do kernel"}),(0,o.jsx)(a.td,{children:"3\xd73 (5\xd75 na primeira camada)"}),(0,o.jsx)(a.td,{children:"Cada vez observa uma regi\xe3o 3\xd73"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Modo de padding"}),(0,o.jsx)(a.td,{children:"same"}),(0,o.jsx)(a.td,{children:"Mant\xe9m o tamanho 19\xd719"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Fun\xe7\xe3o de ativa\xe7\xe3o"}),(0,o.jsx)(a.td,{children:"ReLU"}),(0,o.jsx)(a.td,{children:"max(0, x)"})]})]})]}),"\n",(0,o.jsx)(a.h4,{id:"por-que-192-filtros",children:"Por que 192 filtros?"}),"\n",(0,o.jsx)(a.p,{children:"Este \xe9 um valor emp\xedrico. Muito poucos limitariam a capacidade do modelo, muitos aumentariam o custo computacional e o risco de overfitting. A equipe do DeepMind determinou atrav\xe9s de experimentos que 192 \xe9 um bom ponto de equil\xedbrio."}),"\n",(0,o.jsx)(a.h4,{id:"por-que-kernels-33",children:"Por que kernels 3\xd73?"}),"\n",(0,o.jsx)(a.p,{children:"3\xd73 \xe9 o tamanho mais comum em redes neurais convolucionais, pelas seguintes raz\xf5es:"}),"\n",(0,o.jsxs)(a.ol,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Captura suficiente de padr\xf5es locais"}),": Olhos, conex\xf5es e cortes no Go est\xe3o dentro de uma regi\xe3o 3\xd73"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Alta efici\xeancia computacional"}),": Comparado a kernels maiores, 3\xd73 tem menos par\xe2metros"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Empilh\xe1vel"}),": M\xfaltiplas camadas de convolu\xe7\xe3o 3\xd73 podem alcan\xe7ar um campo receptivo maior"]}),"\n"]}),"\n",(0,o.jsx)(a.h4,{id:"por-que-a-primeira-camada-usa-55",children:"Por que a primeira camada usa 5\xd75?"}),"\n",(0,o.jsx)(a.p,{children:"A primeira camada usa um kernel 5\xd75 maior para capturar padr\xf5es de alcance ligeiramente maior na camada de entrada (como saltos pequenos e grandes). Esta \xe9 uma escolha de design, e o posterior AlphaGo Zero unificou para usar 3\xd73."}),"\n",(0,o.jsx)(a.h3,{id:"fun\xe7\xe3o-de-ativa\xe7\xe3o-relu",children:"Fun\xe7\xe3o de Ativa\xe7\xe3o ReLU"}),"\n",(0,o.jsx)(a.p,{children:"Cada camada convolucional \xe9 seguida pela fun\xe7\xe3o de ativa\xe7\xe3o ReLU (Rectified Linear Unit):"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"ReLU(x) = max(0, x)\n"})}),"\n",(0,o.jsx)(a.p,{children:"Por que usar ReLU?"}),"\n",(0,o.jsxs)(a.ol,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Computa\xe7\xe3o simples"}),": Apenas toma o m\xe1ximo, muito mais r\xe1pido que sigmoid"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Alivia o desaparecimento do gradiente"}),": O gradiente na regi\xe3o positiva \xe9 constantemente 1"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Ativa\xe7\xe3o esparsa"}),": Valores negativos s\xe3o zerados, produzindo representa\xe7\xe3o esparsa"]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"camada-de-sa\xedda",children:"Camada de Sa\xedda"}),"\n",(0,o.jsx)(a.p,{children:"A \xfaltima camada \xe9 uma camada convolucional especial:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"19\xd719\xd7192 \u2192 Convolu\xe7\xe3o(1\xd71, 1 filtro) \u2192 19\xd719\xd71 \u2192 Achatado \u2192 Vetor 362-dim \u2192 Softmax\n"})}),"\n",(0,o.jsx)(a.h4,{id:"convolu\xe7\xe3o-11",children:"Convolu\xe7\xe3o 1\xd71"}),"\n",(0,o.jsx)(a.p,{children:"A camada de sa\xedda usa convolu\xe7\xe3o 1\xd71, comprimindo 192 canais em 1. Isso equivale a fazer uma combina\xe7\xe3o linear das 192 caracter\xedsticas dimensionais em cada posi\xe7\xe3o."}),"\n",(0,o.jsx)(a.h4,{id:"sa\xedda-softmax",children:"Sa\xedda Softmax"}),"\n",(0,o.jsx)(a.p,{children:"O vetor 362-dimensional (361 posi\xe7\xf5es do tabuleiro + 1 para passar) passa pela fun\xe7\xe3o Softmax:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Softmax(z_i) = exp(z_i) / \u03a3_j exp(z_j)\n"})}),"\n",(0,o.jsx)(a.p,{children:"O Softmax garante que a sa\xedda seja uma distribui\xe7\xe3o de probabilidade v\xe1lida:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:"Todos os valores est\xe3o entre 0 e 1"}),"\n",(0,o.jsx)(a.li,{children:"A soma de todos os valores \xe9 1"}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"contagem-de-par\xe2metros",children:"Contagem de Par\xe2metros"}),"\n",(0,o.jsx)(a.p,{children:"Vamos calcular o n\xfamero total de par\xe2metros da rede:"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Camada"}),(0,o.jsx)(a.th,{children:"C\xe1lculo"}),(0,o.jsx)(a.th,{children:"N\xfamero de Par\xe2metros"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Primeira camada conv"}),(0,o.jsx)(a.td,{children:"5\xd75\xd748\xd7192 + 192"}),(0,o.jsx)(a.td,{children:"230.592"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Camadas conv intermedi\xe1rias \xd711"}),(0,o.jsx)(a.td,{children:"(3\xd73\xd7192\xd7192 + 192) \xd7 11"}),(0,o.jsx)(a.td,{children:"3.633.792"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Camada conv de sa\xedda"}),(0,o.jsx)(a.td,{children:"1\xd71\xd7192\xd71 + 1"}),(0,o.jsx)(a.td,{children:"193"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:(0,o.jsx)(a.strong,{children:"Total"})}),(0,o.jsx)(a.td,{}),(0,o.jsx)(a.td,{children:(0,o.jsx)(a.strong,{children:"~3,9M"})})]})]})]}),"\n",(0,o.jsxs)(a.p,{children:["Aproximadamente ",(0,o.jsx)(a.strong,{children:"3,9 milh\xf5es de par\xe2metros"}),", considerada uma rede pequena pelos padr\xf5es atuais."]}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"objetivo-e-m\xe9todos-de-treinamento",children:"Objetivo e M\xe9todos de Treinamento"}),"\n",(0,o.jsx)(a.h3,{id:"dados-de-treinamento",children:"Dados de Treinamento"}),"\n",(0,o.jsxs)(a.p,{children:["A Policy Network usa ",(0,o.jsx)(a.strong,{children:"aprendizado supervisionado"}),", aprendendo a partir de registros de jogos humanos."]}),"\n",(0,o.jsx)(a.p,{children:"Fontes de dados:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"KGS Go Server"}),": Jogos de jogadores amadores e profissionais"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Aproximadamente 30 milh\xf5es de posi\xe7\xf5es"}),": Amostradas de 160 mil jogos"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"R\xf3tulos"}),": A pr\xf3xima jogada humana correspondente a cada posi\xe7\xe3o"]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"fun\xe7\xe3o-de-perda-de-entropia-cruzada",children:"Fun\xe7\xe3o de Perda de Entropia Cruzada"}),"\n",(0,o.jsx)(a.p,{children:"O objetivo do treinamento \xe9 maximizar a probabilidade de prever as jogadas humanas. Usando a fun\xe7\xe3o de perda de entropia cruzada:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"L(\u03b8) = -\u03a3 log p_\u03b8(a | s)\n"})}),"\n",(0,o.jsx)(a.p,{children:"Onde:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.code,{children:"s"}),": Estado do tabuleiro"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.code,{children:"a"}),": Posi\xe7\xe3o onde o humano realmente jogou"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.code,{children:"p_\u03b8(a | s)"}),": Probabilidade do modelo para aquela posi\xe7\xe3o"]}),"\n"]}),"\n",(0,o.jsx)(a.h4,{id:"compreens\xe3o-intuitiva-1",children:"Compreens\xe3o Intuitiva"}),"\n",(0,o.jsx)(a.p,{children:"A perda de entropia cruzada tem um significado simples:"}),"\n",(0,o.jsxs)(a.blockquote,{children:["\n",(0,o.jsx)(a.p,{children:(0,o.jsx)(a.strong,{children:"Quanto maior a probabilidade do modelo para a posi\xe7\xe3o correta, menor a perda"})}),"\n"]}),"\n",(0,o.jsx)(a.p,{children:"Se o humano jogou em K10, e a probabilidade do modelo para K10 \xe9:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:"0,9 \u2192 Perda = -log(0,9) \u2248 0,1 (muito baixa, bom)"}),"\n",(0,o.jsx)(a.li,{children:"0,1 \u2192 Perda = -log(0,1) \u2248 2,3 (alta, ruim)"}),"\n",(0,o.jsx)(a.li,{children:"0,01 \u2192 Perda = -log(0,01) \u2248 4,6 (muito alta, muito ruim)"}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"processo-de-treinamento",children:"Processo de Treinamento"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:"# Pseudoc\xf3digo\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        states, actions = batch\n\n        # Propaga\xe7\xe3o direta\n        policy = network(states)  # Vetor de probabilidade 361-dim\n\n        # Calcular perda (entropia cruzada)\n        loss = cross_entropy(policy, actions)\n\n        # Retropropaga\xe7\xe3o\n        loss.backward()\n        optimizer.step()\n"})}),"\n",(0,o.jsx)(a.p,{children:"Detalhes do treinamento:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Otimizador"}),": SGD com momentum"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Taxa de aprendizado"}),": Inicial 0,003, decaindo gradualmente"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Tamanho do lote"}),": 16"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Tempo de treinamento"}),": Aproximadamente 3 semanas (50 GPUs)"]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"aumento-de-dados",children:"Aumento de Dados"}),"\n",(0,o.jsx)(a.p,{children:"O tabuleiro de Go tem simetria de 8 dobras (4 rota\xe7\xf5es \xd7 2 espelhamentos). Cada amostra de treinamento pode ser transformada em 8 amostras equivalentes:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Original \u2192 Rota\xe7\xe3o 90\xb0 \u2192 Rota\xe7\xe3o 180\xb0 \u2192 Rota\xe7\xe3o 270\xb0\n    \u2193          \u2193              \u2193              \u2193\nEspelhamento horizontal \u2192 ...\n"})}),"\n",(0,o.jsx)(a.p,{children:"Isso aumenta os dados de treinamento efetivos em 8 vezes e garante que os padr\xf5es aprendidos pelo modelo n\xe3o dependam da orienta\xe7\xe3o."}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"resultados-do-treinamento",children:"Resultados do Treinamento"}),"\n",(0,o.jsx)(a.h3,{id:"57-de-precis\xe3o",children:"57% de Precis\xe3o"}),"\n",(0,o.jsxs)(a.p,{children:["Ap\xf3s o treinamento, a Policy Network alcan\xe7ou ",(0,o.jsx)(a.strong,{children:"57% de precis\xe3o top-1"}),"."]}),"\n",(0,o.jsx)(a.p,{children:"Isso significa: dada qualquer posi\xe7\xe3o, o modelo tem 57% de chance de prever a jogada exata que o especialista humano fez."}),"\n",(0,o.jsx)(a.h4,{id:"esta-precis\xe3o-\xe9-alta",children:"Esta precis\xe3o \xe9 alta?"}),"\n",(0,o.jsx)(a.p,{children:"Considerando que cada posi\xe7\xe3o tem em m\xe9dia 250 jogadas legais, a precis\xe3o de adivinha\xe7\xe3o aleat\xf3ria \xe9 de apenas 0,4%."}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"M\xe9todo"}),(0,o.jsx)(a.th,{children:"Precis\xe3o Top-1"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Adivinha\xe7\xe3o aleat\xf3ria"}),(0,o.jsx)(a.td,{children:"0,4%"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Melhor programa de Go anterior"}),(0,o.jsx)(a.td,{children:"~44%"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Policy Network do AlphaGo"}),(0,o.jsx)(a.td,{children:(0,o.jsx)(a.strong,{children:"57%"})})]})]})]}),"\n",(0,o.jsx)(a.p,{children:"Um aumento de 13 pontos percentuais pode parecer pequeno, mas \xe9 muito significativo."}),"\n",(0,o.jsx)(a.h3,{id:"melhoria-na-for\xe7a-de-jogo",children:"Melhoria na For\xe7a de Jogo"}),"\n",(0,o.jsx)(a.p,{children:"Qual for\xe7a de jogo pode ser alcan\xe7ada usando puramente a Policy Network (sem busca)?"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Configura\xe7\xe3o"}),(0,o.jsx)(a.th,{children:"Classifica\xe7\xe3o Elo"}),(0,o.jsx)(a.th,{children:"N\xedvel Aproximado"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Melhor programa anterior (Pachi)"}),(0,o.jsx)(a.td,{children:"2.500"}),(0,o.jsx)(a.td,{children:"Amador 4-5 dan"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Apenas Policy Network"}),(0,o.jsx)(a.td,{children:"2.800"}),(0,o.jsx)(a.td,{children:"Amador 6-7 dan"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"+ MCTS 1600 simula\xe7\xf5es"}),(0,o.jsx)(a.td,{children:"3.200+"}),(0,o.jsx)(a.td,{children:"N\xedvel profissional"})]})]})]}),"\n",(0,o.jsx)(a.p,{children:"A Policy Network sozinha j\xe1 \xe9 de n\xedvel amador alto, e com MCTS salta para n\xedvel profissional."}),"\n",(0,o.jsx)(a.h3,{id:"por-que-apenas-57",children:"Por que apenas 57%?"}),"\n",(0,o.jsx)(a.p,{children:"Os registros de jogos humanos t\xeam as seguintes caracter\xedsticas que limitam a precis\xe3o:"}),"\n",(0,o.jsx)(a.h4,{id:"1-m\xfaltiplas-boas-jogadas",children:"1. M\xfaltiplas Boas Jogadas"}),"\n",(0,o.jsx)(a.p,{children:'Muitas posi\xe7\xf5es t\xeam m\xfaltiplas boas jogadas. Por exemplo, "aproxima\xe7\xe3o do canto" e "defesa do canto" podem ambas ser escolhas corretas. Se o modelo escolhe outra boa jogada, \xe9 contado como "errado".'}),"\n",(0,o.jsx)(a.h4,{id:"2-diferen\xe7as-de-estilo",children:"2. Diferen\xe7as de Estilo"}),"\n",(0,o.jsx)(a.p,{children:'Diferentes jogadores t\xeam estilos diferentes. Jogadores agressivos e jogadores s\xf3lidos podem fazer jogadas diferentes na mesma posi\xe7\xe3o. O modelo aprende um estilo "m\xe9dio".'}),"\n",(0,o.jsx)(a.h4,{id:"3-humanos-tamb\xe9m-erram",children:"3. Humanos Tamb\xe9m Erram"}),"\n",(0,o.jsx)(a.p,{children:'Os dados do KGS incluem jogos de jogadores amadores, cujas escolhas nem sempre s\xe3o \xf3timas. \xc9 normal o modelo aprender alguns "erros".'}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"papel-no-mcts",children:"Papel no MCTS"}),"\n",(0,o.jsx)(a.p,{children:"A Policy Network desempenha dois pap\xe9is-chave no MCTS do AlphaGo:"}),"\n",(0,o.jsx)(a.h3,{id:"1-guiar-a-dire\xe7\xe3o-da-busca",children:"1. Guiar a Dire\xe7\xe3o da Busca"}),"\n",(0,o.jsxs)(a.p,{children:["Na fase de ",(0,o.jsx)(a.strong,{children:"Sele\xe7\xe3o"})," do MCTS, a sa\xedda da Policy Network \xe9 usada para calcular o UCB (Upper Confidence Bound):"]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"UCB(s, a) = Q(s, a) + c_puct \xd7 P(s, a) \xd7 \u221a(N(s)) / (1 + N(s, a))\n"})}),"\n",(0,o.jsxs)(a.p,{children:["Onde ",(0,o.jsx)(a.code,{children:"P(s, a)"})," \xe9 a probabilidade dada pela Policy Network."]}),"\n",(0,o.jsx)(a.p,{children:"Isso significa:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:(0,o.jsx)(a.strong,{children:"Jogadas de alta probabilidade s\xe3o exploradas primeiro"})}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Jogadas de baixa probabilidade tamb\xe9m t\xeam chance de serem exploradas"})," (por causa do termo de explora\xe7\xe3o)"]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"2-prior-para-expans\xe3o-de-n\xf3s",children:"2. Prior para Expans\xe3o de N\xf3s"}),"\n",(0,o.jsxs)(a.p,{children:["Quando o MCTS expande um novo n\xf3, a Policy Network fornece as ",(0,o.jsx)(a.strong,{children:"probabilidades a priori"})," para todos os n\xf3s filhos."]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"Expandir n\xf3 s:\n  for each action a:\n    child = Node()\n    child.prior = policy_network(s)[a]  # Probabilidade a priori\n    child.value = 0\n    child.visits = 0\n"})}),"\n",(0,o.jsx)(a.p,{children:'Essas probabilidades a priori permitem que o MCTS "saiba" quais n\xf3s filhos vale mais a pena explorar, mesmo que ainda n\xe3o tenham sido visitados.'}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"vers\xe3o-leve-vs-vers\xe3o-completa",children:"Vers\xe3o Leve vs. Vers\xe3o Completa"}),"\n",(0,o.jsx)(a.p,{children:"O AlphaGo na verdade tem duas Policy Networks:"}),"\n",(0,o.jsx)(a.h3,{id:"vers\xe3o-completa-sl-policy-network",children:"Vers\xe3o Completa (SL Policy Network)"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Arquitetura"}),": CNN de 13 camadas, 192 filtros"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Precis\xe3o"}),": 57%"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Tempo de infer\xeancia"}),": ~3 milissegundos/posi\xe7\xe3o"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Uso"}),": Sele\xe7\xe3o e Expans\xe3o no MCTS"]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"vers\xe3o-leve-rollout-policy-network",children:"Vers\xe3o Leve (Rollout Policy Network)"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Arquitetura"}),": Modelo linear + caracter\xedsticas manuais"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Precis\xe3o"}),": 24%"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Tempo de infer\xeancia"}),": ~2 microssegundos/posi\xe7\xe3o (1500\xd7 mais r\xe1pido)"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Uso"}),": Simula\xe7\xe3o r\xe1pida (rollout)"]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"por-que-precisamos-da-vers\xe3o-leve",children:"Por que precisamos da vers\xe3o leve?"}),"\n",(0,o.jsxs)(a.p,{children:["Na fase de ",(0,o.jsx)(a.strong,{children:"Simula\xe7\xe3o"})," do MCTS, \xe9 necess\xe1rio jogar desde o n\xf3 atual at\xe9 o fim do jogo, potencialmente 100+ jogadas. Se cada jogada usar a Policy Network completa, \xe9 muito lento."]}),"\n",(0,o.jsx)(a.p,{children:"A vers\xe3o leve tem apenas 24% de precis\xe3o, mas \xe9 1500\xd7 mais r\xe1pida. No rollout, velocidade \xe9 mais importante que precis\xe3o."}),"\n",(0,o.jsx)(a.h3,{id:"caracter\xedsticas-da-vers\xe3o-leve",children:"Caracter\xedsticas da Vers\xe3o Leve"}),"\n",(0,o.jsx)(a.p,{children:"A vers\xe3o leve usa caracter\xedsticas projetadas manualmente, incluindo:"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"Tipo de Caracter\xedstica"}),(0,o.jsx)(a.th,{children:"Exemplos"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Padr\xf5es locais"}),(0,o.jsx)(a.td,{children:"Configura\xe7\xe3o de pedras em regi\xe3o 3\xd73"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Caracter\xedsticas globais"}),(0,o.jsx)(a.td,{children:"Se est\xe1 no canto/borda, pontos grandes"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Caracter\xedsticas t\xe1ticas"}),(0,o.jsx)(a.td,{children:"Atari, escadas, conex\xf5es"})]})]})]}),"\n",(0,o.jsx)(a.p,{children:"Essas caracter\xedsticas s\xe3o alimentadas em um modelo linear (sem camadas ocultas), calculando extremamente r\xe1pido."}),"\n",(0,o.jsx)(a.h3,{id:"melhorias-no-alphago-zero",children:"Melhorias no AlphaGo Zero"}),"\n",(0,o.jsx)(a.p,{children:"O posterior AlphaGo Zero abandonou completamente a vers\xe3o leve e os rollouts. Ele avalia os n\xf3s folha diretamente com a Value Network, n\xe3o precisando de simula\xe7\xe3o r\xe1pida. Esta foi uma simplifica\xe7\xe3o importante."}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"refinamento-com-aprendizado-por-refor\xe7o-rl-policy-network",children:"Refinamento com Aprendizado por Refor\xe7o (RL Policy Network)"}),"\n",(0,o.jsx)(a.h3,{id:"limita\xe7\xf5es-do-aprendizado-supervisionado",children:"Limita\xe7\xf5es do Aprendizado Supervisionado"}),"\n",(0,o.jsx)(a.p,{children:"A Policy Network treinada com aprendizado supervisionado tem um problema fundamental:"}),"\n",(0,o.jsxs)(a.blockquote,{children:["\n",(0,o.jsx)(a.p,{children:(0,o.jsx)(a.strong,{children:'Ela aprende a "imitar humanos", n\xe3o a "vencer"'})}),"\n"]}),"\n",(0,o.jsx)(a.p,{children:"Isso significa que ela aprender\xe1 os maus h\xe1bitos dos humanos e tamb\xe9m ter\xe1 desempenho ruim em posi\xe7\xf5es que os humanos nunca encontraram."}),"\n",(0,o.jsx)(a.h3,{id:"auto-jogo-com-refor\xe7o",children:"Auto-jogo com Refor\xe7o"}),"\n",(0,o.jsxs)(a.p,{children:["A solu\xe7\xe3o do DeepMind \xe9 usar o m\xe9todo de ",(0,o.jsx)(a.strong,{children:"Policy Gradient"})," para aprendizado por refor\xe7o:"]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"1. Deixar a Policy Network jogar contra si mesma\n2. Registrar todas as jogadas de cada partida\n3. Ajustar par\xe2metros baseado no resultado:\n   - Venceu \u2192 Aumentar probabilidade dessas jogadas\n   - Perdeu \u2192 Diminuir probabilidade dessas jogadas\n"})}),"\n",(0,o.jsx)(a.h3,{id:"algoritmo-reinforce",children:"Algoritmo REINFORCE"}),"\n",(0,o.jsx)(a.p,{children:"Especificamente usando o algoritmo REINFORCE:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{children:"\u2207J(\u03b8) = E[\u03a3_t \u2207log \u03c0_\u03b8(a_t | s_t) \xd7 z]\n"})}),"\n",(0,o.jsx)(a.p,{children:"Onde:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.code,{children:"z"}),": Resultado do jogo (+1 vit\xf3ria, -1 derrota)"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.code,{children:"\u03c0_\u03b8(a_t | s_t)"}),": Probabilidade de escolher a\xe7\xe3o ",(0,o.jsx)(a.code,{children:"a_t"})," no estado ",(0,o.jsx)(a.code,{children:"s_t"})]}),"\n"]}),"\n",(0,o.jsx)(a.h3,{id:"resultados",children:"Resultados"}),"\n",(0,o.jsx)(a.p,{children:"Ap\xf3s aproximadamente 1 dia de treinamento com auto-jogo (1,28 milh\xe3o de jogos), a RL Policy Network:"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"M\xe9trica"}),(0,o.jsx)(a.th,{children:"SL Policy"}),(0,o.jsx)(a.th,{children:"RL Policy"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Taxa de vit\xf3ria contra SL Policy"}),(0,o.jsx)(a.td,{children:"50%"}),(0,o.jsx)(a.td,{children:(0,o.jsx)(a.strong,{children:"80%"})})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"Aumento de Elo"}),(0,o.jsx)(a.td,{children:"-"}),(0,o.jsx)(a.td,{children:"+100"})]})]})]}),"\n",(0,o.jsx)(a.p,{children:"A precis\xe3o pode diminuir ligeiramente (porque n\xe3o imita mais completamente os humanos), mas a taxa de vit\xf3ria real melhora significativamente."}),"\n",(0,o.jsx)(a.h3,{id:"de-imitar-para-inovar",children:'De "Imitar" para "Inovar"'}),"\n",(0,o.jsx)(a.p,{children:"O aprendizado por refor\xe7o permitiu que a Policy Network aprendesse algumas jogadas que os humanos nunca pensaram. Essas jogadas nunca apareceram nos dados de treinamento, mas s\xe3o eficazes."}),"\n",(0,o.jsx)(a.p,{children:'\xc9 por isso que o AlphaGo pode fazer "a jogada divina" \u2014 n\xe3o \xe9 limitado pela experi\xeancia humana.'}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"an\xe1lise-visual",children:"An\xe1lise Visual"}),"\n",(0,o.jsx)(a.h3,{id:"distribui\xe7\xf5es-de-probabilidade-em-diferentes-posi\xe7\xf5es",children:"Distribui\xe7\xf5es de Probabilidade em Diferentes Posi\xe7\xf5es"}),"\n",(0,o.jsx)(a.p,{children:"Vamos ver a sa\xedda da Policy Network em diferentes posi\xe7\xf5es:"}),"\n",(0,o.jsx)(a.h4,{id:"abertura-fase-de-fuseki",children:"Abertura (Fase de Fuseki)"}),"\n",(0,o.jsx)(t.dW,{initialPosition:"opening",size:400}),"\n",(0,o.jsx)(a.p,{children:"Na abertura, a probabilidade est\xe1 principalmente concentrada em:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:"Cantos (ocupar cantos)"}),"\n",(0,o.jsx)(a.li,{children:"Bordas (aproxima\xe7\xe3o do canto, defesa do canto)"}),"\n",(0,o.jsx)(a.li,{children:'Posi\xe7\xf5es de "grande escala"'}),"\n"]}),"\n",(0,o.jsx)(a.p,{children:"Isso est\xe1 de acordo com o princ\xedpio b\xe1sico do Go: cantos de ouro, bordas de prata, centro de grama."}),"\n",(0,o.jsx)(a.h4,{id:"posi\xe7\xe3o-de-combate",children:"Posi\xe7\xe3o de Combate"}),"\n",(0,o.jsx)(t.dW,{initialPosition:"fighting",size:400}),"\n",(0,o.jsx)(a.p,{children:"Durante o combate, a probabilidade est\xe1 concentrada em:"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsx)(a.li,{children:"Pontos de corte cr\xedticos"}),"\n",(0,o.jsx)(a.li,{children:"Atari, conex\xf5es"}),"\n",(0,o.jsx)(a.li,{children:"Fazer olhos, destruir olhos"}),"\n"]}),"\n",(0,o.jsx)(a.p,{children:"Isso mostra que o modelo aprendeu t\xe1ticas locais."}),"\n",(0,o.jsx)(a.h4,{id:"fase-de-yose",children:"Fase de Yose"}),"\n",(0,o.jsx)(t.dW,{initialPosition:"endgame",size:400}),"\n",(0,o.jsx)(a.p,{children:"No yose, a probabilidade est\xe1 dispersa em v\xe1rios pontos de fechamento, requerendo c\xe1lculo preciso de pontos."}),"\n",(0,o.jsx)(a.h3,{id:"o-que-as-camadas-ocultas-aprenderam",children:"O que as Camadas Ocultas Aprenderam?"}),"\n",(0,o.jsx)(a.p,{children:'Visualizando a sa\xedda das camadas convolucionais, podemos ver as "caracter\xedsticas" aprendidas pelo modelo:'}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Camadas baixas"}),": Formas b\xe1sicas (olhos, pontos de corte)"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Camadas m\xe9dias"}),": Padr\xf5es t\xe1ticos (atari, escadas)"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Camadas altas"}),": Conceitos globais (influ\xeancia, espessura)"]}),"\n"]}),"\n",(0,o.jsx)(a.p,{children:"Isso \xe9 muito similar \xe0 estrutura hier\xe1rquica de como os humanos compreendem o Go."}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"pontos-de-implementa\xe7\xe3o",children:"Pontos de Implementa\xe7\xe3o"}),"\n",(0,o.jsx)(a.h3,{id:"implementa\xe7\xe3o-em-pytorch",children:"Implementa\xe7\xe3o em PyTorch"}),"\n",(0,o.jsx)(a.p,{children:"Aqui est\xe1 uma implementa\xe7\xe3o simplificada da Policy Network:"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PolicyNetwork(nn.Module):\n    def __init__(self, input_channels=48, num_filters=192, num_layers=12):\n        super().__init__()\n\n        # Primeira camada convolucional (5\xd75)\n        self.conv1 = nn.Conv2d(input_channels, num_filters,\n                               kernel_size=5, padding=2)\n\n        # Camadas convolucionais intermedi\xe1rias (3\xd73)\xd711\n        self.conv_layers = nn.ModuleList([\n            nn.Conv2d(num_filters, num_filters,\n                     kernel_size=3, padding=1)\n            for _ in range(num_layers - 1)\n        ])\n\n        # Camada convolucional de sa\xedda (1\xd71)\n        self.conv_out = nn.Conv2d(num_filters, 1, kernel_size=1)\n\n    def forward(self, x):\n        # x: (batch, 48, 19, 19)\n\n        # Primeira camada\n        x = F.relu(self.conv1(x))\n\n        # Camadas intermedi\xe1rias\n        for conv in self.conv_layers:\n            x = F.relu(conv(x))\n\n        # Camada de sa\xedda\n        x = self.conv_out(x)  # (batch, 1, 19, 19)\n\n        # Achatar + Softmax\n        x = x.view(x.size(0), -1)  # (batch, 361)\n        x = F.softmax(x, dim=1)\n\n        return x\n"})}),"\n",(0,o.jsx)(a.h3,{id:"loop-de-treinamento",children:"Loop de Treinamento"}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'def train_step(model, optimizer, states, actions):\n    """\n    states: (batch, 48, 19, 19) - Caracter\xedsticas do tabuleiro\n    actions: (batch,) - Posi\xe7\xe3o jogada pelo humano (0-360)\n    """\n    # Propaga\xe7\xe3o direta\n    policy = model(states)  # (batch, 361)\n\n    # Perda de entropia cruzada\n    loss = F.cross_entropy(\n        torch.log(policy + 1e-8),  # Prevenir log(0)\n        actions\n    )\n\n    # Retropropaga\xe7\xe3o\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Calcular precis\xe3o\n    predictions = policy.argmax(dim=1)\n    accuracy = (predictions == actions).float().mean()\n\n    return loss.item(), accuracy.item()\n'})}),"\n",(0,o.jsx)(a.h3,{id:"considera\xe7\xf5es-para-infer\xeancia",children:"Considera\xe7\xf5es para Infer\xeancia"}),"\n",(0,o.jsx)(a.p,{children:"Durante o jogo real, note:"}),"\n",(0,o.jsxs)(a.ol,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Filtrar jogadas ilegais"}),": Definir probabilidade de posi\xe7\xf5es ilegais como 0, depois renormalizar"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Ajuste de temperatura"}),': Pode usar par\xe2metro de temperatura para controlar a "nitidez" da distribui\xe7\xe3o de probabilidade']}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Infer\xeancia em lote"}),": No MCTS pode processar m\xfaltiplas posi\xe7\xf5es em lote"]}),"\n"]}),"\n",(0,o.jsx)(a.pre,{children:(0,o.jsx)(a.code,{className:"language-python",children:'def get_move_probabilities(model, state, legal_moves, temperature=1.0):\n    """Obter distribui\xe7\xe3o de probabilidade para jogadas legais"""\n    policy = model(state)  # (361,)\n\n    # Manter apenas jogadas legais\n    mask = torch.zeros(361)\n    mask[legal_moves] = 1\n    policy = policy * mask\n\n    # Ajuste de temperatura\n    if temperature != 1.0:\n        policy = policy ** (1 / temperature)\n\n    # Renormalizar\n    policy = policy / policy.sum()\n\n    return policy\n'})}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"correspond\xeancia-com-anima\xe7\xf5es",children:"Correspond\xeancia com Anima\xe7\xf5es"}),"\n",(0,o.jsx)(a.p,{children:"Os conceitos principais abordados neste artigo e seus n\xfameros de anima\xe7\xe3o:"}),"\n",(0,o.jsxs)(a.table,{children:[(0,o.jsx)(a.thead,{children:(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.th,{children:"N\xfamero"}),(0,o.jsx)(a.th,{children:"Conceito"}),(0,o.jsx)(a.th,{children:"Correspond\xeancia F\xedsica/Matem\xe1tica"})]})}),(0,o.jsxs)(a.tbody,{children:[(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"E1"}),(0,o.jsx)(a.td,{children:"Policy Network"}),(0,o.jsx)(a.td,{children:"Campo de probabilidade"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"D9"}),(0,o.jsx)(a.td,{children:"Extra\xe7\xe3o de caracter\xedsticas CNN"}),(0,o.jsx)(a.td,{children:"Resposta de filtros"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"D3"}),(0,o.jsx)(a.td,{children:"Aprendizado supervisionado"}),(0,o.jsx)(a.td,{children:"Estimativa de m\xe1xima verossimilhan\xe7a"})]}),(0,o.jsxs)(a.tr,{children:[(0,o.jsx)(a.td,{children:"H4"}),(0,o.jsx)(a.td,{children:"Policy gradient"}),(0,o.jsx)(a.td,{children:"Otimiza\xe7\xe3o estoc\xe1stica"})]})]})]}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"leitura-adicional",children:"Leitura Adicional"}),"\n",(0,o.jsxs)(a.ul,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Pr\xf3ximo artigo"}),": ",(0,o.jsx)(a.a,{href:"../value-network",children:"Detalhes da Value Network"})," \u2014 Como o AlphaGo avalia posi\xe7\xf5es"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"T\xf3pico relacionado"}),": ",(0,o.jsx)(a.a,{href:"../input-features",children:"Design de Caracter\xedsticas de Entrada"})," \u2014 Detalhes dos 48 planos de caracter\xedsticas"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Princ\xedpios profundos"}),": ",(0,o.jsx)(a.a,{href:"../cnn-and-go",children:"CNN e Go"})," \u2014 Por que redes neurais convolucionais s\xe3o adequadas para tabuleiros"]}),"\n"]}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"pontos-chave",children:"Pontos-Chave"}),"\n",(0,o.jsxs)(a.ol,{children:["\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Policy Network \xe9 um gerador de distribui\xe7\xe3o de probabilidade"}),": Entrada do tabuleiro, sa\xedda de probabilidades para 361 posi\xe7\xf5es"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"13 camadas CNN + Softmax"}),": Convolu\xe7\xe3o profunda para extra\xe7\xe3o de caracter\xedsticas, Softmax para sa\xedda de probabilidades"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"57% de precis\xe3o"}),": Muito superior aos programas de Go anteriores"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Duas vers\xf5es"}),": Vers\xe3o completa para decis\xf5es no MCTS, vers\xe3o leve para simula\xe7\xe3o r\xe1pida"]}),"\n",(0,o.jsxs)(a.li,{children:[(0,o.jsx)(a.strong,{children:"Refinamento com aprendizado por refor\xe7o"}),': De "imitar humanos" para "buscar vit\xf3ria"']}),"\n"]}),"\n",(0,o.jsx)(a.p,{children:'A Policy Network \xe9 a "intui\xe7\xe3o" do AlphaGo \u2014 ela permite que a IA, como os humanos, identifique rapidamente jogadas que valem a pena considerar.'}),"\n",(0,o.jsx)(a.hr,{}),"\n",(0,o.jsx)(a.h2,{id:"refer\xeancias",children:"Refer\xeancias"}),"\n",(0,o.jsxs)(a.ol,{children:["\n",(0,o.jsxs)(a.li,{children:['Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." ',(0,o.jsx)(a.em,{children:"Nature"}),", 529, 484-489."]}),"\n",(0,o.jsxs)(a.li,{children:['Maddison, C. J., et al. (2014). "Move Evaluation in Go Using Deep Convolutional Neural Networks." ',(0,o.jsx)(a.em,{children:"arXiv:1412.6564"}),"."]}),"\n",(0,o.jsxs)(a.li,{children:["Sutton, R. S., & Barto, A. G. (2018). ",(0,o.jsx)(a.em,{children:"Reinforcement Learning: An Introduction"}),". MIT Press."]}),"\n",(0,o.jsxs)(a.li,{children:['LeCun, Y., Bengio, Y., & Hinton, G. (2015). "Deep learning." ',(0,o.jsx)(a.em,{children:"Nature"}),", 521, 436-444."]}),"\n"]})]})}function p(e={}){const{wrapper:a}={...(0,n.R)(),...e.components};return a?(0,o.jsx)(a,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},42948(e,a,r){r.d(a,{A:()=>n});r(59471);var s=r(61785),o=r(62615);function n({children:e,fallback:a}){return(0,s.A)()?(0,o.jsx)(o.Fragment,{children:e?.()}):a??null}},45695(e,a,r){r.d(a,{$W:()=>P,tO:()=>d,u8:()=>v,dW:()=>x});var s=r(59471),o=r(90989),n=r(62615);const t=19,i=[[3,3],[3,9],[3,15],[9,3],[9,9],[9,15],[15,3],[15,9],[15,15]];function d({size:e=400,stones:a=[],highlights:r=[],labels:d=[],onCellClick:l=null,showCoordinates:c=!0}){const h=(0,s.useRef)(null),p=c?30:15,m=e-2*p,x=m/18;return(0,s.useEffect)(()=>{if(!h.current)return;const e=o.Ltv(h.current);e.selectAll("*").remove();const s=e.append("g").attr("transform",`translate(${p}, ${p})`);s.append("rect").attr("x",-x/2).attr("y",-x/2).attr("width",m+x).attr("height",m+x).attr("fill","#dcb35c").attr("rx",4);const n=s.append("g").attr("class","grid");for(let a=0;a<t;a++)n.append("line").attr("class","grid-line").attr("x1",0).attr("y1",a*x).attr("x2",18*x).attr("y2",a*x);for(let a=0;a<t;a++)n.append("line").attr("class","grid-line").attr("x1",a*x).attr("y1",0).attr("x2",a*x).attr("y2",18*x);const u=s.append("g").attr("class","star-points");if(i.forEach(([e,a])=>{u.append("circle").attr("class","star-point").attr("cx",e*x).attr("cy",a*x).attr("r",x/8)}),r.length>0){const e=s.append("g").attr("class","highlights");r.forEach(({x:a,y:r,intensity:s})=>{e.append("rect").attr("class","heatmap-cell").attr("x",a*x-x/2).attr("y",r*x-x/2).attr("width",x).attr("height",x).attr("fill",o.Q3(s)).attr("opacity",.7*s)})}const j=s.append("g").attr("class","stones");if(a.forEach(({x:e,y:a,color:r})=>{const s="black"===r?"stone-black":"stone-white";j.append("circle").attr("cx",e*x+2).attr("cy",a*x+2).attr("r",.45*x).attr("fill","rgba(0,0,0,0.2)"),j.append("circle").attr("class",s).attr("cx",e*x).attr("cy",a*x).attr("r",.45*x)}),d.length>0){const e=s.append("g").attr("class","labels");d.forEach(({x:r,y:s,text:o})=>{const n=a.find(e=>e.x===r&&e.y===s),t="black"===n?.color?"#fff":"#000";e.append("text").attr("x",r*x).attr("y",s*x).attr("dy","0.35em").attr("text-anchor","middle").attr("fill",t).attr("font-size",.5*x).attr("font-weight","bold").text(o)})}if(c){const a=e.append("g").attr("class","coordinates"),r="ABCDEFGHJKLMNOPQRST";for(let e=0;e<t;e++)a.append("text").attr("x",p+e*x).attr("y",p/2).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(r[e]);for(let e=0;e<t;e++)a.append("text").attr("x",p/2).attr("y",p+e*x).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(t-e)}l&&s.append("g").attr("class","click-targets").selectAll("rect").data(o.y17(361)).enter().append("rect").attr("x",e=>e%t*x-x/2).attr("y",e=>Math.floor(e/t)*x-x/2).attr("width",x).attr("height",x).attr("fill","transparent").attr("cursor","pointer").on("click",(e,a)=>{const r=a%t,s=Math.floor(a/t);l({x:r,y:s})})},[e,a,r,d,c,l,x,p,m]),(0,n.jsx)("div",{className:"go-board-container",children:(0,n.jsx)("svg",{ref:h,width:e,height:e,className:"go-board"})})}var l=r(42948);const c=19,h={empty:function(){const e=[];for(let a=0;a<c;a++)for(let r=0;r<c;r++)e.push({x:r,y:a,prob:1/361});return e}(),corner:function(){const e=[],a=[[3,3],[3,15],[15,3],[15,15]],r=[[2,4],[4,2],[2,14],[4,16],[14,2],[16,4],[14,16],[16,14]];for(let s=0;s<c;s++)for(let o=0;o<c;o++){let n=.001;a.some(([e,a])=>e===o&&a===s)?n=.15:r.some(([e,a])=>e===o&&a===s)?n=.05:0!==o&&18!==o&&0!==s&&18!==s||(n=5e-4),e.push({x:o,y:s,prob:n})}return p(e)}(),move37:function(){const e=[],a={x:9,y:4},r=[[3,2],[15,2],[10,10],[8,6]];for(let s=0;s<c;s++)for(let o=0;o<c;o++){let n=.001;o===a.x&&s===a.y?n=.08:r.some(([e,a])=>e===o&&a===s)?n=.12:o>=5&&o<=13&&s>=5&&s<=13&&(n=.005+.01*Math.random()),e.push({x:o,y:s,prob:n})}return p(e)}()};function p(e){const a=e.reduce((e,a)=>e+a.prob,0);return e.map(e=>({...e,prob:e.prob/a}))}function m({initialPosition:e="corner",stones:a=[],highlightMoves:r=[],size:t=450,showTopN:i=5,interactive:d=!0}){const l=(0,s.useRef)(null),p=(0,s.useRef)(null),[m,x]=(0,s.useState)(h[e]||h.corner),[u,j]=(0,s.useState)(null),v=35,g=t-70,f=g/18;(0,s.useEffect)(()=>{if(!l.current)return;const e=o.Ltv(l.current);e.selectAll("*").remove();const r=e.append("g").attr("transform","translate(35, 35)");r.append("rect").attr("x",-f/2).attr("y",-f/2).attr("width",g+f).attr("height",g+f).attr("fill","#dcb35c").attr("rx",4);const s=Math.max(...m.map(e=>e.prob)),n=o.exT(o.oKI).domain([0,s]);r.append("g").attr("class","heatmap").selectAll("rect").data(m).enter().append("rect").attr("class","heatmap-cell").attr("x",e=>e.x*f-f/2).attr("y",e=>e.y*f-f/2).attr("width",f).attr("height",f).attr("fill",e=>n(e.prob)).attr("opacity",e=>.3+e.prob/s*.6).attr("cursor",d?"pointer":"default").on("mouseover",function(e,a){if(!d)return;o.Ltv(this).attr("stroke","#333").attr("stroke-width",2);o.Ltv(p.current).style("display","block").style("left",`${e.pageX+10}px`).style("top",e.pageY-10+"px").html(`\u4f4d\u7f6e: ${String.fromCharCode(65+a.x)}${19-a.y}<br>\u6a5f\u7387: ${(100*a.prob).toFixed(2)}%`)}).on("mouseout",function(){o.Ltv(this).attr("stroke","none"),o.Ltv(p.current).style("display","none")}).on("click",function(e,a){d&&j(a)});const t=r.append("g").attr("class","grid");for(let a=0;a<c;a++)t.append("line").attr("class","grid-line").attr("x1",0).attr("y1",a*f).attr("x2",18*f).attr("y2",a*f).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5),t.append("line").attr("class","grid-line").attr("x1",a*f).attr("y1",0).attr("x2",a*f).attr("y2",18*f).attr("stroke","#333").attr("stroke-width",.5).attr("opacity",.5);const h=r.append("g").attr("class","stones");a.forEach(({x:e,y:a,color:r})=>{h.append("circle").attr("cx",e*f).attr("cy",a*f).attr("r",.45*f).attr("fill","black"===r?"#1a1a1a":"#f5f5f5").attr("stroke","black"===r?"#000":"#333").attr("stroke-width",1)});const x=[...m].sort((e,a)=>a.prob-e.prob).slice(0,i),u=r.append("g").attr("class","top-labels");x.forEach((e,r)=>{a.some(a=>a.x===e.x&&a.y===e.y)||(u.append("circle").attr("cx",e.x*f).attr("cy",e.y*f).attr("r",.3*f).attr("fill","rgba(255,255,255,0.8)").attr("stroke","#e74c3c").attr("stroke-width",2),u.append("text").attr("x",e.x*f).attr("y",e.y*f).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#e74c3c").attr("font-size",.4*f).attr("font-weight","bold").text(r+1))});const b=e.append("g").attr("class","coordinates");for(let a=0;a<c;a++)b.append("text").attr("x",v+a*f).attr("y",17.5).attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text("ABCDEFGHJKLMNOPQRST"[a]),b.append("text").attr("x",17.5).attr("y",v+a*f).attr("dy","0.35em").attr("text-anchor","middle").attr("fill","#666").attr("font-size",10).text(c-a)},[m,a,i,d,f,v,g]);const b=e=>{x(h[e]||h.corner)};return(0,n.jsxs)("div",{children:[d&&(0,n.jsxs)("div",{className:"d3-controls",children:[(0,n.jsx)("button",{className:"empty"===e?"active":"",onClick:()=>b("empty"),children:"\u5747\u52fb\u5206\u5e03"}),(0,n.jsx)("button",{className:"corner"===e?"active":"",onClick:()=>b("corner"),children:"\u958b\u5c40\u661f\u4f4d"}),(0,n.jsx)("button",{className:"move37"===e?"active":"",onClick:()=>b("move37"),children:"\u7b2c 37 \u624b"})]}),(0,n.jsx)("div",{className:"go-board-container",children:(0,n.jsx)("svg",{ref:l,width:t,height:t,className:"go-board"})}),(0,n.jsx)("div",{ref:p,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),u&&(0,n.jsx)("div",{className:"d3-legend",children:(0,n.jsxs)("div",{className:"d3-legend-item",children:["\u5df2\u9078\u64c7: ",String.fromCharCode(65+u.x),19-u.y,"\u2014 \u6a5f\u7387: ",(100*u.prob).toFixed(2),"%"]})}),(0,n.jsxs)("div",{className:"d3-legend",children:[(0,n.jsxs)("div",{className:"d3-legend-item",children:[(0,n.jsx)("div",{className:"d3-legend-color",style:{background:"#ffffb2"}}),"\u4f4e\u6a5f\u7387"]}),(0,n.jsxs)("div",{className:"d3-legend-item",children:[(0,n.jsx)("div",{className:"d3-legend-color",style:{background:"#fd8d3c"}}),"\u4e2d\u6a5f\u7387"]}),(0,n.jsxs)("div",{className:"d3-legend-item",children:[(0,n.jsx)("div",{className:"d3-legend-color",style:{background:"#bd0026"}}),"\u9ad8\u6a5f\u7387"]})]})]})}function x(e){return(0,n.jsx)(l.A,{fallback:(0,n.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,n.jsx)(m,{...e})})}const u={name:"Root",visits:1600,value:.55,prior:1,children:[{name:"D4",visits:800,value:.62,prior:.35,selected:!0,children:[{name:"Q16",visits:400,value:.58,prior:.3},{name:"R4",visits:300,value:.65,prior:.25,selected:!0},{name:"C16",visits:100,value:.55,prior:.2}]},{name:"Q4",visits:500,value:.52,prior:.3,children:[{name:"D16",visits:300,value:.5,prior:.28},{name:"Q16",visits:200,value:.54,prior:.22}]},{name:"D16",visits:200,value:.48,prior:.2},{name:"Q16",visits:100,value:.45,prior:.15}]};function j({data:e=u,width:a=700,height:r=450,showPUCT:t=!0,cPuct:i=1.5,interactive:d=!0}){const l=(0,s.useRef)(null),c=(0,s.useRef)(null),[h,p]=(0,s.useState)(null),[m,x]=(0,s.useState)(i),j=40,v=40,g=a-v-40,f=r-j-40;return(0,s.useEffect)(()=>{if(!l.current)return;const s=o.Ltv(l.current);s.selectAll("*").remove();const n=o.B22().size([g,f-50]),i=o.Sk5(e);n(i);const h=s.append("g").attr("transform",`translate(${v}, ${j})`);h.append("g").attr("class","links").selectAll("path").data(i.links()).enter().append("path").attr("class",e=>"link "+(e.target.data.selected?"selected":"")).attr("fill","none").attr("stroke",e=>e.target.data.selected?"#4a90d9":"#999").attr("stroke-width",e=>e.target.data.selected?3:1.5).attr("d",o.vu().x(e=>e.x).y(e=>e.y));const x=h.append("g").attr("class","nodes").selectAll("g").data(i.descendants()).enter().append("g").attr("class","node").attr("transform",e=>`translate(${e.x}, ${e.y})`).attr("cursor",d?"pointer":"default").on("mouseover",function(e,a){if(!d)return;o.Ltv(this).select("circle").transition().duration(200).attr("r",30);const r=a.parent?a.parent.data.visits:a.data.visits,s=((e,a)=>{if(!a)return 0;const r=e.value,s=e.prior,o=e.visits;return r+m*s*Math.sqrt(a)/(1+o)})(a.data,r);o.Ltv(c.current).style("display","block").style("left",`${e.pageX+15}px`).style("top",e.pageY-10+"px").html(`\n            <strong>${a.data.name}</strong><br>\n            \u8a2a\u554f\u6b21\u6578 (N): ${a.data.visits}<br>\n            \u5e73\u5747\u50f9\u503c (Q): ${a.data.value.toFixed(3)}<br>\n            \u5148\u9a57\u6a5f\u7387 (P): ${(100*a.data.prior).toFixed(1)}%<br>\n            ${t?`PUCT \u5206\u6578: ${s.toFixed(3)}`:""}\n          `)}).on("mouseout",function(){o.Ltv(this).select("circle").transition().duration(200).attr("r",25),o.Ltv(c.current).style("display","none")}).on("click",function(e,a){d&&p(a.data)});x.append("circle").attr("r",25).attr("fill",e=>e.data.selected?"#4a90d9":"#fff").attr("stroke",a=>{if(a.data.selected)return"#2c5282";const r=a.data.visits/e.visits;return o.dM(.3+.5*r)}).attr("stroke-width",e=>e.data.selected?3:2),x.append("text").attr("dy",-5).attr("text-anchor","middle").attr("fill",e=>e.data.selected?"#fff":"#333").attr("font-size",11).attr("font-weight","bold").text(e=>e.data.name),x.append("text").attr("dy",10).attr("text-anchor","middle").attr("fill",e=>e.data.selected?"#fff":"#666").attr("font-size",9).text(e=>`N=${e.data.visits}`),s.append("text").attr("x",a/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("MCTS \u641c\u7d22\u6a39"),t&&s.append("text").attr("x",a/2).attr("y",r-10).attr("text-anchor","middle").attr("font-size",11).attr("fill","#666").text("\u85cd\u8272\u8def\u5f91\uff1aPUCT \u9078\u64c7\u7684\u6700\u4f73\u8def\u5f91")},[e,a,r,t,m,d,g,f]),(0,n.jsxs)("div",{children:[t&&d&&(0,n.jsx)("div",{className:"d3-controls",children:(0,n.jsxs)("div",{className:"d3-slider",children:[(0,n.jsxs)("label",{children:["c_puct: ",m.toFixed(1)]}),(0,n.jsx)("input",{type:"range",min:"0.5",max:"3",step:"0.1",value:m,onChange:e=>x(parseFloat(e.target.value))})]})}),(0,n.jsx)("div",{className:"mcts-tree-container",children:(0,n.jsx)("svg",{ref:l,width:a,height:r,className:"mcts-tree"})}),(0,n.jsx)("div",{ref:c,className:"d3-tooltip",style:{display:"none",position:"fixed"}}),h&&(0,n.jsxs)("div",{className:"d3-legend",style:{background:"#f5f5f5",padding:"1rem",borderRadius:"4px"},children:[(0,n.jsxs)("strong",{children:["\u5df2\u9078\u64c7\u7bc0\u9ede: ",h.name]}),(0,n.jsxs)("div",{children:["\u8a2a\u554f\u6b21\u6578: ",h.visits]}),(0,n.jsxs)("div",{children:["\u5e73\u5747\u50f9\u503c: ",h.value.toFixed(3)]}),(0,n.jsxs)("div",{children:["\u5148\u9a57\u6a5f\u7387: ",(100*h.prior).toFixed(1),"%"]})]}),(0,n.jsxs)("div",{className:"d3-legend",children:[(0,n.jsxs)("div",{className:"d3-legend-item",children:[(0,n.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"\u9078\u4e2d\u8def\u5f91"]}),(0,n.jsxs)("div",{className:"d3-legend-item",children:[(0,n.jsx)("div",{className:"d3-legend-color",style:{background:"#fff",border:"2px solid #999"}}),"\u5176\u4ed6\u7bc0\u9ede"]}),(0,n.jsx)("div",{className:"d3-legend-item",children:(0,n.jsx)("span",{style:{fontSize:"12px"},children:"\u7bc0\u9ede\u5927\u5c0f \u221d \u8a2a\u554f\u6b21\u6578"})})]})]})}function v(e){return(0,n.jsx)(l.A,{fallback:(0,n.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,n.jsx)(j,{...e})})}const g=[{hours:0,elo:0,label:"\u96a8\u6a5f"},{hours:3,elo:1e3,label:"\u767c\u73fe\u898f\u5247"},{hours:6,elo:2e3},{hours:12,elo:3e3,label:"\u767c\u73fe\u5b9a\u5f0f"},{hours:24,elo:4e3},{hours:36,elo:4500,label:"\u8d85\u8d8a Fan Hui"},{hours:48,elo:5e3},{hours:60,elo:5200,label:"\u8d85\u8d8a Lee Sedol"},{hours:72,elo:5400,label:"\u8d85\u8d8a\u539f\u7248 AlphaGo"}],f=[{elo:2700,label:"\u696d\u9918\u5f37\u8c6a"},{elo:3500,label:"Fan Hui (\u8077\u696d\u4e8c\u6bb5)"},{elo:4500,label:"Lee Sedol (\u4e16\u754c\u51a0\u8ecd)"},{elo:5e3,label:"\u539f\u7248 AlphaGo"}],b=[{epochs:0,elo:0},{epochs:10,elo:1500},{epochs:20,elo:2500},{epochs:30,elo:3e3},{epochs:40,elo:3200},{epochs:50,elo:3300}],y=[{games:0,elo:3300},{games:1e3,elo:3800},{games:5e3,elo:4200},{games:1e4,elo:4500},{games:5e4,elo:4800},{games:1e5,elo:5e3}];function k({mode:e="zero",width:a=600,height:r=400,animated:t=!0,showMilestones:i=!0}){const d=(0,s.useRef)(null),[l,c]=(0,s.useState)(e),h=40,p=70,m=a-p-100,x=r-h-60;return(0,s.useEffect)(()=>{if(!d.current)return;const e=o.Ltv(d.current);let r,s,n;e.selectAll("*").remove(),"zero"===l?(r=g,s="\u8a13\u7df4\u6642\u9593\uff08\u5c0f\u6642\uff09",n=[0,80]):"sl"===l?(r=b,s="\u8a13\u7df4\u8f2a\u6578\uff08Epochs\uff09",n=[0,60]):(r=y,s="\u81ea\u6211\u5c0d\u5f08\u5c40\u6578",n=[0,12e4]);const c="selfplay"===l?o.ZEH().domain([1,n[1]]).range([0,m]):o.m4Y().domain(n).range([0,m]),u=o.m4Y().domain([0,6e3]).range([x,0]),j=e.append("g").attr("transform",`translate(${p}, ${h})`);if(j.append("g").attr("class","grid").selectAll(".grid-line-y").data(u.ticks(6)).enter().append("line").attr("class","grid-line-y").attr("x1",0).attr("x2",m).attr("y1",e=>u(e)).attr("y2",e=>u(e)).attr("stroke","#ddd").attr("stroke-dasharray","3,3"),i&&"zero"===l){const e=j.append("g").attr("class","human-levels");f.forEach(a=>{e.append("line").attr("x1",0).attr("x2",m).attr("y1",u(a.elo)).attr("y2",u(a.elo)).attr("stroke","#e74c3c").attr("stroke-dasharray","5,5").attr("opacity",.6),e.append("text").attr("x",m+5).attr("y",u(a.elo)).attr("dy","0.35em").attr("fill","#e74c3c").attr("font-size",10).text(a.label)})}const v=o.n8j().x(e=>c("zero"===l?e.hours:"sl"===l?e.epochs:Math.max(1,e.games))).y(e=>u(e.elo)).curve(o.nVG),k=o.Wcw().x(e=>c("zero"===l?e.hours:"sl"===l?e.epochs:Math.max(1,e.games))).y0(x).y1(e=>u(e.elo)).curve(o.nVG);j.append("path").datum(r).attr("class","area").attr("fill","#4a90d9").attr("opacity",.1).attr("d",k);const P=j.append("path").datum(r).attr("class","line").attr("fill","none").attr("stroke","#4a90d9").attr("stroke-width",3).attr("d",v);if(t){const e=P.node().getTotalLength();P.attr("stroke-dasharray",`${e} ${e}`).attr("stroke-dashoffset",e).transition().duration(2e3).ease(o.yfw).attr("stroke-dashoffset",0)}if(i&&"zero"===l){const e=r.filter(e=>e.label),a=j.append("g").attr("class","milestones");a.selectAll("circle").data(e).enter().append("circle").attr("cx",e=>c(e.hours)).attr("cy",e=>u(e.elo)).attr("r",6).attr("fill","#e74c3c").attr("stroke","#fff").attr("stroke-width",2),a.selectAll("text").data(e).enter().append("text").attr("x",e=>c(e.hours)).attr("y",e=>u(e.elo)-15).attr("text-anchor","middle").attr("fill","#333").attr("font-size",10).text(e=>e.label)}const C="selfplay"===l?o.l78(c).ticks(5,"~s"):o.l78(c);j.append("g").attr("class","x-axis").attr("transform",`translate(0, ${x})`).call(C),j.append("text").attr("class","axis-label").attr("x",m/2).attr("y",x+45).attr("text-anchor","middle").attr("fill","#666").text(s),j.append("g").attr("class","y-axis").call(o.V4s(u).ticks(6)),j.append("text").attr("class","axis-label").attr("transform","rotate(-90)").attr("x",-x/2).attr("y",-50).attr("text-anchor","middle").attr("fill","#666").text("ELO \u8a55\u5206"),e.append("text").attr("x",a/2).attr("y",20).attr("text-anchor","middle").attr("font-size",14).attr("font-weight","bold").attr("fill","#333").text("zero"===l?"AlphaGo Zero \u8a13\u7df4\u66f2\u7dda":"sl"===l?"\u76e3\u7763\u5b78\u7fd2\u68cb\u529b\u6210\u9577":"\u81ea\u6211\u5c0d\u5f08\u68cb\u529b\u6210\u9577")},[l,a,r,t,i,m,x]),(0,n.jsxs)("div",{children:[(0,n.jsxs)("div",{className:"d3-controls",children:[(0,n.jsx)("button",{className:"zero"===l?"active":"",onClick:()=>c("zero"),children:"AlphaGo Zero"}),(0,n.jsx)("button",{className:"sl"===l?"active":"",onClick:()=>c("sl"),children:"\u76e3\u7763\u5b78\u7fd2"}),(0,n.jsx)("button",{className:"selfplay"===l?"active":"",onClick:()=>c("selfplay"),children:"\u81ea\u6211\u5c0d\u5f08"})]}),(0,n.jsx)("div",{className:"elo-chart-container",children:(0,n.jsx)("svg",{ref:d,width:a,height:r,className:"elo-chart"})}),(0,n.jsxs)("div",{className:"d3-legend",children:[(0,n.jsxs)("div",{className:"d3-legend-item",children:[(0,n.jsx)("div",{className:"d3-legend-color",style:{background:"#4a90d9"}}),"ELO \u8a55\u5206"]}),i&&"zero"===l&&(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)("div",{className:"d3-legend-item",children:[(0,n.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c"}}),"\u91cc\u7a0b\u7891"]}),(0,n.jsxs)("div",{className:"d3-legend-item",children:[(0,n.jsx)("div",{className:"d3-legend-color",style:{background:"#e74c3c",opacity:.3}}),"\u4eba\u985e\u6c34\u5e73\u53c3\u8003\u7dda"]})]})]})]})}function P(e){return(0,n.jsx)(l.A,{fallback:(0,n.jsx)("div",{children:"\u8f09\u5165\u4e2d..."}),children:()=>(0,n.jsx)(k,{...e})})}},30416(e,a,r){r.d(a,{R:()=>t,x:()=>i});var s=r(59471);const o={},n=s.createContext(o);function t(e){const a=s.useContext(n);return s.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function i(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),s.createElement(n.Provider,{value:a},e.children)}}}]);