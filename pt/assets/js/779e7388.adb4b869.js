"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[5233],{19402(e,a,o){o.r(a),o.d(a,{assets:()=>l,contentTitle:()=>n,default:()=>h,frontMatter:()=>d,metadata:()=>s,toc:()=>t});const s=JSON.parse('{"id":"alphago/explained/alphago-zero","title":"Vis\xe3o Geral do AlphaGo Zero","description":"Come\xe7ando do zero, completamente autodidata, como o AlphaGo Zero superou todas as vers\xf5es anteriores sem usar qualquer registro de partidas humanas","source":"@site/i18n/pt/docusaurus-plugin-content-docs/current/alphago/explained/16-alphago-zero.mdx","sourceDirName":"alphago/explained","slug":"/alphago/explained/alphago-zero","permalink":"/pt/docs/alphago/explained/alphago-zero","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/explained/16-alphago-zero.mdx","tags":[],"version":"current","sidebarPosition":17,"frontMatter":{"sidebar_position":17,"title":"Vis\xe3o Geral do AlphaGo Zero","description":"Come\xe7ando do zero, completamente autodidata, como o AlphaGo Zero superou todas as vers\xf5es anteriores sem usar qualquer registro de partidas humanas","keywords":["AlphaGo Zero","auto-jogo","aprendizado por refor\xe7o","aprendizado profundo","IA de Go","aprendizado n\xe3o supervisionado"]},"sidebar":"tutorialSidebar","previous":{"title":"F\xf3rmula PUCT em Detalhes","permalink":"/pt/docs/alphago/explained/puct-formula"},"next":{"title":"Rede de Cabe\xe7a Dupla e Rede Residual","permalink":"/pt/docs/alphago/explained/dual-head-resnet"}}');var r=o(62615),i=o(30416);const d={sidebar_position:17,title:"Vis\xe3o Geral do AlphaGo Zero",description:"Come\xe7ando do zero, completamente autodidata, como o AlphaGo Zero superou todas as vers\xf5es anteriores sem usar qualquer registro de partidas humanas",keywords:["AlphaGo Zero","auto-jogo","aprendizado por refor\xe7o","aprendizado profundo","IA de Go","aprendizado n\xe3o supervisionado"]},n="Vis\xe3o Geral do AlphaGo Zero",l={},t=[{value:"Por que N\xe3o Precisa de Registros de Partidas Humanas?",id:"por-que-n\xe3o-precisa-de-registros-de-partidas-humanas",level:2},{value:"Limita\xe7\xf5es dos Registros de Partidas Humanas",id:"limita\xe7\xf5es-dos-registros-de-partidas-humanas",level:3},{value:"1. Os registros humanos t\xeam um teto",id:"1-os-registros-humanos-t\xeam-um-teto",level:4},{value:"2. O gargalo do aprendizado supervisionado",id:"2-o-gargalo-do-aprendizado-supervisionado",level:4},{value:"3. Custo de coleta de dados",id:"3-custo-de-coleta-de-dados",level:4},{value:"O Avan\xe7o do Zero",id:"o-avan\xe7o-do-zero",level:3},{value:"Compara\xe7\xe3o com o AlphaGo Original: 100:0",id:"compara\xe7\xe3o-com-o-alphago-original-1000",level:2},{value:"Vit\xf3ria Esmagadora",id:"vit\xf3ria-esmagadora",level:3},{value:"Menos Recursos, Mais For\xe7a",id:"menos-recursos-mais-for\xe7a",level:3},{value:"Por que o Zero \xe9 Mais Forte?",id:"por-que-o-zero-\xe9-mais-forte",level:3},{value:"1. Aprendizado sem vi\xe9s",id:"1-aprendizado-sem-vi\xe9s",level:4},{value:"2. Objetivo de aprendizado consistente",id:"2-objetivo-de-aprendizado-consistente",level:4},{value:"3. Arquitetura mais simples",id:"3-arquitetura-mais-simples",level:4},{value:"Caracter\xedsticas de Entrada Simplificadas: De 48 para 17",id:"caracter\xedsticas-de-entrada-simplificadas-de-48-para-17",level:2},{value:"48 Planos de Caracter\xedsticas do AlphaGo Original",id:"48-planos-de-caracter\xedsticas-do-alphago-original",level:3},{value:"17 Planos de Caracter\xedsticas do AlphaGo Zero",id:"17-planos-de-caracter\xedsticas-do-alphago-zero",level:3},{value:"Por que a Simplifica\xe7\xe3o \xe9 Boa?",id:"por-que-a-simplifica\xe7\xe3o-\xe9-boa",level:3},{value:"1. Deixar a rede descobrir caracter\xedsticas",id:"1-deixar-a-rede-descobrir-caracter\xedsticas",level:4},{value:"2. Melhor generalizabilidade",id:"2-melhor-generalizabilidade",level:4},{value:"3. Redu\xe7\xe3o de erros humanos",id:"3-redu\xe7\xe3o-de-erros-humanos",level:4},{value:"Arquitetura de Rede \xdanica",id:"arquitetura-de-rede-\xfanica",level:2},{value:"Design de Rede Dupla da Vers\xe3o Original",id:"design-de-rede-dupla-da-vers\xe3o-original",level:3},{value:"Rede de Cabe\xe7a Dupla do Zero",id:"rede-de-cabe\xe7a-dupla-do-zero",level:3},{value:"1. Efici\xeancia de par\xe2metros",id:"1-efici\xeancia-de-par\xe2metros",level:4},{value:"2. Compartilhamento de caracter\xedsticas",id:"2-compartilhamento-de-caracter\xedsticas",level:4},{value:"3. Estabilidade de treinamento",id:"3-estabilidade-de-treinamento",level:4},{value:"O Poder da Rede Residual",id:"o-poder-da-rede-residual",level:3},{value:"Melhoria na Efici\xeancia de Treinamento",id:"melhoria-na-efici\xeancia-de-treinamento",level:2},{value:"Crescimento Exponencial do Auto-jogo",id:"crescimento-exponencial-do-auto-jogo",level:3},{value:"Por que T\xe3o R\xe1pido?",id:"por-que-t\xe3o-r\xe1pido",level:3},{value:"1. Guia de busca mais forte",id:"1-guia-de-busca-mais-forte",level:4},{value:"2. Auto-jogo mais r\xe1pido",id:"2-auto-jogo-mais-r\xe1pido",level:4},{value:"3. Aprendizado mais eficaz",id:"3-aprendizado-mais-eficaz",level:4},{value:"Compara\xe7\xe3o com Aprendizado Humano",id:"compara\xe7\xe3o-com-aprendizado-humano",level:3},{value:"Generalidade: Xadrez, Shogi",id:"generalidade-xadrez-shogi",level:2},{value:"O Nascimento do AlphaZero",id:"o-nascimento-do-alphazero",level:3},{value:"O Significado da Generalidade",id:"o-significado-da-generalidade",level:3},{value:"Impacto na IA Tradicional",id:"impacto-na-ia-tradicional",level:3},{value:"Estilo de Jogo do AlphaGo Zero",id:"estilo-de-jogo-do-alphago-zero",level:2},{value:"Est\xe9tica Al\xe9m dos Humanos",id:"est\xe9tica-al\xe9m-dos-humanos",level:3},{value:"Redescoberta da Teoria de Go Humana",id:"redescoberta-da-teoria-de-go-humana",level:3},{value:"Inova\xe7\xe3o Al\xe9m dos Humanos",id:"inova\xe7\xe3o-al\xe9m-dos-humanos",level:3},{value:"Resumo dos Detalhes T\xe9cnicos",id:"resumo-dos-detalhes-t\xe9cnicos",level:2},{value:"Compara\xe7\xe3o Completa com o AlphaGo Original",id:"compara\xe7\xe3o-completa-com-o-alphago-original",level:3},{value:"Algoritmo Central",id:"algoritmo-central",level:3},{value:"Li\xe7\xf5es para Pesquisa em IA",id:"li\xe7\xf5es-para-pesquisa-em-ia",level:2},{value:"Aprendizado de Primeiros Princ\xedpios",id:"aprendizado-de-primeiros-princ\xedpios",level:3},{value:"O Poder do Auto-jogo",id:"o-poder-do-auto-jogo",level:3},{value:"A Import\xe2ncia da Simplifica\xe7\xe3o",id:"a-import\xe2ncia-da-simplifica\xe7\xe3o",level:3},{value:"Correspond\xeancia de Anima\xe7\xf5es",id:"correspond\xeancia-de-anima\xe7\xf5es",level:2},{value:"Leitura Adicional",id:"leitura-adicional",level:2},{value:"Refer\xeancias",id:"refer\xeancias",level:2}];function c(e){const a={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"vis\xe3o-geral-do-alphago-zero",children:"Vis\xe3o Geral do AlphaGo Zero"})}),"\n",(0,r.jsxs)(a.p,{children:["Em outubro de 2017, a DeepMind publicou um resultado que chocou o mundo da IA: ",(0,r.jsx)(a.strong,{children:"AlphaGo Zero"}),", sem usar qualquer registro de partidas humanas, come\xe7ando a treinar de um estado completamente aleat\xf3rio, superou o AlphaGo original que derrotou Lee Sedol em apenas tr\xeas dias, e venceu por ",(0,r.jsx)(a.strong,{children:"100:0"}),"."]}),"\n",(0,r.jsxs)(a.p,{children:["Isso n\xe3o \xe9 apenas um progresso num\xe9rico. Representa um novo paradigma: ",(0,r.jsx)(a.strong,{children:"A IA n\xe3o precisa de conhecimento humano, pode descobrir tudo do zero"}),"."]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"por-que-n\xe3o-precisa-de-registros-de-partidas-humanas",children:"Por que N\xe3o Precisa de Registros de Partidas Humanas?"}),"\n",(0,r.jsx)(a.h3,{id:"limita\xe7\xf5es-dos-registros-de-partidas-humanas",children:"Limita\xe7\xf5es dos Registros de Partidas Humanas"}),"\n",(0,r.jsx)(a.p,{children:"O processo de treinamento do AlphaGo original era dividido em duas etapas:"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Aprendizado supervisionado"}),": Treinar a Policy Network com 30 milh\xf5es de partidas humanas"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Aprendizado por refor\xe7o"}),": Melhorar ainda mais atrav\xe9s de auto-jogo"]}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Este m\xe9todo tem v\xe1rios problemas fundamentais:"}),"\n",(0,r.jsx)(a.h4,{id:"1-os-registros-humanos-t\xeam-um-teto",children:"1. Os registros humanos t\xeam um teto"}),"\n",(0,r.jsx)(a.p,{children:"A habilidade dos jogadores humanos tem limites, os registros cont\xeam a compreens\xe3o humana, mas tamb\xe9m incluem erros e vieses humanos. Quando a IA aprende com registros humanos, ela aprende:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Jogadas que os humanos consideram boas (mas n\xe3o necessariamente \xf3timas)"}),"\n",(0,r.jsx)(a.li,{children:"Padr\xf5es de pensamento humano (que podem limitar a inova\xe7\xe3o)"}),"\n",(0,r.jsx)(a.li,{children:"Erros humanos (que s\xe3o aprendidos como exemplos corretos)"}),"\n"]}),"\n",(0,r.jsx)(a.h4,{id:"2-o-gargalo-do-aprendizado-supervisionado",children:"2. O gargalo do aprendizado supervisionado"}),"\n",(0,r.jsx)(a.p,{children:'O objetivo do aprendizado supervisionado \xe9 "imitar humanos" \u2014 prever qual jogada um jogador humano faria. Isso significa que o limite de capacidade da IA \xe9 limitado pela habilidade dos jogadores humanos.'}),"\n",(0,r.jsx)(a.p,{children:"\xc9 como um aprendiz que s\xf3 pode imitar o mestre, nunca podendo superar o mestre."}),"\n",(0,r.jsx)(a.h4,{id:"3-custo-de-coleta-de-dados",children:"3. Custo de coleta de dados"}),"\n",(0,r.jsx)(a.p,{children:'Registros de partidas humanas de alta qualidade levam muitos anos para acumular, e s\xf3 existem para jogos com longa hist\xf3ria como o Go. Se quisermos aplicar IA a novos campos (como previs\xe3o de estrutura de prote\xednas), simplesmente n\xe3o existem "registros de especialistas humanos".'}),"\n",(0,r.jsx)(a.h3,{id:"o-avan\xe7o-do-zero",children:"O Avan\xe7o do Zero"}),"\n",(0,r.jsxs)(a.p,{children:["O AlphaGo Zero pula completamente a etapa de aprendizado supervisionado, come\xe7ando diretamente o auto-jogo a partir de ",(0,r.jsx)(a.strong,{children:"inicializa\xe7\xe3o aleat\xf3ria"}),". Isso resolve todos os problemas acima:"]}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Problema"}),(0,r.jsx)(a.th,{children:"AlphaGo Original"}),(0,r.jsx)(a.th,{children:"AlphaGo Zero"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Limite do conhecimento humano"}),(0,r.jsx)(a.td,{children:"Limitado pela qualidade dos registros"}),(0,r.jsx)(a.td,{children:"Sem esta limita\xe7\xe3o"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Objetivo de aprendizado"}),(0,r.jsx)(a.td,{children:"Imitar humanos"}),(0,r.jsx)(a.td,{children:"Maximizar taxa de vit\xf3ria"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Requisitos de dados"}),(0,r.jsx)(a.td,{children:"30 milh\xf5es de partidas"}),(0,r.jsx)(a.td,{children:"0"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Generalizabilidade"}),(0,r.jsx)(a.td,{children:"Apenas Go"}),(0,r.jsx)(a.td,{children:"Pode ser generalizado para outros campos"})]})]})]}),"\n",(0,r.jsx)(a.p,{children:'Esta \xe9 uma mudan\xe7a de paradigma fundamental: de "aprender conhecimento humano" para "descobrir conhecimento a partir de primeiros princ\xedpios".'}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"compara\xe7\xe3o-com-o-alphago-original-1000",children:"Compara\xe7\xe3o com o AlphaGo Original: 100:0"}),"\n",(0,r.jsx)(a.h3,{id:"vit\xf3ria-esmagadora",children:"Vit\xf3ria Esmagadora"}),"\n",(0,r.jsx)(a.p,{children:"A DeepMind fez o AlphaGo Zero treinado jogar contra v\xe1rias vers\xf5es do AlphaGo:"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Oponente"}),(0,r.jsx)(a.th,{children:"Resultado do AlphaGo Zero"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"AlphaGo Fan (vers\xe3o que derrotou Fan Hui)"}),(0,r.jsx)(a.td,{children:"100:0"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"AlphaGo Lee (vers\xe3o que derrotou Lee Sedol)"}),(0,r.jsx)(a.td,{children:"100:0"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"AlphaGo Master (vers\xe3o das 60 vit\xf3rias consecutivas)"}),(0,r.jsx)(a.td,{children:"89:11"})]})]})]}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"100:0"})," \u2014 isso significa que em 100 partidas, o AlphaGo original n\xe3o conseguiu vencer sequer uma."]}),"\n",(0,r.jsx)(a.h3,{id:"menos-recursos-mais-for\xe7a",children:"Menos Recursos, Mais For\xe7a"}),"\n",(0,r.jsx)(a.p,{children:"N\xe3o apenas venceu, o AlphaGo Zero tamb\xe9m alcan\xe7ou maior for\xe7a de jogo com menos recursos:"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"M\xe9trica"}),(0,r.jsx)(a.th,{children:"AlphaGo Lee"}),(0,r.jsx)(a.th,{children:"AlphaGo Zero"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Tempo de treinamento"}),(0,r.jsx)(a.td,{children:"V\xe1rios meses"}),(0,r.jsx)(a.td,{children:"40 dias (3 dias para superar AlphaGo Lee)"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Partidas de treinamento"}),(0,r.jsx)(a.td,{children:"30 milh\xf5es de partidas humanas + auto-jogo"}),(0,r.jsx)(a.td,{children:"4,9 milh\xf5es de partidas de auto-jogo"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"TPUs (treinamento)"}),(0,r.jsx)(a.td,{children:"50+"}),(0,r.jsx)(a.td,{children:"4"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"TPUs (infer\xeancia)"}),(0,r.jsx)(a.td,{children:"48"}),(0,r.jsx)(a.td,{children:"4"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Caracter\xedsticas de entrada"}),(0,r.jsx)(a.td,{children:"48 planos"}),(0,r.jsx)(a.td,{children:"17 planos"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Rede neural"}),(0,r.jsx)(a.td,{children:"Redes duplas SL + RL"}),(0,r.jsx)(a.td,{children:"\xdanica rede de cabe\xe7a dupla"})]})]})]}),"\n",(0,r.jsxs)(a.p,{children:["Esta \xe9 uma melhoria de efici\xeancia impressionante: ",(0,r.jsx)(a.strong,{children:"recursos reduzidos em mais de 10 vezes, mas a for\xe7a de jogo aumentou significativamente"}),"."]}),"\n",(0,r.jsx)(a.h3,{id:"por-que-o-zero-\xe9-mais-forte",children:"Por que o Zero \xe9 Mais Forte?"}),"\n",(0,r.jsx)(a.p,{children:"As raz\xf5es pelas quais o AlphaGo Zero \xe9 mais forte podem ser entendidas de v\xe1rios \xe2ngulos:"}),"\n",(0,r.jsx)(a.h4,{id:"1-aprendizado-sem-vi\xe9s",children:"1. Aprendizado sem vi\xe9s"}),"\n",(0,r.jsx)(a.p,{children:"O AlphaGo original aprendeu com registros humanos, herdando vieses humanos. Por exemplo, jogadores humanos podem supervalorizar certos josekis, ou ter avalia\xe7\xf5es incorretas de certas posi\xe7\xf5es."}),"\n",(0,r.jsx)(a.p,{children:"O AlphaGo Zero n\xe3o tem essa bagagem. Ele come\xe7a de uma tela em branco, aprendendo o que \xe9 uma boa jogada apenas atrav\xe9s dos resultados de vit\xf3ria/derrota. Isso permite descobrir jogadas que os humanos nunca imaginaram."}),"\n",(0,r.jsx)(a.h4,{id:"2-objetivo-de-aprendizado-consistente",children:"2. Objetivo de aprendizado consistente"}),"\n",(0,r.jsx)(a.p,{children:"O treinamento do AlphaGo original tinha dois objetivos diferentes:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Aprendizado supervisionado: Maximizar a precis\xe3o de previs\xe3o das jogadas humanas"}),"\n",(0,r.jsx)(a.li,{children:"Aprendizado por refor\xe7o: Maximizar a taxa de vit\xf3ria"}),"\n"]}),"\n",(0,r.jsxs)(a.p,{children:["Estes dois objetivos podem entrar em conflito. O AlphaGo Zero tem apenas um objetivo: ",(0,r.jsx)(a.strong,{children:"maximiza\xe7\xe3o da taxa de vit\xf3ria"}),". Isso torna o processo de aprendizado mais consistente e eficaz."]}),"\n",(0,r.jsx)(a.h4,{id:"3-arquitetura-mais-simples",children:"3. Arquitetura mais simples"}),"\n",(0,r.jsx)(a.p,{children:"O AlphaGo original usava Policy Network e Value Network separadas. O AlphaGo Zero usa uma \xfanica rede de cabe\xe7a dupla (veja o pr\xf3ximo artigo), permitindo que a representa\xe7\xe3o de caracter\xedsticas seja compartilhada, aumentando a efici\xeancia de aprendizado."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"caracter\xedsticas-de-entrada-simplificadas-de-48-para-17",children:"Caracter\xedsticas de Entrada Simplificadas: De 48 para 17"}),"\n",(0,r.jsx)(a.h3,{id:"48-planos-de-caracter\xedsticas-do-alphago-original",children:"48 Planos de Caracter\xedsticas do AlphaGo Original"}),"\n",(0,r.jsx)(a.p,{children:"A entrada da rede neural do AlphaGo original inclu\xeda 48 planos de caracter\xedsticas 19x19, codificando muitas caracter\xedsticas projetadas por humanos:"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Categoria"}),(0,r.jsx)(a.th,{children:"N\xfamero de caracter\xedsticas"}),(0,r.jsx)(a.th,{children:"Conte\xfado"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Posi\xe7\xf5es das pedras"}),(0,r.jsx)(a.td,{children:"3"}),(0,r.jsx)(a.td,{children:"Pedras pretas, pedras brancas, pontos vazios"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Liberdades"}),(0,r.jsx)(a.td,{children:"8"}),(0,r.jsx)(a.td,{children:"Grupos com 1-8 liberdades"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Capturas"}),(0,r.jsx)(a.td,{children:"8"}),(0,r.jsx)(a.td,{children:"Pode capturar 1-8 pedras"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Ko"}),(0,r.jsx)(a.td,{children:"1"}),(0,r.jsx)(a.td,{children:"Posi\xe7\xe3o do ko"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Dist\xe2ncia da borda"}),(0,r.jsx)(a.td,{children:"4"}),(0,r.jsx)(a.td,{children:"Primeira a quarta linha"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Legalidade de jogada"}),(0,r.jsx)(a.td,{children:"1"}),(0,r.jsx)(a.td,{children:"Quais posi\xe7\xf5es podem ser jogadas"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Estado hist\xf3rico"}),(0,r.jsx)(a.td,{children:"8"}),(0,r.jsx)(a.td,{children:"Posi\xe7\xf5es das \xfaltimas 8 jogadas"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Turno"}),(0,r.jsx)(a.td,{children:"1"}),(0,r.jsx)(a.td,{children:"Pretas ou brancas"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Outros"}),(0,r.jsx)(a.td,{children:"14"}),(0,r.jsx)(a.td,{children:"Escada, olhos, etc."})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"Estas 48 caracter\xedsticas foram cuidadosamente projetadas por especialistas de Go, contendo muito conhecimento do dom\xednio."}),"\n",(0,r.jsx)(a.h3,{id:"17-planos-de-caracter\xedsticas-do-alphago-zero",children:"17 Planos de Caracter\xedsticas do AlphaGo Zero"}),"\n",(0,r.jsx)(a.p,{children:"O AlphaGo Zero simplificou drasticamente a entrada, usando apenas 17 planos de caracter\xedsticas:"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"N\xfamero do plano"}),(0,r.jsx)(a.th,{children:"Conte\xfado"}),(0,r.jsx)(a.th,{children:"Quantidade"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"1-8"}),(0,r.jsx)(a.td,{children:"Posi\xe7\xf5es das pedras pretas (\xfaltimas 8 jogadas)"}),(0,r.jsx)(a.td,{children:"8"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"9-16"}),(0,r.jsx)(a.td,{children:"Posi\xe7\xf5es das pedras brancas (\xfaltimas 8 jogadas)"}),(0,r.jsx)(a.td,{children:"8"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"17"}),(0,r.jsx)(a.td,{children:"Turno atual (todo 1 ou todo 0)"}),(0,r.jsx)(a.td,{children:"1"})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"Estes 17 planos cont\xeam apenas:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Estado atual do tabuleiro"}),": Cada posi\xe7\xe3o tem pedra preta, pedra branca ou vazia"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Informa\xe7\xe3o hist\xf3rica"}),": Estado do tabuleiro das \xfaltimas 8 jogadas"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Informa\xe7\xe3o de turno"}),": De quem \xe9 a vez de jogar"]}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:'Sem liberdades, sem julgamento de escada, sem dist\xe2ncia da borda \u2014 todo esse "conhecimento de Go" \xe9 deixado para a rede neural aprender sozinha.'}),"\n",(0,r.jsx)(a.h3,{id:"por-que-a-simplifica\xe7\xe3o-\xe9-boa",children:"Por que a Simplifica\xe7\xe3o \xe9 Boa?"}),"\n",(0,r.jsx)(a.h4,{id:"1-deixar-a-rede-descobrir-caracter\xedsticas",children:"1. Deixar a rede descobrir caracter\xedsticas"}),"\n",(0,r.jsx)(a.p,{children:"Caracter\xedsticas manuais complexas podem perder informa\xe7\xf5es importantes, ou codificar suposi\xe7\xf5es incorretas. Deixar a rede neural aprender a partir de dados brutos pode levar a descobrir melhores representa\xe7\xf5es de caracter\xedsticas."}),"\n",(0,r.jsx)(a.p,{children:"De fato, o AlphaGo Zero aprendeu todas as caracter\xedsticas que os humanos projetaram (liberdades, escadas, etc.), e tamb\xe9m aprendeu alguns padr\xf5es que os humanos n\xe3o tinham consci\xeancia expl\xedcita."}),"\n",(0,r.jsx)(a.h4,{id:"2-melhor-generalizabilidade",children:"2. Melhor generalizabilidade"}),"\n",(0,r.jsx)(a.p,{children:"Muitas das 48 caracter\xedsticas s\xe3o espec\xedficas do Go (como escadas, dist\xe2ncia da borda). Os 17 planos simplificados s\xe3o gen\xe9ricos \u2014 qualquer jogo de tabuleiro pode ser codificado de forma similar."}),"\n",(0,r.jsxs)(a.p,{children:["Isso estabeleceu as bases para o posterior ",(0,r.jsx)(a.strong,{children:"AlphaZero"})," (IA de jogos gen\xe9rica)."]}),"\n",(0,r.jsx)(a.h4,{id:"3-redu\xe7\xe3o-de-erros-humanos",children:"3. Redu\xe7\xe3o de erros humanos"}),"\n",(0,r.jsx)(a.p,{children:"Caracter\xedsticas projetadas manualmente podem conter defini\xe7\xf5es incorretas ou incompletas. Simplificar a entrada elimina a possibilidade desses problemas."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"arquitetura-de-rede-\xfanica",children:"Arquitetura de Rede \xdanica"}),"\n",(0,r.jsx)(a.h3,{id:"design-de-rede-dupla-da-vers\xe3o-original",children:"Design de Rede Dupla da Vers\xe3o Original"}),"\n",(0,r.jsx)(a.p,{children:"O AlphaGo original usava duas redes neurais independentes:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Policy Network:  Entrada \u2192 CNN \u2192 Probabilidades de jogada 19x19\nValue Network:   Entrada \u2192 CNN \u2192 Estimativa de taxa de vit\xf3ria (-1 a 1)\n"})}),"\n",(0,r.jsx)(a.p,{children:"Estas duas redes:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Tinham arquiteturas diferentes (n\xfamero de camadas e canais ligeiramente diferentes)"}),"\n",(0,r.jsx)(a.li,{children:"Eram treinadas independentemente (primeiro Policy, depois Value)"}),"\n",(0,r.jsx)(a.li,{children:"N\xe3o compartilhavam nenhum par\xe2metro"}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"rede-de-cabe\xe7a-dupla-do-zero",children:"Rede de Cabe\xe7a Dupla do Zero"}),"\n",(0,r.jsx)(a.p,{children:"O AlphaGo Zero usa uma \xfanica rede, mas com duas cabe\xe7as de sa\xedda (heads):"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Entrada \u2192 Backbone ResNet compartilhado \u2192 Policy Head \u2192 Probabilidades de jogada 19x19\n                                       \u2192 Value Head  \u2192 Estimativa de taxa de vit\xf3ria\n"})}),"\n",(0,r.jsxs)(a.p,{children:["As duas Heads compartilham o mesmo backbone ResNet (veja o pr\xf3ximo artigo: ",(0,r.jsx)(a.a,{href:"../dual-head-resnet",children:"Rede de Cabe\xe7a Dupla e Rede Residual"}),"), o que traz v\xe1rias vantagens:"]}),"\n",(0,r.jsx)(a.h4,{id:"1-efici\xeancia-de-par\xe2metros",children:"1. Efici\xeancia de par\xe2metros"}),"\n",(0,r.jsx)(a.p,{children:"Backbone compartilhado significa que a maioria dos par\xe2metros \xe9 usada por ambas as tarefas. Isso reduz o n\xfamero total de par\xe2metros e diminui o risco de overfitting."}),"\n",(0,r.jsx)(a.h4,{id:"2-compartilhamento-de-caracter\xedsticas",children:"2. Compartilhamento de caracter\xedsticas"}),"\n",(0,r.jsx)(a.p,{children:'"Onde devo jogar" (Policy) e "Quem vai ganhar" (Value) precisam entender padr\xf5es de tabuleiro similares. O backbone compartilhado permite que essas caracter\xedsticas sejam aprendidas e utilizadas por ambas as tarefas simultaneamente.'}),"\n",(0,r.jsx)(a.h4,{id:"3-estabilidade-de-treinamento",children:"3. Estabilidade de treinamento"}),"\n",(0,r.jsx)(a.p,{children:"O treinamento conjunto faz com que os sinais de gradiente venham de duas fontes, fornecendo sinais de supervis\xe3o mais ricos, tornando o treinamento mais est\xe1vel."}),"\n",(0,r.jsx)(a.h3,{id:"o-poder-da-rede-residual",children:"O Poder da Rede Residual"}),"\n",(0,r.jsxs)(a.p,{children:["O backbone do AlphaGo Zero usa uma ",(0,r.jsx)(a.strong,{children:"rede residual (ResNet) de 40 camadas"}),", muito mais profunda que a CNN de 13 camadas do AlphaGo original."]}),"\n",(0,r.jsx)(a.p,{children:"As conex\xf5es residuais (skip connections) permitem que redes profundas sejam treinadas efetivamente, evitando o problema do gradiente desvanecente. Esta foi a tecnologia inovadora da competi\xe7\xe3o ImageNet de 2015, aplicada com sucesso pelo AlphaGo Zero ao campo do Go."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"melhoria-na-efici\xeancia-de-treinamento",children:"Melhoria na Efici\xeancia de Treinamento"}),"\n",(0,r.jsx)(a.h3,{id:"crescimento-exponencial-do-auto-jogo",children:"Crescimento Exponencial do Auto-jogo"}),"\n",(0,r.jsx)(a.p,{children:"O processo de treinamento do AlphaGo Zero demonstra uma efici\xeancia impressionante:"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Tempo de Treinamento"}),(0,r.jsx)(a.th,{children:"Classifica\xe7\xe3o ELO"}),(0,r.jsx)(a.th,{children:"Equivalente a"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"0 horas"}),(0,r.jsx)(a.td,{children:"0"}),(0,r.jsx)(a.td,{children:"Jogando aleatoriamente"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"3 horas"}),(0,r.jsx)(a.td,{children:"~1000"}),(0,r.jsx)(a.td,{children:"Descobriu regras b\xe1sicas"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"12 horas"}),(0,r.jsx)(a.td,{children:"~3000"}),(0,r.jsx)(a.td,{children:"Descobriu josekis"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"36 horas"}),(0,r.jsx)(a.td,{children:"~4500"}),(0,r.jsx)(a.td,{children:"Superou vers\xe3o Fan Hui"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"60 horas"}),(0,r.jsx)(a.td,{children:"~5200"}),(0,r.jsx)(a.td,{children:"Superou vers\xe3o Lee Sedol"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"72 horas"}),(0,r.jsx)(a.td,{children:"~5400"}),(0,r.jsx)(a.td,{children:"Superou AlphaGo original"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"40 dias"}),(0,r.jsx)(a.td,{children:"~5600"}),(0,r.jsx)(a.td,{children:"Vers\xe3o mais forte"})]})]})]}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"Tr\xeas dias para superar humanos, tr\xeas dias para superar IA que levou meses para treinar"})," \u2014 isso \xe9 uma melhoria de efici\xeancia exponencial."]}),"\n",(0,r.jsx)(a.h3,{id:"por-que-t\xe3o-r\xe1pido",children:"Por que T\xe3o R\xe1pido?"}),"\n",(0,r.jsx)(a.h4,{id:"1-guia-de-busca-mais-forte",children:"1. Guia de busca mais forte"}),"\n",(0,r.jsx)(a.p,{children:"O MCTS do AlphaGo Zero \xe9 completamente guiado pela rede neural, n\xe3o usa mais a pol\xedtica de jogada r\xe1pida (rollout). Isso torna a busca mais eficiente e precisa."}),"\n",(0,r.jsx)(a.h4,{id:"2-auto-jogo-mais-r\xe1pido",children:"2. Auto-jogo mais r\xe1pido"}),"\n",(0,r.jsx)(a.p,{children:"Como precisa de apenas uma rede (em vez de duas), o custo computacional de cada partida de auto-jogo \xe9 reduzido. Isso significa que mais dados de treinamento podem ser gerados no mesmo tempo."}),"\n",(0,r.jsx)(a.h4,{id:"3-aprendizado-mais-eficaz",children:"3. Aprendizado mais eficaz"}),"\n",(0,r.jsx)(a.p,{children:"O treinamento conjunto da rede de cabe\xe7a dupla faz com que a informa\xe7\xe3o de cada partida seja utilizada de forma mais eficiente. Os gradientes de Policy e Value se refor\xe7am mutuamente, acelerando a converg\xeancia."}),"\n",(0,r.jsx)(a.h3,{id:"compara\xe7\xe3o-com-aprendizado-humano",children:"Compara\xe7\xe3o com Aprendizado Humano"}),"\n",(0,r.jsx)(a.p,{children:"Quanto tempo jogadores humanos precisam para alcan\xe7ar diferentes n\xedveis?"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"N\xedvel"}),(0,r.jsx)(a.th,{children:"Tempo necess\xe1rio humano"}),(0,r.jsx)(a.th,{children:"AlphaGo Zero"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Iniciante"}),(0,r.jsx)(a.td,{children:"V\xe1rias semanas"}),(0,r.jsx)(a.td,{children:"Alguns minutos"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Amateur 1 dan"}),(0,r.jsx)(a.td,{children:"V\xe1rios anos"}),(0,r.jsx)(a.td,{children:"Algumas horas"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"N\xedvel profissional"}),(0,r.jsx)(a.td,{children:"10-20 anos"}),(0,r.jsx)(a.td,{children:"1-2 dias"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Campe\xe3o mundial"}),(0,r.jsx)(a.td,{children:"20+ anos de dedica\xe7\xe3o em tempo integral"}),(0,r.jsx)(a.td,{children:"3 dias"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Superar humanos"}),(0,r.jsx)(a.td,{children:"Imposs\xedvel"}),(0,r.jsx)(a.td,{children:"3 dias"})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"Esta compara\xe7\xe3o n\xe3o \xe9 para diminuir jogadores humanos \u2014 eles usam neur\xf4nios biol\xf3gicos, enquanto o AlphaGo Zero usa TPUs especialmente projetados e v\xe1rios quilowatts de eletricidade. Mas isso realmente demonstra qu\xe3o eficiente o m\xe9todo de aprendizado correto pode ser."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"generalidade-xadrez-shogi",children:"Generalidade: Xadrez, Shogi"}),"\n",(0,r.jsx)(a.h3,{id:"o-nascimento-do-alphazero",children:"O Nascimento do AlphaZero"}),"\n",(0,r.jsxs)(a.p,{children:["Em dezembro de 2017, a DeepMind publicou o ",(0,r.jsx)(a.strong,{children:"AlphaZero"})," \u2014 a vers\xe3o gen\xe9rica do AlphaGo Zero. O mesmo algoritmo, apenas modificando as regras do jogo, alcan\xe7ou n\xedvel mundial em tr\xeas jogos de tabuleiro:"]}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Jogo"}),(0,r.jsx)(a.th,{children:"Tempo de Treinamento"}),(0,r.jsx)(a.th,{children:"Oponente"}),(0,r.jsx)(a.th,{children:"Resultado"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Go"}),(0,r.jsx)(a.td,{children:"8 horas"}),(0,r.jsx)(a.td,{children:"AlphaGo Zero"}),(0,r.jsx)(a.td,{children:"60:40"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Xadrez"}),(0,r.jsx)(a.td,{children:"4 horas"}),(0,r.jsx)(a.td,{children:"Stockfish 8"}),(0,r.jsx)(a.td,{children:"28 vit\xf3rias 72 empates 0 derrotas"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Shogi"}),(0,r.jsx)(a.td,{children:"2 horas"}),(0,r.jsx)(a.td,{children:"Elmo"}),(0,r.jsx)(a.td,{children:"90:8:2"})]})]})]}),"\n",(0,r.jsx)(a.p,{children:"Note os oponentes:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Stockfish"})," era a engine de xadrez mais forte na \xe9poca, usando d\xe9cadas de conhecimento humano e otimiza\xe7\xe3o"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Elmo"})," era a IA de shogi mais forte na \xe9poca"]}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"O AlphaZero com algumas horas de treinamento superou esses sistemas especializados que levaram anos para desenvolver."}),"\n",(0,r.jsx)(a.h3,{id:"o-significado-da-generalidade",children:"O Significado da Generalidade"}),"\n",(0,r.jsx)(a.p,{children:"AlphaGo Zero / AlphaZero provou algo importante:"}),"\n",(0,r.jsxs)(a.blockquote,{children:["\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"O mesmo algoritmo de aprendizado pode alcan\xe7ar n\xedvel sobre-humano em diferentes dom\xednios."})}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"N\xe3o s\xe3o tr\xeas IAs diferentes, mas um framework de aprendizado gen\xe9rico:"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Auto-jogo"})," gera experi\xeancia"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Busca em \xc1rvore de Monte Carlo"})," explora possibilidades"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Rede Neural"})," aprende fun\xe7\xe3o de pol\xedtica e valor"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Aprendizado por refor\xe7o"})," otimiza a fun\xe7\xe3o objetivo"]}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Este framework n\xe3o depende de conhecimento espec\xedfico do dom\xednio, isso \xe9 um passo importante para a generaliza\xe7\xe3o da IA."}),"\n",(0,r.jsx)(a.h3,{id:"impacto-na-ia-tradicional",children:"Impacto na IA Tradicional"}),"\n",(0,r.jsx)(a.p,{children:'Antes do AlphaZero, as IAs mais fortes de xadrez e shogi eram do estilo "sistema especialista":'}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Muito conhecimento humano"}),": Livros de abertura, tabelas de finais, fun\xe7\xf5es de avalia\xe7\xe3o"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"D\xe9cadas de otimiza\xe7\xe3o"}),": Sangue e suor de incont\xe1veis jogadores e engenheiros"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Altamente especializadas"}),": Stockfish n\xe3o consegue jogar Go, Elmo n\xe3o consegue jogar xadrez"]}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"O AlphaZero superou tudo isso em horas com um algoritmo gen\xe9rico. Isso fez muitos pesquisadores de IA reconsiderarem:"}),"\n",(0,r.jsxs)(a.blockquote,{children:["\n",(0,r.jsx)(a.p,{children:'Devemos investir mais esfor\xe7os em "algoritmos de aprendizado gen\xe9ricos" ou "codifica\xe7\xe3o de conhecimento especializado"?'}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"A resposta parece cada vez mais clara: deixar a m\xe1quina aprender sozinha \xe9 mais eficaz do que ensin\xe1-la conhecimento."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"estilo-de-jogo-do-alphago-zero",children:"Estilo de Jogo do AlphaGo Zero"}),"\n",(0,r.jsx)(a.h3,{id:"est\xe9tica-al\xe9m-dos-humanos",children:"Est\xe9tica Al\xe9m dos Humanos"}),"\n",(0,r.jsxs)(a.p,{children:["O mundo do Go tem uma avalia\xe7\xe3o comum das jogadas do AlphaGo Zero: ",(0,r.jsx)(a.strong,{children:"mais elegantes"}),"."]}),"\n",(0,r.jsx)(a.p,{children:'As jogadas do AlphaGo Lee \xe0s vezes pareciam "estranhas" \u2014 como a jogada 37, onde os humanos precisaram de an\xe1lise posterior para entender sua profundidade. Mas as jogadas do AlphaGo Zero s\xe3o frequentemente avaliadas posteriormente como "imediatamente reconhec\xedveis como boas jogadas".'}),"\n",(0,r.jsx)(a.p,{children:"Isso pode ser porque:"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"For\xe7a de jogo mais forte"}),": Zero pode ver mais profundamente, jogar com mais calma"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Sem vieses humanos"}),": N\xe3o limitado por josekis tradicionais"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Objetivo consistente"}),": Busca apenas taxa de vit\xf3ria, n\xe3o imita humanos"]}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"redescoberta-da-teoria-de-go-humana",children:"Redescoberta da Teoria de Go Humana"}),"\n",(0,r.jsx)(a.p,{children:'Curiosamente, o AlphaGo Zero "redescobriu" o conhecimento de Go que os humanos acumularam ao longo de milhares de anos durante o treinamento:'}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Josekis"}),": Zero descobriu sozinho muitos josekis comuns, porque estes s\xe3o de fato as solu\xe7\xf5es \xf3timas para ambos os lados"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Princ\xedpios de abertura"}),": A ordem de import\xe2ncia de cantos, bordas e centro"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Conhecimento de formas"}),": A diferen\xe7a entre formas ruins e formas boas"]}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Isso valida a racionalidade da teoria de Go humana \u2014 este conhecimento n\xe3o \xe9 coincid\xeancia, mas reflexo da ess\xeancia do Go."}),"\n",(0,r.jsx)(a.h3,{id:"inova\xe7\xe3o-al\xe9m-dos-humanos",children:"Inova\xe7\xe3o Al\xe9m dos Humanos"}),"\n",(0,r.jsx)(a.p,{children:"Mas o Zero tamb\xe9m descobriu jogadas que os humanos nunca imaginaram:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Aberturas n\xe3o convencionais"}),": Varia\xe7\xf5es sobre aberturas tradicionais"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Sacrif\xedcios agressivos"}),": Mais disposto que humanos a desistir localmente em troca de vantagem global"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Formas contra-intuitivas"}),': "Formas ruins" superficiais que na verdade s\xe3o a solu\xe7\xe3o \xf3tima']}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Estas inova\xe7\xf5es est\xe3o mudando a compreens\xe3o humana do Go. Muitos jogadores profissionais dizem que estudar os registros de partidas do AlphaGo Zero lhes deu uma compreens\xe3o completamente nova do Go."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"resumo-dos-detalhes-t\xe9cnicos",children:"Resumo dos Detalhes T\xe9cnicos"}),"\n",(0,r.jsx)(a.h3,{id:"compara\xe7\xe3o-completa-com-o-alphago-original",children:"Compara\xe7\xe3o Completa com o AlphaGo Original"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Aspecto"}),(0,r.jsx)(a.th,{children:"AlphaGo (Original)"}),(0,r.jsx)(a.th,{children:"AlphaGo Zero"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"Dados de treinamento"})}),(0,r.jsx)(a.td,{children:"Registros humanos + auto-jogo"}),(0,r.jsx)(a.td,{children:"Puro auto-jogo"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"M\xe9todo de aprendizado"})}),(0,r.jsx)(a.td,{children:"Supervisionado + por refor\xe7o"}),(0,r.jsx)(a.td,{children:"Puro por refor\xe7o"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"Caracter\xedsticas de entrada"})}),(0,r.jsx)(a.td,{children:"48 planos"}),(0,r.jsx)(a.td,{children:"17 planos"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"Arquitetura de rede"})}),(0,r.jsx)(a.td,{children:"Policy/Value separadas"}),(0,r.jsx)(a.td,{children:"ResNet de cabe\xe7a dupla"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"Profundidade da rede"})}),(0,r.jsx)(a.td,{children:"13 camadas"}),(0,r.jsx)(a.td,{children:"40 camadas (ou mais)"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"Avalia\xe7\xe3o MCTS"})}),(0,r.jsx)(a.td,{children:"Rede neural + Rollout"}),(0,r.jsx)(a.td,{children:"Pura rede neural"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"Simula\xe7\xf5es"})}),(0,r.jsx)(a.td,{children:"~100.000 por jogada"}),(0,r.jsx)(a.td,{children:"~1.600 por jogada"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"TPUs de treinamento"})}),(0,r.jsx)(a.td,{children:"50+"}),(0,r.jsx)(a.td,{children:"4"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"TPUs de infer\xeancia"})}),(0,r.jsx)(a.td,{children:"48"}),(0,r.jsx)(a.td,{children:"4 (escal\xe1vel)"})]})]})]}),"\n",(0,r.jsx)(a.h3,{id:"algoritmo-central",children:"Algoritmo Central"}),"\n",(0,r.jsx)(a.p,{children:"O ciclo de treinamento do AlphaGo Zero \xe9 muito simples:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"1. Auto-jogo\n   - Usar rede atual para MCTS\n   - Selecionar jogadas pela probabilidade de busca MCTS\n   - Registrar cada passo (posi\xe7\xe3o, probabilidade MCTS, resultado da partida)\n\n2. Treinar rede\n   - Amostrar do pool de experi\xeancia\n   - Policy Head: minimizar entropia cruzada com probabilidades MCTS\n   - Value Head: minimizar erro quadr\xe1tico m\xe9dio com resultado real\n   - Otimizar ambos os objetivos conjuntamente\n\n3. Atualizar rede\n   - Substituir rede antiga pela nova (verificar que nova \xe9 mais forte por auto-jogo)\n   - Voltar ao passo 1\n"})}),"\n",(0,r.jsx)(a.p,{children:"Este ciclo roda continuamente, e a rede fica cada vez mais forte. Sem dados humanos, sem conhecimento humano, apenas regras do jogo e objetivo de vit\xf3ria/derrota."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"li\xe7\xf5es-para-pesquisa-em-ia",children:"Li\xe7\xf5es para Pesquisa em IA"}),"\n",(0,r.jsx)(a.h3,{id:"aprendizado-de-primeiros-princ\xedpios",children:"Aprendizado de Primeiros Princ\xedpios"}),"\n",(0,r.jsx)(a.p,{children:'O AlphaGo Zero demonstrou um m\xe9todo de aprendizado de "primeiros princ\xedpios":'}),"\n",(0,r.jsxs)(a.blockquote,{children:["\n",(0,r.jsx)(a.p,{children:"N\xe3o diga \xe0 IA como fazer, apenas diga qual \xe9 o objetivo, e deixe-a descobrir o m\xe9todo por conta pr\xf3pria."}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Isso forma um contraste marcante com a abordagem tradicional de sistemas especialistas. Sistemas especialistas tentam codificar conhecimento humano na IA, enquanto o AlphaGo Zero deixa a IA descobrir conhecimento por conta pr\xf3pria."}),"\n",(0,r.jsx)(a.p,{children:"O resultado \xe9: o conhecimento que a IA descobre pode ser mais completo e preciso que o conhecimento humano."}),"\n",(0,r.jsx)(a.h3,{id:"o-poder-do-auto-jogo",children:"O Poder do Auto-jogo"}),"\n",(0,r.jsx)(a.p,{children:"O AlphaGo Zero provou que o auto-jogo pode gerar dados de treinamento infinitos, e a qualidade desses dados melhora \xe0 medida que a rede melhora."}),"\n",(0,r.jsx)(a.p,{children:'Este \xe9 um "ciclo positivo":'}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Rede mais forte \u2192 Dados de auto-jogo melhores"}),"\n",(0,r.jsx)(a.li,{children:"Dados melhores \u2192 Rede mais forte"}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Este ciclo pode continuar rodando at\xe9 atingir o limite te\xf3rico do jogo (se existir)."}),"\n",(0,r.jsx)(a.h3,{id:"a-import\xe2ncia-da-simplifica\xe7\xe3o",children:"A Import\xe2ncia da Simplifica\xe7\xe3o"}),"\n",(0,r.jsx)(a.p,{children:'O sucesso do AlphaGo Zero prova a import\xe2ncia da "simplifica\xe7\xe3o":'}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Simplificar entrada (48 \u2192 17)"}),"\n",(0,r.jsx)(a.li,{children:"Simplificar arquitetura (rede dupla \u2192 rede \xfanica)"}),"\n",(0,r.jsx)(a.li,{children:"Simplificar treinamento (supervisionado + refor\xe7o \u2192 puro refor\xe7o)"}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Cada simplifica\xe7\xe3o tornou o sistema mais poderoso. Isso nos diz: complexo n\xe3o significa bom, a solu\xe7\xe3o mais simples frequentemente \xe9 a melhor."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"correspond\xeancia-de-anima\xe7\xf5es",children:"Correspond\xeancia de Anima\xe7\xf5es"}),"\n",(0,r.jsx)(a.p,{children:"Conceitos centrais discutidos neste artigo e n\xfameros de anima\xe7\xe3o:"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"N\xfamero"}),(0,r.jsx)(a.th,{children:"Conceito"}),(0,r.jsx)(a.th,{children:"Correspond\xeancia F\xedsica/Matem\xe1tica"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"\ud83c\udfac E7"}),(0,r.jsx)(a.td,{children:"Treinamento do zero"}),(0,r.jsx)(a.td,{children:"Fen\xf4meno de auto-organiza\xe7\xe3o"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"\ud83c\udfac E5"}),(0,r.jsx)(a.td,{children:"Auto-jogo"}),(0,r.jsx)(a.td,{children:"Converg\xeancia de ponto fixo"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"\ud83c\udfac E12"}),(0,r.jsx)(a.td,{children:"Curva de crescimento de for\xe7a"}),(0,r.jsx)(a.td,{children:"Crescimento em forma de S"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"\ud83c\udfac D12"}),(0,r.jsx)(a.td,{children:"Rede residual"}),(0,r.jsx)(a.td,{children:"Rodovia de gradientes"})]})]})]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"leitura-adicional",children:"Leitura Adicional"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Pr\xf3ximo artigo"}),": ",(0,r.jsx)(a.a,{href:"../dual-head-resnet",children:"Rede de Cabe\xe7a Dupla e Rede Residual"})," \u2014 An\xe1lise detalhada da arquitetura de rede neural do AlphaGo Zero"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Artigo relacionado"}),": ",(0,r.jsx)(a.a,{href:"../self-play",children:"Auto-jogo"})," \u2014 Por que o auto-jogo pode produzir n\xedvel sobre-humano"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Aprofundamento t\xe9cnico"}),": ",(0,r.jsx)(a.a,{href:"../training-from-scratch",children:"O Processo de Treinamento do Zero"})," \u2014 Evolu\xe7\xe3o detalhada dos Dias 0-3"]}),"\n"]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"refer\xeancias",children:"Refer\xeancias"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:['Silver, D., et al. (2017). "Mastering the game of Go without human knowledge." ',(0,r.jsx)(a.em,{children:"Nature"}),", 550, 354-359."]}),"\n",(0,r.jsxs)(a.li,{children:['Silver, D., et al. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." ',(0,r.jsx)(a.em,{children:"Science"}),", 362(6419), 1140-1144."]}),"\n",(0,r.jsxs)(a.li,{children:['DeepMind. (2017). "AlphaGo Zero: Starting from scratch." ',(0,r.jsx)(a.em,{children:"DeepMind Blog"}),"."]}),"\n",(0,r.jsxs)(a.li,{children:['Schrittwieser, J., et al. (2020). "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model." ',(0,r.jsx)(a.em,{children:"Nature"}),", 588, 604-609."]}),"\n"]})]})}function h(e={}){const{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},30416(e,a,o){o.d(a,{R:()=>d,x:()=>n});var s=o(59471);const r={},i=s.createContext(r);function d(e){const a=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function n(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:d(e.components),s.createElement(i.Provider,{value:a},e.children)}}}]);