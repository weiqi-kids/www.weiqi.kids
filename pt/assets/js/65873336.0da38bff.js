"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[10],{55466(e,a,i){i.r(a),i.d(a,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>d,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"for-engineers/deep-dive/papers","title":"Guia de Artigos Importantes","description":"Analise dos pontos-chave dos artigos marcos da IA de Go: AlphaGo, AlphaZero, KataGo","source":"@site/i18n/pt/docusaurus-plugin-content-docs/current/for-engineers/deep-dive/papers.md","sourceDirName":"for-engineers/deep-dive","slug":"/for-engineers/deep-dive/papers","permalink":"/pt/docs/for-engineers/deep-dive/papers","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/for-engineers/deep-dive/papers.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11,"title":"Guia de Artigos Importantes","description":"Analise dos pontos-chave dos artigos marcos da IA de Go: AlphaGo, AlphaZero, KataGo"},"sidebar":"tutorialSidebar","previous":{"title":"Regras Personalizadas e Variantes","permalink":"/pt/docs/for-engineers/deep-dive/custom-rules"},"next":{"title":"Construindo uma IA de Go do Zero","permalink":"/pt/docs/for-engineers/deep-dive/build-from-scratch"}}');var r=i(62615),n=i(30416);const d={sidebar_position:11,title:"Guia de Artigos Importantes",description:"Analise dos pontos-chave dos artigos marcos da IA de Go: AlphaGo, AlphaZero, KataGo"},s="Guia de Artigos Importantes",l={},c=[{value:"Visao Geral dos Artigos",id:"visao-geral-dos-artigos",level:2},{value:"Linha do Tempo",id:"linha-do-tempo",level:3},{value:"Sugestoes de Leitura",id:"sugestoes-de-leitura",level:3},{value:"1. O Nascimento do MCTS (2006)",id:"1-o-nascimento-do-mcts-2006",level:2},{value:"Informacoes do Artigo",id:"informacoes-do-artigo",level:3},{value:"Contribuicao Principal",id:"contribuicao-principal",level:3},{value:"Conceitos-Chave",id:"conceitos-chave",level:3},{value:"Formula UCB1",id:"formula-ucb1",level:4},{value:"Quatro Etapas do MCTS",id:"quatro-etapas-do-mcts",level:4},{value:"Impacto",id:"impacto",level:3},{value:"2. AlphaGo (2016)",id:"2-alphago-2016",level:2},{value:"Informacoes do Artigo",id:"informacoes-do-artigo-1",level:3},{value:"Contribuicao Principal",id:"contribuicao-principal-1",level:3},{value:"Arquitetura do Sistema",id:"arquitetura-do-sistema",level:3},{value:"Pontos Tecnicos",id:"pontos-tecnicos",level:3},{value:"1. Policy Network com Aprendizado Supervisionado",id:"1-policy-network-com-aprendizado-supervisionado",level:4},{value:"2. Melhoria com Aprendizado por Reforco",id:"2-melhoria-com-aprendizado-por-reforco",level:4},{value:"3. Treinamento da Value Network",id:"3-treinamento-da-value-network",level:4},{value:"4. Integracao MCTS",id:"4-integracao-mcts",level:4},{value:"Dados-Chave",id:"dados-chave",level:3},{value:"3. AlphaGo Zero (2017)",id:"3-alphago-zero-2017",level:2},{value:"Informacoes do Artigo",id:"informacoes-do-artigo-2",level:3},{value:"Contribuicao Principal",id:"contribuicao-principal-2",level:3},{value:"Diferencas em Relacao ao AlphaGo",id:"diferencas-em-relacao-ao-alphago",level:3},{value:"Inovacoes-Chave",id:"inovacoes-chave",level:3},{value:"1. Rede Unica com Duas Cabecas",id:"1-rede-unica-com-duas-cabecas",level:4},{value:"2. Recursos de Entrada Simplificados",id:"2-recursos-de-entrada-simplificados",level:4},{value:"3. Avaliacao Pura com Value Network",id:"3-avaliacao-pura-com-value-network",level:4},{value:"4. Fluxo de Treinamento",id:"4-fluxo-de-treinamento",level:4},{value:"Curva de Aprendizado",id:"curva-de-aprendizado",level:3},{value:"4. AlphaZero (2017)",id:"4-alphazero-2017",level:2},{value:"Informacoes do Artigo",id:"informacoes-do-artigo-3",level:3},{value:"Contribuicao Principal",id:"contribuicao-principal-3",level:3},{value:"Arquitetura Geral",id:"arquitetura-geral",level:3},{value:"Adaptacao Entre Jogos",id:"adaptacao-entre-jogos",level:3},{value:"Melhorias no MCTS",id:"melhorias-no-mcts",level:3},{value:"Formula PUCT",id:"formula-puct",level:4},{value:"Ruido de Exploracao",id:"ruido-de-exploracao",level:4},{value:"5. KataGo (2019)",id:"5-katago-2019",level:2},{value:"Informacoes do Artigo",id:"informacoes-do-artigo-4",level:3},{value:"Contribuicao Principal",id:"contribuicao-principal-4",level:3},{value:"Inovacoes-Chave",id:"inovacoes-chave-1",level:3},{value:"1. Objetivos de Treinamento Auxiliares",id:"1-objetivos-de-treinamento-auxiliares",level:4},{value:"2. Recursos Globais",id:"2-recursos-globais",level:4},{value:"3. Randomizacao de Playout Cap",id:"3-randomizacao-de-playout-cap",level:4},{value:"4. Tamanho de Tabuleiro Progressivo",id:"4-tamanho-de-tabuleiro-progressivo",level:4},{value:"Comparacao de Eficiencia",id:"comparacao-de-eficiencia",level:3},{value:"6. Artigos Relacionados",id:"6-artigos-relacionados",level:2},{value:"MuZero (2020)",id:"muzero-2020",level:3},{value:"EfficientZero (2021)",id:"efficientzero-2021",level:3},{value:"Gumbel AlphaZero (2022)",id:"gumbel-alphazero-2022",level:3},{value:"Sugestoes de Leitura de Artigos",id:"sugestoes-de-leitura-de-artigos",level:2},{value:"Ordem para Iniciantes",id:"ordem-para-iniciantes",level:3},{value:"Ordem Avancada",id:"ordem-avancada",level:3},{value:"Tecnicas de Leitura",id:"tecnicas-de-leitura",level:3},{value:"Links de Recursos",id:"links-de-recursos",level:2},{value:"PDFs dos Artigos",id:"pdfs-dos-artigos",level:3},{value:"Implementacoes Open Source",id:"implementacoes-open-source",level:3},{value:"Leitura Adicional",id:"leitura-adicional",level:2}];function t(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,n.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"guia-de-artigos-importantes",children:"Guia de Artigos Importantes"})}),"\n",(0,r.jsx)(a.p,{children:"Este artigo organiza os artigos mais importantes na historia do desenvolvimento da IA de Go, fornecendo resumos e pontos tecnicos para compreensao rapida."}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"visao-geral-dos-artigos",children:"Visao Geral dos Artigos"}),"\n",(0,r.jsx)(a.h3,{id:"linha-do-tempo",children:"Linha do Tempo"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"2006  Coulom - MCTS aplicado ao Go pela primeira vez\n2016  Silver et al. - AlphaGo (Nature)\n2017  Silver et al. - AlphaGo Zero (Nature)\n2017  Silver et al. - AlphaZero\n2019  Wu - KataGo\n2020+ Varias melhorias e aplicacoes\n"})}),"\n",(0,r.jsx)(a.h3,{id:"sugestoes-de-leitura",children:"Sugestoes de Leitura"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Objetivo"}),(0,r.jsx)(a.th,{children:"Artigo Recomendado"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Entender o basico"}),(0,r.jsx)(a.td,{children:"AlphaGo (2016)"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Entender auto-jogo"}),(0,r.jsx)(a.td,{children:"AlphaGo Zero (2017)"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Entender metodo geral"}),(0,r.jsx)(a.td,{children:"AlphaZero (2017)"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Referencia de implementacao"}),(0,r.jsx)(a.td,{children:"KataGo (2019)"})]})]})]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"1-o-nascimento-do-mcts-2006",children:"1. O Nascimento do MCTS (2006)"}),"\n",(0,r.jsx)(a.h3,{id:"informacoes-do-artigo",children:"Informacoes do Artigo"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Titulo: Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search\nAutor: Remi Coulom\nPublicacao: Computers and Games 2006\n"})}),"\n",(0,r.jsx)(a.h3,{id:"contribuicao-principal",children:"Contribuicao Principal"}),"\n",(0,r.jsx)(a.p,{children:"Primeira aplicacao sistematica de metodos de Monte Carlo ao Go:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Antes: Simulacao puramente aleatoria, sem estrutura de arvore\nDepois: Construcao de arvore de busca + selecao UCB + retropropagacao de estatisticas\n"})}),"\n",(0,r.jsx)(a.h3,{id:"conceitos-chave",children:"Conceitos-Chave"}),"\n",(0,r.jsx)(a.h4,{id:"formula-ucb1",children:"Formula UCB1"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Pontuacao de selecao = Taxa media de vitoria + C \xd7 sqrt(ln(N) / n)\n\nOnde:\n- N: Contagem de visitas do no pai\n- n: Contagem de visitas do no filho\n- C: Constante de exploracao\n"})}),"\n",(0,r.jsx)(a.h4,{id:"quatro-etapas-do-mcts",children:"Quatro Etapas do MCTS"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"1. Selection: Seleciona nos usando UCB\n2. Expansion: Expande novos nos\n3. Simulation: Simula aleatoriamente ate o fim\n4. Backpropagation: Retropropaga vitoria/derrota\n"})}),"\n",(0,r.jsx)(a.h3,{id:"impacto",children:"Impacto"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Elevou a IA de Go ao nivel amador dan"}),"\n",(0,r.jsx)(a.li,{children:"Tornou-se a base de todas as IAs de Go subsequentes"}),"\n",(0,r.jsx)(a.li,{children:"Conceito UCB influenciou o desenvolvimento do PUCT"}),"\n"]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"2-alphago-2016",children:"2. AlphaGo (2016)"}),"\n",(0,r.jsx)(a.h3,{id:"informacoes-do-artigo-1",children:"Informacoes do Artigo"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Titulo: Mastering the game of Go with deep neural networks and tree search\nAutores: Silver, D., Huang, A., Maddison, C.J., et al.\nPublicacao: Nature, 2016\nDOI: 10.1038/nature16961\n"})}),"\n",(0,r.jsx)(a.h3,{id:"contribuicao-principal-1",children:"Contribuicao Principal"}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"Primeira combinacao de deep learning com MCTS"}),", derrotando o campeao mundial humano."]}),"\n",(0,r.jsx)(a.h3,{id:"arquitetura-do-sistema",children:"Arquitetura do Sistema"}),"\n",(0,r.jsx)(a.mermaid,{value:'flowchart TB\n    subgraph AlphaGo["Arquitetura AlphaGo"]\n        subgraph SL["Policy Network (SL)"]\n            SL1["Entrada: Estado do tabuleiro (48 planos)"]\n            SL2["Arquitetura: CNN de 13 camadas"]\n            SL3["Saida: Probabilidade de 361 posicoes"]\n            SL4["Treinamento: 30 milhoes de registros humanos"]\n        end\n\n        subgraph RL["Policy Network (RL)"]\n            RL1["Inicializado a partir de SL Policy"]\n            RL2["Aprendizado por reforco via auto-jogo"]\n        end\n\n        subgraph VN["Value Network"]\n            VN1["Entrada: Estado do tabuleiro"]\n            VN2["Saida: Valor unico de taxa de vitoria"]\n            VN3["Treinamento: Posicoes geradas por auto-jogo"]\n        end\n\n        subgraph MCTS["MCTS"]\n            MCTS1["Usa Policy Network para guiar busca"]\n            MCTS2["Usa Value Network + Rollout para avaliacao"]\n        end\n    end'}),"\n",(0,r.jsx)(a.h3,{id:"pontos-tecnicos",children:"Pontos Tecnicos"}),"\n",(0,r.jsx)(a.h4,{id:"1-policy-network-com-aprendizado-supervisionado",children:"1. Policy Network com Aprendizado Supervisionado"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# Recursos de entrada (48 planos)\n- Posicao das pedras proprias\n- Posicao das pedras do oponente\n- Numero de liberdades\n- Estado apos captura\n- Posicoes de jogadas legais\n- Posicoes das ultimas jogadas\n...\n"})}),"\n",(0,r.jsx)(a.h4,{id:"2-melhoria-com-aprendizado-por-reforco",children:"2. Melhoria com Aprendizado por Reforco"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"SL Policy \u2192 Auto-jogo \u2192 RL Policy\n\nRL Policy e ~80% mais forte que SL Policy em taxa de vitoria\n"})}),"\n",(0,r.jsx)(a.h4,{id:"3-treinamento-da-value-network",children:"3. Treinamento da Value Network"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Chave para evitar overfitting:\n- Pegar apenas uma posicao de cada jogo\n- Evitar repeticao de posicoes similares\n"})}),"\n",(0,r.jsx)(a.h4,{id:"4-integracao-mcts",children:"4. Integracao MCTS"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Avaliacao de no folha = 0.5 \xd7 Value Network + 0.5 \xd7 Rollout\n\nRollout usa Policy Network rapida (menor precisao mas mais velocidade)\n"})}),"\n",(0,r.jsx)(a.h3,{id:"dados-chave",children:"Dados-Chave"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Item"}),(0,r.jsx)(a.th,{children:"Valor"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Precisao SL Policy"}),(0,r.jsx)(a.td,{children:"57%"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Taxa de vitoria RL Policy vs SL Policy"}),(0,r.jsx)(a.td,{children:"80%"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"GPUs de treinamento"}),(0,r.jsx)(a.td,{children:"176"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"TPUs de jogo"}),(0,r.jsx)(a.td,{children:"48"})]})]})]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"3-alphago-zero-2017",children:"3. AlphaGo Zero (2017)"}),"\n",(0,r.jsx)(a.h3,{id:"informacoes-do-artigo-2",children:"Informacoes do Artigo"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Titulo: Mastering the game of Go without human knowledge\nAutores: Silver, D., Schrittwieser, J., Simonyan, K., et al.\nPublicacao: Nature, 2017\nDOI: 10.1038/nature24270\n"})}),"\n",(0,r.jsx)(a.h3,{id:"contribuicao-principal-2",children:"Contribuicao Principal"}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"Nao precisa de registros humanos"}),", aprende do zero por auto-aprendizado."]}),"\n",(0,r.jsx)(a.h3,{id:"diferencas-em-relacao-ao-alphago",children:"Diferencas em Relacao ao AlphaGo"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Aspecto"}),(0,r.jsx)(a.th,{children:"AlphaGo"}),(0,r.jsx)(a.th,{children:"AlphaGo Zero"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Registros humanos"}),(0,r.jsx)(a.td,{children:"Precisa"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"Nao precisa"})})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Numero de redes"}),(0,r.jsx)(a.td,{children:"4"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"1 com duas cabecas"})})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Planos de entrada"}),(0,r.jsx)(a.td,{children:"48"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"17"})})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Rollout"}),(0,r.jsx)(a.td,{children:"Usa"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"Nao usa"})})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Rede residual"}),(0,r.jsx)(a.td,{children:"Nao"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"Sim"})})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Tempo de treinamento"}),(0,r.jsx)(a.td,{children:"Meses"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"3 dias"})})]})]})]}),"\n",(0,r.jsx)(a.h3,{id:"inovacoes-chave",children:"Inovacoes-Chave"}),"\n",(0,r.jsx)(a.h4,{id:"1-rede-unica-com-duas-cabecas",children:"1. Rede Unica com Duas Cabecas"}),"\n",(0,r.jsx)(a.mermaid,{value:'flowchart TB\n    Input["Entrada (17 planos)"]\n    Input --\x3e ResNet\n    subgraph ResNet["Torre Residual (19 ou 39 camadas)"]\n        R[" "]\n    end\n    ResNet --\x3e Policy["Policy (361)"]\n    ResNet --\x3e Value["Value (1)"]'}),"\n",(0,r.jsx)(a.h4,{id:"2-recursos-de-entrada-simplificados",children:"2. Recursos de Entrada Simplificados"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# Apenas 17 planos de recursos necessarios\nfeatures = [\n    current_player_stones,      # Pedras proprias\n    opponent_stones,            # Pedras do oponente\n    history_1_player,           # Estado historico 1\n    history_1_opponent,\n    ...                         # Estados historicos 2-7\n    color_to_play               # De quem e a vez\n]\n"})}),"\n",(0,r.jsx)(a.h4,{id:"3-avaliacao-pura-com-value-network",children:"3. Avaliacao Pura com Value Network"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Nao usa mais Rollout\nAvaliacao de no folha = Saida da Value Network\n\nMais simples e rapido\n"})}),"\n",(0,r.jsx)(a.h4,{id:"4-fluxo-de-treinamento",children:"4. Fluxo de Treinamento"}),"\n",(0,r.jsx)(a.mermaid,{value:'flowchart TB\n    Init["Inicializar rede aleatoria"]\n    Init --\x3e SelfPlay\n    SelfPlay["Auto-jogo gera registros"]\n    SelfPlay --\x3e Train\n    Train["Treinar rede neural<br/>- Policy: minimizar entropia cruzada<br/>- Value: minimizar MSE"]\n    Train --\x3e Eval\n    Eval["Avaliar nova rede<br/>Se mais forte, substitui"]\n    Eval --\x3e SelfPlay'}),"\n",(0,r.jsx)(a.h3,{id:"curva-de-aprendizado",children:"Curva de Aprendizado"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Tempo de treinamento    Elo\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n3 horas                 Iniciante\n24 horas                Supera AlphaGo Lee\n72 horas                Supera AlphaGo Master\n"})}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"4-alphazero-2017",children:"4. AlphaZero (2017)"}),"\n",(0,r.jsx)(a.h3,{id:"informacoes-do-artigo-3",children:"Informacoes do Artigo"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Titulo: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\nAutores: Silver, D., Hubert, T., Schrittwieser, J., et al.\nPublicacao: arXiv:1712.01815 (depois publicado na Science, 2018)\n"})}),"\n",(0,r.jsx)(a.h3,{id:"contribuicao-principal-3",children:"Contribuicao Principal"}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"Generalizacao"}),": Mesmo algoritmo aplicado a Go, xadrez e shogi."]}),"\n",(0,r.jsx)(a.h3,{id:"arquitetura-geral",children:"Arquitetura Geral"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Codificacao de entrada (especifica do jogo) \u2192 Rede residual (geral) \u2192 Saida com duas cabecas (geral)\n"})}),"\n",(0,r.jsx)(a.h3,{id:"adaptacao-entre-jogos",children:"Adaptacao Entre Jogos"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Jogo"}),(0,r.jsx)(a.th,{children:"Planos de Entrada"}),(0,r.jsx)(a.th,{children:"Espaco de Acoes"}),(0,r.jsx)(a.th,{children:"Tempo de Treinamento"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Go"}),(0,r.jsx)(a.td,{children:"17"}),(0,r.jsx)(a.td,{children:"362"}),(0,r.jsx)(a.td,{children:"40 dias"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Xadrez"}),(0,r.jsx)(a.td,{children:"119"}),(0,r.jsx)(a.td,{children:"4672"}),(0,r.jsx)(a.td,{children:"9 horas"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Shogi"}),(0,r.jsx)(a.td,{children:"362"}),(0,r.jsx)(a.td,{children:"11259"}),(0,r.jsx)(a.td,{children:"12 horas"})]})]})]}),"\n",(0,r.jsx)(a.h3,{id:"melhorias-no-mcts",children:"Melhorias no MCTS"}),"\n",(0,r.jsx)(a.h4,{id:"formula-puct",children:"Formula PUCT"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Pontuacao de selecao = Q(s,a) + c(s) \xd7 P(s,a) \xd7 sqrt(N(s)) / (1 + N(s,a))\n\nc(s) = log((1 + N(s) + c_base) / c_base) + c_init\n"})}),"\n",(0,r.jsx)(a.h4,{id:"ruido-de-exploracao",children:"Ruido de Exploracao"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# Adicionar ruido de Dirichlet no no raiz\nP(s,a) = (1 - epsilon) \xd7 p_a + epsilon \xd7 eta_a\n\neta ~ Dir(alpha)\nalpha = 0.03 (Go), 0.3 (xadrez), 0.15 (shogi)\n"})}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"5-katago-2019",children:"5. KataGo (2019)"}),"\n",(0,r.jsx)(a.h3,{id:"informacoes-do-artigo-4",children:"Informacoes do Artigo"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Titulo: Accelerating Self-Play Learning in Go\nAutor: David J. Wu\nPublicacao: arXiv:1902.10565\n"})}),"\n",(0,r.jsx)(a.h3,{id:"contribuicao-principal-4",children:"Contribuicao Principal"}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"Aumento de eficiencia de 50x"}),", permitindo que desenvolvedores individuais treinem IAs de Go poderosas."]}),"\n",(0,r.jsx)(a.h3,{id:"inovacoes-chave-1",children:"Inovacoes-Chave"}),"\n",(0,r.jsx)(a.h4,{id:"1-objetivos-de-treinamento-auxiliares",children:"1. Objetivos de Treinamento Auxiliares"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Perda total = Policy Loss + Value Loss +\n              Score Loss + Ownership Loss + ...\n\nObjetivos auxiliares fazem a rede convergir mais rapido\n"})}),"\n",(0,r.jsx)(a.h4,{id:"2-recursos-globais",children:"2. Recursos Globais"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# Camada de pooling global\nglobal_features = global_avg_pool(conv_features)\n# Combinar com recursos locais\ncombined = concat(conv_features, broadcast(global_features))\n"})}),"\n",(0,r.jsx)(a.h4,{id:"3-randomizacao-de-playout-cap",children:"3. Randomizacao de Playout Cap"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Tradicional: Busca fixa de N vezes\nKataGo: N amostrado de uma distribuicao\n\nPermite que a rede tenha bom desempenho em varias profundidades de busca\n"})}),"\n",(0,r.jsx)(a.h4,{id:"4-tamanho-de-tabuleiro-progressivo",children:"4. Tamanho de Tabuleiro Progressivo"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"if training_step < 1000000:\n    board_size = random.choice([9, 13, 19])\nelse:\n    board_size = 19\n"})}),"\n",(0,r.jsx)(a.h3,{id:"comparacao-de-eficiencia",children:"Comparacao de Eficiencia"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Metrica"}),(0,r.jsx)(a.th,{children:"AlphaZero"}),(0,r.jsx)(a.th,{children:"KataGo"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Dias de GPU para nivel sobre-humano"}),(0,r.jsx)(a.td,{children:"5000"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"100"})})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Aumento de eficiencia"}),(0,r.jsx)(a.td,{children:"Base"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.strong,{children:"50x"})})]})]})]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"6-artigos-relacionados",children:"6. Artigos Relacionados"}),"\n",(0,r.jsx)(a.h3,{id:"muzero-2020",children:"MuZero (2020)"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Titulo: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\nContribuicao: Aprende modelo de dinamica do ambiente, nao precisa de regras do jogo\n"})}),"\n",(0,r.jsx)(a.h3,{id:"efficientzero-2021",children:"EfficientZero (2021)"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Titulo: Mastering Atari Games with Limited Data\nContribuicao: Grande melhoria na eficiencia de amostragem\n"})}),"\n",(0,r.jsx)(a.h3,{id:"gumbel-alphazero-2022",children:"Gumbel AlphaZero (2022)"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"Titulo: Policy Improvement by Planning with Gumbel\nContribuicao: Metodo melhorado de melhoria de politica\n"})}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"sugestoes-de-leitura-de-artigos",children:"Sugestoes de Leitura de Artigos"}),"\n",(0,r.jsx)(a.h3,{id:"ordem-para-iniciantes",children:"Ordem para Iniciantes"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"1. AlphaGo (2016) - Entender arquitetura basica\n2. AlphaGo Zero (2017) - Entender auto-jogo\n3. KataGo (2019) - Entender detalhes de implementacao\n"})}),"\n",(0,r.jsx)(a.h3,{id:"ordem-avancada",children:"Ordem Avancada"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"4. AlphaZero (2017) - Generalizacao\n5. MuZero (2020) - Aprender modelo do mundo\n6. Artigo original do MCTS - Entender fundamentos\n"})}),"\n",(0,r.jsx)(a.h3,{id:"tecnicas-de-leitura",children:"Tecnicas de Leitura"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Ver resumo e conclusao primeiro"}),": Captar rapidamente a contribuicao principal"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Ver figuras e tabelas"}),": Entender arquitetura geral"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Ver secao de metodos"}),": Entender detalhes tecnicos"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Ver apendice"}),": Encontrar detalhes de implementacao e hiperparametros"]}),"\n"]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"links-de-recursos",children:"Links de Recursos"}),"\n",(0,r.jsx)(a.h3,{id:"pdfs-dos-artigos",children:"PDFs dos Artigos"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Artigo"}),(0,r.jsx)(a.th,{children:"Link"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"AlphaGo"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.a,{href:"https://www.nature.com/articles/nature16961",children:"Nature"})})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"AlphaGo Zero"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.a,{href:"https://www.nature.com/articles/nature24270",children:"Nature"})})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"AlphaZero"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.a,{href:"https://www.science.org/doi/10.1126/science.aar6404",children:"Science"})})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"KataGo"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.a,{href:"https://arxiv.org/abs/1902.10565",children:"arXiv"})})]})]})]}),"\n",(0,r.jsx)(a.h3,{id:"implementacoes-open-source",children:"Implementacoes Open Source"}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Projeto"}),(0,r.jsx)(a.th,{children:"Link"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"KataGo"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.a,{href:"https://github.com/lightvector/KataGo",children:"GitHub"})})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"Leela Zero"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.a,{href:"https://github.com/leela-zero/leela-zero",children:"GitHub"})})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"MiniGo"}),(0,r.jsx)(a.td,{children:(0,r.jsx)(a.a,{href:"https://github.com/tensorflow/minigo",children:"GitHub"})})]})]})]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"leitura-adicional",children:"Leitura Adicional"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.a,{href:"../neural-network",children:"Arquitetura de Rede Neural Detalhada"})," \u2014 Entendimento profundo do design de redes"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.a,{href:"../mcts-implementation",children:"Detalhes de Implementacao do MCTS"})," \u2014 Implementacao do algoritmo de busca"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.a,{href:"../training",children:"Analise do Mecanismo de Treinamento do KataGo"})," \u2014 Detalhes do fluxo de treinamento"]}),"\n"]})]})}function h(e={}){const{wrapper:a}={...(0,n.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(t,{...e})}):t(e)}},30416(e,a,i){i.d(a,{R:()=>d,x:()=>s});var o=i(59471);const r={},n=o.createContext(r);function d(e){const a=o.useContext(n);return o.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function s(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:d(e.components),o.createElement(n.Provider,{value:a},e.children)}}}]);