<!doctype html>
<html lang="zh-cn" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-alphago/cnn-and-go" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">CNN 与围棋的结合 | 好棋寶寶協會 | Weiqi.Kids</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://www.weiqi.kids/zh-cn/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://www.weiqi.kids/zh-cn/img/social-card.png"><meta data-rh="true" property="og:url" content="https://www.weiqi.kids/zh-cn/docs/alphago/cnn-and-go/"><meta data-rh="true" property="og:locale" content="zh_cn"><meta data-rh="true" property="og:locale:alternate" content="zh_tw"><meta data-rh="true" property="og:locale:alternate" content="zh_hk"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" property="og:locale:alternate" content="ja"><meta data-rh="true" property="og:locale:alternate" content="ko"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="pt"><meta data-rh="true" property="og:locale:alternate" content="hi"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" property="og:locale:alternate" content="ar"><meta data-rh="true" name="docusaurus_locale" content="zh-cn"><meta data-rh="true" name="docsearch:language" content="zh-cn"><meta data-rh="true" name="keywords" content="圍棋, Go, 好棋寶寶, AI, KataGo, AlphaGo, 圍棋教學, 圍棋入門"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="CNN 与围棋的结合 | 好棋寶寶協會 | Weiqi.Kids"><meta data-rh="true" name="description" content="深入探讨卷积神经网络为何特别适合围棋，从感受野到批次正规化的完整解析"><meta data-rh="true" property="og:description" content="深入探讨卷积神经网络为何特别适合围棋，从感受野到批次正规化的完整解析"><link data-rh="true" rel="icon" href="/zh-cn/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://www.weiqi.kids/zh-cn/docs/alphago/cnn-and-go/"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/cnn-and-go/" hreflang="zh-tw"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-cn/docs/alphago/cnn-and-go/" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/zh-hk/docs/alphago/cnn-and-go/" hreflang="zh-hk"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/en/docs/alphago/cnn-and-go/" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ja/docs/alphago/cnn-and-go/" hreflang="ja"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ko/docs/alphago/cnn-and-go/" hreflang="ko"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/es/docs/alphago/cnn-and-go/" hreflang="es"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/pt/docs/alphago/cnn-and-go/" hreflang="pt"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/hi/docs/alphago/cnn-and-go/" hreflang="hi"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/id/docs/alphago/cnn-and-go/" hreflang="id"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/ar/docs/alphago/cnn-and-go/" hreflang="ar"><link data-rh="true" rel="alternate" href="https://www.weiqi.kids/docs/alphago/cnn-and-go/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://www.weiqi.kids#organization","name":"台灣好棋寶寶協會","alternateName":"Taiwan Good Go Baby Association","url":"https://www.weiqi.kids","logo":{"@type":"ImageObject","url":"https://www.weiqi.kids/img/logo.svg","width":200,"height":200},"sameAs":["https://mastodon.weiqi.kids/","https://peertube.weiqi.kids/"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://www.weiqi.kids/docs/aboutus"},"description":"推動圍棋文化與 AI 研究的台灣非營利組織"},{"@type":"WebSite","@id":"https://www.weiqi.kids#website","name":"好棋寶寶協會官網","url":"https://www.weiqi.kids","publisher":{"@id":"https://www.weiqi.kids#organization"},"inLanguage":["zh-TW","zh-CN","zh-HK","en","ja","ko","es","pt","hi","id","ar"],"potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://www.weiqi.kids/search?q={search_term_string}"},"query-input":"required name=search_term_string"}}]}</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"AlphaGo","item":"https://www.weiqi.kids/zh-cn/docs/alphago/"},{"@type":"ListItem","position":2,"name":"CNN 与围棋的结合","item":"https://www.weiqi.kids/zh-cn/docs/alphago/cnn-and-go"}]}</script><meta property="og:type" content="website">
<meta property="og:site_name" content="好棋寶寶協會">
<meta name="twitter:card" content="summary_large_image">
<meta name="robots" content="index, follow">
<meta name="author" content="台灣好棋寶寶協會">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/zh-cn/assets/css/styles.f23bf74b.css">
<script src="/zh-cn/assets/js/runtime~main.23815985.js" defer="defer"></script>
<script src="/zh-cn/assets/js/main.c7733d2c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/zh-cn/img/logo.svg"><div role="region" aria-label="跳到主要内容"><a class="skipToContent_WWLT" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zh-cn/"><div class="navbar__logo"><img src="/zh-cn/img/logo.svg" alt="好棋宝宝协会标志" class="themedComponent_rBb5 themedComponent--light_t3c2"><img src="/zh-cn/img/logo.svg" alt="好棋宝宝协会标志" class="themedComponent_rBb5 themedComponent--dark_Z76H"></div><b class="navbar__title text--truncate">围棋宝宝</b></a><a class="navbar__item navbar__link" href="/zh-cn/docs/learn/">学围棋</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/zh-cn/docs/alphago/">AlphaGo</a><a class="navbar__item navbar__link" href="/zh-cn/docs/animations/">动画教室</a><a class="navbar__item navbar__link" href="/zh-cn/docs/tech/">技术文档</a><a class="navbar__item navbar__link" href="/zh-cn/docs/about/">关于我们</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link navbar__item--compact"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_LXmZ"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>简体中文</a><ul class="dropdown__menu"><li><a href="/docs/alphago/cnn-and-go/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-tw">繁體中文</a></li><li><a href="/zh-cn/docs/alphago/cnn-and-go/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-cn">简体中文</a></li><li><a href="/zh-hk/docs/alphago/cnn-and-go/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hk">粵語（香港）</a></li><li><a href="/en/docs/alphago/cnn-and-go/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/ja/docs/alphago/cnn-and-go/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/ko/docs/alphago/cnn-and-go/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ko">한국어</a></li><li><a href="/es/docs/alphago/cnn-and-go/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/pt/docs/alphago/cnn-and-go/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/hi/docs/alphago/cnn-and-go/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="hi">हिन्दी</a></li><li><a href="/id/docs/alphago/cnn-and-go/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/ar/docs/alphago/cnn-and-go/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li></ul></div><a href="https://github.com/weiqi-kids/www.weiqi.kids" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link navbar__item--compact" aria-label="GitHub"></a><div class="navbarSearchContainer_MDM0"><div class="navbar__search searchBarContainer_Qpyg" dir="ltr"><input placeholder="搜索" aria-label="Search" class="navbar__search-input searchInput_KBva" value=""><div class="loadingRing_riNO searchBarLoadingRing_XquT"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_SQvo"><div class="docsWrapper_HjTU"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_91RE" type="button"></button><div class="docRoot_SzOB"><aside class="theme-doc-sidebar-container docSidebarContainer_Q0cb"><div class="sidebarViewport_TEb6"><div class="sidebar_LiQ3"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_iDgU"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/zh-cn/docs/intro/"><span title="使用指南" class="linkLabel_REp1">使用指南</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist menu__link--active" href="/zh-cn/docs/alphago/"><span title="AlphaGo" class="categoryLinkLabel_ezQx">AlphaGo</span></a><button aria-label="折叠侧边栏分类 &#x27;AlphaGo&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/birth-of-alphago/"><span title="AlphaGo 的诞生" class="linkLabel_REp1">AlphaGo 的诞生</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/key-matches/"><span title="关键对局回顾" class="linkLabel_REp1">关键对局回顾</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/move-37/"><span title="「神之一手」深度分析" class="linkLabel_REp1">「神之一手」深度分析</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/why-go-is-hard/"><span title="围棋为什么难？" class="linkLabel_REp1">围棋为什么难？</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/traditional-limits/"><span title="传统方法的极限" class="linkLabel_REp1">传统方法的极限</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/board-representation/"><span title="棋盘状态表示" class="linkLabel_REp1">棋盘状态表示</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/policy-network/"><span title="Policy Network 详解" class="linkLabel_REp1">Policy Network 详解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/value-network/"><span title="Value Network 详解" class="linkLabel_REp1">Value Network 详解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/input-features/"><span title="输入特征设计" class="linkLabel_REp1">输入特征设计</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/zh-cn/docs/alphago/cnn-and-go/"><span title="CNN 与围棋的结合" class="linkLabel_REp1">CNN 与围棋的结合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/supervised-learning/"><span title="监督学习阶段" class="linkLabel_REp1">监督学习阶段</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/reinforcement-intro/"><span title="强化学习入门" class="linkLabel_REp1">强化学习入门</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/self-play/"><span title="自我对弈" class="linkLabel_REp1">自我对弈</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/mcts-neural-combo/"><span title="MCTS 与神经网络的结合" class="linkLabel_REp1">MCTS 与神经网络的结合</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/puct-formula/"><span title="PUCT 公式详解" class="linkLabel_REp1">PUCT 公式详解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/alphago-zero/"><span title="AlphaGo Zero 概述" class="linkLabel_REp1">AlphaGo Zero 概述</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/dual-head-resnet/"><span title="双头网络与残差网络" class="linkLabel_REp1">双头网络与残差网络</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/training-from-scratch/"><span title="从零训练的过程" class="linkLabel_REp1">从零训练的过程</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/distributed-systems/"><span title="分布式系统与 TPU" class="linkLabel_REp1">分布式系统与 TPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh-cn/docs/alphago/legacy-and-impact/"><span title="AlphaGo 的遗产" class="linkLabel_REp1">AlphaGo 的遗产</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/zh-cn/docs/learn/"><span title="學圍棋" class="categoryLinkLabel_ezQx">學圍棋</span></a><button aria-label="展开侧边栏分类 &#x27;學圍棋&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/zh-cn/docs/animations/"><span title="動畫教室" class="linkLabel_REp1">動畫教室</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/zh-cn/docs/tech/"><span title="技術文件" class="categoryLinkLabel_ezQx">技術文件</span></a><button aria-label="展开侧边栏分类 &#x27;技術文件&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_nItb menu__link menu__link--sublist" href="/zh-cn/docs/about/"><span title="關於我們" class="categoryLinkLabel_ezQx">關於我們</span></a><button aria-label="展开侧边栏分类 &#x27;關於我們&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_C0wL"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_txwF"><div class="docItemContainer_FUEM"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_DlgT" aria-label="页面路径"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/zh-cn/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_BMIH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/zh-cn/docs/alphago/"><span>AlphaGo</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">CNN 与围棋的结合</span></li></ul></nav><div class="tocCollapsible_LBsY theme-doc-toc-mobile tocMobile_eMWB"><button type="button" class="clean-btn tocCollapsibleButton_fVBn">本页总览</button></div><div class="theme-doc-markdown markdown"><header><h1>CNN 与围棋的结合</h1></header>
<p>当 DeepMind 选择用**卷积神经网络（CNN）**来处理围棋时，这是一个天才的设计决策。</p>
<p>CNN 原本是为图像识别设计的。为什么它也适合围棋？这篇文章将深入探讨 CNN 的运作原理，以及它与围棋的完美契合。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="为什么-cnn-适合棋盘">为什么 CNN 适合棋盘？<a href="#为什么-cnn-适合棋盘" class="hash-link" aria-label="为什么 CNN 适合棋盘？的直接链接" title="为什么 CNN 适合棋盘？的直接链接" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="棋盘是图像">棋盘是「图像」<a href="#棋盘是图像" class="hash-link" aria-label="棋盘是「图像」的直接链接" title="棋盘是「图像」的直接链接" translate="no">​</a></h3>
<p>从某种角度看，19×19 的围棋棋盘就是一张<strong>图像</strong>：</p>
<table><thead><tr><th>图像</th><th>围棋棋盘</th></tr></thead><tbody><tr><td>像素</td><td>交叉点</td></tr><tr><td>RGB 通道</td><td>特征平面（黑、白、空...）</td></tr><tr><td>224×224</td><td>19×19</td></tr><tr><td>辨识猫狗</td><td>判断好棋坏棋</td></tr></tbody></table>
<p>这个类比并非偶然。CNN 擅长图像的原因，也让它擅长棋盘。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="三个关键特性">三个关键特性<a href="#三个关键特性" class="hash-link" aria-label="三个关键特性的直接链接" title="三个关键特性的直接链接" translate="no">​</a></h3>
<p>CNN 有三个特性，让它特别适合棋盘类型的数据：</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="1-局部连接local-connectivity">1. 局部连接（Local Connectivity）<a href="#1-局部连接local-connectivity" class="hash-link" aria-label="1. 局部连接（Local Connectivity）的直接链接" title="1. 局部连接（Local Connectivity）的直接链接" translate="no">​</a></h4>
<p>CNN 的卷积核只看局部区域，这与围棋的特性完美匹配：</p>
<p><strong>图像识别 vs 围棋局部特征比较：</strong></p>
<table><thead><tr><th>图像识别</th><th>围棋</th></tr></thead><tbody><tr><td>猫耳朵是局部特征</td><td>「眼」是局部棋形</td></tr><tr><td>不需要看整张图</td><td>不需要看整个棋盘</td></tr></tbody></table>
<p><strong>3×3 区域示例（眼位）：</strong></p>
<table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">○</td><td style="text-align:center">●</td><td style="text-align:center">○</td></tr><tr><td style="text-align:center">●</td><td style="text-align:center">·</td><td style="text-align:center">●</td></tr><tr><td style="text-align:center">○</td><td style="text-align:center">●</td><td style="text-align:center">○</td></tr></tbody></table>
<p>很多围棋概念都是「局部」的：</p>
<ul>
<li class=""><strong>眼</strong>：2×2 或 3×3 的区域</li>
<li class=""><strong>叫吃</strong>：3×3 的区域</li>
<li class=""><strong>接、断</strong>：2×2 的区域</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="2-权重共享weight-sharing">2. 权重共享（Weight Sharing）<a href="#2-权重共享weight-sharing" class="hash-link" aria-label="2. 权重共享（Weight Sharing）的直接链接" title="2. 权重共享（Weight Sharing）的直接链接" translate="no">​</a></h4>
<p>同一个卷积核会扫描整个棋盘，这意味着：</p>
<blockquote>
<p><strong>棋盘左上角的「眼」和右下角的「眼」，用同样的方式识别</strong></p>
</blockquote>
<p>这是合理的——围棋规则不因位置而异（边角例外，但可以用边角特征平面处理）。</p>
<p>权重共享也大幅减少了参数量：</p>
<table><thead><tr><th>方法</th><th>参数量</th></tr></thead><tbody><tr><td>全连接网络</td><td>361 × 361 × 通道数 = 数千万</td></tr><tr><td>CNN</td><td>3 × 3 × 通道数 × 滤波器数 = 数百万</td></tr></tbody></table>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="3-平移等变性translation-equivariance">3. 平移等变性（Translation Equivariance）<a href="#3-平移等变性translation-equivariance" class="hash-link" aria-label="3. 平移等变性（Translation Equivariance）的直接链接" title="3. 平移等变性（Translation Equivariance）的直接链接" translate="no">​</a></h4>
<p>如果输入平移，CNN 的输出也会相应平移：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">输入:                    输出 (高概率区域):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  A B C D E               A B C D E</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1 . . . . .            1  . . . . .</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2 . ● . . .   →        2  . * . . .</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3 . . . . .            3  . . . . .</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">输入平移后:              输出也平移:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  A B C D E               A B C D E</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1 . . . . .            1  . . . . .</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2 . . . . .   →        2  . . . . .</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3 . . ● . .            3  . . * . .</span><br></span></code></pre></div></div>
<p>这对围棋很重要：相同的局部棋形，无论出现在棋盘哪里，应该有类似的评估。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="卷积运算">卷积运算<a href="#卷积运算" class="hash-link" aria-label="卷积运算的直接链接" title="卷积运算的直接链接" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="基本原理">基本原理<a href="#基本原理" class="hash-link" aria-label="基本原理的直接链接" title="基本原理的直接链接" translate="no">​</a></h3>
<p>卷积运算是 CNN 的核心。它是一种「滑动窗口」操作：</p>
<!-- -->
<p>计算过程（以中心点为例）：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">输出[2,2] = 1×1 + 1×0 + 1×1 +</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            1×0 + 1×1 + 1×0 +</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            1×1 + 1×0 + 1×1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          = 1 + 0 + 1 + 0 + 1 + 0 + 1 + 0 + 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          = 5</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="多通道卷积">多通道卷积<a href="#多通道卷积" class="hash-link" aria-label="多通道卷积的直接链接" title="多通道卷积的直接链接" translate="no">​</a></h3>
<p>当输入有多个通道（如 48 个特征平面）时，卷积核也变成 3D：</p>
<!-- -->
<p>每个卷积核会跨所有输入通道计算，产生一个输出通道。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="多个滤波器">多个滤波器<a href="#多个滤波器" class="hash-link" aria-label="多个滤波器的直接链接" title="多个滤波器的直接链接" translate="no">​</a></h3>
<p>AlphaGo 使用 192 个滤波器，每个滤波器学习不同的特征：</p>
<!-- -->
<p>每个滤波器可能学到不同的棋形：</p>
<ul>
<li class="">滤波器 1：眼位检测</li>
<li class="">滤波器 2：断点检测</li>
<li class="">滤波器 3：连接检测</li>
<li class="">...</li>
<li class="">滤波器 192：某种复杂模式</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="感受野">感受野<a href="#感受野" class="hash-link" aria-label="感受野的直接链接" title="感受野的直接链接" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="什么是感受野">什么是感受野？<a href="#什么是感受野" class="hash-link" aria-label="什么是感受野？的直接链接" title="什么是感受野？的直接链接" translate="no">​</a></h3>
<p>**感受野（Receptive Field）**是指输出的一个位置，受到输入的哪些位置影响。</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="单层卷积">单层卷积<a href="#单层卷积" class="hash-link" aria-label="单层卷积的直接链接" title="单层卷积的直接链接" translate="no">​</a></h4>
<p>使用 3×3 卷积核时，输出的每个位置只受输入 3×3 区域影响：</p>
<!-- -->
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="多层卷积">多层卷积<a href="#多层卷积" class="hash-link" aria-label="多层卷积的直接链接" title="多层卷积的直接链接" translate="no">​</a></h4>
<p>堆叠多层卷积后，感受野会扩大：</p>
<table><thead><tr><th>层数</th><th>感受野</th><th>计算</th></tr></thead><tbody><tr><td>1</td><td>3×3</td><td>3</td></tr><tr><td>2</td><td>5×5</td><td>3 + (3-1) = 5</td></tr><tr><td>3</td><td>7×7</td><td>5 + (3-1) = 7</td></tr><tr><td>...</td><td>...</td><td>...</td></tr><tr><td>12</td><td>25×25</td><td>3 + 11×2 = 25</td></tr></tbody></table>
<p>AlphaGo 的 12 层卷积给出 <strong>25×25 的感受野</strong>，已经超过 19×19 的棋盘！</p>
<p>这意味着：</p>
<ul>
<li class=""><strong>输出的每个位置都能「看到」整个棋盘</strong></li>
<li class="">但「看」的方式不同：近处细节清楚，远处概括</li>
<li class="">这与人类棋手的思维方式类似</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="感受野与围棋">感受野与围棋<a href="#感受野与围棋" class="hash-link" aria-label="感受野与围棋的直接链接" title="感受野与围棋的直接链接" translate="no">​</a></h3>
<p>感受野的概念解释了为什么 AlphaGo 能处理「全局」问题：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">局部问题（3×3 感受野）:     全局问题（25×25 感受野）:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- 这里有眼吗？              - 这块棋有眼位吗？</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- 可以叫吃吗？              - 征子有利吗？</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- 能接上吗？                - 全局形势如何？</span><br></span></code></pre></div></div>
<p>浅层处理局部特征，深层处理全局特征。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="局部-vs-全局特征">局部 vs 全局特征<a href="#局部-vs-全局特征" class="hash-link" aria-label="局部 vs 全局特征的直接链接" title="局部 vs 全局特征的直接链接" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="cnn-的层次结构">CNN 的层次结构<a href="#cnn-的层次结构" class="hash-link" aria-label="CNN 的层次结构的直接链接" title="CNN 的层次结构的直接链接" translate="no">​</a></h3>
<p>CNN 自然形成层次结构：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">输入层:        黑子、白子、空点</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">浅层 (1-3):    眼、接、断、叫吃</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">中层 (4-8):    棋形、活棋、死棋</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">深层 (9-12):   势力、厚薄、大场</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">输出层:        落子概率 / 胜率</span><br></span></code></pre></div></div>
<p>这与人类学习围棋的过程惊人地相似：</p>
<ol>
<li class="">先学规则（哪里有子）</li>
<li class="">再学战术（如何吃子）</li>
<li class="">然后学棋形（什么是好形）</li>
<li class="">最后学大局观（全局判断）</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="可视化隐藏层">可视化隐藏层<a href="#可视化隐藏层" class="hash-link" aria-label="可视化隐藏层的直接链接" title="可视化隐藏层的直接链接" translate="no">​</a></h3>
<p>研究人员发现，CNN 的隐藏层确实学到了有意义的特征：</p>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="浅层滤波器">浅层滤波器<a href="#浅层滤波器" class="hash-link" aria-label="浅层滤波器的直接链接" title="浅层滤波器的直接链接" translate="no">​</a></h4>
<p><strong>滤波器 A（眼位检测）：</strong></p>
<table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">+</td><td style="text-align:center">-</td><td style="text-align:center">+</td></tr><tr><td style="text-align:center">-</td><td style="text-align:center">+</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">+</td><td style="text-align:center">-</td><td style="text-align:center">+</td></tr></tbody></table>
<p><strong>滤波器 B（叫吃检测）：</strong></p>
<table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">+</td><td style="text-align:center">+</td><td style="text-align:center">+</td></tr><tr><td style="text-align:center">+</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">+</td><td style="text-align:center">+</td><td style="text-align:center">+</td></tr></tbody></table>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="深层滤波器">深层滤波器<a href="#深层滤波器" class="hash-link" aria-label="深层滤波器的直接链接" title="深层滤波器的直接链接" translate="no">​</a></h4>
<p>深层的滤波器更抽象，难以直接解释，但它们捕捉了复杂的棋形模式。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="激活函数的选择">激活函数的选择<a href="#激活函数的选择" class="hash-link" aria-label="激活函数的选择的直接链接" title="激活函数的选择的直接链接" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="relu简单而有效">ReLU：简单而有效<a href="#relu简单而有效" class="hash-link" aria-label="ReLU：简单而有效的直接链接" title="ReLU：简单而有效的直接链接" translate="no">​</a></h3>
<p>AlphaGo 在所有卷积层后使用 <strong>ReLU（Rectified Linear Unit）</strong>：</p>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">relu</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> </span><span class="token builtin">max</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p><strong>ReLU 函数特性：</strong></p>
<ul>
<li class="">当输入 x ≤ 0 时，输出为 0（水平线）</li>
<li class="">当输入 x &gt; 0 时，输出等于 x（斜率为 1 的直线）</li>
<li class="">图形呈「折线」状：在原点左侧为零，右侧为线性递增</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="为什么不用其他函数">为什么不用其他函数？<a href="#为什么不用其他函数" class="hash-link" aria-label="为什么不用其他函数？的直接链接" title="为什么不用其他函数？的直接链接" translate="no">​</a></h3>
<table><thead><tr><th>激活函数</th><th>公式</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>ReLU</td><td>max(0, x)</td><td>计算快、梯度好</td><td>负值死亡</td></tr><tr><td>Sigmoid</td><td>1/(1+e^-x)</td><td>输出有界</td><td>梯度消失</td></tr><tr><td>Tanh</td><td>(e^x-e^-x)/(e^x+e^-x)</td><td>零中心</td><td>梯度消失</td></tr><tr><td>LeakyReLU</td><td>max(0.01x, x)</td><td>解决死亡问题</td><td>多一个超参数</td></tr></tbody></table>
<p>对于深度网络，ReLU 的优势明显：</p>
<ol>
<li class=""><strong>计算简单</strong>：只是比较和取最大值</li>
<li class=""><strong>梯度不消失</strong>：正区间梯度恒为 1</li>
<li class=""><strong>稀疏激活</strong>：很多神经元输出 0，提高效率</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="relu-在围棋中的含义">ReLU 在围棋中的含义<a href="#relu-在围棋中的含义" class="hash-link" aria-label="ReLU 在围棋中的含义的直接链接" title="ReLU 在围棋中的含义的直接链接" translate="no">​</a></h3>
<p>ReLU 的稀疏性在围棋中有有趣的解释：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">某个滤波器检测「断点」:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- 有断点 → 正值输出（激活）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- 无断点 → 零输出（不激活）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">这就像棋手只关注「有事」的位置</span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="批次正规化">批次正规化<a href="#批次正规化" class="hash-link" aria-label="批次正规化的直接链接" title="批次正规化的直接链接" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="什么是批次正规化">什么是批次正规化？<a href="#什么是批次正规化" class="hash-link" aria-label="什么是批次正规化？的直接链接" title="什么是批次正规化？的直接链接" translate="no">​</a></h3>
<p>**批次正规化（Batch Normalization）**是一种技术，让每层的输出维持稳定的分布：</p>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">batch_norm</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> gamma</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> beta</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 计算批次的均值和标准差</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mean </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">mean</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">axis</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    std </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">std</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">axis</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 正规化</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    x_norm </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> mean</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">std </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1e-8</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 缩放和平移</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> gamma </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> x_norm </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> beta</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="为什么需要">为什么需要？<a href="#为什么需要" class="hash-link" aria-label="为什么需要？的直接链接" title="为什么需要？的直接链接" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_l_sM" id="内部协变量偏移">内部协变量偏移<a href="#内部协变量偏移" class="hash-link" aria-label="内部协变量偏移的直接链接" title="内部协变量偏移的直接链接" translate="no">​</a></h4>
<p>当网络训练时，每层的输入分布会随着前面层的权重变化而改变。这被称为「内部协变量偏移」：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">第一层权重更新 → 第一层输出分布改变</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">               第二层输入分布改变 → 第二层需要重新适应</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                        ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                   ... (传递下去)</span><br></span></code></pre></div></div>
<p>批次正规化通过强制每层输入有固定的分布（均值 0，标准差 1），来稳定训练。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="在-alphago-中的应用">在 AlphaGo 中的应用<a href="#在-alphago-中的应用" class="hash-link" aria-label="在 AlphaGo 中的应用的直接链接" title="在 AlphaGo 中的应用的直接链接" translate="no">​</a></h3>
<p>AlphaGo 在每个卷积层后、激活函数前使用批次正规化：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">Conv → BatchNorm → ReLU → Conv → BatchNorm → ReLU → ...</span><br></span></code></pre></div></div>
<p>好处：</p>
<ol>
<li class=""><strong>训练更快</strong>：可以使用更大的学习率</li>
<li class=""><strong>更稳定</strong>：减少对初始化的敏感性</li>
<li class=""><strong>正则化效果</strong>：有轻微的 dropout 效果</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="推理时的处理">推理时的处理<a href="#推理时的处理" class="hash-link" aria-label="推理时的处理的直接链接" title="推理时的处理的直接链接" translate="no">​</a></h3>
<p>训练时，使用当前批次的统计量。推理时，使用整个训练集的统计量（移动平均）：</p>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 训练时</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mean </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> batch_mean</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">var </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> batch_var</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 推理时</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mean </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> running_mean  </span><span class="token comment" style="color:#999988;font-style:italic"># 训练期间累积的均值</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">var </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> running_var    </span><span class="token comment" style="color:#999988;font-style:italic"># 训练期间累积的方差</span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="alphago-的具体配置">AlphaGo 的具体配置<a href="#alphago-的具体配置" class="hash-link" aria-label="AlphaGo 的具体配置的直接链接" title="AlphaGo 的具体配置的直接链接" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="完整架构">完整架构<a href="#完整架构" class="hash-link" aria-label="完整架构的直接链接" title="完整架构的直接链接" translate="no">​</a></h3>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">输入: 19×19×48</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">第 1 层:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv2D(5×5, 192 filters, padding=&#x27;same&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  BatchNorm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ReLU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  输出: 19×19×192</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">第 2-12 层 (共 11 层):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv2D(3×3, 192 filters, padding=&#x27;same&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  BatchNorm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ReLU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  输出: 19×19×192</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">输出层 (Policy):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv2D(1×1, 1 filter)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Flatten</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Softmax</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  输出: 361 维概率</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">输出层 (Value):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Conv2D(1×1, 1 filter)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Flatten</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Dense(256)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ReLU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Dense(1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Tanh</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  输出: 单一数值</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="参数配置">参数配置<a href="#参数配置" class="hash-link" aria-label="参数配置的直接链接" title="参数配置的直接链接" translate="no">​</a></h3>
<table><thead><tr><th>参数</th><th>数值</th><th>说明</th></tr></thead><tbody><tr><td>输入通道</td><td>48</td><td>特征平面数</td></tr><tr><td>滤波器数</td><td>192</td><td>每层的通道数</td></tr><tr><td>卷积核大小</td><td>3×3（第一层 5×5）</td><td>感受野</td></tr><tr><td>层数</td><td>13（含输出层）</td><td>深度</td></tr><tr><td>激活函数</td><td>ReLU</td><td>非线性</td></tr><tr><td>正规化</td><td>BatchNorm</td><td>稳定训练</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="pytorch-实现">PyTorch 实现<a href="#pytorch-实现" class="hash-link" aria-label="PyTorch 实现的直接链接" title="PyTorch 实现的直接链接" translate="no">​</a></h3>
<div class="language-python codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-python codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> nn</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">AlphaGoCNN</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Module</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> input_channels</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">48</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_filters</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">192</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_layers</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">12</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 第一层（5×5 卷积）</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv1 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Sequential</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Conv2d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">input_channels</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_filters</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> kernel_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">5</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> padding</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">BatchNorm2d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_filters</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ReLU</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">inplace</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 中间层（3×3 卷积）</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv_layers </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Sequential</span><span class="token punctuation" style="color:#393A34">(</span><span class="token operator" style="color:#393A34">*</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Sequential</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Conv2d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_filters</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_filters</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> kernel_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> padding</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">BatchNorm2d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_filters</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ReLU</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">inplace</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> _ </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_layers </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Policy 输出头</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">policy_head </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Sequential</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Conv2d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_filters</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> kernel_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Flatten</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Softmax</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">dim</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Value 输出头</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">value_head </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Sequential</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Conv2d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_filters</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> kernel_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Flatten</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Linear</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">361</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">256</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ReLU</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">inplace</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Linear</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">256</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Tanh</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">forward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 共享特征提取</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv1</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conv_layers</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 分头输出</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        policy </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">policy_head</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        value </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">value_head</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> policy</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> value</span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="与其他架构的比较">与其他架构的比较<a href="#与其他架构的比较" class="hash-link" aria-label="与其他架构的比较的直接链接" title="与其他架构的比较的直接链接" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="全连接网络">全连接网络<a href="#全连接网络" class="hash-link" aria-label="全连接网络的直接链接" title="全连接网络的直接链接" translate="no">​</a></h3>
<p>如果用全连接网络处理围棋：</p>
<table><thead><tr><th>特性</th><th>全连接</th><th>CNN</th></tr></thead><tbody><tr><td>参数量</td><td>极大（数亿）</td><td>较小（数百万）</td></tr><tr><td>位置不变性</td><td>无</td><td>有</td></tr><tr><td>局部特征</td><td>难学</td><td>自然捕捉</td></tr><tr><td>训练效率</td><td>低</td><td>高</td></tr></tbody></table>
<p>全连接网络无法利用棋盘的空间结构，效率极低。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="循环神经网络rnn">循环神经网络（RNN）<a href="#循环神经网络rnn" class="hash-link" aria-label="循环神经网络（RNN）的直接链接" title="循环神经网络（RNN）的直接链接" translate="no">​</a></h3>
<p>RNN 适合序列数据（如棋局历史），但：</p>
<table><thead><tr><th>特性</th><th>RNN</th><th>CNN</th></tr></thead><tbody><tr><td>空间处理</td><td>弱</td><td>强</td></tr><tr><td>序列处理</td><td>强</td><td>弱（需要历史平面）</td></tr><tr><td>并行化</td><td>难</td><td>易</td></tr><tr><td>长距离依赖</td><td>需要 LSTM</td><td>深层即可</td></tr></tbody></table>
<p>AlphaGo 选择 CNN + 历史平面，而非 CNN + RNN。</p>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="残差网络resnet">残差网络（ResNet）<a href="#残差网络resnet" class="hash-link" aria-label="残差网络（ResNet）的直接链接" title="残差网络（ResNet）的直接链接" translate="no">​</a></h3>
<p>AlphaGo Zero 升级为 ResNet：</p>
<div class="language-text codeBlockContainer_huE5 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_oko_"><pre tabindex="0" class="prism-code language-text codeBlock_BBgU thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_t9f9"><span class="token-line" style="color:#393A34"><span class="token plain">普通 CNN:                ResNet:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  x                        x</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ↓                        ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> Conv                     Conv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ↓                        ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> ReLU                    ReLU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ↓                        ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> Conv                     Conv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ↓                        ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  y                      y + x  ← 残差连接</span><br></span></code></pre></div></div>
<p>残差连接让梯度更容易流动，可以训练更深的网络（40 层 vs 12 层）。</p>
<p>详见 <a class="" href="/zh-cn/docs/alphago/dual-head-resnet/">双头网络与残差网络</a>。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="可视化理解">可视化理解<a href="#可视化理解" class="hash-link" aria-label="可视化理解的直接链接" title="可视化理解的直接链接" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="卷积过程">卷积过程<a href="#卷积过程" class="hash-link" aria-label="卷积过程的直接链接" title="卷积过程的直接链接" translate="no">​</a></h3>
<p><strong>输入棋盘（简化为 5×5）：</strong></p>
<table><thead><tr><th style="text-align:center"></th><th style="text-align:center">A</th><th style="text-align:center">B</th><th style="text-align:center">C</th><th style="text-align:center">D</th><th style="text-align:center">E</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">·</td><td style="text-align:center">·</td><td style="text-align:center">·</td><td style="text-align:center">·</td><td style="text-align:center">·</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">·</td><td style="text-align:center">●</td><td style="text-align:center">·</td><td style="text-align:center">·</td><td style="text-align:center">·</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">·</td><td style="text-align:center">·</td><td style="text-align:center">○</td><td style="text-align:center">·</td><td style="text-align:center">·</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">·</td><td style="text-align:center">·</td><td style="text-align:center">·</td><td style="text-align:center">●</td><td style="text-align:center">·</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">·</td><td style="text-align:center">·</td><td style="text-align:center">·</td><td style="text-align:center">·</td><td style="text-align:center">·</td></tr></tbody></table>
<p><strong>滤波器（3×3，检测「十字形」）：</strong></p>
<table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr></tbody></table>
<p><strong>卷积输出（中心有强响应，十字形匹配）：</strong></p>
<table><thead><tr><th style="text-align:center"></th><th style="text-align:center">A</th><th style="text-align:center">B</th><th style="text-align:center">C</th><th style="text-align:center">D</th><th style="text-align:center">E</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center"><strong>1</strong></td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_l_sM" id="多层特征">多层特征<a href="#多层特征" class="hash-link" aria-label="多层特征的直接链接" title="多层特征的直接链接" translate="no">​</a></h3>
<p><strong>第 1 层输出（192 个通道中的 4 个）：</strong></p>
<p><strong>通道 1（眼位）：</strong></p>
<table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">0.9</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr></tbody></table>
<p><strong>通道 2（边线）：</strong></p>
<table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">0.8</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0.8</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0.8</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0.8</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr></tbody></table>
<p><strong>通道 3（断点）：</strong></p>
<table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0.7</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr></tbody></table>
<p><strong>通道 4（连接）：</strong></p>
<table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">0.5</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">0.8</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">0.5</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr></tbody></table>
<p>这些特征在更深层会被组合成更复杂的概念...</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="动画对应">动画对应<a href="#动画对应" class="hash-link" aria-label="动画对应的直接链接" title="动画对应的直接链接" translate="no">​</a></h2>
<p>本文涉及的核心概念与动画编号：</p>
<table><thead><tr><th>编号</th><th>概念</th><th>物理/数学对应</th></tr></thead><tbody><tr><td>🎬 D9</td><td>卷积运算</td><td>滤波器响应</td></tr><tr><td>🎬 D10</td><td>感受野</td><td>局部→全局</td></tr><tr><td>🎬 D11</td><td>批次正规化</td><td>分布稳定</td></tr><tr><td>🎬 D1</td><td>多通道输入</td><td>张量运算</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="延伸阅读">延伸阅读<a href="#延伸阅读" class="hash-link" aria-label="延伸阅读的直接链接" title="延伸阅读的直接链接" translate="no">​</a></h2>
<ul>
<li class=""><strong>上一篇</strong>：<a class="" href="/zh-cn/docs/alphago/input-features/">输入特征设计</a> — 48 个特征平面详解</li>
<li class=""><strong>下一篇</strong>：<a class="" href="/zh-cn/docs/alphago/supervised-learning/">监督学习阶段</a> — 如何从人类棋谱学习</li>
<li class=""><strong>进阶主题</strong>：<a class="" href="/zh-cn/docs/alphago/dual-head-resnet/">双头网络与残差网络</a> — AlphaGo Zero 的网络升级</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="关键要点">关键要点<a href="#关键要点" class="hash-link" aria-label="关键要点的直接链接" title="关键要点的直接链接" translate="no">​</a></h2>
<ol>
<li class=""><strong>CNN 天然适合棋盘</strong>：局部连接、权重共享、平移等变性</li>
<li class=""><strong>卷积提取局部特征</strong>：3×3 区域的模式识别</li>
<li class=""><strong>深层网络获得全局视野</strong>：12 层 → 25×25 感受野</li>
<li class=""><strong>ReLU 快速有效</strong>：简单的非线性激活</li>
<li class=""><strong>BatchNorm 稳定训练</strong>：标准化每层输出</li>
</ol>
<p>CNN 让 AlphaGo 能够「看」棋盘——就像人类用眼睛看图像一样自然。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_l_sM" id="参考资料">参考资料<a href="#参考资料" class="hash-link" aria-label="参考资料的直接链接" title="参考资料的直接链接" translate="no">​</a></h2>
<ol>
<li class="">LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). &quot;Deep learning.&quot; <em>Nature</em>, 521, 436-444.</li>
<li class="">He, K., et al. (2015). &quot;Deep Residual Learning for Image Recognition.&quot; <em>CVPR</em>.</li>
<li class="">Ioffe, S., &amp; Szegedy, C. (2015). &quot;Batch Normalization: Accelerating Deep Network Training.&quot; <em>ICML</em>.</li>
<li class="">Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). &quot;ImageNet Classification with Deep Convolutional Neural Networks.&quot; <em>NeurIPS</em>.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_YM8j"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/alphago/10-cnn-and-go.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_hRvp" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_LNbR"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="文件选项卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/zh-cn/docs/alphago/input-features/"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">输入特征设计</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/zh-cn/docs/alphago/supervised-learning/"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">监督学习阶段</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_VU0O thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#为什么-cnn-适合棋盘" class="table-of-contents__link toc-highlight">为什么 CNN 适合棋盘？</a><ul><li><a href="#棋盘是图像" class="table-of-contents__link toc-highlight">棋盘是「图像」</a></li><li><a href="#三个关键特性" class="table-of-contents__link toc-highlight">三个关键特性</a></li></ul></li><li><a href="#卷积运算" class="table-of-contents__link toc-highlight">卷积运算</a><ul><li><a href="#基本原理" class="table-of-contents__link toc-highlight">基本原理</a></li><li><a href="#多通道卷积" class="table-of-contents__link toc-highlight">多通道卷积</a></li><li><a href="#多个滤波器" class="table-of-contents__link toc-highlight">多个滤波器</a></li></ul></li><li><a href="#感受野" class="table-of-contents__link toc-highlight">感受野</a><ul><li><a href="#什么是感受野" class="table-of-contents__link toc-highlight">什么是感受野？</a></li><li><a href="#感受野与围棋" class="table-of-contents__link toc-highlight">感受野与围棋</a></li></ul></li><li><a href="#局部-vs-全局特征" class="table-of-contents__link toc-highlight">局部 vs 全局特征</a><ul><li><a href="#cnn-的层次结构" class="table-of-contents__link toc-highlight">CNN 的层次结构</a></li><li><a href="#可视化隐藏层" class="table-of-contents__link toc-highlight">可视化隐藏层</a></li></ul></li><li><a href="#激活函数的选择" class="table-of-contents__link toc-highlight">激活函数的选择</a><ul><li><a href="#relu简单而有效" class="table-of-contents__link toc-highlight">ReLU：简单而有效</a></li><li><a href="#为什么不用其他函数" class="table-of-contents__link toc-highlight">为什么不用其他函数？</a></li><li><a href="#relu-在围棋中的含义" class="table-of-contents__link toc-highlight">ReLU 在围棋中的含义</a></li></ul></li><li><a href="#批次正规化" class="table-of-contents__link toc-highlight">批次正规化</a><ul><li><a href="#什么是批次正规化" class="table-of-contents__link toc-highlight">什么是批次正规化？</a></li><li><a href="#为什么需要" class="table-of-contents__link toc-highlight">为什么需要？</a></li><li><a href="#在-alphago-中的应用" class="table-of-contents__link toc-highlight">在 AlphaGo 中的应用</a></li><li><a href="#推理时的处理" class="table-of-contents__link toc-highlight">推理时的处理</a></li></ul></li><li><a href="#alphago-的具体配置" class="table-of-contents__link toc-highlight">AlphaGo 的具体配置</a><ul><li><a href="#完整架构" class="table-of-contents__link toc-highlight">完整架构</a></li><li><a href="#参数配置" class="table-of-contents__link toc-highlight">参数配置</a></li><li><a href="#pytorch-实现" class="table-of-contents__link toc-highlight">PyTorch 实现</a></li></ul></li><li><a href="#与其他架构的比较" class="table-of-contents__link toc-highlight">与其他架构的比较</a><ul><li><a href="#全连接网络" class="table-of-contents__link toc-highlight">全连接网络</a></li><li><a href="#循环神经网络rnn" class="table-of-contents__link toc-highlight">循环神经网络（RNN）</a></li><li><a href="#残差网络resnet" class="table-of-contents__link toc-highlight">残差网络（ResNet）</a></li></ul></li><li><a href="#可视化理解" class="table-of-contents__link toc-highlight">可视化理解</a><ul><li><a href="#卷积过程" class="table-of-contents__link toc-highlight">卷积过程</a></li><li><a href="#多层特征" class="table-of-contents__link toc-highlight">多层特征</a></li></ul></li><li><a href="#动画对应" class="table-of-contents__link toc-highlight">动画对应</a></li><li><a href="#延伸阅读" class="table-of-contents__link toc-highlight">延伸阅读</a></li><li><a href="#关键要点" class="table-of-contents__link toc-highlight">关键要点</a></li><li><a href="#参考资料" class="table-of-contents__link toc-highlight">参考资料</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Weiqi.Kids. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>